{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time, random\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from semanticscholar import SemanticScholar\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S2_API_KEY = os.environ['S2_API_KEY']\n",
    "S2_API_KEY is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Rei\n"
     ]
    }
   ],
   "source": [
    "# search by title\n",
    "sch = SemanticScholar(api_key=S2_API_KEY)\n",
    "#example = 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'\n",
    "example = 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'\n",
    "print(example[:79])\n",
    "results = sch.search_paper(example) # limit to 79 characters -> 넘어가면 에러남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c', 'externalIds': {'ArXiv': '2209.12072', 'DBLP': 'journals/corr/abs-2209-12072', 'DOI': '10.48550/arXiv.2209.12072', 'CorpusId': 252531963}, 'corpusId': 252531963, 'publicationVenue': {'id': '1901e811-ee72-4b20-8f7e-de08cd395a10', 'name': 'arXiv.org', 'alternate_names': ['ArXiv'], 'issn': '2331-8422', 'url': 'http://bibpurl.oclc.org/web/7130'}, 'url': 'https://www.semanticscholar.org/paper/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c', 'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills', 'abstract': '—Efﬁcient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demon- strations or designed for speciﬁc tasks can beneﬁt the exploration, but they are usually costly-collected, unbalanced/sub-optimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efﬁcient and structural explorations in the entire skill space rather than a limited space with task-speciﬁc skills. Inspired by the above fact, we propose an RL algorithm exploring all feasible motion skills instead of a limited set of task-speciﬁc and object-centric skills. Without demonstrations, our method can still perform well in diverse tasks. First, we build a task-agnostic and ego-centric (TaEc) motion skill library in a pure motion perspective, which is diverse enough to be reusable in different complex tasks. The motion skills are then encoded into a low-dimension latent skill space, in which RL can do exploration efﬁciently. Validations in various challenging driving scenarios demonstrate that our proposed method, TaEc-RL, outperforms its counterparts signiﬁcantly in learning efﬁciency and task performance.', 'venue': 'arXiv.org', 'year': 2022, 'referenceCount': 46, 'citationCount': 0, 'influentialCitationCount': 0, 'isOpenAccess': False, 'openAccessPdf': None, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'publicationTypes': ['JournalArticle'], 'publicationDate': '2022-09-24', 'journal': {'name': 'ArXiv', 'volume': 'abs/2209.12072'}, 'authors': [{'authorId': '2045308050', 'name': 'Tong Zhou'}, {'authorId': '2143431388', 'name': 'Letian Wang'}, {'authorId': '2993340', 'name': 'Ruobing Chen'}, {'authorId': '2108270996', 'name': 'Wenshuo Wang'}, {'authorId': '2146400675', 'name': 'Y. Liu'}]}\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022\n",
      "https://www.semanticscholar.org/paper/a17a7256c04afee68f9aa0b7bfdc67fbca998b9c\n",
      "0\n",
      "a17a7256c04afee68f9aa0b7bfdc67fbca998b9c\n",
      "dict_keys(['paperId', 'externalIds', 'corpusId', 'publicationVenue', 'url', 'title', 'abstract', 'venue', 'year', 'referenceCount', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'openAccessPdf', 'fieldsOfStudy', 's2FieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal', 'authors'])\n"
     ]
    }
   ],
   "source": [
    "print(results[0])\n",
    "print(results[0].title)\n",
    "print(results[0].year)\n",
    "print(results[0].url)\n",
    "print(results[0].citationCount)\n",
    "print(results[0].paperId)\n",
    "print(results[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search by paperId\n",
    "sch = SemanticScholar(api_key=S2_API_KEY)\n",
    "example = 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c'\n",
    "results = sch.get_paper(example, fields=['title','year','paperId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c', 'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills', 'year': 2022}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = {}\n",
    "paper['title'] = results[0].title\n",
    "paper['paperId'] = results[0].paperId\n",
    "paper['year'] = results[0].year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCitationInfo(paperId):\n",
    "    headers = {'x-api-key': S2_API_KEY}\n",
    "    r = requests.post(\n",
    "        'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "        headers=headers,\n",
    "        params={'fields': 'citations,references,title'},\n",
    "        json={\"ids\": [paperId]}\n",
    "    )\n",
    "    return r.json()[0]['citations'], r.json()[0]['references']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper['citation'], paper['reference'] = getCitationInfo(paper['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "  'title': 'NetHack is Hard to Hack'},\n",
       " {'paperId': 'e91bdc6d78cef19648c468acf9bf16a3905a5008',\n",
       "  'title': 'Constraint‐based multi‐agent reinforcement learning for collaborative tasks'},\n",
       " {'paperId': '85bc55ba9ab93c09713b0891bbcf0541b8f27ea9',\n",
       "  'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       " {'paperId': 'ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7',\n",
       "  'title': 'Behavior Contrastive Learning for Unsupervised Skill Discovery'},\n",
       " {'paperId': '71c508996120b06b2a056d9d073f6e6c60a671a6',\n",
       "  'title': 'Progressive Transfer Learning for Dexterous In-Hand Manipulation with Multi-Fingered Anthropomorphic Hand'},\n",
       " {'paperId': 'f811132d3a737ee1c71b52706f0cd78b8904f056',\n",
       "  'title': 'Learning Diverse Policies with Soft Self-Generated Guidance'},\n",
       " {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "  'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '1f346f74e8eabececa4896d734ab9b261f30830d',\n",
       "  'title': 'Modular Deep Learning'},\n",
       " {'paperId': 'ff106b68484cc45a296390d14bb2cb88b75a1e0d',\n",
       "  'title': 'CERiL: Continuous Event-based Reinforcement Learning'},\n",
       " {'paperId': 'd5781022f211bc2bc8eaeeb574e0fef86a58ec45',\n",
       "  'title': 'PushWorld: A benchmark for manipulation planning with tools and movable obstacles'},\n",
       " {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "  'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       " {'paperId': '3ecbf75ca51133eb59a66ddb28d057a14dd538c1',\n",
       "  'title': 'Reusable Options through Gradient-based Meta Learning'},\n",
       " {'paperId': '2be3222b6b9888746d5239c35ef867b10d7228a6',\n",
       "  'title': 'Planning Irregular Object Packing via Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '6ace8342b5bb52e5db1bc7f6bc42b4c4c4d7b938',\n",
       "  'title': 'A Context-based Multi-task Hierarchical Inverse Reinforcement Learning Algorithm'},\n",
       " {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "  'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       " {'paperId': '18d02042724a7fe29ae6aa9460353d7e2425ca86',\n",
       "  'title': 'Matching options to tasks using Option-Indexed Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'beb6cc3a60d1c511b869e4bb2c744cec99cd3ef1',\n",
       "  'title': 'Adjacency Constraint for Efficient Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '988dae20df8d69869aa41097a05d821446cff621',\n",
       "  'title': 'DisTop: Discovering a Topological representation to learn diverse and rewarding skills'},\n",
       " {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "  'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       " {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "  'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "  'title': 'Learning Options via Compression'},\n",
       " {'paperId': 'ec1e08dc5cbec362ea0a331e32bfa3d30e47c24f',\n",
       "  'title': 'Learning Landmark-Oriented Subgoals for Visual Navigation Using Trajectory Memory'},\n",
       " {'paperId': '9fa422789f9c3ae2a11aa6c624d8e23382fd07cd',\n",
       "  'title': 'CandyRL: A Hybrid Reinforcement Learning Model for Gameplay'},\n",
       " {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "  'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       " {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "  'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       " {'paperId': '23d9e8319f58451a20fac7fc1316c4e664d66eb1',\n",
       "  'title': 'Knowing the Past to Predict the Future: Reinforcement Virtual Learning'},\n",
       " {'paperId': '594cc7ce6690ad6d3dd79ea57391ab1bd4d41119',\n",
       "  'title': 'Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning'},\n",
       " {'paperId': '1cc13cff6f12d450457f51eb8d5d8e20bce47b56',\n",
       "  'title': 'Skill-Based Reinforcement Learning with Intrinsic Reward Matching'},\n",
       " {'paperId': '0c0b7fb0066e8c1a5a2b4e4856135650eeef7702',\n",
       "  'title': 'Causality-driven Hierarchical Structure Discovery for Reinforcement Learning'},\n",
       " {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "  'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "  'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       " {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "  'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       " {'paperId': '75175ec0a794875a1b089f6de6ed57e04f352168',\n",
       "  'title': 'Towards Run-time Efficient Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '88f48480ffb3c36683a63b0f6d5932b40bfbef64',\n",
       "  'title': 'Celebrating Robustness in Efficient Off-Policy Meta-Reinforcement Learning'},\n",
       " {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "  'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       " {'paperId': '8e21576387f46f1b9090bdbff1ceadf187feeada',\n",
       "  'title': 'CompoSuite: A Compositional Reinforcement Learning Benchmark'},\n",
       " {'paperId': 'e032a053dc934cb938c79c8adab8fde38a9eb157',\n",
       "  'title': 'Variational Diversity Maximization for Hierarchical Skill Discovery'},\n",
       " {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "  'title': 'Deep Hierarchical Planning from Pixels'},\n",
       " {'paperId': '4014bf79a220a09d1e380624adff53f5314a7e41',\n",
       "  'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks'},\n",
       " {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "  'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       " {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "  'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       " {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "  'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       " {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "  'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       " {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "  'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       " {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "  'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       " {'paperId': '6d8f5366a04ed1955bb62759369c938f05da7f27',\n",
       "  'title': 'Open-Ended Reinforcement Learning with Neural Reward Functions'},\n",
       " {'paperId': '8ada79148436cb18ed4fe84d1b2165047447462a',\n",
       "  'title': 'ASC me to Do Anything: Multi-task Training for Embodied AI'},\n",
       " {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "  'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       " {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "  'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       " {'paperId': 'e617b03e14bbdde9f0fb3f9fabca0dd29c91f174',\n",
       "  'title': 'GrASP: Gradient-Based Affordance Selection for Planning'},\n",
       " {'paperId': '1e5b39d523392d234484aa43318329114541b527',\n",
       "  'title': 'Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity'},\n",
       " {'paperId': '9bf925ecb1e6c6bfeecfc15aec1d0c6d7c28e135',\n",
       "  'title': 'CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery'},\n",
       " {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "  'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       " {'paperId': '33e3f13087abd5241d55523140720f5e684b7bee',\n",
       "  'title': 'Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning'},\n",
       " {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "  'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       " {'paperId': 'fa66e527a84666355bb50323a12c4f79147ea3b1',\n",
       "  'title': 'RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN'},\n",
       " {'paperId': '0213fa01c7b8aa7668b11fd9edf283fe10d5719e',\n",
       "  'title': 'Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning'},\n",
       " {'paperId': 'a01fae01cd9a3067fa6b8a777e70efe86bdc4699',\n",
       "  'title': 'Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching'},\n",
       " {'paperId': '90b237b41ca5541f7e7d0337fe053be4e6cee853',\n",
       "  'title': 'Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via Multi-Agent Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '8eb73addbf8c2b52637af040755cf3ca13cdbf40',\n",
       "  'title': 'Training Transition Policies via Distribution Matching for Complex Tasks'},\n",
       " {'paperId': '40888b859c5b40868943162e3c4769dae1aed716',\n",
       "  'title': 'The Information Geometry of Unsupervised Reinforcement Learning'},\n",
       " {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "  'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       " {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "  'title': 'Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "  'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       " {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "  'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       " {'paperId': '3f037bef6e3acc7c66c5aca8ca6426f984a719c9',\n",
       "  'title': 'Multi-Agent Reinforcement Learning'},\n",
       " {'paperId': 'dde41671ebd2a16408d0e61c37efca60b7720895',\n",
       "  'title': 'Toward Human Cognition-inspired High-Level Decision Making For Hierarchical Reinforcement Learning Agents'},\n",
       " {'paperId': '959a5b0021c44690c92426df0c516e0ab7a997f3',\n",
       "  'title': 'EAT-C: Environment-Adversarial sub-Task Curriculum for RL'},\n",
       " {'paperId': '800a1917c57c5701cc974e5498ad27a61ae0f292',\n",
       "  'title': 'Exploring Long-Horizon Reasoning with Deep RL in Combinatorially Hard Tasks'},\n",
       " {'paperId': 'ff33db486b561f36617bfee1d55fbd42d3045969',\n",
       "  'title': 'O PEN -E NDED R EINFORCEMENT L EARNING WITH N EU RAL R EWARD F UNCTIONS'},\n",
       " {'paperId': 'eedca44bc8bd69e4b62742306ff70582984ca7c5',\n",
       "  'title': 'Optimizing Multi-Agent Coordination via Hierarchical Graph Probabilistic Recursive Reasoning'},\n",
       " {'paperId': '7319249a853288dce45ca331a87dce052a2abfba',\n",
       "  'title': 'Disentangling Controlled Effects for Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "  'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       " {'paperId': '1af3755f1f66df99184c0ab3d92590e28aaf9cba',\n",
       "  'title': 'Agile Control For Quadruped Robot In Complex Environment Based on Deep Reinforcement Learning Method'},\n",
       " {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "  'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       " {'paperId': '27bc680bf6a115cc3f28c4da462b6d25cf04cb09',\n",
       "  'title': 'Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '1718384deb5a0298c31b902fff4e0caba6aaf298',\n",
       "  'title': 'Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "  'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       " {'paperId': '56796e5fcf12f9bcdfc4aef96c12b97b6c63746e',\n",
       "  'title': 'Shared Trained Models Selection and Management for Transfer Reinforcement Learning in Open IoT'},\n",
       " {'paperId': '98d0821e7165c1f5e08123e46efea804dddfb577',\n",
       "  'title': 'Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization'},\n",
       " {'paperId': 'e5d128eee302958cca0d744046ceed841c5b3d8e',\n",
       "  'title': 'Pick Your Battles: Interaction Graphs as Population-Level Objectives for Strategic Diversity'},\n",
       " {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "  'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       " {'paperId': 'a8262348003a9d93ea7ceffc887729cf88dedf06',\n",
       "  'title': 'Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms'},\n",
       " {'paperId': '61e6674bcdf297dda6744c8fe69cbc0d8ec6006c',\n",
       "  'title': 'Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning'},\n",
       " {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "  'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       " {'paperId': 'a30904d356f61dea1a1966571dbec8d2375e862e',\n",
       "  'title': 'The Multi-Dimensional Actions Control Approach for Obstacle Avoidance Based on Reinforcement Learning'},\n",
       " {'paperId': '0ff1247950819a6beafa369178c6c9489de3ceaa',\n",
       "  'title': 'Unsupervised Discovery of Transitional Skills for Deep Reinforcement Learning'},\n",
       " {'paperId': 'e2dc201c06500b30ac337d7e6e85e1e76955dd4a',\n",
       "  'title': 'Unsupervised Skill-Discovery and Skill-Learning in Minecraft'},\n",
       " {'paperId': '2d84f9f1483a73acff0f349826c6bc0ac4025075',\n",
       "  'title': 'Adaptable Agent Populations via a Generative Model of Policies'},\n",
       " {'paperId': 'cfcbeec1ae2f7d13ec1576aa30cb98f5326ffa71',\n",
       "  'title': 'Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '107e4ea37d2e5364893107a8ce072972c4a10dfb',\n",
       "  'title': 'Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       " {'paperId': '4ff9a9248dceb00a7d02073815c085911d5c0a59',\n",
       "  'title': 'Discovering Generalizable Skills via Automated Generation of Diverse Tasks'},\n",
       " {'paperId': 'a3a6bce70faccd8b33e44a3c9a838289a870f6ec',\n",
       "  'title': 'On Study of Mutual Information and its Estimation Methods'},\n",
       " {'paperId': '45f573f302dc7e77cbc5d1a74ccbac3564bbebc8',\n",
       "  'title': 'PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training'},\n",
       " {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "  'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       " {'paperId': '26d601215e16b7b69e3ee2f88282312ba4577519',\n",
       "  'title': 'Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning'},\n",
       " {'paperId': '5c37023c35fc1c95565d56b4fc4821fcf768651a',\n",
       "  'title': 'Reward is enough for convex MDPs'},\n",
       " {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "  'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       " {'paperId': '4f9f09d1ab684b145627f5cbad5560f364e51559',\n",
       "  'title': 'Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning'},\n",
       " {'paperId': '32a921c37bc5d4d9b1b906f812e7a41fe5689085',\n",
       "  'title': 'Scalable, Decentralized Multi-Agent Reinforcement Learning Methods Inspired by Stigmergy and Ant Colonies'},\n",
       " {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "  'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       " {'paperId': '17051a6fb19dfa224bd4c4de5825ea15d765e723',\n",
       "  'title': 'Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning'},\n",
       " {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "  'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       " {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "  'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       " {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "  'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       " {'paperId': '46130875c8c2d89ea23dfb29c3784a6e5e510e54',\n",
       "  'title': 'Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning'},\n",
       " {'paperId': '38fa4bd7e944b73d128b93f2d279416f93074222',\n",
       "  'title': 'Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems'},\n",
       " {'paperId': '1adbeb95eae6bad58425cdb40565e627032f72aa',\n",
       "  'title': 'State-Aware Variational Thompson Sampling for Deep Q-Networks'},\n",
       " {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "  'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       " {'paperId': '37a1050c1c5dab50c30d14f34ae00c723f06eb9f',\n",
       "  'title': 'Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp'},\n",
       " {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "  'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       " {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "  'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       " {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "  'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       " {'paperId': '0d6a4e45acde6f47d704ed0752f17f7ab52223af',\n",
       "  'title': 'Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning'},\n",
       " {'paperId': '1213756da1b99702ab45b4745b6053365686f4ca',\n",
       "  'title': 'A development cycle for automated self-exploration of robot behaviors'},\n",
       " {'paperId': 'c88c99fc89a32883384b5a629a8905504e42ac72',\n",
       "  'title': 'Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning'},\n",
       " {'paperId': 'a2e0f316b9bfe24f66464edb55a8615b01f38904',\n",
       "  'title': 'Trajectory Diversity for Zero-Shot Coordination'},\n",
       " {'paperId': '530f1b3f399f4bdee1d009e1bc2824ec861f6fba',\n",
       "  'title': 'Generalized Reinforcement Learning for Gameplay'},\n",
       " {'paperId': '5546d2a7ab3b7a8150b72e641af90a199537f7a8',\n",
       "  'title': 'E NVIRONMENT -A DVERSARIAL SUB -T ASK C URRICU LUM FOR E FFICIENT R EINFORCEMENT L EARNING A BSTRACT'},\n",
       " {'paperId': '567c75aa0c146f92db19134806b5b41266b13f59',\n",
       "  'title': 'V ALUE F UNCTION S PACES : S KILL -C ENTRIC S TATE A BSTRACTIONS FOR L ONG -H ORIZON R EASONING'},\n",
       " {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "  'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       " {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "  'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       " {'paperId': '599fcb9ca476098f33334852cc360bae9f1f7ee2',\n",
       "  'title': '[Appendix] Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       " {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "  'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       " {'paperId': '13c4a790dd099ded14d424df332b10195ad1ee14',\n",
       "  'title': 'Discovery and Learning of Navigation Goals from Pixels in Minecraft'},\n",
       " {'paperId': '0cff1642794d9628b6e5388a4af97bf9a77ccdba',\n",
       "  'title': 'L ATENT S KILL P LANNING FOR E XPLORATION AND T RANSFER'},\n",
       " {'paperId': '637cb5b797697930d7d1577031134801af99f83d',\n",
       "  'title': 'Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning'},\n",
       " {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "  'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       " {'paperId': '4d1537347d8f5c463188166ae96c3c0d7a3260fa',\n",
       "  'title': 'Skill Transfer via Partially Amortized Hierarchical Planning'},\n",
       " {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "  'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       " {'paperId': 'b667641dc6acd7c0233503615942ea00ea9875f5',\n",
       "  'title': 'Continual Learning of Control Primitives: Skill Discovery via Reset-Games'},\n",
       " {'paperId': '3da314388876286aacb6d9b355439ee68700576d',\n",
       "  'title': 'Automatische Programmierung von Produktionsmaschinen'},\n",
       " {'paperId': '2be22d8a3f39c7bcd1c7639b849e640a0003c831',\n",
       "  'title': 'Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity'},\n",
       " {'paperId': '7fad5ce5ef04b84d3cee1ab79f16532b93c8aad5',\n",
       "  'title': 'BSE-MAML: Model Agnostic Meta-Reinforcement Learning via Bayesian Structured Exploration'},\n",
       " {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "  'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       " {'paperId': '07d251a8d721b5f3da6cc8a92e75840b563927f2',\n",
       "  'title': 'Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness'},\n",
       " {'paperId': '79f054f309a5e104aac046522346e53ad4fc7fd5',\n",
       "  'title': 'Temporal Difference Uncertainties as a Signal for Exploration'},\n",
       " {'paperId': '560e9a8a1b027adbefb838ca337aa73af7998ca9',\n",
       "  'title': 'Disentangling causal effects for hierarchical reinforcement learning'},\n",
       " {'paperId': 'f22d1a958c5fe235d454469a82f2e4d61d2aaeac',\n",
       "  'title': 'Robust RL-Based Map-Less Local Planning: Using 2D Point Clouds as Observations'},\n",
       " {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "  'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       " {'paperId': '6be61525ee8b21c3bef6564df17b435fc4f84282',\n",
       "  'title': 'Action and Perception as Divergence Minimization'},\n",
       " {'paperId': 'cbd2935be2cfc61ead29d3af9529519e8a5f4172',\n",
       "  'title': 'OCEAN: Online Task Inference for Compositional Tasks with Context Adaptation'},\n",
       " {'paperId': '50e0d675bc64e4648b5ceda1268f00cc9c3269c6',\n",
       "  'title': 'The formation and use of hierarchical cognitive maps in the brain: A neural network model'},\n",
       " {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "  'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       " {'paperId': '019820cbb73d0651a913bb74cbfb713c8ad772df',\n",
       "  'title': 'ELSIM: End-to-end learning of reusable skills through intrinsic motivation'},\n",
       " {'paperId': '41382835ae60fb3280ea9a5b3004a236af1eb01b',\n",
       "  'title': 'CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information'},\n",
       " {'paperId': '361ccae6cb40343c8824c9d64104ff8261a7c089',\n",
       "  'title': 'Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "  'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       " {'paperId': '99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45',\n",
       "  'title': 'Novel Policy Seeking with Constrained Optimization'},\n",
       " {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "  'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       " {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "  'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       " {'paperId': 'b4873e3a17058c81a8d2bba838cdd0c415ee80e7',\n",
       "  'title': 'Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning'},\n",
       " {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "  'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       " {'paperId': 'e014af8ae8d7dbd3c1c908dfba334a6d2181b8e1',\n",
       "  'title': 'Task-oriented Dialogue System for Automatic Disease Diagnosis via Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "  'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       " {'paperId': 'f423e28e9a42c0a6a37e5a09fe490d5f27c464fe',\n",
       "  'title': 'Particle-Based Adaptive Discretization for Continuous Control using Deep Reinforcement Learning'},\n",
       " {'paperId': 'bbac680797af0f7ce4cdcc6430ff001fa0dfe670',\n",
       "  'title': 'Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations'},\n",
       " {'paperId': '845aeb9dcf12efba0760c6eb2e2ac56ea27f3247',\n",
       "  'title': 'Option Discovery in the Absence of Rewards with Manifold Analysis'},\n",
       " {'paperId': '11236ae4f31b428b6313559fb99300643c172cf9',\n",
       "  'title': 'Meta-learning curiosity algorithms'},\n",
       " {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "  'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       " {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "  'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       " {'paperId': '886b604f74d65bb420bc1311e9647c44fe8fa91e',\n",
       "  'title': 'Temporal-adaptive Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '7b0871c783e721bfbf9b5d16e575130a07a672cd',\n",
       "  'title': 'Generalized Hindsight for Reinforcement Learning'},\n",
       " {'paperId': '5a30917733db939b004d6a637a5d316373914af4',\n",
       "  'title': 'Combining primitive DQNs for improved reinforcement learning in Minecraft'},\n",
       " {'paperId': '81a5864c21bf8e4018ac9004d618ccb99e261965',\n",
       "  'title': 'Efficient hindsight reinforcement learning using demonstrations for robotic tasks with sparse rewards'},\n",
       " {'paperId': '3f787ac280c3d0dbb16eb2456e62ffbf1a56ff62',\n",
       "  'title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation'},\n",
       " {'paperId': '46106fcd08223540d080f674b26770c1fa8a52ff',\n",
       "  'title': 'Influence-Based Multi-Agent Exploration'},\n",
       " {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "  'title': 'Dynamics-aware Embeddings'},\n",
       " {'paperId': '4cd909c37ee27a97b57f36bd0cad1a2fcb23c441',\n",
       "  'title': 'Deep Reinforcement Learning with Adaptive Update Target Combination'},\n",
       " {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "  'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       " {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "  'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       " {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "  'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       " {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "  'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "  'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       " {'paperId': 'edcd3a5e8e0e6fd27f95c34348404ea449b6927d',\n",
       "  'title': 'Mega-Reward: Achieving Human-Level Play without Extrinsic Rewards'},\n",
       " {'paperId': 'a7859b059cfe01d01f1bd795e86eb3f0771fb53b',\n",
       "  'title': 'Model primitives for hierarchical lifelong reinforcement learning'},\n",
       " {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "  'title': 'Multitask Soft Option Learning'},\n",
       " {'paperId': '7388826b5ee00efe17cb7f19a623d9b5e955ae70',\n",
       "  'title': 'Skew-Fit: State-Covering Self-Supervised Reinforcement Learning'},\n",
       " {'paperId': '5330b2732c12c7027811869a921f544a0bf581ca',\n",
       "  'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks'},\n",
       " {'paperId': 'ec684b9cf2433680f6bd70779186f34bcd5b4f06',\n",
       "  'title': 'MaxEnt Reward Expected Reward Latent Representations Missing Data Controllable Future Factorized Target Perception Action Both Low Entropy Preferences Empowerment Skill Discovery Amortized Inference Maximum Likelihood Variational Inference Input Density Exploration Information GainFiltering Latent S'},\n",
       " {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "  'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       " {'paperId': '728cfe9697d7f7a9940dca17a4045fd10d6c0bf4',\n",
       "  'title': 'IHRL: Interactive Influence-based Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "  'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       " {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "  'title': 'Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'd802b949ff627d51fcb148685256f9fd25f848d5',\n",
       "  'title': 'Hindsight Planner'},\n",
       " {'paperId': '839ea5421bc5515f1465d49613972b64cce61302',\n",
       "  'title': 'Effective, interpretable algorithms for curiosity automatically discovered by evolutionary search'},\n",
       " {'paperId': 'c5e1cbf8e76fb074bb666c695763cefb16381000',\n",
       "  'title': 'Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning'},\n",
       " {'paperId': 'd9e14b81e7acf2e17de20df113018943978509ae',\n",
       "  'title': 'Inter-Level Cooperation in Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '0d2bda1c16a1e4907175df2a1041068ccbe701ae',\n",
       "  'title': 'Learning Stochastic Weight Masking to Resist Adversarial Attacks'},\n",
       " {'paperId': '6fcc8c61041b495c82d339d0cb17147bee2cf0e1',\n",
       "  'title': 'Learning from Trajectories via Subgoal Discovery'},\n",
       " {'paperId': 'd861cab02cbb314fa7f1e14103a238d66e5d8809',\n",
       "  'title': 'Research on Learning Method Based on Hierarchical Decomposition'},\n",
       " {'paperId': 'f58a77a92b795241943b7ff1740dfcc58039589c',\n",
       "  'title': 'TendencyRL: Multi-stage Discriminative Hints for Efficient Goal-Oriented Reverse Curriculum Learning'},\n",
       " {'paperId': '3f116db230b58e6908f5d5791c9761260ad2d7b2',\n",
       "  'title': 'Graph-Based Design of Hierarchical Reinforcement Learning Agents'},\n",
       " {'paperId': '38eb1086cebd6d92b061d8282dc22d6cf4181b6b',\n",
       "  'title': 'Multi-controller multi-objective locomotion planning for legged robots'},\n",
       " {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "  'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       " {'paperId': '701e5d53031ebcd6955dc6da1bd9525e392fe770',\n",
       "  'title': 'Imagined Value Gradients: Model-Based Policy Optimization with Transferable Latent Dynamics Models'},\n",
       " {'paperId': '5225b6a414254e6d8e4a474d84011a974b5eb6ba',\n",
       "  'title': 'Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks'},\n",
       " {'paperId': '68d4d8a1b2cf589de9f116cf748c9e8f11ab852f',\n",
       "  'title': 'MAVEN: Multi-Agent Variational Exploration'},\n",
       " {'paperId': 'df3ac75ec8ad937b7e1d43d6e4f40aa0cfa6bc01',\n",
       "  'title': 'Playing Atari Ball Games with Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '4c52f87830d6c0f9d7e61defa695bd65a8aca067',\n",
       "  'title': 'Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks'},\n",
       " {'paperId': 'd85cc7cd2e424f598c447c72473078957e55b74b',\n",
       "  'title': 'Skew-Explore: Learn faster in continuous spaces with sparse rewards'},\n",
       " {'paperId': '89f11c2e2bfc1af9ec3e703bb62c4547c4de33bd',\n",
       "  'title': 'Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes'},\n",
       " {'paperId': '5a44e0877ae4d3b6322164a02d204429604e3daf',\n",
       "  'title': 'Construction of Macro Actions for Deep Reinforcement Learning'},\n",
       " {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "  'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       " {'paperId': '57c84b34c8516659d8e1aec2be736f9adce61878',\n",
       "  'title': 'Stochastic Activation Actor Critic Methods'},\n",
       " {'paperId': '7723086a4a09c39a1cd91c62b2234cdf72dc1f63',\n",
       "  'title': 'Multi-Task Hierarchical Imitation Learning for Home Automation'},\n",
       " {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "  'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       " {'paperId': 'a77df5291ee644022067f34eec790ea31380792b',\n",
       "  'title': 'Are You for Real? Detecting Identity Fraud via Dialogue Interactions'},\n",
       " {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "  'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       " {'paperId': 'f5cf9f849b67d19b6bb378fe14433d7d8f4b4ddd',\n",
       "  'title': 'Constructive Policy: Reinforcement Learning Approach for Connected Multi-Agent Systems'},\n",
       " {'paperId': '127a8f944a8010f768aac9d01cdba5548456b217',\n",
       "  'title': 'Skill based transfer learning with domain adaptation for continuous reinforcement learning domains'},\n",
       " {'paperId': '3944f15e0c2d39d114e5a44aedd079630607521e',\n",
       "  'title': 'Semantic RL with Action Grammars: Data-Efficient Learning of Hierarchical Task Abstractions'},\n",
       " {'paperId': '6225f9fe891b233db76cef6d48ef786a7ecaffc5',\n",
       "  'title': 'Hybrid system identification using switching density networks'},\n",
       " {'paperId': 'fad3d8d8ba17799f362ba1436cf0dccb00d12cee',\n",
       "  'title': 'Online Multi-modal Imitation Learning via Lifelong Intention Encoding'},\n",
       " {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "  'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       " {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "  'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       " {'paperId': '6d399bff0205977f51ff2334168c89320206493d',\n",
       "  'title': 'Goal-conditioned Imitation Learning'},\n",
       " {'paperId': '390c7c230c223498c281a204006c5fc141759460',\n",
       "  'title': 'Transfer Learning by Modeling a Distribution over Policies'},\n",
       " {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "  'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       " {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "  'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       " {'paperId': '6b36776e5c0473d82cbdd2c92cd97cca7925ae08',\n",
       "  'title': 'TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning'},\n",
       " {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "  'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       " {'paperId': '7bd95a62fd6320730cbb24a0e4fafac97d840652',\n",
       "  'title': 'Meta Reinforcement Learning with Task Embedding and Shared Policy'},\n",
       " {'paperId': '074b4702465630da81a3b40600183e58a42c404d',\n",
       "  'title': 'Apprentissage séquentiel budgétisé pour la classification extrême et la découverte de hiérarchie en apprentissage par renforcement. (Budgeted sequential learning for extreme classification and for the discovery of hierarchy in reinforcement learning)'},\n",
       " {'paperId': '27d5c1a6b35a591739c10a04e5fc1a96b10e3177',\n",
       "  'title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning'},\n",
       " {'paperId': 'fb960d0ea21ff4f1c51444dd6e644efeb16b5dd7',\n",
       "  'title': 'Continual and Multi-task Reinforcement Learning With Shared Episodic Memory'},\n",
       " {'paperId': '6570c7cab46d2b0f3315f5abfbbd209140529c8b',\n",
       "  'title': 'Hierarchical Policy Learning is Sensitive to Goal Space Design'},\n",
       " {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "  'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       " {'paperId': 'be928f91385999fa90d1e2fe06058f9dbcfd7186',\n",
       "  'title': 'Routing Networks and the Challenges of Modular and Compositional Computation'},\n",
       " {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "  'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       " {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "  'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       " {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "  'title': 'The Termination Critic'},\n",
       " {'paperId': '70a3f24292bdcf6e630d5b32eacf93aa3f913c59',\n",
       "  'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent networks.'},\n",
       " {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "  'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       " {'paperId': '7d5dd17e1a10c0388d2e134dedcfe4e8b224352b',\n",
       "  'title': 'Self-supervised Learning of Image Embedding for Continuous Control'},\n",
       " {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "  'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       " {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "  'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751',\n",
       "  'title': 'Deep Reinforcement Learning'},\n",
       " {'paperId': 'a88d54c168a84ed8a04d2a32be0b5939586b5792',\n",
       "  'title': 'NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning'},\n",
       " {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "  'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       " {'paperId': '1f26116ff8758190e4a5d7fa65f137b2b4befed1',\n",
       "  'title': 'InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics'},\n",
       " {'paperId': '5bdf7fde60ad8cb413eab7201c28c796d5f23698',\n",
       "  'title': 'Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications'},\n",
       " {'paperId': '653bdfb3c35621ee04ee5d5253dc7e3a422d69e1',\n",
       "  'title': 'Learning Self-Imitating Diverse Policies'},\n",
       " {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "  'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       " {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "  'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       " {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "  'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       " {'paperId': 'af3ecce4aa6ba955d25fa2f7455a82c9b2b328cf',\n",
       "  'title': 'UvA-DARE (Digital Academic Repository) Stochastic Activation Actor Critic Methods'},\n",
       " {'paperId': '5c6a0b7cc9f2455de3494baf92b07a10a711b679',\n",
       "  'title': 'UvA-DARE (Digital Academic Repository) Stochastic Activation Actor Critic Methods Stochastic Activation Actor Critic Methods ∗'},\n",
       " {'paperId': '3eda1c2c8b6727ce5f32576a6001d7ef9b722f71',\n",
       "  'title': 'Stochastic Activation Actor Critic Methods*'},\n",
       " {'paperId': '02feb390612858d745ce324303bfc8f9d0148c42',\n",
       "  'title': 'Building structured hierarchical agents'},\n",
       " {'paperId': 'c558c56245d4e5b3ba1cb3ba650cb6e7f4bd0cbc',\n",
       "  'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, Sujoy; van Baar, Jeroen; Roy-Chowdhury, Amit K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       " {'paperId': '822cd314662479bc4bad911a3768d921614fcaa3',\n",
       "  'title': 'WHY DOES HIERARCHY (SOMETIMES) WORK'},\n",
       " {'paperId': '7fb2f8458dc50b0317e8431862345fb313cd28fa',\n",
       "  'title': 'Modern Optimization for Statistics and Learning'},\n",
       " {'paperId': 'a3a8861363fdfbd6a0792fdffb64517e366fea01',\n",
       "  'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, S.; van Baar, J.; Roy Chowdhury, A.K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       " {'paperId': 'e7069f324d16847e9939bded648032e54c8c9331',\n",
       "  'title': '10-708 Final Report:Investigating Max-Entropy Latent-Space Policiesfor Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'b668ba0900ddcdacd0a07ff9983172f525c3c4d6',\n",
       "  'title': 'Goal-conditioned Imitation Learning'},\n",
       " {'paperId': '5654028d5193dbf8eaa5ab9ef6af21f6da265878',\n",
       "  'title': 'SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-'},\n",
       " {'paperId': '4c19fcb0f8c041996c7cbb0deab15fbec5d1b5d9',\n",
       "  'title': 'Meta-Learning via Weighted Gradient Update'},\n",
       " {'paperId': '98b41528c58e6f5b7b28be5b54029e52ca90c4ab',\n",
       "  'title': 'Learning to Learn: Hierarchical Meta-Critic Networks'},\n",
       " {'paperId': '39995a05908b23249025de5a6c3c439d0dd33d9d',\n",
       "  'title': 'MUTUAL-INFORMATION REGULARIZATION'},\n",
       " {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "  'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       " {'paperId': '4b61c25a86083c20730c9b12737ac6ac4178c364',\n",
       "  'title': 'An Introduction to Deep Reinforcement Learning'},\n",
       " {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "  'title': 'Modulated Policy Hierarchies'},\n",
       " {'paperId': '415e3cf34d45f92a0515bb85611f099d86e92e1f',\n",
       "  'title': 'Learning Physically Based Humanoid Climbing Movements'},\n",
       " {'paperId': '0a01766797da6701034a9b4947bb2201ef2f3380',\n",
       "  'title': 'Hierarchical reinforcement learning of multiple grasping strategies with human instructions'},\n",
       " {'paperId': '97b802a9b094594f5778682ccaa56863280a00d5',\n",
       "  'title': 'An Approach to Hierarchical Deep Reinforcement Learning for a Decentralized Walking Control Architecture'},\n",
       " {'paperId': 'a83eeb55896c963cc56244335eca2d4fee5f7a99',\n",
       "  'title': 'Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback'},\n",
       " {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "  'title': 'Variational Option Discovery Algorithms'},\n",
       " {'paperId': '4c03497f2e17900cbf4066fbf68a7cbaad8376be',\n",
       "  'title': 'Representational efficiency outweighs action efficiency in human program induction'},\n",
       " {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "  'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       " {'paperId': '2e7b6e73398af01bc975e9bf9374ee5f255252b3',\n",
       "  'title': 'VFunc: a Deep Generative Model for Functions'},\n",
       " {'paperId': 'd2eaa230a68d38e9fe508dc8f2e712712e978cdc',\n",
       "  'title': 'Budgeted Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '1cb6edbedc4a1ac5c32f61a435a23264e42a9071',\n",
       "  'title': 'Towards Sample Efficient Reinforcement Learning'},\n",
       " {'paperId': '9bc8fdfd393701dbfdd64e0a6f8bb9a1f66804c3',\n",
       "  'title': 'Marginal Policy Gradients for Complex Control'},\n",
       " {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "  'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       " {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "  'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       " {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "  'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       " {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "  'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '5c55162deeea9870f765e0a9cd3e8387b05a0ea2',\n",
       "  'title': 'Disentangling the independently controllable factors of variation by interacting with the world'},\n",
       " {'paperId': '68c108795deef06fa929d1f6e96b75dbf7ce8531',\n",
       "  'title': 'Meta-Reinforcement Learning of Structured Exploration Strategies'},\n",
       " {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "  'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       " {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "  'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       " {'paperId': '90d3f103b6b03accff2799cb2bf8ca95d3d71669',\n",
       "  'title': 'Hierarchical Learning for Modular Robots'},\n",
       " {'paperId': '6938ddf69008ac0a13afd5855c854c8a7520adc5',\n",
       "  'title': 'Hierarchical Policy Search via Return-Weighted Density Estimation'},\n",
       " {'paperId': '58fb60c5592224901a26dd84220a2f3332c1fcf5',\n",
       "  'title': 'Eigenoption Discovery through the Deep Successor Representation'},\n",
       " {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "  'title': 'Meta Learning Shared Hierarchies'},\n",
       " {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "  'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       " {'paperId': '64643ab9a5f70945c2b171558e121006a99d27a2',\n",
       "  'title': 'NADPE X : A N ON-POLICY TEMPORALLY CONSISTENT EXPLORATION METHOD FOR DEEP REINFORCEMENT LEARNING'},\n",
       " {'paperId': '5beaeff056549019926075746f8c4f78e30494b0',\n",
       "  'title': 'EARNING AN E MBEDDING S PACE FOR T RANSFERABLE R OBOT S KILLS'},\n",
       " {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "  'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'},\n",
       " {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "  'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       " {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "  'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       " {'paperId': '2f6bf6b57cba8be7aa4631645cc7313824bfb674',\n",
       "  'title': 'Slowness-based neural visuomotor control with an Intrinsically motivated Continuous Actor-Critic'},\n",
       " {'paperId': '4155ecb89086261704bae0040abcf326c41c21f8',\n",
       "  'title': 'Extending the Hierarchical Deep Reinforcement Learning framework'},\n",
       " {'paperId': 'b65a6be07ce9c86797e6917258cf5ba45273ee73',\n",
       "  'title': 'NON-PARAMETRIC DISCRIMINATIVE REWARDS'},\n",
       " {'paperId': '4ab08b2f1193d770c241b41f5d9f1c841a3663d3',\n",
       "  'title': 'Optimizing Chemical Reactions with Deep Reinforcement Learning'},\n",
       " {'paperId': '7f64121eaf74b8204e0445e804f93f3b53a0a64a',\n",
       "  'title': 'The Eigenoption-Critic Framework'},\n",
       " {'paperId': 'd53d49ec21c372d4781d54346c95faa17b332e98',\n",
       "  'title': 'Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck'},\n",
       " {'paperId': '72e87d27e8b3493981daca533b3956fae8b4f316',\n",
       "  'title': 'Learning Robot Skill Embeddings'},\n",
       " {'paperId': '7a5196d05b145ec552912dccedd16a42c88718f1',\n",
       "  'title': 'Learning Skill Embeddings for Transferable Robot Skills'},\n",
       " {'paperId': '52b18b9b31d942b8fc83dc69db097557c881a641',\n",
       "  'title': 'Hierarchical Reinforcement Learning with Parameters'},\n",
       " {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "  'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       " {'paperId': 'd672baf56986a3bc5748c25362b2d2b4d65efcb8',\n",
       "  'title': 'Multi-task Learning with Gradient Guided Policy Specialization'},\n",
       " {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "  'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       " {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "  'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       " {'paperId': '0c2954912936b59162881374164fe79e7b2bb66f',\n",
       "  'title': 'Discrete Sequential Prediction of Continuous Actions for Deep RL'},\n",
       " {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "  'title': 'Multi-Level Discovery of Deep Options'},\n",
       " {'paperId': '850d78496304829d16d14701e4d81692f088f47d',\n",
       "  'title': 'EX2: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       " {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "  'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       " {'paperId': '9f1e9e56d80146766bc2316efbc54d8b770a23df',\n",
       "  'title': 'Deep Reinforcement Learning: An Overview'},\n",
       " {'paperId': '4eb38b3460606a4042b04fc52d0044ab948b4a17',\n",
       "  'title': 'EX: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       " {'paperId': 'f8a257006599de5899506959de5f4a8a1b2d2fec',\n",
       "  'title': 'Options Discovery with Budgeted Reinforcement Learning'}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper['citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'paperId': 'bcd857d75841aa3e92cd4284a8818aba9f6c0c3f',\n",
       "  'title': 'Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS'},\n",
       " {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "  'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       " {'paperId': '515a21e90117941150923e559729c59f5fdade1c',\n",
       "  'title': 'The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables'},\n",
       " {'paperId': '8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a',\n",
       "  'title': 'Learning modular neural network policies for multi-task and multi-robot transfer'},\n",
       " {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "  'title': 'The Option-Critic Architecture'},\n",
       " {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "  'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       " {'paperId': '136cf66392f1d6bf42da4cc070888996dc472b91',\n",
       "  'title': 'On Multiplicative Integration with Recurrent Neural Networks'},\n",
       " {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "  'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       " {'paperId': '35da0a2001eea88486a5de677ab97868c93d0824',\n",
       "  'title': 'InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets'},\n",
       " {'paperId': 'fddc15480d086629b960be5bff96232f967f2252',\n",
       "  'title': 'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding'},\n",
       " {'paperId': '6e90fd78e8a3b98af3954aae5209703aa966603e',\n",
       "  'title': 'Unifying Count-Based Exploration and Intrinsic Motivation'},\n",
       " {'paperId': '317cd4522b1f4a6f889743578143bb8823623f8b',\n",
       "  'title': 'VIME: Variational Information Maximizing Exploration'},\n",
       " {'paperId': '1464776f20e2bccb6182f183b5ff2e15b0ae5e56',\n",
       "  'title': 'Benchmarking Deep Reinforcement Learning for Continuous Control'},\n",
       " {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "  'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       " {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "  'title': 'Continuous control with deep reinforcement learning'},\n",
       " {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "  'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       " {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "  'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       " {'paperId': '1389772b8a0f9c7fc43057f9da41a7d0ebf0308b',\n",
       "  'title': 'Generalization and Exploration via Randomized Value Functions'},\n",
       " {'paperId': 'ce2e0bd9135814f4018106bc31d87902b358e251',\n",
       "  'title': 'Variational Information Maximizing Exploration'},\n",
       " {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "  'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       " {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "  'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       " {'paperId': 'bb1a17010254abfa5e1f2a17553582ce449f8e16',\n",
       "  'title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images'},\n",
       " {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "  'title': 'Human-level control through deep reinforcement learning'},\n",
       " {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "  'title': 'Trust Region Policy Optimization'},\n",
       " {'paperId': 'a2785f66c20fbdf30ec26c0931584c6d6a0f4fca',\n",
       "  'title': 'DRAW: A Recurrent Neural Network For Image Generation'},\n",
       " {'paperId': 'b6cc21b30912bdaecd9f178d700a4c545b1d0838',\n",
       "  'title': 'Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning'},\n",
       " {'paperId': '512ea8d0c5b5de896129e76d4276f7b996fe88d8',\n",
       "  'title': 'Learning Stochastic Feedforward Neural Networks'},\n",
       " {'paperId': 'b9e4fc24106bfced345c9cd6e24695a1e5c2e483',\n",
       "  'title': 'Autonomous reinforcement learning with hierarchical REPS'},\n",
       " {'paperId': '225fbfd99465033e993460a1bc838a87fbf42346',\n",
       "  'title': 'Gaussian-Bernoulli deep Boltzmann machine'},\n",
       " {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "  'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       " {'paperId': '2ffc1cbe7488ba3d054b482bac5edf9d272cf99c',\n",
       "  'title': 'Autonomous Skill Acquisition on a Mobile Manipulator'},\n",
       " {'paperId': '33224ad0cdf6e2dc4893194dd587309c7887f0ba',\n",
       "  'title': 'Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)'},\n",
       " {'paperId': '274f4ad2f9be649c297eba6bffa97540599569df',\n",
       "  'title': 'Bayesian Multi-Task Reinforcement Learning'},\n",
       " {'paperId': 'a9c7e2a415cac344e1b4b3126bd43f6f53264e3e',\n",
       "  'title': 'Intrinsically Motivated Hierarchical Skill Learning in Structured Environments'},\n",
       " {'paperId': '467568f1777bc51a15a5100516cd4fe8de62b9ab',\n",
       "  'title': 'Transfer Learning for Reinforcement Learning Domains: A Survey'},\n",
       " {'paperId': '2a8a076c26875208d52c66e07aa7f6db9a4f34b7',\n",
       "  'title': 'Representational Power of Restricted Boltzmann Machines and Deep Belief Networks'},\n",
       " {'paperId': 'ab19a482195f4299f96b98e4eb15cb3ad4753f3b',\n",
       "  'title': 'Multi-task reinforcement learning: a hierarchical Bayesian approach'},\n",
       " {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "  'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       " {'paperId': '8978cf7574ceb35f4c3096be768c7547b28a35d0',\n",
       "  'title': 'A Fast Learning Algorithm for Deep Belief Nets'},\n",
       " {'paperId': '1c59bfa0e8654ebea94277064f82062875cae8b6',\n",
       "  'title': 'Identifying useful subgoals in reinforcement learning by local graph partitioning'},\n",
       " {'paperId': 'cc45fa649a3153a61182222f496eb38554caf2bc',\n",
       "  'title': 'A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems'},\n",
       " {'paperId': '12d6fde053e2c7174a76fe1bbdb97dd039a3b662',\n",
       "  'title': 'Intrinsically Motivated Reinforcement Learning'},\n",
       " {'paperId': '42af0ed020c2caecafb7dbe826064d7f9ba2022b',\n",
       "  'title': 'Dynamic abstraction in reinforcement learning via clustering'},\n",
       " {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "  'title': 'Learning Movement Primitives'},\n",
       " {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "  'title': 'Learning Options in Reinforcement Learning'},\n",
       " {'paperId': '9360e5ce9c98166bb179ad479a9d2919ff13d022',\n",
       "  'title': 'Training Products of Experts by Minimizing Contrastive Divergence'},\n",
       " {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "  'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       " {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "  'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       " {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "  'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       " {'paperId': 'a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5',\n",
       "  'title': 'Connectionist Learning of Belief Networks'},\n",
       " {'paperId': '94db34f4b68189bfcba22beab33ee3b54f10b876',\n",
       "  'title': 'Curious model-building control systems'},\n",
       " {'paperId': '4f7476037408ac3d993f5088544aab427bc319c1',\n",
       "  'title': 'Information processing in dynamical systems: foundations of harmony theory'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper['reference']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class 화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelatedWorkAnalyzer:\n",
    "    def __init__(self) -> None:\n",
    "        self.paper_list = {'keypaper':[]}\n",
    "        self.S2_API_KEY = os.environ['S2_API_KEY']\n",
    "        self.sch = SemanticScholar(api_key=self.S2_API_KEY)\n",
    "        self.is_updated = False\n",
    "        self.load_paper_list()\n",
    "\n",
    "    def build_paper_list_from_txt(self):\n",
    "        if os.path.exists('./paper_list.txt'): \n",
    "            f = open('paper_list.txt', 'r')\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                items = line.split('_')\n",
    "                year = items[0]\n",
    "                title = items[-1].split('\\n')[0]\n",
    "                self.addNewPaper(title, keypaper=True)\n",
    "            self.is_updated = True\n",
    "            self.update_pkl()\n",
    "        else: # txt 파일이 없으면 빈 딕셔너리 반환\n",
    "            print('No txt file')\n",
    "            \n",
    "        # Save data_dict\n",
    "        self.update_pkl()\n",
    "\n",
    "    def load_paper_list(self):\n",
    "        if os.path.exists('./paper_list_from_semantic.pkl'): # pkl 파일이 존재하면 불러옴\n",
    "            print('load from pkl')\n",
    "            self.load_pkl()\n",
    "        else: # pkl 파일이 없으면 \n",
    "            print('load from txt')\n",
    "            self.build_paper_list_from_txt()\n",
    "\n",
    "    def addNewPaper(self, paper, keypaper=False):\n",
    "        if ' ' in paper: # title\n",
    "            title, paperId, year = self.searchByTitle(paper)\n",
    "            if self.isExistInList(paperId):\n",
    "                return False\n",
    "        else: # paperId\n",
    "            paperId = paper\n",
    "            if self.isExistInList(paperId):\n",
    "                return False\n",
    "            title, paperId, year = self.searchByPaperId(paperId) # get the rest of info.\n",
    "\n",
    "        # add a new paper\n",
    "        citations, references = self.getCitationInfo(paperId)\n",
    "        self.paper_list[paperId] = {'title': title, \n",
    "                                    'year': year, \n",
    "                                    'references': references, \n",
    "                                    'citations': citations, \n",
    "                                    'citnuminlist': 0, \n",
    "                                    'refnuminlist': 0, \n",
    "                                    'isKeypaper': keypaper}\n",
    "        print('Added {} in the list'.format(title))\n",
    "        if keypaper:\n",
    "            self.paper_list['keypaper'].append(paperId)\n",
    "            print('Added to key paper list')\n",
    "\n",
    "        return True\n",
    "    \n",
    "    def load_pkl(self):\n",
    "        f =  open('paper_list_from_semantic.pkl','rb')\n",
    "        self.paper_list = pickle.load(f)\n",
    "\n",
    "    def update_pkl(self):\n",
    "        if self.is_updated: # 업데이트 사항이 있는 경우에만\n",
    "            print('update pkl')\n",
    "            with open('paper_list_from_semantic.pkl','wb') as f:\n",
    "               pickle.dump(self.paper_list, f)\n",
    "            self.is_updated = False\n",
    "        else:\n",
    "            print('no update') \n",
    "\n",
    "    def searchByTitle(self, title, fields=['title','year','paperId']):\n",
    "        print(\"Searching by title... {}\".format(title))\n",
    "        results = self.sch.search_paper(title, fields=fields)\n",
    "        paperId = results[0].paperId\n",
    "        title = results[0].title\n",
    "        year = results[0].year\n",
    "        return title, paperId, year\n",
    "    \n",
    "    def searchByPaperId(self, paperId, fields=['title','year','paperId']):\n",
    "        results = self.sch.get_paper(paperId, fields=fields)\n",
    "        paperId = results.paperId\n",
    "        title = results.title\n",
    "        year = results.year\n",
    "        return title, paperId, year\n",
    "\n",
    "\n",
    "    def isExistInList(self, paperId):\n",
    "        if paperId in self.paper_list.keys():\n",
    "            print('Already exists in the list!')\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def getCitationInfo(self, paperId):\n",
    "        headers = {'x-api-key': self.S2_API_KEY}\n",
    "        r = requests.post(\n",
    "            'https://api.semanticscholar.org/graph/v1/paper/batch',\n",
    "            headers=headers,\n",
    "            params={'fields': 'citations,references,title'},\n",
    "            json={\"ids\": [paperId]}\n",
    "        )\n",
    "        return r.json()[0]['citations'], r.json()[0]['references']\n",
    "    \n",
    "    def updateCitationInfo(self, paperId, reset=False):\n",
    "        keys = self.paper_list[paperId].keys()\n",
    "        if 'citations' not in keys or 'references' not in keys or reset: # if there is no citation info or reset is True\n",
    "            citations, references = self.getCitationInfo(paperId)\n",
    "            self.paper_list[paperId]['citations'] = citations\n",
    "            self.paper_list[paperId]['references'] = references\n",
    "            print(self.paper_list[paperId]['title'] + \"'s citations, references are updated)\")\n",
    "            self.is_updated = True\n",
    "\n",
    "    def updataAllCitationInfo(self, reset=False):\n",
    "        for paperId in self.paper_list.keys():\n",
    "            if str(type(self.paper_list[paperId])) == \"<class 'list'>\":\n",
    "                continue\n",
    "            self.updateCitationInfo(paperId, reset)\n",
    "    \n",
    "    def updateRefToList(self, paperId):\n",
    "        for paper in self.paper_list[paperId]['references']:\n",
    "            title = paper['title']\n",
    "            paperId = paper['paperId']\n",
    "            if not self.isExistInList(paper['paperId']):\n",
    "                self.paper_list[paperId] = {'title': title, \n",
    "                                            'year': None, \n",
    "                                            #'references': [], \n",
    "                                            #'citations': [], \n",
    "                                            'citnuminlist': 0, \n",
    "                                            'refnuminlist': 0, \n",
    "                                            'isKeypaper': False}\n",
    "                print(\"Added {} in the list\".format(title))\n",
    "                self.is_updated = True\n",
    "\n",
    "\n",
    "    def updateCitToList(self, paperId):\n",
    "        for paper in self.paper_list[paperId]['citations']:\n",
    "            title = paper['title']\n",
    "            paperId = paper['paperId']\n",
    "            if not self.isExistInList(paper['paperId']):\n",
    "                self.paper_list[paperId] = {'title': title, \n",
    "                                            'year': None, \n",
    "                                            #'references': [], \n",
    "                                            #'citations': [], \n",
    "                                            'citnuminlist': 0, \n",
    "                                            'refnuminlist': 0, \n",
    "                                            'isKeypaper': False}\n",
    "                self.is_updated = True\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    def checkCitationInList(self, paperId): # citation 목록 중 리스트에 있는 논문이 몇 개인지 체크\n",
    "        if 'citations' in self.paper_list[paperId].keys():\n",
    "            self.paper_list[paperId]['citnuminlist'] = 0\n",
    "            for paper in self.paper_list[paperId]['citations']:\n",
    "                if paper['paperId'] in self.paper_list.keys():\n",
    "                    self.paper_list[paperId]['citnuminlist'] += 1\n",
    "                    self.is_updated = True\n",
    "\n",
    "    def checkReferenceInList(self, paperId): # reference 목록 중 리스트에 있는 논문이 몇 개인지 체크\n",
    "        if 'references' in self.paper_list[paperId].keys():\n",
    "            self.paper_list[paperId]['refnuminlist'] = 0\n",
    "            for paper in self.paper_list[paperId]['references']:\n",
    "                if paper['paperId'] in self.paper_list.keys():\n",
    "                    self.paper_list[paperId]['refnuminlist'] += 1\n",
    "                    self.is_updated = True\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from pkl\n"
     ]
    }
   ],
   "source": [
    "rwa = RelatedWorkAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching by title... Stochastic Neural Networks for Hierarchical Reinforcement Learning\n",
      "Already exists in the list!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa.addNewPaper('Stochastic Neural Networks for Hierarchical Reinforcement Learning', keypaper=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       " '45644c7f952d2a5a5b4e594998e2e6dff9088118',\n",
       " '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8',\n",
       " 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       " '03eebc19358c4bddb4a987b9e94ecbcb7e58b5d2',\n",
       " 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       " '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       " 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       " '1509169e79337e7e1628b266e9475aac779f9c00',\n",
       " '3b51a29424b619ec5ce29125c4b88d8e24a09328',\n",
       " '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       " '2a88d01f3079e68ad9b5bcb1ebe56da25679e331',\n",
       " 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       " '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       " '14444e96f58ff0ce449fd6c61abfffaec1c83f76',\n",
       " 'f8cbd6e934ff43e6227bc14bc77c2934c0b66e23',\n",
       " '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       " '271081730bfce9117ad4432bffa8e5f18dbac133',\n",
       " '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       " '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       " '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       " '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       " '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       " '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       " '119639e61c1f88c3d675dac2d3cf47530969276d',\n",
       " 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       " '23bb22710f7be585305bf01841b74ed167a706ce',\n",
       " 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       " 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       " 'd669358916608af804c20329b7287d02c75b1311',\n",
       " '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       " 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       " 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       " '6ec8797952213227eea2e63620f4d7c060d598d5',\n",
       " 'eadbe2e4f9de47dd357589cf59e3d1f0199e5075',\n",
       " 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       " '872edada2165ad65c1664b813efdb92e3bec1b36',\n",
       " '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0',\n",
       " 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       " 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       " 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       " '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       " '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       " '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       " '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       " '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       " '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       " 'a13a62d16ebeaff7c5125a2b60cfc30a35cd55af',\n",
       " 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       " 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       " '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       " '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       " 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       " '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       " '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       " '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       " 'e2bd18c1039f27675bd64014117db648d969452e']"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa.paper_list['keypaper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['keypaper', '74dd51db773ea883d9804d1845345a46ab908ccd', '45644c7f952d2a5a5b4e594998e2e6dff9088118', '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8', 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c', '03eebc19358c4bddb4a987b9e94ecbcb7e58b5d2', 'b27fd9ea29cabe6afedd01e446b96c34e956ce84', '0af6a63167df299a1556a560d6884ae38eda390d', 'e6548d97d82aa2710019951eb4eac034e1747aa1', '1509169e79337e7e1628b266e9475aac779f9c00', '3b51a29424b619ec5ce29125c4b88d8e24a09328', '546bff6c12ea395690292f204a7e019a8b3b87a0', '2a88d01f3079e68ad9b5bcb1ebe56da25679e331', 'bd2ff852e86d16df09376f2dfdc934c533bb04a2', '947070ff65dc9a0b0024d299acdcfa8251b5118b', '14444e96f58ff0ce449fd6c61abfffaec1c83f76', 'f8cbd6e934ff43e6227bc14bc77c2934c0b66e23', '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9', '271081730bfce9117ad4432bffa8e5f18dbac133', '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6', '372715a73955b7fbc1daf816bd52c0641b3ff5f2', '105f44c9d445de2b93d1297c2d5ac10cc776d654', '259b4f5ed43fda5dd3510821b40fac13021e7605', '45afe2d85f2896ce569be0d27678edcff68017e2', '13dfb80b184a6568485fbfd11e5b24d51b0f503f', '119639e61c1f88c3d675dac2d3cf47530969276d', 'c85662dcd17eed4452019b640a30a323970472ef', '23bb22710f7be585305bf01841b74ed167a706ce', 'd3c6e0b80c36c14f7d1761fb881f20c35165f507', 'b68b8b980db62308864b2a7d33718182c5f8335b', 'd669358916608af804c20329b7287d02c75b1311', '1d6d157f4586ee5fffa172b7198ecb8f7101f921', 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc', 'e90323d515a024be8a6d0465dd90eefd681f9245', '6ec8797952213227eea2e63620f4d7c060d598d5', 'eadbe2e4f9de47dd357589cf59e3d1f0199e5075', 'd242950c9d4903d078055b3f5bbbad1b5e626e74', '872edada2165ad65c1664b813efdb92e3bec1b36', '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0', 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac', 'b43d8c8b25bc65cbf3097480e9000649c79b7a51', 'ffb3886a253ff927bcc46b78e00409893865a68e', '99a7df93a2e16bd7ac3349d52cc34417cda7909d', '17704b148b5c20ddf92acbaf1addda134ecbb474', '7aea82f3b7726b0bd3bb3931dff10c93d1907abf', '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca', '8c54e8575e7c17a4097838305915e6e7b00fd4af', '2fed116dea9c36914b52b55e0f9688ccf641ee07', 'a13a62d16ebeaff7c5125a2b60cfc30a35cd55af', 'fbf03bf621ffee283911e765d525a75fc0d11bae', 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef', '565af8f2ef461b1d7368f3e9899e0f576e4f0a24', '3c3861c607fb79f3fbf79552018724617fc8ba1b', 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69', '049c6e5736313374c6e594c34b9be89a3a09dced', '3deecaee4ec1a37de3cb10420eaabff067669e17', '15b26d8cb35d7e795c8832fe08794224ee1e9f84', 'e2bd18c1039f27675bd64014117db648d969452e'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa.paper_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcd857d75841aa3e92cd4284a8818aba9f6c0c3f\n",
      "29e944711a354c396fad71936f536e83025b6ce0\n",
      "515a21e90117941150923e559729c59f5fdade1c\n",
      "8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a\n",
      "15b26d8cb35d7e795c8832fe08794224ee1e9f84\n",
      "e2bd18c1039f27675bd64014117db648d969452e\n",
      "136cf66392f1d6bf42da4cc070888996dc472b91\n",
      "4ba25cb493ac7a03fc15d3b936257c9a6c689c1d\n",
      "35da0a2001eea88486a5de677ab97868c93d0824\n",
      "fddc15480d086629b960be5bff96232f967f2252\n",
      "6e90fd78e8a3b98af3954aae5209703aa966603e\n",
      "317cd4522b1f4a6f889743578143bb8823623f8b\n",
      "1464776f20e2bccb6182f183b5ff2e15b0ae5e56\n",
      "846aedd869a00c09b40f1f1f35673cb22bc87490\n",
      "024006d4c2a89f7acacc6e4438d156525b60a98f\n",
      "d316c82c12cf4c45f9e85211ef3d1fa62497bff8\n",
      "b6b8a1b80891c96c28cc6340267b58186157e536\n",
      "1389772b8a0f9c7fc43057f9da41a7d0ebf0308b\n",
      "ce2e0bd9135814f4018106bc31d87902b358e251\n",
      "a696aeab7b4c6bb47630663e7638fc0f60b584b8\n",
      "6640f4e4beae786f301928d82a9f8eb037aa6935\n",
      "bb1a17010254abfa5e1f2a17553582ce449f8e16\n",
      "e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d\n",
      "66cdc28dc084af6507e979767755e99fe0b46b39\n",
      "a2785f66c20fbdf30ec26c0931584c6d6a0f4fca\n",
      "b6cc21b30912bdaecd9f178d700a4c545b1d0838\n",
      "512ea8d0c5b5de896129e76d4276f7b996fe88d8\n",
      "b9e4fc24106bfced345c9cd6e24695a1e5c2e483\n",
      "225fbfd99465033e993460a1bc838a87fbf42346\n",
      "8101ec9a994551edfdc7c79ebc89ed939cd07eb3\n",
      "2ffc1cbe7488ba3d054b482bac5edf9d272cf99c\n",
      "33224ad0cdf6e2dc4893194dd587309c7887f0ba\n",
      "274f4ad2f9be649c297eba6bffa97540599569df\n",
      "a9c7e2a415cac344e1b4b3126bd43f6f53264e3e\n",
      "467568f1777bc51a15a5100516cd4fe8de62b9ab\n",
      "2a8a076c26875208d52c66e07aa7f6db9a4f34b7\n",
      "ab19a482195f4299f96b98e4eb15cb3ad4753f3b\n",
      "16050a256dd6add1e9187e8c4f5c30c85f342fd8\n",
      "8978cf7574ceb35f4c3096be768c7547b28a35d0\n",
      "1c59bfa0e8654ebea94277064f82062875cae8b6\n",
      "cc45fa649a3153a61182222f496eb38554caf2bc\n",
      "12d6fde053e2c7174a76fe1bbdb97dd039a3b662\n",
      "42af0ed020c2caecafb7dbe826064d7f9ba2022b\n",
      "82673205bf76c6fc788790308bc14a9a2d8e41ad\n",
      "48bf148ca96f928d762c5be9231f1cdff8090cc7\n",
      "9360e5ce9c98166bb179ad479a9d2919ff13d022\n",
      "4c96ca25d889251e20e33d01f24eec175301ab94\n",
      "0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d\n",
      "52e2ac397f0c8d5f533959905df899bc328d9f85\n",
      "a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5\n",
      "94db34f4b68189bfcba22beab33ee3b54f10b876\n",
      "4f7476037408ac3d993f5088544aab427bc319c1\n"
     ]
    }
   ],
   "source": [
    "for paper in rwa.paper_list['3deecaee4ec1a37de3cb10420eaabff067669e17']['references']:\n",
    "    print(paper['paperId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS in the list\n",
      "Added Categorical Reparameterization with Gumbel-Softmax in the list\n",
      "Added The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables in the list\n",
      "Added Learning modular neural network policies for multi-task and multi-robot transfer in the list\n",
      "Added The Option-Critic Architecture in the list\n",
      "Added Learning and Transfer of Modulated Locomotor Controllers in the list\n",
      "Added On Multiplicative Integration with Recurrent Neural Networks in the list\n",
      "Added Strategic Attentive Writer for Learning Macro-Actions in the list\n",
      "Added InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets in the list\n",
      "Added Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding in the list\n",
      "Added Unifying Count-Based Exploration and Intrinsic Motivation in the list\n",
      "Added VIME: Variational Information Maximizing Exploration in the list\n",
      "Added Benchmarking Deep Reinforcement Learning for Continuous Control in the list\n",
      "Added Mastering the game of Go with deep neural networks and tree search in the list\n",
      "Added Continuous control with deep reinforcement learning in the list\n",
      "Added High-Dimensional Continuous Control Using Generalized Advantage Estimation in the list\n",
      "Added End-to-End Training of Deep Visuomotor Policies in the list\n",
      "Added Generalization and Exploration via Randomized Value Functions in the list\n",
      "Added Variational Information Maximizing Exploration in the list\n",
      "Added Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning in the list\n",
      "Added Learning Continuous Control Policies by Stochastic Value Gradients in the list\n",
      "Added Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images in the list\n",
      "Added Human-level control through deep reinforcement learning in the list\n",
      "Added Trust Region Policy Optimization in the list\n",
      "Added DRAW: A Recurrent Neural Network For Image Generation in the list\n",
      "Added Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning in the list\n",
      "Added Learning Stochastic Feedforward Neural Networks in the list\n",
      "Added Autonomous reinforcement learning with hierarchical REPS in the list\n",
      "Added Gaussian-Bernoulli deep Boltzmann machine in the list\n",
      "Added Hierarchical Relative Entropy Policy Search in the list\n",
      "Added Autonomous Skill Acquisition on a Mobile Manipulator in the list\n",
      "Added Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010) in the list\n",
      "Added Bayesian Multi-Task Reinforcement Learning in the list\n",
      "Added Intrinsically Motivated Hierarchical Skill Learning in Structured Environments in the list\n",
      "Added Transfer Learning for Reinforcement Learning Domains: A Survey in the list\n",
      "Added Representational Power of Restricted Boltzmann Machines and Deep Belief Networks in the list\n",
      "Added Multi-task reinforcement learning: a hierarchical Bayesian approach in the list\n",
      "Added Building Portable Options: Skill Transfer in Reinforcement Learning in the list\n",
      "Added A Fast Learning Algorithm for Deep Belief Nets in the list\n",
      "Added Identifying useful subgoals in reinforcement learning by local graph partitioning in the list\n",
      "Added A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems in the list\n",
      "Added Intrinsically Motivated Reinforcement Learning in the list\n",
      "Added Dynamic abstraction in reinforcement learning via clustering in the list\n",
      "Added Learning Movement Primitives in the list\n",
      "Added Learning Options in Reinforcement Learning in the list\n",
      "Added Training Products of Experts by Minimizing Contrastive Divergence in the list\n",
      "Added Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition in the list\n",
      "Added Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning in the list\n",
      "Added Reinforcement Learning with Hierarchies of Machines in the list\n",
      "Added Connectionist Learning of Belief Networks in the list\n",
      "Added Curious model-building control systems in the list\n",
      "Added Information processing in dynamical systems: foundations of harmony theory in the list\n"
     ]
    }
   ],
   "source": [
    "rwa.updateRefToList('3deecaee4ec1a37de3cb10420eaabff067669e17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keypaper': ['74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "  '45644c7f952d2a5a5b4e594998e2e6dff9088118',\n",
       "  '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8',\n",
       "  'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "  '03eebc19358c4bddb4a987b9e94ecbcb7e58b5d2',\n",
       "  'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "  '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       "  'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "  '1509169e79337e7e1628b266e9475aac779f9c00',\n",
       "  '3b51a29424b619ec5ce29125c4b88d8e24a09328',\n",
       "  '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "  '2a88d01f3079e68ad9b5bcb1ebe56da25679e331',\n",
       "  'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "  '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "  '14444e96f58ff0ce449fd6c61abfffaec1c83f76',\n",
       "  'f8cbd6e934ff43e6227bc14bc77c2934c0b66e23',\n",
       "  '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "  '271081730bfce9117ad4432bffa8e5f18dbac133',\n",
       "  '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "  '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "  '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "  '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "  '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "  '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "  '119639e61c1f88c3d675dac2d3cf47530969276d',\n",
       "  'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "  '23bb22710f7be585305bf01841b74ed167a706ce',\n",
       "  'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "  'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "  'd669358916608af804c20329b7287d02c75b1311',\n",
       "  '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "  'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "  'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "  '6ec8797952213227eea2e63620f4d7c060d598d5',\n",
       "  'eadbe2e4f9de47dd357589cf59e3d1f0199e5075',\n",
       "  'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "  '872edada2165ad65c1664b813efdb92e3bec1b36',\n",
       "  '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0',\n",
       "  'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "  'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "  'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "  '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "  '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "  '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "  '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "  '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "  '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "  'a13a62d16ebeaff7c5125a2b60cfc30a35cd55af',\n",
       "  'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "  'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "  '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "  '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "  'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "  '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "  '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "  '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "  'e2bd18c1039f27675bd64014117db648d969452e'],\n",
       " '74dd51db773ea883d9804d1845345a46ab908ccd': {'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning',\n",
       "  'year': 2023,\n",
       "  'references': [{'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': '8a913111f23fbded7f2e9d2d6c9c4278e7c682c9',\n",
       "    'title': 'APS: Active Pretraining with Successor Features'},\n",
       "   {'paperId': '107e4ea37d2e5364893107a8ce072972c4a10dfb',\n",
       "    'title': 'Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '982a3218ec07cae66ffd2151e025305bc08f7764',\n",
       "    'title': 'Efficient Batch-Mode Reinforcement Learning Using Extreme Learning Machines'},\n",
       "   {'paperId': '8343b6f3c8424ac1a8069d31b7a0de1e8f3c40b8',\n",
       "    'title': 'Discovery of Options via Meta-Learned Subgoals'},\n",
       "   {'paperId': '876d5bbc6b145264572696a71e85bca731de903e',\n",
       "    'title': 'Looking Back on the Actor–Critic Architecture'},\n",
       "   {'paperId': 'c10a513140aa640385b3c60ad293b8d8554fc980',\n",
       "    'title': 'Proximal Parameter Distribution Optimization'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "    'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '0556f74fd9acfd35c858e362ce4777a71ceb00d8',\n",
       "    'title': 'Two-Level Master–Slave RFID Networks Planning via Hybrid Multiobjective Artificial Bee Colony Optimizer'},\n",
       "   {'paperId': '4625628163a2ee0e6cd320cd7a14b4ccded2a631',\n",
       "    'title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '68c108795deef06fa929d1f6e96b75dbf7ce8531',\n",
       "    'title': 'Meta-Reinforcement Learning of Structured Exploration Strategies'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '5e2c4e7b3302549b3718601c44d9af6c7554efef',\n",
       "    'title': 'Learning Robust Rewards with Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "    'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': '0c908739fbff75f03469d13d4a1a07de3414ee19',\n",
       "    'title': 'Distilling the Knowledge in a Neural Network'},\n",
       "   {'paperId': 'df2da95303bedf417c76fa8439844d671fb056da',\n",
       "    'title': 'Multiobjective Reinforcement Learning: A Comprehensive Overview'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': 'd1e4365de165463e51134f10bf3939f2b00a6667',\n",
       "    'title': 'Deep learning with Elastic Averaging SGD'},\n",
       "   {'paperId': '54e325aee6b2d476bbbb88615ac15e251c6e8214',\n",
       "    'title': 'Generative Adversarial Nets'},\n",
       "   {'paperId': '9b1de5d93854d9dc364a4bc6a462193ccc3ea895',\n",
       "    'title': 'Simulated Car Racing Championship: Competition Software Manual'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '11b6bdfe36c48b11367b27187da11d95892f0361',\n",
       "    'title': 'Maximum Entropy Inverse Reinforcement Learning'},\n",
       "   {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "    'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': '396b2edb73c02e8536c7ea5b916b0422a2f1fc0d',\n",
       "    'title': 'A Metaverse-Based Teaching Building Evacuation Training System With Deep Reinforcement Learning'},\n",
       "   {'paperId': '959465859a5812aa5cb54e85de018825cab33f44',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning With Experience Sharing for Metaverse in Education'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 8,\n",
       "  'isKeypaper': True},\n",
       " '45644c7f952d2a5a5b4e594998e2e6dff9088118': {'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '7a9846fbb9a580f522ff93f201a6bf15f80d112b',\n",
       "    'title': 'CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents'},\n",
       "   {'paperId': '8e128a1b2efb0ddf688902ade4405d22d5b61eec',\n",
       "    'title': 'Benchmarking the Spectrum of Agent Capabilities'},\n",
       "   {'paperId': '43ea4f5d999d35d4fc6c544eedbc100d8c3a5e00',\n",
       "    'title': 'MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research'},\n",
       "   {'paperId': None,\n",
       "    'title': 'The minerl 2020 competition on sample efficient reinforcement learning using human priors, 2021'},\n",
       "   {'paperId': '3228f6fc7902b23c26378e59f8c8820412de7f42',\n",
       "    'title': 'The NetHack Learning Environment'},\n",
       "   {'paperId': '8d814620a1ca77e745bc8a33b96b86148f2804fe',\n",
       "    'title': 'Leveraging Procedural Generation to Benchmark Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Effective diversity in populationbased reinforcement learning'},\n",
       "   {'paperId': 'b81d6c9138a51e7e109ec59d04f22f7186b3b7dc',\n",
       "    'title': 'TorchBeast: A PyTorch Platform for Distributed RL'},\n",
       "   {'paperId': 'ff84c46d4653782218549bd99631130df2d2859e',\n",
       "    'title': 'Distilling Policy Distillation'},\n",
       "   {'paperId': '4cb3fd057949624aa4f0bbe7a6dcc8777ff04758',\n",
       "    'title': 'Exploration by Random Network Distillation'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ebf3b0c284cb776d89951e4e67a59df6403fc9a6',\n",
       "    'title': 'Kickstarting Deep Reinforcement Learning'},\n",
       "   {'paperId': '80196cdfcd0c6ce2953bf65a7f019971e2026386',\n",
       "    'title': 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures'},\n",
       "   {'paperId': 'af10f3c1c0859aa620623f760c8a29e78f177f7f',\n",
       "    'title': 'Population Based Training of Neural Networks'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Meta learning shared hierarchies. CoRR, abs/1710.09767'},\n",
       "   {'paperId': 'a473f545318325ba23b7a6b477485d29777ba873',\n",
       "    'title': 'ViZDoom: A Doom-based AI research platform for visual reinforcement learning'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '1def5d3711ebd1d86787b1ed57c91832c5ddc90b',\n",
       "    'title': 'Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning'},\n",
       "   {'paperId': '1c4927af526d5c28f7c2cfa492ece192d80a61d4',\n",
       "    'title': 'Policy Distillation'},\n",
       "   {'paperId': '0c908739fbff75f03469d13d4a1a07de3414ee19',\n",
       "    'title': 'Distilling the Knowledge in a Neural Network'},\n",
       "   {'paperId': 'f82e4ff4f003581330338aaae71f60316e58dd26',\n",
       "    'title': 'The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)'},\n",
       "   {'paperId': 'fb144a1d31aec3b2bece6a59bd11a876a9fafb34',\n",
       "    'title': 'Exploiting Open-Endedness to Solve Problems Through the Search for Novelty'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'ac4af1df88e178386d782705acc159eaa0c3904a',\n",
       "    'title': 'Actor-Critic Algorithms'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': '40bfcb5901a3c9cd9581433fd2eb1906abcfae6d',\n",
       "    'title': 'Hierarchical Learning with Unsupervised Skill Discovery for Highway Merging Applications'},\n",
       "   {'paperId': 'a6c6e464b2ab15bb04176c26da3816d4038d5176',\n",
       "    'title': 'Dungeons and Data: A Large-Scale NetHack Dataset'},\n",
       "   {'paperId': '15ddeb9b812e4063a8b907d50c720e01c753b2b4',\n",
       "    'title': 'Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 3,\n",
       "  'isKeypaper': True},\n",
       " '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8': {'title': 'Hierarchical Adversarial Inverse Reinforcement Learning',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': 'acff9c4e2ad66dd785f75cf91dd0aa442c6cae14',\n",
       "    'title': 'Adversarial Option-Aware Hierarchical Imitation Learning'},\n",
       "   {'paperId': '3ff0d5dde1594dd2a2ff8ce750ebe9ab6600a980',\n",
       "    'title': 'Decision Making for Autonomous Driving via Augmented Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '3047992223549cc394e201a326eaa3337dc4b4b2',\n",
       "    'title': 'Provable Hierarchical Imitation Learning via EM'},\n",
       "   {'paperId': None,\n",
       "    'title': 'The skill-action architecture: Learning abstract action embeddings for reinforcement learning'},\n",
       "   {'paperId': '52c77192011106c2f2e3911f0e8ed7bb9f5ac7a2',\n",
       "    'title': 'Augmenting GAIL with BC for sample efficient imitation learning'},\n",
       "   {'paperId': '2ee463bba9d4db6aec0eab17e54431a6dc80bf17',\n",
       "    'title': 'Superhuman AI for multiplayer poker'},\n",
       "   {'paperId': '46c53faeaf2f52215adb165559c5ce056a71146b',\n",
       "    'title': 'Stacked Capsule Autoencoders'},\n",
       "   {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "    'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '54cd5a5ddd286442fa94da7ec344a7e76b9a6ccd',\n",
       "    'title': 'Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control'},\n",
       "   {'paperId': 'fb9693183bc74568c72188431c18cb2b07c87213',\n",
       "    'title': 'Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '5e2c4e7b3302549b3718601c44d9af6c7554efef',\n",
       "    'title': 'Learning Robust Rewards with Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "    'title': 'Attention is All you Need'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': '6fab0b3b321988cedd0a017c1dad997f6d4da930',\n",
       "    'title': 'Playing Atari Games with Deep Reinforcement Learning and Human Checkpoint Replay'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': '0c3b69b5247ef18fd5bab1109d87a04184ea8f4b',\n",
       "    'title': 'A Recurrent Latent Variable Model for Sequential Data'},\n",
       "   {'paperId': '61dcbbc4bf4a7fb5973cd37e6e8b863ed2bb24c9',\n",
       "    'title': 'Three tutorial lectures on entropy and counting'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '215838674eb124c4482ba89dbda94293ae3bfae6',\n",
       "    'title': 'Expectation Maximization Algorithm'},\n",
       "   {'paperId': 'fbc6562814e08e416e28a268ce7beeaa3d0708c8',\n",
       "    'title': 'Large-Scale Machine Learning with Stochastic Gradient Descent'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': '11b6bdfe36c48b11367b27187da11d95892f0361',\n",
       "    'title': 'Maximum Entropy Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'f836987f4b7892698b95593b11dfe185854307c1',\n",
       "    'title': 'An introduction to hidden Markov models.'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': 'fa0f137fb007d169d368873484633ef7c45de003',\n",
       "    'title': 'The Hierarchical Hidden Markov Model: Analysis and Applications'},\n",
       "   {'paperId': 'b05b67aca720d0bc39bc9afad02a19f522c7a1bc',\n",
       "    'title': 'Pharmacokinetics of a novel formulation of ivermectin after administration to goats'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '8d652a1980e743c7c85ff6066409ea1e3be4d685',\n",
       "    'title': 'Efficient Training of Artificial Neural Networks for Autonomous Navigation'},\n",
       "   {'paperId': '7dbdb4209626fd92d2436a058663206216036e68',\n",
       "    'title': 'Elements of Information Theory'},\n",
       "   {'paperId': '557668619327081a6b77aa5b181fa84722a875a4',\n",
       "    'title': 'CAUSALITY, FEEDBACK AND DIRECTED INFORMATION'},\n",
       "   {'paperId': '6a53541fb412356f0965d1aad34a4938353cf0b6',\n",
       "    'title': 'Sur les fonctions convexes et les inégalités entre les valeurs moyennes'}],\n",
       "  'citations': [{'paperId': '85bc55ba9ab93c09713b0891bbcf0541b8f27ea9',\n",
       "    'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c': {'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '9dae8d598f9dd9d8c96f771699f5e37a27e71df3',\n",
       "    'title': 'MetaDrive: Composing Diverse Driving Scenarios for Generalizable Reinforcement Learning'},\n",
       "   {'paperId': '2ce42614bbffe69dfbcda4d4b13ffaac87213412',\n",
       "    'title': 'Transferable and Adaptable Driving Behavior Prediction'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': 'b69a7afd74c6a70fa3a1bf2934143afcd1f54dc3',\n",
       "    'title': 'Hierarchical Adaptable and Transferable Networks (HATN) for Driving Behavior Prediction'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': 'c3bba4ae7442747a980fdd370b1d31bf4da5600d',\n",
       "    'title': 'Learning Interaction-aware Guidance Policies for Motion Planning in Dense Traffic Scenarios'},\n",
       "   {'paperId': 'a66df6a55c25e8f131502c922000b9197debe14a',\n",
       "    'title': 'Shaping Rewards for Reinforcement Learning with Imperfect Demonstrations using Generative Models'},\n",
       "   {'paperId': '538f97d36125b704f91894839107c8f29c129a9d',\n",
       "    'title': 'Socially-Compatible Behavior Design of Autonomous Vehicles With Verification on Real Human Data'},\n",
       "   {'paperId': '076d1585bc4603c47a0735b599164604ef1b0199',\n",
       "    'title': 'Learning compositional models of robot skills for task and motion planning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '57a121e51b4cd1cd2a81506ce32196217b439a46',\n",
       "    'title': 'Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving'},\n",
       "   {'paperId': '7583ce60cc38983e3f568591d7590ecf34f82f84',\n",
       "    'title': 'Navigation Command Matching for Vision-based Autonomous Driving'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'be97603d18c9ea29a42877de05465caedf7e3e49',\n",
       "    'title': 'Driving in Dense Traffic with Model-Free Reinforcement Learning'},\n",
       "   {'paperId': '4f0c4189d9a82f94dcd84fe879cfbe124aaf270b',\n",
       "    'title': 'Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning'},\n",
       "   {'paperId': '14e956857571057afa71d1e86720877060af7aae',\n",
       "    'title': 'Adaptive sampling-based motion planning with a non-conservatively defensive strategy for autonomous driving'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "    'title': 'Composing Complex Skills by Learning Transition Policies'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '7cd316505f52aa337ef8a2aff10bc6bf1df561d0', 'title': 'and s'},\n",
       "   {'paperId': '3e0e337b477e0c226da00eae03fd29882275a469',\n",
       "    'title': '“A and B”:'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': '032db195efd97fe2bcd20c4ad04628c70ff4e79c',\n",
       "    'title': 'and a at'},\n",
       "   {'paperId': '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba',\n",
       "    'title': 'Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review'},\n",
       "   {'paperId': 'd356a5603f14c7a6873272774782d7812871f952',\n",
       "    'title': 'Reinforcement and Imitation Learning for Diverse Visuomotor Skills'},\n",
       "   {'paperId': '79e8af07d7be3e41ebdc2d1a219d5c867693bf4c',\n",
       "    'title': 'How Would Surround Vehicles Move? A Unified Framework for Maneuver Classification and Motion Prediction'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'ead1abc362819539d454f2d85216e58d540f5124',\n",
       "    'title': 'Improved Trajectory Planning for On-Road Self-Driving Vehicles Via Combined Graph Search, Optimization & Topology Analysis'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '30b0baee14e9b0e9f9e6afa972912eb4a3e593ba',\n",
       "    'title': 'A Computationally Efficient Motion Primitive for Quadrocopter Trajectory Generation'},\n",
       "   {'paperId': '3d2218b17e7898a222e5fc2079a3f1531990708f', 'title': 'I and J'},\n",
       "   {'paperId': '2a65434d43ffa6554eaf14b728780919ad4f33eb',\n",
       "    'title': 'Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy'},\n",
       "   {'paperId': 'eb5b459c8a3e56064158fb3514eeab763486e437',\n",
       "    'title': 'Reinforcement learning of motor skills with policy gradients'},\n",
       "   {'paperId': '0515b1803656762dcad0ad66097578e810d7d5dd',\n",
       "    'title': 'Optimal Rough Terrain Trajectory Generation for Wheeled Mobile Robots'},\n",
       "   {'paperId': 'a364338bf39c405f78035ebef19a919442fd9c57',\n",
       "    'title': 'The Construction of Movement with Behavior-Specific and Behavior-Independent Modules'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': 'and P'}],\n",
       "  'citations': [],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 8,\n",
       "  'isKeypaper': True},\n",
       " '03eebc19358c4bddb4a987b9e94ecbcb7e58b5d2': {'title': 'Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '5da5c2167a85ecb5d1ea22656ae36fdf995df0f2',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Complex motion decomposition: combining offline motion libraries with online MPC'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Complex motion decomposition: combining offline motion libraries with online MPC,\" under review for'},\n",
       "   {'paperId': 'ca6096142016a2ba8133f6cb2c04ad30f5eae730',\n",
       "    'title': 'Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning'},\n",
       "   {'paperId': '49142e3e381c0dc7fee0049ea41d2ef02c0340d7',\n",
       "    'title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'be864a16ec597c76d1ab36453d01471723a37bac',\n",
       "    'title': 'Understanding Catastrophic Forgetting and Remembering in Continual Learning with Optimal Relevance Mapping'},\n",
       "   {'paperId': '267b1b3017378169a21f93f8e77907cfed57681c',\n",
       "    'title': 'Whole-Body MPC and Online Gait Sequence Generation for Wheeled-Legged Robots'},\n",
       "   {'paperId': '9718971bd0e36b32145623d4fdd566d1b3291108',\n",
       "    'title': 'Pseudo-Rehearsal: Achieving Deep Reinforcement Learning without Catastrophic Forgetting'},\n",
       "   {'paperId': 'eadbe2e4f9de47dd357589cf59e3d1f0199e5075',\n",
       "    'title': 'Learning quadrupedal locomotion over challenging terrain'},\n",
       "   {'paperId': '9c1746732a01810484b2c7b79f934c302837293d', 'title': 'AMP'},\n",
       "   {'paperId': 'bb0ee42d406f2361fee89cf1274073185a0e9eec',\n",
       "    'title': 'Learning agile and dynamic motor skills for legged robots'},\n",
       "   {'paperId': '482376177d6ed10aa2975f9858a91e49ec121b00',\n",
       "    'title': 'Physics-based motion capture imitation with deep reinforcement learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '5151d6cb3a4eaec14a56944d58338251fca344ab',\n",
       "    'title': 'Overcoming catastrophic forgetting in neural networks'},\n",
       "   {'paperId': '571b0750085ae3d939525e62af510ee2cee9d5ea',\n",
       "    'title': 'Improved Techniques for Training GANs'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': '42a990e548eab8985b053dc076240f6f581d434f',\n",
       "    'title': 'Generalizing locomotion style to new animals with inverse optimal regression'},\n",
       "   {'paperId': '0722077581460d8f060a96956a79c782d4fca57c',\n",
       "    'title': 'Continuous character control with low-dimensional embeddings'},\n",
       "   {'paperId': '8ce229884f3e823a7d7e873b199fc2e21c72cc0a',\n",
       "    'title': 'Synthesis of Responsive Motion Using a Dynamic Model'},\n",
       "   {'paperId': 'f65020fc3b1692d7989e099d6b6e698be5a50a93',\n",
       "    'title': 'Apprenticeship learning via inverse reinforcement learning'},\n",
       "   {'paperId': '8639de4f0a209a740ef1c5949bf4d854275cd743',\n",
       "    'title': 'Motion capture-driven simulations that hit and react'},\n",
       "   {'paperId': '16f5015bf514c95be1ec87b9ee098dd227cfc0ae',\n",
       "    'title': 'Animation of dynamic legged locomotion'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Complex motion decomposition : combining offline motion libraries with online MPC Animation of dynamic legged locomotion'}],\n",
       "  'citations': [{'paperId': 'f3c0e8a4693a69836e93e3ad313863886fb53a6c',\n",
       "    'title': 'LeggedWalking on Inclined Surfaces'},\n",
       "   {'paperId': '05e3dd7135500150a38a362bece1d42c5b4d5740',\n",
       "    'title': 'Learning and Adapting Agile Locomotion Skills by Transferring Experience'},\n",
       "   {'paperId': '0caf19c0cf585146880d241984fac3833b60d228',\n",
       "    'title': 'Learning to generate pointing gestures in situated embodied conversational agents'},\n",
       "   {'paperId': 'e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e',\n",
       "    'title': 'A Survey of Demonstration Learning'},\n",
       "   {'paperId': '8d32b150ce19200a2d2d71f68b63972080ef99ad',\n",
       "    'title': 'Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach'},\n",
       "   {'paperId': '9fda8dbbc030dbf9dae798b051505756be6ffd3a',\n",
       "    'title': 'Robust and Versatile Bipedal Jumping Control through Reinforcement Learning'},\n",
       "   {'paperId': '228382ac9f728874a17c1182d1e660dcb00403ac',\n",
       "    'title': 'Robust and Versatile Bipedal Jumping Control through Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': '039cac821166586d4e9bebf1070941f3939e0d71',\n",
       "    'title': 'Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior'},\n",
       "   {'paperId': '2b5137052cea0b49f83c48bfb6f2176ad5499144',\n",
       "    'title': 'A Bipedal Wheel-Legged Robot with High-frequency Force Control by Qausi-Direct Drive: Design and Experiments'},\n",
       "   {'paperId': 'e367fba2a2227dccedba0d2874ba30112c216ee4',\n",
       "    'title': 'OPT-Mimic: Imitation of Optimized Trajectories for Dynamic Quadruped Behaviors'},\n",
       "   {'paperId': '44b6fc338220fcd83448f7093c586c18be299236',\n",
       "    'title': 'Saving the Limping: Fault-tolerant Quadruped Locomotion via Reinforcement Learning'},\n",
       "   {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "    'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       "   {'paperId': '5e2b96cd085a32d64081045c1a7a6923461ca9b8',\n",
       "    'title': 'Upright and Crawling Locomotion and Its Transition for a Wheel-Legged Robot'},\n",
       "   {'paperId': '06f73c93d7902355752abbc1de330436caa9e35f',\n",
       "    'title': 'Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations'},\n",
       "   {'paperId': 'bafbb3c535d9ee0fbffaad266f732a3892f53b4e',\n",
       "    'title': 'Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " 'b27fd9ea29cabe6afedd01e446b96c34e956ce84': {'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269',\n",
       "    'title': 'Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback'},\n",
       "   {'paperId': 'f71da178cd63958fe659ad613d474b67c5615bd3',\n",
       "    'title': 'Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL'},\n",
       "   {'paperId': 'e7c33544f157974083e9b106605f417722777352',\n",
       "    'title': 'Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning'},\n",
       "   {'paperId': '335f33b9fbbfd0a7da6eb36af4942829d1104ffb',\n",
       "    'title': 'Toward Robust Long Range Policy Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '3cb8e96faba73efa027fa858e2a78cd1fc3c6e4d',\n",
       "    'title': 'Model-Based Offline Planning'},\n",
       "   {'paperId': '79ebde314ab90d066cee3b82193ef05666323394',\n",
       "    'title': 'Deployment-Efficient Reinforcement Learning via Model-Based Offline Optimization'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'cf34efc663284da131e747407ac3f389f898e471',\n",
       "    'title': 'robosuite: A Modular Simulation Framework and Benchmark for Robot Learning'},\n",
       "   {'paperId': '7acbdb961f67d50fef359066f2a1d7755cf16ee2',\n",
       "    'title': 'Critic Regularized Regression'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '28db20a81eec74a50204686c3cf796c42a020d2e',\n",
       "    'title': 'Conservative Q-Learning for Offline Reinforcement Learning'},\n",
       "   {'paperId': '7490d73dc5ba02204407bb6ef630d2e8ec47bb4f',\n",
       "    'title': 'MOPO: Model-based Offline Policy Optimization'},\n",
       "   {'paperId': '309c2c5ee60e725244da09180f913cd8d4b8d4e9',\n",
       "    'title': 'MOReL : Model-Based Offline Reinforcement Learning'},\n",
       "   {'paperId': '5e7bc93622416f14e6948a500278bfbe58cd3890',\n",
       "    'title': 'Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': 'cc4435c2c1ea079721f48d08d0ae3436599d1533',\n",
       "    'title': 'Striving for Simplicity in Off-policy Deep Reinforcement Learning'},\n",
       "   {'paperId': '57daffd65a5d73a439903f3e50950c21c9eba687',\n",
       "    'title': 'Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '3b70d0cb2aea1e2bf8afb178daae64fa59fd8639',\n",
       "    'title': 'Composing Value Functions in Reinforcement Learning'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '5e8c7276897c1c744dfabffb9197e6689ac5e68e',\n",
       "    'title': 'Off-Policy Policy Gradient with State Distribution Correction'},\n",
       "   {'paperId': '894536f2ac4728850bc18705daeeda6e88f3d6f1',\n",
       "    'title': 'Universal Successor Features Approximators'},\n",
       "   {'paperId': '5285cb8faada5de8a92a47622950f6cfd476ac1d',\n",
       "    'title': 'Off-Policy Deep Reinforcement Learning without Exploration'},\n",
       "   {'paperId': 'd4b34b12d3515680837c024bfa53fd370628aa51',\n",
       "    'title': 'Composing Entropic Policies using Divergence Correction'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': 'f650f1fd44ab0778d30577f8c2077b2ff58830da',\n",
       "    'title': 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'dee85f0ed3571d7b591e23000848c584242186ef',\n",
       "    'title': 'Composable Deep Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '3167b590e47b08828555938d3126fde1bb3c038e',\n",
       "    'title': 'Stein Variational Policy Gradient'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'd8686b657b61a37da351af2952aabd8b281de408',\n",
       "    'title': 'Successor Features for Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '768f7353718c6d95f2d63f954f2236369a409135',\n",
       "    'title': 'Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm'},\n",
       "   {'paperId': 'b23c97e59f44bc88121c65d1ac41f2cbddcefbd2',\n",
       "    'title': 'Deep Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': '0f09cfccdf269c578b45936664552b3a89996281',\n",
       "    'title': 'Proceedings of the Institution of Mechanical Engineers , Part I : Journal of Systems and Control Engineering'},\n",
       "   {'paperId': '9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f',\n",
       "    'title': 'Probabilistic policy reuse in a reinforcement learning agent'},\n",
       "   {'paperId': '149a748f024abbe0024cb3631ddd7e813b453be0',\n",
       "    'title': 'STP: Skills, tactics, and plays for multi-robot control in adversarial environments'},\n",
       "   {'paperId': 'c8d90974c3f3b40fa05e322df2905fc16204aa56',\n",
       "    'title': 'Adaptive Mixtures of Local Experts'}],\n",
       "  'citations': [],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 10,\n",
       "  'isKeypaper': True},\n",
       " '0af6a63167df299a1556a560d6884ae38eda390d': {'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '96c9b1f1da5368f5f900e2091633b1139d92ecc8',\n",
       "    'title': 'Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning'},\n",
       "   {'paperId': '0e6799d4bdd33cd992147c4ac7b82a760f74edcc',\n",
       "    'title': 'RoLoMa: Robust Loco-Manipulation for Quadruped Robots with Arms'},\n",
       "   {'paperId': '5da5c2167a85ecb5d1ea22656ae36fdf995df0f2',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': 'c132f6e6e472497514646e8aa2d84a70f4501c9d',\n",
       "    'title': 'Coupling Vision and Proprioception for Navigation of Legged Robots'},\n",
       "   {'paperId': '35efc3a4c5f64d96ded6daea692f3935c96f0415',\n",
       "    'title': 'Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World'},\n",
       "   {'paperId': '53b27d420ed21f0e22ca1e8efba46f5ba2dbf707',\n",
       "    'title': 'Graph-based Cluttered Scene Generation and Interactive Exploration using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'c263702a419d7119ca9b9804e0b44e59b03511d2',\n",
       "    'title': 'Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots'},\n",
       "   {'paperId': 'ca6096142016a2ba8133f6cb2c04ad30f5eae730',\n",
       "    'title': 'Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning'},\n",
       "   {'paperId': '49142e3e381c0dc7fee0049ea41d2ef02c0340d7',\n",
       "    'title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning'},\n",
       "   {'paperId': '1ca5ff6555d9fc634d3858d1fda9b3de2a91b13a',\n",
       "    'title': 'RMA: Rapid Motor Adaptation for Legged Robots'},\n",
       "   {'paperId': '597ae6a9020637439a0501d6985a469885a05804',\n",
       "    'title': 'Go Fetch! - Dynamic Grasps using Boston Dynamics Spot with External Robotic Arm'},\n",
       "   {'paperId': 'b7386dbdf7dcf344124408693c0e2075143dd5dd',\n",
       "    'title': 'Pushing it out of the Way: Interactive Visual Navigation'},\n",
       "   {'paperId': 'c35f3f4f344c4f056e89cf0ab84ba96595f62986',\n",
       "    'title': 'Mechanical Search on Shelves using Lateral Access X-RAY'},\n",
       "   {'paperId': '13f6165633f67604ecaa4662448a0cc62d1674a5',\n",
       "    'title': 'Learning Human Search Behavior from Egocentric Visual Inputs'},\n",
       "   {'paperId': 'a22cbd162066b01639aefd3deceeadd0170a6afe',\n",
       "    'title': 'Estimating Mass Distribution of Articulated Objects using Non-prehensile Manipulation'},\n",
       "   {'paperId': '872edada2165ad65c1664b813efdb92e3bec1b36',\n",
       "    'title': 'Multi-expert learning of adaptive legged locomotion'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '62516303058a1322450b58e4cd778ab873b5e531',\n",
       "    'title': 'Object Goal Navigation using Goal-Oriented Semantic Exploration'},\n",
       "   {'paperId': '19c8c5a32cb0d77cd22ee1201e31306f0cd6100a',\n",
       "    'title': 'ObjectNav Revisited: On Evaluation of Embodied Agents Navigating to Objects'},\n",
       "   {'paperId': '49e3d03e0f43a05367f0c99e23f09e28c834f20c',\n",
       "    'title': 'X-Ray: Mechanical Search for an Occluded Object by Minimizing Support of Learned Occupancy Distributions'},\n",
       "   {'paperId': '003987bfff295e76946bf430376af4fe3d466cb4',\n",
       "    'title': 'Learning to Walk in the Real World with Minimal Human Effort'},\n",
       "   {'paperId': '6b5f4b0052859259e21467e1758d0b26e6dee3af',\n",
       "    'title': 'DD-PPO: Learning Near-Perfect PointGoal Navigators from 2.5 Billion Frames'},\n",
       "   {'paperId': '2b10c90d2aef5ad62e746d809065251768108541',\n",
       "    'title': 'Interactive Gibson Benchmark: A Benchmark for Interactive Navigation in Cluttered Environments'},\n",
       "   {'paperId': '05d57ef604913b8a1b12537fafd53799d65b10db',\n",
       "    'title': 'Learning Fast Adaptation With Meta Strategy Optimization'},\n",
       "   {'paperId': '817fc6b231599a257302cd853ce4fa9572c8e849',\n",
       "    'title': 'Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': '90e8e65532a647074fa705e8c84d70fa0e1af266',\n",
       "    'title': 'Highly Dynamic Quadruped Locomotion via Whole-Body Impulse Control and Model Predictive Control'},\n",
       "   {'paperId': 'dbdb47913244562ff6cd2a70956997cbdeffd6cb',\n",
       "    'title': 'DensePhysNet: Learning Dense Physical Object Representations via Multi-step Dynamic Interactions'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '0d6c52ac0424321e08cee82dd2ccb1fe0e826c01',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': 'bb0ee42d406f2361fee89cf1274073185a0e9eec',\n",
       "    'title': 'Learning agile and dynamic motor skills for legged robots'},\n",
       "   {'paperId': 'ae4d32f05cf40e4cc01c69d7787149a258c95eda',\n",
       "    'title': 'Residual Reinforcement Learning for Robot Control'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': '91a19620e40c3bad71b5c52eff40d86eaed32d29',\n",
       "    'title': 'MIT Cheetah 3: Design and Control of a Robust, Dynamic Quadruped Robot'},\n",
       "   {'paperId': '608d53fcd69173d30914e29d9b8ca4b37efe9ac4',\n",
       "    'title': 'Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control'},\n",
       "   {'paperId': '6654ba1d3e61cdf5f4decc7464436046cf602ed1',\n",
       "    'title': 'On Evaluation of Embodied Navigation Agents'},\n",
       "   {'paperId': '35648a156667ffeb02d7642d5153766faa7f1eeb',\n",
       "    'title': 'Fast Online Trajectory Optimization for the Bipedal Robot Cassie'},\n",
       "   {'paperId': '4d3b69bdcd1d325d29badc6a38f2d6cc504fe7d1',\n",
       "    'title': 'Sim-to-Real: Learning Agile Locomotion For Quadruped Robots'},\n",
       "   {'paperId': '05893041d24dd404963960e73220aca83d19add4',\n",
       "    'title': 'Deep Reinforcement Learning: A Brief Survey'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '9917363277c783a01bff32af1c27fc9b373ad55d',\n",
       "    'title': 'DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'bfba319e021ec261ad7cad164749cc5eb85951f5',\n",
       "    'title': 'ANYmal - a highly mobile and dynamic quadrupedal robot'},\n",
       "   {'paperId': '2ffc1cbe7488ba3d054b482bac5edf9d272cf99c',\n",
       "    'title': 'Autonomous Skill Acquisition on a Mobile Manipulator'},\n",
       "   {'paperId': 'ea2ef0d66632b9469eaf6a7ea531b79fa7542f23',\n",
       "    'title': 'A whole-body control framework for humanoids operating in human environments'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '12ca15a0e03ecd2f6494e415ea7c94a74f8bf53b',\n",
       "    'title': 'Trotting, pacing and bounding by a quadruped robot.'}],\n",
       "  'citations': [{'paperId': '055a1f9c5254235fd764db875db783532bd5706b',\n",
       "    'title': 'Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot using Scalable Motion Imitation'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 4,\n",
       "  'isKeypaper': True},\n",
       " 'e6548d97d82aa2710019951eb4eac034e1747aa1': {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '580d40a8847897886193c31e2eec0947978e42fb',\n",
       "    'title': 'IL-flOw: Imitation Learning from Observation using Normalizing Flows'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'c35831b0adef60e4497815fae077ff31137c73e1',\n",
       "    'title': 'Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation'},\n",
       "   {'paperId': '00438218d81c2d50fc96592e16c07ae720440bb6',\n",
       "    'title': 'Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration'},\n",
       "   {'paperId': '5e39c030eb7e3ad516cbe1bdcd7b8c4a9e51a6a9',\n",
       "    'title': 'The Surprising Effectiveness of Representation Learning for Visual Imitation'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "    'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models'},\n",
       "   {'paperId': 'a8f97a65f1cb6106b83d3c21a1dad5fa005aee43',\n",
       "    'title': 'RotoGrad: Gradient Homogenization in Multitask Learning'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '5bd9095fb3654bd2875232f73bb1733fd45eebc6',\n",
       "    'title': 'Learning a Universal Template for Few-shot Dataset Generalization'},\n",
       "   {'paperId': '5d0143965645f2dfdc4b217560071f355957f2f4',\n",
       "    'title': 'Action Priors for Large Action Spaces in Robotics'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '556410853b41fcb228e6138bbaa7723f55503645',\n",
       "    'title': 'Learning Stable Normalizing-Flow Control for Robotic Manipulation'},\n",
       "   {'paperId': 'beffe798cff58416e06372904de204dcf9fb9157',\n",
       "    'title': 'Scalable Transfer Learning with Expert Models'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': '6e1efe22d5696269aff7addcb438f77ff6cc2508',\n",
       "    'title': 'A Universal Representation Transformer Layer for Few-Shot Image Classification'},\n",
       "   {'paperId': '31b38a19d87711489786ad54a5a00d5f0b2ead43',\n",
       "    'title': 'Normalizing Flows: An Introduction and Review of Current Methods'},\n",
       "   {'paperId': 'e3fc5b5627af62ee6981a02090cf6bae368202d7',\n",
       "    'title': 'Stable-Baselines3: Reliable Reinforcement Learning Implementations'},\n",
       "   {'paperId': 'fab87cc094c0aedf2a283371de8339f466fdf3f8',\n",
       "    'title': 'Learning to Weight Imperfect Demonstrations'},\n",
       "   {'paperId': 'f204041dd567025217adc8070ca292e89cc80488',\n",
       "    'title': 'COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'eae5a9efa1da2f3f053684869564bf4c1aa41096',\n",
       "    'title': 'Variational MIxture of Normalizing Flows'},\n",
       "   {'paperId': '9566da1b6af07462bc0ba54f24e47ba8ea82adcf',\n",
       "    'title': 'Not All Unlabeled Data are Equal: Learning to Weight Data in Semi-supervised Learning'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '5c378ca2e4699eaf763de9f8ec02ca89860bb1cf',\n",
       "    'title': 'Neural Topological SLAM for Visual Navigation'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': 'b1f73ed3e27b9d219c3b469bb579e70cdfd591f9',\n",
       "    'title': 'Selecting Relevant Features from a Multi-domain Representation for Few-Shot Classification'},\n",
       "   {'paperId': 'e13ca1bd60af3325afc64dc09979e3322818e365',\n",
       "    'title': 'Meta-Learning with Warped Gradient Descent'},\n",
       "   {'paperId': '712f4f21b9d3e6a7f110a2ecd9b3a2f900397b9f',\n",
       "    'title': 'Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples'},\n",
       "   {'paperId': 'f8e06d60a883b6b0e1f686c224ca1336712d4933',\n",
       "    'title': 'Normalizing Flow Policies for Multi-agent Systems'},\n",
       "   {'paperId': 'c0dabdc9036909e10c05628b395784078c0c8f6b',\n",
       "    'title': 'A Divergence Minimization Perspective on Imitation Learning Methods'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '3aade591195b41898f4b67b9dea280582ae10229',\n",
       "    'title': 'Improving Exploration in Soft-Actor-Critic with Normalizing Flows Policies'},\n",
       "   {'paperId': 'c3bd5dd10f555bc1622cd37598bdbdf5635c94fe',\n",
       "    'title': 'Leveraging exploration in off-policy algorithms via normalizing flows'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '16481307a862ddae514045b13b944ca043c36a4b',\n",
       "    'title': 'Generative Adversarial User Model for Reinforcement Learning Based Recommendation System'},\n",
       "   {'paperId': '2f240424ca0761d0549252dacfbbeece14bb3cb6',\n",
       "    'title': 'Fast Context Adaptation via Meta-Learning'},\n",
       "   {'paperId': '6879ecec797cd0f1319b54a963f91abb5f7325de',\n",
       "    'title': 'Randomized Value Functions via Multiplicative Normalizing Flows'},\n",
       "   {'paperId': 'fb363b670b6a48020e3504714c444291785b3fbc',\n",
       "    'title': 'Boosting Trust Region Policy Optimization by Normalizing Flows Policy'},\n",
       "   {'paperId': 'b227f3e4c0dc96e5ac5426b85485a70f2175a205',\n",
       "    'title': 'Representation Learning with Contrastive Predictive Coding'},\n",
       "   {'paperId': '21b786b3f870fc7fa247c143aa41de88b1fc6141',\n",
       "    'title': 'Glow: Generative Flow with Invertible 1x1 Convolutions'},\n",
       "   {'paperId': 'f07d6814c33cd3384354a29d1d413b10540b10b3',\n",
       "    'title': 'Neural Autoregressive Flows'},\n",
       "   {'paperId': 'b79e5e4622a95417deec313cd543617b19611bea',\n",
       "    'title': 'Deep Learning using Rectified Linear Units (ReLU)'},\n",
       "   {'paperId': '533e651177354d6687e8d0a24217a6bf65692bcc',\n",
       "    'title': 'Sylvester Normalizing Flows for Variational Inference'},\n",
       "   {'paperId': 'ce1c28ca2f52a42c6e60d792cd71ba894abc47d5',\n",
       "    'title': 'Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': '38fb1902c6a2ab4f767d4532b28a92473ea737aa',\n",
       "    'title': 'Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '585bf7bea8fa5267738bc465611d6f197e0f87dd',\n",
       "    'title': 'Masked Autoregressive Flow for Density Estimation'},\n",
       "   {'paperId': '6a97d2668187965743d1b825b306defccbabbb4c',\n",
       "    'title': 'Improved Variational Inference with Inverse Autoregressive Flow'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': '5ce030f1650145a103527e883e7a9d9a25c45547',\n",
       "    'title': 'A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots'},\n",
       "   {'paperId': 'f926d3bf410875857effe1b5000a4fbc25397b74',\n",
       "    'title': 'Reinforcement Learning from Demonstration through Shaping'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'a9da09d1e63686706d64782e654d69f13fd292ad',\n",
       "    'title': 'Learning by Demonstration'},\n",
       "   {'paperId': '82479b544924ee734fb22f9dce78aace0f90cd3c',\n",
       "    'title': 'Robot juggling: implementation of memory-based learning'},\n",
       "   {'paperId': 'ad690ce0b049c5bbd269129e1f62a45ebf5f24af',\n",
       "    'title': 'Mixtures of Normalizing Flows'}],\n",
       "  'citations': [],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 7,\n",
       "  'isKeypaper': True},\n",
       " '1509169e79337e7e1628b266e9475aac779f9c00': {'title': 'MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer',\n",
       "  'year': 2022,\n",
       "  'references': [],\n",
       "  'citations': [{'paperId': 'a861bc73b75c09e8bbe6f4b61563f2778e74554a',\n",
       "    'title': 'Multi-Task Reinforcement Learning With Attention-Based Mixture of Experts'},\n",
       "   {'paperId': 'b831a722ffa93beb6aac782c5884f793cb42e54d',\n",
       "    'title': 'Deep reinforcement learning in smart manufacturing: A review and prospects'},\n",
       "   {'paperId': '8d79b7d0d1c8e1bfc5ef20428e4566116e8eb862',\n",
       "    'title': 'Transferring Knowledge for Reinforcement Learning in Contact-Rich Manipulation'},\n",
       "   {'paperId': '1b8d686ecfa6b3743dc17d8022c7fa595e0a14f6',\n",
       "    'title': 'TEAM: a parameter-free algorithm to teach collaborative robots motions from user demonstrations'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " '3b51a29424b619ec5ce29125c4b88d8e24a09328': {'title': 'Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '52aeb38922f1f60ef4032012c70f9d5363547e03',\n",
       "    'title': 'C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks'},\n",
       "   {'paperId': '348a855fe01f3f4273bf0ecf851ca688686dbfcc',\n",
       "    'title': 'Offline Reinforcement Learning with Implicit Q-Learning'},\n",
       "   {'paperId': '2db0fbdb68919bbaccdd67a610a4e30c41fc41d6',\n",
       "    'title': 'Offline Pre-trained Multi-Agent Decision Transformer: One Big Sequence Model Tackles All SMAC Tasks'},\n",
       "   {'paperId': '62272403114c67a85e6fde9e428334d89e143485',\n",
       "    'title': 'AW-Opt: Learning Robotic Skills with Imitation and Reinforcement at Scale'},\n",
       "   {'paperId': '6c1a13f1479227f921a20e92671407b195159dee',\n",
       "    'title': 'Conservative Data Sharing for Multi-Task Offline Reinforcement Learning'},\n",
       "   {'paperId': '95e078afe47c574da17c14d52614b0ccfceb1eb4',\n",
       "    'title': 'Autonomous Reinforcement Learning via Subgoal Curricula'},\n",
       "   {'paperId': '33b456eb43e5391761540f17a29e598d7595565b',\n",
       "    'title': 'Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble'},\n",
       "   {'paperId': 'fb95d6e6e5f78f6e5c339e2058ce9ae9e803182b',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning with Imagined Subgoals'},\n",
       "   {'paperId': 'c879b25308026d6538e52b27bcf4fd3cb60855f3',\n",
       "    'title': 'A Minimalist Approach to Offline Reinforcement Learning'},\n",
       "   {'paperId': 'adcae35901c36325478a03b647e14222a53ea9fc',\n",
       "    'title': 'What Can I Do Here? Learning New Skills by Imagining Visual Affordances'},\n",
       "   {'paperId': '3e85d208b1b927fdb69ecf8336c70995818aaebd',\n",
       "    'title': 'MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale'},\n",
       "   {'paperId': '677b103eecc4d34e378502d60147456875e8741b',\n",
       "    'title': 'Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills'},\n",
       "   {'paperId': '2c442a7ef8bb18ab27fdbe3bc1dc3a1a47fe0b2e',\n",
       "    'title': 'Robust Model Predictive Path Integral Control: Analysis and Performance Guarantees'},\n",
       "   {'paperId': 'e77c0dcc5704c1c5de1ee091600ab6ca14b92784',\n",
       "    'title': 'C-Learning: Learning to Achieve Goals via Recursive Classification'},\n",
       "   {'paperId': '831e7cbafed2dca05db1e7f5ef16d1a7614f44ec',\n",
       "    'title': 'Learning to Reach Goals via Iterated Supervised Learning'},\n",
       "   {'paperId': 'f204041dd567025217adc8070ca292e89cc80488',\n",
       "    'title': 'COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning'},\n",
       "   {'paperId': '8dd3ec3ca1b7400d998e747356d07763a7ac1fb0',\n",
       "    'title': 'Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '28db20a81eec74a50204686c3cf796c42a020d2e',\n",
       "    'title': 'Conservative Q-Learning for Offline Reinforcement Learning'},\n",
       "   {'paperId': '50670f6746f08d3a8c7233ae97b1a00d94d8028a',\n",
       "    'title': 'PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals'},\n",
       "   {'paperId': '55999400a3eed52ea9dd2f4b9f1b71ccb5c51238',\n",
       "    'title': 'Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement'},\n",
       "   {'paperId': '59f606de7c073b4179f627325702d242eea51ba1',\n",
       "    'title': 'Scaling data-driven robotics with reward sketching and batch reinforcement learning'},\n",
       "   {'paperId': '4d40f7df809576d4db22b95c4ca9cc4c66e6928d',\n",
       "    'title': 'Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation'},\n",
       "   {'paperId': '7388826b5ee00efe17cb7f19a623d9b5e955ae70',\n",
       "    'title': 'Skew-Fit: State-Covering Self-Supervised Reinforcement Learning'},\n",
       "   {'paperId': '7b4848bad51ebd38fb068e73abc3c6d865fd692f',\n",
       "    'title': 'Planning with Goal-Conditioned Policies'},\n",
       "   {'paperId': '1674008abd47f1ce1e894c672074a47ee6c3288c',\n",
       "    'title': 'Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'beb4a4cb4b6ce49e58a2acd831f48a38b02fcf93',\n",
       "    'title': 'Contextual Imagined Goals for Self-Supervised Robotic Learning'},\n",
       "   {'paperId': 'c92780cd2c90b0393efb5d5b3a49b1ec9503df11',\n",
       "    'title': 'Policy Continuation with Hindsight Inverse Dynamics'},\n",
       "   {'paperId': '6d399bff0205977f51ff2334168c89320206493d',\n",
       "    'title': 'Goal-conditioned Imitation Learning'},\n",
       "   {'paperId': 'e0889fcee1acd985af76a3907d5d0029bf260be9',\n",
       "    'title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '5285cb8faada5de8a92a47622950f6cfd476ac1d',\n",
       "    'title': 'Off-Policy Deep Reinforcement Learning without Exploration'},\n",
       "   {'paperId': '7706a6aa39fedb5cff6c954d81a825b140216240',\n",
       "    'title': 'Curriculum-guided Hindsight Experience Replay'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': 'c7fddb4a67d703f059179ee1410e5b37b002d31e',\n",
       "    'title': 'Learning Sampling Distributions for Robot Motion Planning'},\n",
       "   {'paperId': '494e2d5b40dcebde349f9872c7317e5003f9c5d2',\n",
       "    'title': 'Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection'},\n",
       "   {'paperId': 'f466157848d1a7772fb6d02cdac9a7a5e7ef982e',\n",
       "    'title': 'Neural Discrete Representation Learning'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Pybullet, a python module for physics simulation for games, robotics and machine learning'},\n",
       "   {'paperId': '3f25e17eb717e5894e0404ea634451332f85d287',\n",
       "    'title': 'Learning Structured Output Representation using Deep Conditional Generative Models'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': '6364fdaa0a0eccd823a779fcdd489173f938e91a',\n",
       "    'title': 'U-Net: Convolutional Networks for Biomedical Image Segmentation'},\n",
       "   {'paperId': '680345b8799e659b60018c77cddcfff2b781d529',\n",
       "    'title': 'Combined task and motion planning through an extensible planner-independent interface layer'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'f358d770a1c31a67a06d467827bfde9f298278ac',\n",
       "    'title': 'CHOMP: Covariant Hamiltonian optimization for motion planning'},\n",
       "   {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff',\n",
       "    'title': 'ImageNet classification with deep convolutional neural networks'},\n",
       "   {'paperId': '5c65d095600d6c647426fa3bc45031b208882d5f',\n",
       "    'title': 'Batch Reinforcement Learning'},\n",
       "   {'paperId': '56ca783385dfa46a3917850bb52179d0bba25903',\n",
       "    'title': 'STOMP: Stochastic trajectory optimization for motion planning'},\n",
       "   {'paperId': '4326d7e9933c77ff9dc53056c62ef6712d90c633',\n",
       "    'title': 'Sampling-based algorithms for optimal motion planning'},\n",
       "   {'paperId': '89d487427c6614e033544d33a161a6f8da1d984a', 'title': 'D*lite'},\n",
       "   {'paperId': '7e98801ab0b438115ea4d86fdd4babe7ae178561',\n",
       "    'title': 'Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces'},\n",
       "   {'paperId': '6df43f70f383007a946448122b75918e3a9d6682',\n",
       "    'title': 'Learning to Achieve Goals'},\n",
       "   {'paperId': 'c547e1f79e6039d05c5ae433a36612d7f8e4d3f5',\n",
       "    'title': 'STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving'}],\n",
       "  'citations': [{'paperId': 'd11ae7f22045a2217fb2ef169037fba216153c63',\n",
       "    'title': 'Stabilizing Contrastive RL: Techniques for Offline Goal Reaching'},\n",
       "   {'paperId': '4a34c55239899ccb29ea75b7184403f9c1858da5',\n",
       "    'title': 'Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale'},\n",
       "   {'paperId': '87a00037444092e8ada8d3bb4c1f8c6baededdc0',\n",
       "    'title': 'Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " '546bff6c12ea395690292f204a7e019a8b3b87a0': {'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '9faecf3e18a833f2d49b030d591cc2ded0b54336',\n",
       "    'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "    'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '6fb1c349f407bb40add9c3f743a8fe040859cd6e',\n",
       "    'title': 'Diversity-Enriched Option-Critic'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'e2658b9abcbc170ae71744c7f27c0e6e27256ce2',\n",
       "    'title': 'Learning Off-Policy with Online Planning'},\n",
       "   {'paperId': '5e7bc93622416f14e6948a500278bfbe58cd3890',\n",
       "    'title': 'Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': '60c8b24913090e168bdb174f7d3e9e96dd0c5a8c',\n",
       "    'title': 'Options of Interest: Temporal Abstraction with Interest Functions'},\n",
       "   {'paperId': '0bd0db63a5d635287b3de4e537748e4242a53450',\n",
       "    'title': 'Attention-Privileged Reinforcement Learning'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': 'ad14227e4f51276892ffc37aa43fd8750bb5eba8',\n",
       "    'title': 'Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': 'eea259a6d2f08e1e3af5f210b5937d4654c325f4',\n",
       "    'title': 'Attention Privileged Reinforcement Learning For Domain Transfer'},\n",
       "   {'paperId': 'a4a2d99d1c237d0818971ec9205e89128c57fb02',\n",
       "    'title': 'Towards Interpretable Reinforcement Learning Using Attention Augmented Agents'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': 'bf7f1ada5feecc0992f71b39c1ebeccb19ae631b',\n",
       "    'title': 'InfoBot: Transfer and Exploration via the Information Bottleneck'},\n",
       "   {'paperId': '2ed619fbc7902155d54f6f21da16ad6c120eac63',\n",
       "    'title': 'Learning to Walk via Deep Reinforcement Learning'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '9ea50b3408f993853f1c5e374690e5fbe73c2a3c',\n",
       "    'title': 'Continual Lifelong Learning with Neural Networks: A Review'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85',\n",
       "    'title': 'Learning Abstract Options'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fb9693183bc74568c72188431c18cb2b07c87213',\n",
       "    'title': 'Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'ce1c28ca2f52a42c6e60d792cd71ba894abc47d5',\n",
       "    'title': 'Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '96e81cabed55630f2ad3e1346300bd7a7a17f060',\n",
       "    'title': 'When Waiting is not an Option : Learning Options with a Deliberation Cost'},\n",
       "   {'paperId': None,\n",
       "    'title': 'IWs are commonly ignored (Lillicrap et al., 2015'},\n",
       "   {'paperId': 'c27db32efa8137cbf654902f8f728f338e55cd1c',\n",
       "    'title': 'Mastering the game of Go without human knowledge'},\n",
       "   {'paperId': 'cf90552b5d2e992e93ab838fd615e1c36618e31c',\n",
       "    'title': 'Distral: Robust multitask reinforcement learning'},\n",
       "   {'paperId': 'd0352057e2b99f65f8b5244a0b912026c86d7b21',\n",
       "    'title': 'Equivalence Between Policy Gradients and Soft Q-Learning'},\n",
       "   {'paperId': '5151d6cb3a4eaec14a56944d58338251fca344ab',\n",
       "    'title': 'Overcoming catastrophic forgetting in neural networks'},\n",
       "   {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "    'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       "   {'paperId': 'dc3e905bfb27d21675ee1720413e007b014b37d3',\n",
       "    'title': 'Safe and Efficient Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '4b8f52da1aa977de0ad3ede54b36730cfbf700fd',\n",
       "    'title': 'On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference'},\n",
       "   {'paperId': '9a25c8d370d5bae018706e92cd3b975e7ec737fd',\n",
       "    'title': 'Optimal control as a graphical model inference problem'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '644a079073969a92674f69483c4a85679d066545',\n",
       "    'title': 'Double Q-learning'},\n",
       "   {'paperId': '8570302f7b63e8fcf87030f556b065fd8c260021',\n",
       "    'title': 'Linearly-solvable Markov decision problems'},\n",
       "   {'paperId': 'a91635f8d0e7fb804efd1c38d9c24ee952ba7076',\n",
       "    'title': 'Learning to Predict by the Methods of Temporal Differences'},\n",
       "   {'paperId': 'ec0a3723a829de0ef865199ab326a044e6c87b8b',\n",
       "    'title': 'Monte Carlo POMDPs'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '69d7086300e7f5322c06f2f242a565b3a182efb5',\n",
       "    'title': 'In Advances in Neural Information Processing Systems'}],\n",
       "  'citations': [{'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 10,\n",
       "  'isKeypaper': True},\n",
       " '2a88d01f3079e68ad9b5bcb1ebe56da25679e331': {'title': 'Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '8abea1b488813901470a0b2c9bb35eb13f57b407',\n",
       "    'title': 'Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing'},\n",
       "   {'paperId': '5c85edd4b333e78b2c42bdde6f3eec5f911bdbdc',\n",
       "    'title': 'Lifelong Robotic Reinforcement Learning by Retaining Experiences'},\n",
       "   {'paperId': 'c7b01136da36d68e229f268e5489c81c94586481',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation'},\n",
       "   {'paperId': 'a026fb990836c9c36f0d24bc3d079656938f5193',\n",
       "    'title': 'Curriculum Learning with Hindsight Experience Replay for Sequential Object Manipulation Tasks'},\n",
       "   {'paperId': '007017ca4b9760bbd72f7ea1a4357c4d52dfd205',\n",
       "    'title': 'Exploiting Reward Shifting in Value-Based Deep RL'},\n",
       "   {'paperId': '0bc75aebf563bbc7802a9e461f4fb24b022dbe8e',\n",
       "    'title': 'Imaginary Hindsight Experience Replay: Curious Model-based Learning for Sparse Reward Tasks'},\n",
       "   {'paperId': '76cad7c2e31b9c0243b3045b7af89e9ad0be15d6',\n",
       "    'title': 'An Open-Source Multi-Goal Reinforcement Learning Environment for Robotic Manipulation with Pybullet'},\n",
       "   {'paperId': '3e85d208b1b927fdb69ecf8336c70995818aaebd',\n",
       "    'title': 'MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale'},\n",
       "   {'paperId': '7f823f7711eee4db094f844e8a25cf8e4f80f2be',\n",
       "    'title': 'Continual Model-Based Reinforcement Learning with Hypernetworks'},\n",
       "   {'paperId': '4f0d3bd7a319989415ad94ac5ba281304c75a125',\n",
       "    'title': 'Bias-Reduced Hindsight Experience Replay with Virtual Goal Prioritization'},\n",
       "   {'paperId': '9c60e97762aeea4b5b7a01c835975b749772227b',\n",
       "    'title': 'Generating attentive goals for prioritized hindsight reinforcement learning'},\n",
       "   {'paperId': '0a38ed285c159e07cbf5edaf4b73ea4e045406d2',\n",
       "    'title': 'ACDER: Augmented Curiosity-Driven Experience Replay'},\n",
       "   {'paperId': '92e0c1697bc4630903501185d11676f9542c9c80',\n",
       "    'title': 'Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards'},\n",
       "   {'paperId': '191461705e9961a3b07273c19d89eb214819002c',\n",
       "    'title': 'Continual learning with hypernetworks'},\n",
       "   {'paperId': '4fc4533b33a4131ee287118fe17868992accad3b',\n",
       "    'title': 'Dynamical Distance Learning for Unsupervised and Semi-Supervised Skill Discovery'},\n",
       "   {'paperId': '4a954b3e72a61968ab235076bcc242aca3a05520',\n",
       "    'title': 'Efficient Lifelong Learning with A-GEM'},\n",
       "   {'paperId': 'c575d724615a2852f04dfce84547aa3654101150',\n",
       "    'title': 'Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference'},\n",
       "   {'paperId': 'e3fee9244fc47aa9e80006e39352af90f64631fe',\n",
       "    'title': 'Measuring and regularizing networks in function space'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '7706a6aa39fedb5cff6c954d81a825b140216240',\n",
       "    'title': 'Curriculum-guided Hindsight Experience Replay'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': '0e93ee6176197f758dd5e8e9cff8b9610274c402',\n",
       "    'title': 'Modular meta-learning'},\n",
       "   {'paperId': '4debb99c0c63bfaa97dd433bc2828e4dac81c48b',\n",
       "    'title': 'Addressing Function Approximation Error in Actor-Critic Methods'},\n",
       "   {'paperId': 'ce1c28ca2f52a42c6e60d792cd71ba894abc47d5',\n",
       "    'title': 'Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research'},\n",
       "   {'paperId': '713b0d9005944f80af00addc81b162ca74ea4b14',\n",
       "    'title': 'Memory Aware Synapses: Learning what (not) to forget'},\n",
       "   {'paperId': 'fc9d5be5e3f14b4c4b145b6c4bd96a9182f39fd2',\n",
       "    'title': 'Routing Networks: Adaptive Selection of Non-linear Functions for Multi-Task Learning'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': 'e37b999f0c96d7136db07b0185b837d5decd599a',\n",
       "    'title': 'Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '24a0686eec31394a98b233161befdeb41b942cca',\n",
       "    'title': 'IEEE International Conference on Robotics and Automation (ICRA) におけるフルードパワー技術の研究動向'},\n",
       "   {'paperId': '9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f',\n",
       "    'title': 'Probabilistic policy reuse in a reinforcement learning agent'}],\n",
       "  'citations': [],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " 'bd2ff852e86d16df09376f2dfdc934c533bb04a2': {'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': '5c22bec75b80233c4c95a1986ca45bdc6c262108',\n",
       "    'title': 'Jump-Start Reinforcement Learning'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '6ec8797952213227eea2e63620f4d7c060d598d5',\n",
       "    'title': 'Hierarchical reinforcement learning for efficient exploration and transfer'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0',\n",
       "    'title': 'Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer'},\n",
       "   {'paperId': 'f8a54ba839f6194198b4886097169a53905fbb37',\n",
       "    'title': 'Uncertainty in Neural Networks: Approximately Bayesian Ensembling'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'ae4d32f05cf40e4cc01c69d7787149a258c95eda',\n",
       "    'title': 'Residual Reinforcement Learning for Robot Control'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '25e433197844c239742f67fbb4171e913e0b9fe2',\n",
       "    'title': 'Analyzing Inverse Problems with Invertible Neural Networks'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '7cd316505f52aa337ef8a2aff10bc6bf1df561d0', 'title': 'and s'},\n",
       "   {'paperId': None, 'title': '299 and N'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': '6f45c4c8912551c5d76c217245296fcf9a5db291',\n",
       "    'title': 'Uncertainty in Neural Networks: Bayesian Ensembling'},\n",
       "   {'paperId': '1a1a22a3281e00d4ffd69b6490abc266d4eed0b3',\n",
       "    'title': 'Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '3f25e17eb717e5894e0404ea634451332f85d287',\n",
       "    'title': 'Learning Structured Output Representation using Deep Conditional Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'b70dd1fccbba3466f22b592ca090836513ebf494',\n",
       "    'title': 'Data-Efficient Generalization of Robot Skills with Contextual Policy Search'},\n",
       "   {'paperId': '7304b5ae51dc9d91bd988141d6d7b1f02aa3f86f',\n",
       "    'title': 'A Family of Nonparametric Density Estimation Algorithms'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': 'bc6dff14a130c57a91d5a21339c23471faf1d46f', 'title': 'Et al'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'a414c2111a042c0e53f4cb4297ddb39a3801fc1e',\n",
       "    'title': 'An image synthesizer'}],\n",
       "  'citations': [{'paperId': '8fce3142bc144bdc08bf0cab1db908c7ad3f8454',\n",
       "    'title': 'Contrastive Language, Action, and State Pre-training for Robot Learning'},\n",
       "   {'paperId': 'b8a3a030a36fbb74deea909aec5a959c94477fd2',\n",
       "    'title': 'CLAS: Coordinating Multi-Robot Manipulation with Central Latent Action Spaces'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 19,\n",
       "  'isKeypaper': True},\n",
       " '947070ff65dc9a0b0024d299acdcfa8251b5118b': {'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': '156dcc0a20da91fd9f0874bceaf195b5f9345fe7',\n",
       "    'title': 'Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile Manipulation'},\n",
       "   {'paperId': '1909317ccf9d01052bbe9eafbbe06380848effb8',\n",
       "    'title': 'Boosted Curriculum Reinforcement Learning'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '6f362c4e5952f027725d3bb21da0145553e63421',\n",
       "    'title': 'Fully Autonomous Real-World Reinforcement Learning with Applications to Mobile Manipulation'},\n",
       "   {'paperId': '23bb22710f7be585305bf01841b74ed167a706ce',\n",
       "    'title': 'Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning'},\n",
       "   {'paperId': '7bbba63ad5f8786c29b80e3c3098b720f658114a',\n",
       "    'title': 'Mobile Manipulation Hackathon: Moving into Real World Applications'},\n",
       "   {'paperId': '400811ee31020a3f002551476dac25973e13035e',\n",
       "    'title': 'How to train your robot with deep reinforcement learning: lessons we have learned'},\n",
       "   {'paperId': '2fe9fbe6c5ba35697abb67a2bb8df5e3c3fee0a7',\n",
       "    'title': 'Learning Kinematic Feasibility for Mobile Manipulation Through Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ff0282b34d758a4aaad524ea554f6545852e3c68',\n",
       "    'title': 'ACRONYM: A Large-Scale Grasp Dataset Based on Simulation'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': 'fa3e3b3ae026a226351ab988374319e0cde010e6',\n",
       "    'title': 'MushroomRL: Simplifying Reinforcement Learning Research'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'eadbe2e4f9de47dd357589cf59e3d1f0199e5075',\n",
       "    'title': 'Learning quadrupedal locomotion over challenging terrain'},\n",
       "   {'paperId': 'd052f58cfa47ff972280063682a78516aa500353',\n",
       "    'title': 'Learning Dexterous Manipulation from Suboptimal Experts'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': 'f441e637980a8b427474dbdc0141f38dd78bb831',\n",
       "    'title': 'Recent Advances in Robot Learning from Demonstration'},\n",
       "   {'paperId': '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0',\n",
       "    'title': 'Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer'},\n",
       "   {'paperId': 'dc36611384b28d2926f919fb806a1f2d6015e33c',\n",
       "    'title': 'Whole-Body Control of a Mobile Manipulator using End-to-End Reinforcement Learning'},\n",
       "   {'paperId': 'c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d',\n",
       "    'title': 'Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': 'ea5d0091e349fd96c54512b32711ce3bd53a8418',\n",
       "    'title': 'Combined Task and Action Learning from Human Demonstrations for Mobile Manipulation Applications'},\n",
       "   {'paperId': 'a62158491e3c88712277a8947a736ed8f17bbd4c',\n",
       "    'title': 'Control Regularization for Reduced Variance Reinforcement Learning'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': 'ae4d32f05cf40e4cc01c69d7787149a258c95eda',\n",
       "    'title': 'Residual Reinforcement Learning for Robot Control'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': 'bd6114d4e418d2669b6bbb0dfd5f2b4c0d83e7e4',\n",
       "    'title': 'Coupling Mobile Base and End-Effector Motion in Task Space'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '3e710e9a58c7da0eddca063ce5f517b30199b4a6',\n",
       "    'title': 'Reuleaux: Robot Base Placement by Reachability Analysis'},\n",
       "   {'paperId': '3b2bb0bc47a8b8df4d3f3d1534c6e0b92ada0bd8',\n",
       "    'title': 'Identifying good poses when doing your household chores: Creation and exploitation of inverse surface reachability maps'},\n",
       "   {'paperId': '270f43a1de06f613761b448c10e9a8ca20d452ca',\n",
       "    'title': 'Learning mobile manipulation actions from human demonstrations'},\n",
       "   {'paperId': 'f95e2f1c3fb37dc82621408ca5e9fcc72f056dee',\n",
       "    'title': 'Boosted Fitted Q-Iteration'},\n",
       "   {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "    'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       "   {'paperId': '515a21e90117941150923e559729c59f5fdade1c',\n",
       "    'title': 'The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables'},\n",
       "   {'paperId': '0537dff479f1f0d26fd1ead20d42a2aa92a4463f',\n",
       "    'title': 'BI2RRT*: An efficient sampling-based path planning framework for task-constrained mobile manipulation'},\n",
       "   {'paperId': '058f1f89f2383c2b5e0dd159f54b41f7b2cf906d',\n",
       "    'title': 'Mobility and Manipulation'},\n",
       "   {'paperId': '3e9faf8b1feff0d54eddb11fb0b092ae40e09a59',\n",
       "    'title': 'Representing the robot’s workspace through constrained manipulability analysis'},\n",
       "   {'paperId': 'bcc2e4700bc0ab4df0144167223f285c17892b22',\n",
       "    'title': 'Robot placement based on reachability inversion'},\n",
       "   {'paperId': 'f3566be263583b136c51451cba7ea4ace6947f83',\n",
       "    'title': 'Mobile Manipulation in Unstructured Environments: Perception, Planning, and Execution'},\n",
       "   {'paperId': '5cbfbbca3a1ea8ee39254dd4ef07b3d67761c39a',\n",
       "    'title': 'Relative Entropy Policy Search'},\n",
       "   {'paperId': '415f36dfa55b8c2a66942e8080e182c489e9c291',\n",
       "    'title': 'Combined Task and Motion Planning for Mobile Manipulation'},\n",
       "   {'paperId': 'b824cb051ffbdd81b529c4b82379a3af270fb6f7',\n",
       "    'title': 'Boosting a weak learning algorithm by majority'}],\n",
       "  'citations': [{'paperId': '7be614daf2a8074d587be2651ebe9f0d63ef6c88',\n",
       "    'title': 'PRIMP: PRobabilistically-Informed Motion Primitives for Efficient Affordance Learning from Demonstration'},\n",
       "   {'paperId': '8269c424aa7bfc6ac216e291f4e5e985b9727031',\n",
       "    'title': 'M-EMBER: Tackling Long-Horizon Mobile Manipulation via Factorized Domain Transfer'},\n",
       "   {'paperId': '606877f3cb1109b0399799b085b5202511f9da3b',\n",
       "    'title': 'Enabling Failure Recovery for On-The-Move Mobile Manipulation'},\n",
       "   {'paperId': 'b1e76ccc5ac58f7a08fe4253d402d7b1011f05b5',\n",
       "    'title': 'Causal Policy Gradient for Whole-Body Mobile Manipulation'},\n",
       "   {'paperId': '61fab51f8aa6049d9e3d5973ce8d3626e23c5e08',\n",
       "    'title': 'CLARA: Building a Socially Assistive Robot to Interact with Elderly People'},\n",
       "   {'paperId': '897cedbb9f947d1130ce659559845013bcc77124',\n",
       "    'title': 'Hierarchical Policy Blending As Optimal Transport'},\n",
       "   {'paperId': '920c99d6b0d2497c4323e7fdd5c95fc2d8f1bb65',\n",
       "    'title': 'Hierarchical Policy Blending as Inference for Reactive Robot Control'},\n",
       "   {'paperId': 'c74ba54cc1b29a99619f8d3cf13b8a35e3307396',\n",
       "    'title': 'Obstacle Avoidance for Robotic Manipulator in Joint Space via Improved Proximal Policy Optimization'},\n",
       "   {'paperId': '5051947cbac4223a68edc8e7e7319b5cdb2dc712',\n",
       "    'title': 'N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 6,\n",
       "  'isKeypaper': True},\n",
       " '14444e96f58ff0ce449fd6c61abfffaec1c83f76': {'title': 'Cache-Assisted Collaborative Task Offloading and Resource Allocation Strategy: A Metareinforcement Learning Approach',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '5f265df48c9fe3201551a71c9d57f140c49eaaa0',\n",
       "    'title': 'MR-DRO: A Fast and Efficient Task Offloading Algorithm in Heterogeneous Edge/Cloud Computing Environments'},\n",
       "   {'paperId': '020bb2ba5f3923858cd6882ba5c5a44ea8041ab6',\n",
       "    'title': 'Meta-Learning in Neural Networks: A Survey'},\n",
       "   {'paperId': '4f163a55bc42126541f53ac20bf3a35ecf0ad9f6',\n",
       "    'title': 'A DRL Agent for Jointly Optimizing Computation Offloading and Resource Allocation in MEC'},\n",
       "   {'paperId': '5e28612144635ed0ef4f3c2edb0c359f12ae71b9',\n",
       "    'title': 'Optimizing Age of Information Through Aerial Reconfigurable Intelligent Surfaces: A Deep Reinforcement Learning Approach'},\n",
       "   {'paperId': 'c40a927a558ad5a5ffe254605ed3bfebd18be39c',\n",
       "    'title': 'Fast Adaptive Task Offloading in Edge Computing Based on Meta Reinforcement Learning'},\n",
       "   {'paperId': '9bb535e18c064698b739e2068a616156f878a15f',\n",
       "    'title': 'Intelligent Reflecting Surface Assisted Anti-Jamming Communications: A Fast Reinforcement Learning Approach'},\n",
       "   {'paperId': '6ac78be17a44e7836a5d20bdee2492901aaa90c2',\n",
       "    'title': 'Deep Reinforcement Learning-Based Intelligent Reflecting Surface for Secure Wireless Communications'},\n",
       "   {'paperId': '6a7569a831e29ff32644514516569c9ffc023c05',\n",
       "    'title': 'Collaborative Service Placement for Edge Computing in Dense Small Cell Networks'},\n",
       "   {'paperId': '86ee7dc6a91a0104ff04aadbfd7fa8241e36dcbc',\n",
       "    'title': 'Offloading Tasks With Dependency and Service Caching in Mobile Edge Computing'},\n",
       "   {'paperId': 'a18bf429ac14fb67a028fc113fc6851187e759f9',\n",
       "    'title': 'DCoL: Distributed Collaborative Learning for Proactive Content Caching at Edge Networks'},\n",
       "   {'paperId': 'd5318a4ebb4160743e3af568b310ebc5aaebc09e',\n",
       "    'title': 'Federated learning for 6G communications: Challenges, methods, and future directions'},\n",
       "   {'paperId': '33cb916f17f1124e0e749830a850f8998dbd2eac',\n",
       "    'title': 'Edge QoE: Computation Offloading With Deep Reinforcement Learning for Internet of Things'},\n",
       "   {'paperId': 'cdd1f585d75533d8f4dc7b5bc183a4fe5747d45e',\n",
       "    'title': 'Deep-Reinforcement-Learning-Based Offloading Scheduling for Vehicular Edge Computing'},\n",
       "   {'paperId': '790e61d213be35d2164ff1869d06dbb9cc6858a2',\n",
       "    'title': 'Cache-Aware Computation Offloading in IoT Systems'},\n",
       "   {'paperId': '4bfdc3910684396e22f6675470002ef162bebdbf',\n",
       "    'title': 'Artificial-Intelligence-Enabled Intelligent 6G Networks'},\n",
       "   {'paperId': '3ca142ea88ac2b3d8d38db14be86f3f9cd62d47e',\n",
       "    'title': 'Joint Optimization of Service Caching Placement and Computation Offloading in Mobile Edge Computing Systems'},\n",
       "   {'paperId': 'abe3c1f51e6781eb4320771f792728cd9cd6defb',\n",
       "    'title': 'Deep Reinforcement Learning for Online Computation Offloading in Wireless Powered Mobile-Edge Computing Networks'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Multi-agent metareinforcement learning for self-powered and sustainable edge computing systems'},\n",
       "   {'paperId': '8a8194b271f32ad1bc3efb9f9293b6ad5ed7718c',\n",
       "    'title': 'Joint Multi-User Computation Offloading and Data Caching for Hybrid Mobile Cloud/Edge Computing'},\n",
       "   {'paperId': 'ec187491b58dce48ab547a19fa4e87d1f9755742',\n",
       "    'title': 'QoE-Aware Computation Offloading to Capture Energy-Latency-Pricing Tradeoff in Mobile Clouds'},\n",
       "   {'paperId': '34c65ff9691581ef251faeb35d4b48d37ce76a4c',\n",
       "    'title': 'Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy'},\n",
       "   {'paperId': 'ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b',\n",
       "    'title': 'Reinforcement Learning, Fast and Slow'},\n",
       "   {'paperId': '2deed11218a514356b48dabfbf41a5a2656c1499',\n",
       "    'title': 'Deep Reinforcement Learning for Mobile 5G and Beyond: Fundamentals, Applications, and Challenges'},\n",
       "   {'paperId': '4f4be04c73a99910e2d3cd94ed3f3c15ba28aa6d',\n",
       "    'title': 'Online Proactive Caching in Mobile Edge Computing Using Bidirectional Deep Recurrent Neural Network'},\n",
       "   {'paperId': '8c673cb6de4c38ba295e2557319c4e63c3654dd2',\n",
       "    'title': 'Optimized Computation Offloading Performance in Virtual Edge Computing Systems Via Deep Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'An edgecomputing based architecture for mobile augmented reality'},\n",
       "   {'paperId': '3f6d196b05805122e9acdafce092095cdc36e266',\n",
       "    'title': 'Computation Offloading With Data Caching Enhancement for Mobile Edge Computing'},\n",
       "   {'paperId': 'ddc283ff3f84f42460cb3b32eacc7d549eda5d50',\n",
       "    'title': 'Task Proactive Caching Based Computation Offloading and Resource Allocation in Mobile-Edge Computing Systems'},\n",
       "   {'paperId': '64078bc74a65f520fe2f87ac9c0507f1748b245a',\n",
       "    'title': 'Resource allocation and distributed uplink offloading mechanism in fog environment'},\n",
       "   {'paperId': '1f8dfacb1bdd035327c0fa40f8c8b68176a7887f',\n",
       "    'title': 'QoE-Based Big Data Analysis with Deep Learning in Pervasive Edge Environment'},\n",
       "   {'paperId': '90dc22818bd2d97d8deaff168b0137b75a962767',\n",
       "    'title': 'On First-Order Meta-Learning Algorithms'},\n",
       "   {'paperId': 'b97265ec7cd48942949660051686a957ddffe781',\n",
       "    'title': 'Energy Efficient Task Caching and Offloading for Mobile Edge Computing'},\n",
       "   {'paperId': '28007ec1eb3d95e3cb2f6230cec3879af56cc76d',\n",
       "    'title': 'Online Scheduling and Interference Alleviation for Low-Latency, High-Throughput Processing of Data Streams'},\n",
       "   {'paperId': '252780a5c41e6c96e91f89c898e7071b1c76aaf7',\n",
       "    'title': 'Joint computation offloading and data caching with delay optimization in mobile-edge computing systems'},\n",
       "   {'paperId': 'd43e501bb2a33227eab2327241075c860fcaa346',\n",
       "    'title': 'Cachier: Edge-Caching for Recognition Applications'},\n",
       "   {'paperId': '1aa57fd9f06e73c5fb05f070cc88b2f7cc3aa6d3',\n",
       "    'title': 'Offloading in Mobile Edge Computing: Task Allocation and Computational Frequency Scaling'},\n",
       "   {'paperId': '1be70c1cc40865c281b3f59e973f9bd8a8cf06c8',\n",
       "    'title': 'A Survey on Mobile Edge Networks: Convergence of Computing, Caching and Communications'},\n",
       "   {'paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'},\n",
       "   {'paperId': 'afd9dadac8d3354615e26b2038887ecdbc9e33e5',\n",
       "    'title': 'Mobile Edge Computing: A Survey on Architecture and Computation Offloading'},\n",
       "   {'paperId': '8c5293da3ad1a463cb9694edfbf1bf19b8cbd698',\n",
       "    'title': 'A Survey on Mobile Edge Computing: The Communication Perspective'},\n",
       "   {'paperId': '05a5c937531d42c9947127e4de81da7a2e03dc00',\n",
       "    'title': 'Energy-Efficient Resource Allocation for Mobile Edge Computing-Based Augmented Reality Applications'},\n",
       "   {'paperId': '0d88252e3a8777618d680fbb7fe64f8c1bdd1483',\n",
       "    'title': 'Efficient Multi-User Computation Offloading for Mobile-Edge Cloud Computing'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'b25744552b75c50979bbc122e48cd640f2595544',\n",
       "    'title': 'List Scheduling Algorithm for Heterogeneous Systems by an Optimistic Cost Table'},\n",
       "   {'paperId': 'e36f29ec882aba87135b590ea5bcdabc2e036b9f',\n",
       "    'title': 'Markov Decision Processes: A Tool for Sequential Decision Making under Uncertainty'}],\n",
       "  'citations': [{'paperId': '9c77e61acb8141be4484f431f8330992a4d04ad3',\n",
       "    'title': 'DDPG with Transfer Learning and Meta Learning Framework for Resource Allocation in Underlay Cognitive Radio Network'},\n",
       "   {'paperId': '42d34b617f3a725d210b91ddfadb42e00937741a',\n",
       "    'title': 'Caching in Narrow-Band Burst-Error Channels via Meta Self-Supervision Learning'},\n",
       "   {'paperId': 'a2987bbb5577f1727706716481e0ecdce950c6b1',\n",
       "    'title': 'Fast Spectrum Sharing in Vehicular Networks: A Meta Reinforcement Learning Approach'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'f8cbd6e934ff43e6227bc14bc77c2934c0b66e23': {'title': 'STRUCTURAL ACTIVE CONTROL FRAMEWORK USING REINFORCEMENT LEARNING',\n",
       "  'year': 2022,\n",
       "  'references': [],\n",
       "  'citations': [{'paperId': 'acb46399a0810a8d21a82fd0f5def1d122b66fb5',\n",
       "    'title': 'Active structural control framework using policy-gradient reinforcement learning'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9': {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': '9406e95bcafaf5dbce5629467a5e4282d40b667d',\n",
       "    'title': 'NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields'},\n",
       "   {'paperId': '69b80ce5ab6c2263b145b1eaa23664145938f351',\n",
       "    'title': 'The Primacy Bias in Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bd6a0d440cfb790e781b88cad57c7f6edbe421b1',\n",
       "    'title': 'Revisiting Gaussian mixture critics in off-policy reinforcement learning: a sample-based approach'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '90ab9f71262e03ba7429b3fc4b631aa8ab1ddd28',\n",
       "    'title': 'Evaluating model-based planning and planner amortization for continuous control'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': '257432aaaba92fc878293028b63131d87cb61712',\n",
       "    'title': 'On Multi-objective Policy Optimization as a Tool for Reinforcement Learning'},\n",
       "   {'paperId': '39cc9292cb602970453c7677ada5d575d03d1d77',\n",
       "    'title': 'SKID RAW: Skill Discovery From Raw Trajectories'},\n",
       "   {'paperId': '46130875c8c2d89ea23dfb29c3784a6e5e510e54',\n",
       "    'title': 'Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '90974d9e0df8466a50338601e839fa0ea69c9872',\n",
       "    'title': 'Transient Non-stationarity and Generalisation in Deep Reinforcement Learning'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'd052f58cfa47ff972280063682a78516aa500353',\n",
       "    'title': 'Learning Dexterous Manipulation from Suboptimal Experts'},\n",
       "   {'paperId': '0922fc9d7aa9610896890f4d417ca7e72e417cdf',\n",
       "    'title': 'Importance Weighted Policy Learning and Adaption'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '7acbdb961f67d50fef359066f2a1d7755cf16ee2',\n",
       "    'title': 'Critic Regularized Regression'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': '19ee55abcb71c87a5f53778cefcd660f4fdd7773',\n",
       "    'title': 'On Warm-Starting Neural Network Training'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d',\n",
       "    'title': 'Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics'},\n",
       "   {'paperId': 'ad14227e4f51276892ffc37aa43fd8750bb5eba8',\n",
       "    'title': 'Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '57daffd65a5d73a439903f3e50950c21c9eba687',\n",
       "    'title': 'Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'df73c1505f49dcc354fb2b97e3b06bf06e20f3f7',\n",
       "    'title': 'Using probabilistic movement primitives in robotics'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'd6ef620ed29b94edfc28c8ec263b29ffb4f4b89d',\n",
       "    'title': 'Learning movement primitive libraries through probabilistic segmentation'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'b941e362eaaed23293c7ec602ac842bc5253f1fd',\n",
       "    'title': 'Efficient Unsupervised Temporal Segmentation of Motion Data'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '1c4927af526d5c28f7c2cfa492ece192d80a61d4',\n",
       "    'title': 'Policy Distillation'},\n",
       "   {'paperId': '1def5d3711ebd1d86787b1ed57c91832c5ddc90b',\n",
       "    'title': 'Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning'},\n",
       "   {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "    'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       "   {'paperId': '43fc6ab25ba5236688a87a591ed58c778bdcd417',\n",
       "    'title': 'Probabilistic segmentation applied to an assembly task'},\n",
       "   {'paperId': '3a81cfb4a7a880b7cf8979f6067732e961aceb7c',\n",
       "    'title': 'Probabilistic Movement Primitives'},\n",
       "   {'paperId': 'afa621e449527ae3306286d0fe6cd443fe38d722',\n",
       "    'title': 'Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '1695dbabf8e905db0b391ff522c323db5fc8b958',\n",
       "    'title': 'Learning to select and generalize striking movements in robot table tennis'},\n",
       "   {'paperId': '21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8',\n",
       "    'title': 'Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '0ab699a1022a15f8f38e336e1f9431d5a7bfad2c',\n",
       "    'title': 'Strength Through Diversity: Robust Behavior Learning via Mixture Policies'},\n",
       "   {'paperId': 'bfb456caf5e71d426bd3e2fd529ee833a6c3b7e7',\n",
       "    'title': 'An efficient orientation filter for inertial and inertial / magnetic sensor arrays'},\n",
       "   {'paperId': '68ca859820a3c7b672e279b4e952e0306310e6ab',\n",
       "    'title': 'Relational Macros for Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': '3307e0ff313497b8a1dc3358982cc679125d14cd',\n",
       "    'title': 'PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'fa26574f17f5cf43538c8162c40c5cab801cacfd',\n",
       "    'title': 'Reusing Old Policies to Accelerate Learning on New MDPs'},\n",
       "   {'paperId': '5dd31d8088caa71e5aec1509305eb927f3a2dede',\n",
       "    'title': 'Reusing Learned Policies Between Similar Problems'},\n",
       "   {'paperId': 'e41cda7b81cc49640210173fd45eb06cdbd6e824',\n",
       "    'title': 'Finding Structure in Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': '• Scheduler target critic update period'},\n",
       "   {'paperId': None, 'title': '• Component network size for Mixture policies'},\n",
       "   {'paperId': None,\n",
       "    'title': 'As discussed in Section 5 of the main text, fine-tuning often leads to sub-optimal performance since useful information can be lost early in training'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Scheduler specific parameters • Scheduler batch size: 128 • Scheduler learning rate'},\n",
       "   {'paperId': None,\n",
       "    'title': '• Min replay size to sample for scheduler: 64 • Steps per update: 4 • Available skill lengths: K = {n · 10} 10'},\n",
       "   {'paperId': None,\n",
       "    'title': ') • RHPO: Fine-tune or freeze parameters size: 256 • Trajectory length: 48 • Learning rate'},\n",
       "   {'paperId': None,\n",
       "    'title': '• Min replay size to sample: 200 • Samples per insert: 50 • Replay size: 1e6 • Target Actor update period: 25 • Target Critic update period'},\n",
       "   {'paperId': None,\n",
       "    'title': '(mean) and 1e-5 (covariance) Network parameters • Torso for Gaussian policies: (128, 256, 128) • Component network size for Mixture policies: (256, 128) • Categorical network size for MoG policies'},\n",
       "   {'paperId': None,\n",
       "    'title': 'This differs from the way the data is collected at training time, where the new skill can interact with the environment only when selected by the scheduler (b)'}],\n",
       "  'citations': [{'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': '98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2',\n",
       "    'title': 'Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 16,\n",
       "  'isKeypaper': True},\n",
       " '271081730bfce9117ad4432bffa8e5f18dbac133': {'title': 'Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback',\n",
       "  'year': 2022,\n",
       "  'references': [{'paperId': None,\n",
       "    'title': '“ Zero - shot sim - to - real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback'},\n",
       "   {'paperId': 'e50d0d7d42960d148420875054e1974aaa06b5da',\n",
       "    'title': 'A Unified Parametric Representation for Robotic Compliant Skills With Adaptation of Impedance and Force'},\n",
       "   {'paperId': '156e6bfc8dd475119eee12c93eb3ed1c7e8a198e',\n",
       "    'title': 'An Approach for Robotic Leaning Inspired by Biomimetic Adaptive Control'},\n",
       "   {'paperId': 'b27e09102b327cdd04a7ab22c911ae92a52b4814',\n",
       "    'title': 'Learning of Parameters in Behavior Trees for Movement Skills'},\n",
       "   {'paperId': '49142e3e381c0dc7fee0049ea41d2ef02c0340d7',\n",
       "    'title': 'Isaac Gym: High Performance GPU-Based Physics Simulation For Robot Learning'},\n",
       "   {'paperId': '30b28c0a7611b85b2c93637955a096069edbaab9',\n",
       "    'title': 'Zero-Shot Sim-to-Real Transfer of Tactile Control Policies for Aggressive Swing-Up Manipulation'},\n",
       "   {'paperId': '8c152e1b08b5aefd8ae15181dee89c0d9ebd5314',\n",
       "    'title': 'MRAC-RL: A Framework for On-Line Policy Adaptation Under Parametric Model Uncertainty'},\n",
       "   {'paperId': '3e4f080a9158b80d98b64bf4b0467c213b0c8387',\n",
       "    'title': 'The Development of Digital Twin Technology Review'},\n",
       "   {'paperId': '5a1b92aa50797a7c1e99b8840ff01aad66038596',\n",
       "    'title': 'Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey'},\n",
       "   {'paperId': '343101bbbe5b5614f38427e70983318f9c5f4247',\n",
       "    'title': 'A digital twin to train deep reinforcement learning agent for smart manufacturing plants: Environment, interfaces and intelligence'},\n",
       "   {'paperId': '84aa55e2969d5e1fcae9622a1ab1100e1da9205b',\n",
       "    'title': 'Learning Force Control for Contact-Rich Manipulation Tasks With Rigid Position-Controlled Robots'},\n",
       "   {'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': 'f2ba0fff8e4ef7a633c388e4477893e36a92ff79',\n",
       "    'title': 'Crowdsourcing Mechanism for Trust Evaluation in CPCS Based on Intelligent Mobile Edge Computing'},\n",
       "   {'paperId': '9e0e385662961562324482860dca531413d5c4af',\n",
       "    'title': 'Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks'},\n",
       "   {'paperId': '66a6f04d594d61962628cb52ec69ceedb4193740',\n",
       "    'title': 'Review of Deep Reinforcement Learning for Robot Manipulation'},\n",
       "   {'paperId': 'dafa29f1f0534448d205365796d68873a0068c6b',\n",
       "    'title': 'A Survey of Zero-Shot Learning'},\n",
       "   {'paperId': 'b2174399c04a7d894bcd2dc7848a35aed4c67f80',\n",
       "    'title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172',\n",
       "    'title': 'Sim-to-Real Transfer of Robotic Control with Dynamics Randomization'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': '5d1864d759d272c5dc928b641d113527a3e81f99',\n",
       "    'title': 'Domain randomization for transferring deep neural networks from simulation to the real world'},\n",
       "   {'paperId': 'b9f8a1a9a5aec3dcdd155c4594f25274f6418e11',\n",
       "    'title': 'Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning'},\n",
       "   {'paperId': '3ed67ded2b4d3614b38798b3f17a8e69803d0980',\n",
       "    'title': 'Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'}],\n",
       "  'citations': [{'paperId': '65686bed452b981129b8384bad502041181f5917',\n",
       "    'title': 'Reinforced Meta-Learning Method for Shape-Dependent Regulation of Cutting Force in Pork Carcass Operation Robots'},\n",
       "   {'paperId': 'c74ba54cc1b29a99619f8d3cf13b8a35e3307396',\n",
       "    'title': 'Obstacle Avoidance for Robotic Manipulator in Joint Space via Improved Proximal Policy Optimization'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6': {'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '4e93cea327e1420078f09d3377e4ff3e51eade5a',\n",
       "    'title': 'LASER: Learning a Latent Action Space for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '39cc9292cb602970453c7677ada5d575d03d1d77',\n",
       "    'title': 'SKID RAW: Skill Discovery From Raw Trajectories'},\n",
       "   {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "    'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': 'b44bb1762640ed72091fd5f5fdc20719a6dc24af',\n",
       "    'title': 'Mastering Atari with Discrete World Models'},\n",
       "   {'paperId': '5fa8b76256a2125c7a72db372b6e0d6be90d3a54',\n",
       "    'title': 'Neural Dynamic Policies for End-to-End Sensorimotor Learning'},\n",
       "   {'paperId': '67ecbbefe1a18a52f4f3e71f8bd2dbee552548e6',\n",
       "    'title': 'A Long Horizon Planning Framework for Manipulating Rigid Pointcloud Objects'},\n",
       "   {'paperId': 'cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3',\n",
       "    'title': 'PLAS: Latent Action Space for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'd9ceb68a016be1dcd8d246497d7d964ed4b22751',\n",
       "    'title': 'Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'cf34efc663284da131e747407ac3f389f898e471',\n",
       "    'title': 'robosuite: A Modular Simulation Framework and Benchmark for Robot Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '8095bdd5861d1dbe43b77997bc0dbc2fd51acb93',\n",
       "    'title': 'Grounding Language in Play'},\n",
       "   {'paperId': '3a71c306eb6232658c9e5fd48aed1ef3befe5fbe',\n",
       "    'title': 'Planning to Explore via Self-Supervised World Models'},\n",
       "   {'paperId': '744139d65c3bf6da6a6acd384a32d94a06f44f62',\n",
       "    'title': 'Reinforcement Learning with Augmented Data'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '0cc956565c7d249d4197eeb1dbab6523c648b2c9',\n",
       "    'title': 'Dream to Control: Learning Behaviors by Latent Imagination'},\n",
       "   {'paperId': '5bf83e8224dae5f123e42b07eb9363ec120d7eba',\n",
       "    'title': 'Efficient Bimanual Manipulation Using Learned Task Schemas'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Learning dexterous in-hand manipulation. The International'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '0bc855f84668b35cb65618d996d09f6e434d28c9',\n",
       "    'title': 'Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning'},\n",
       "   {'paperId': '7a450675968d31b8363e21fb5d5b72474c128076',\n",
       "    'title': 'Deep Dynamics Models for Learning Dexterous Manipulation'},\n",
       "   {'paperId': '9e0e385662961562324482860dca531413d5c4af',\n",
       "    'title': 'Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks'},\n",
       "   {'paperId': 'ec80965b72de5076a183026ba182068f6e0a928a',\n",
       "    'title': 'Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '36d83559c34d67ee09ab0bdaf88fdb1c6dfec20b',\n",
       "    'title': 'Hybrid Actor-Critic Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '7cd316505f52aa337ef8a2aff10bc6bf1df561d0', 'title': 'and s'},\n",
       "   {'paperId': '1f9160fb21d80498842f137181ab12f1beeaa1f4',\n",
       "    'title': 'Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': '328233806be18fe15176ce71debaecd95e771fcf',\n",
       "    'title': 'Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space'},\n",
       "   {'paperId': '032db195efd97fe2bcd20c4ad04628c70ff4e79c',\n",
       "    'title': 'and a at'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Pytorch implementations of reinforcement learning algorithms'},\n",
       "   {'paperId': '28d68b2064607c02dd1e549e870a894b11674f48',\n",
       "    'title': 'Learning composable models of parameterized skills'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'b23c97e59f44bc88121c65d1ac41f2cbddcefbd2',\n",
       "    'title': 'Deep Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': '3f59bce00434b432dfd0b9ab20903acadaefd456',\n",
       "    'title': 'Reinforcement Learning with Parameterized Actions'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': 'bc6dff14a130c57a91d5a21339c23471faf1d46f', 'title': 'Et al'},\n",
       "   {'paperId': '3d2218b17e7898a222e5fc2079a3f1531990708f', 'title': 'I and J'},\n",
       "   {'paperId': '9b7ae896675c71ac50fa1fbc555cb19f80863f0e',\n",
       "    'title': 'Hierarchical task and motion planning in the now'},\n",
       "   {'paperId': '5cbfbbca3a1ea8ee39254dd4ef07b3d67761c39a',\n",
       "    'title': 'Relative Entropy Policy Search'},\n",
       "   {'paperId': '0645c8792d5095f0de45e95c5fd9de5238f426e6',\n",
       "    'title': 'Learning motor primitives for robotics'},\n",
       "   {'paperId': '56f2d1c73574679261d2d414a0d517afa8967438',\n",
       "    'title': 'A Hybrid Approach to Intricate Motion, Manipulation and Task Planning'},\n",
       "   {'paperId': '2065d9eb28be0700a235afb78e4a073845bfb67d',\n",
       "    'title': 'Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics'},\n",
       "   {'paperId': '38688edefc7591ea2fc7d4294070e8bfe9d9ac3d',\n",
       "    'title': 'Learning Attractor Landscapes for Learning Motor Primitives'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '9696cc9905a8d84b6786c1f1cfef963dc35069a3',\n",
       "    'title': 'Control of systems integrating logic, dynamics, and constraints'},\n",
       "   {'paperId': '697bca60bd1d9624d2087ccede90740bad6e28cc',\n",
       "    'title': 'A unified framework for hybrid control: model and optimal control theory'},\n",
       "   {'paperId': '33576c0fc316c45c3672523114b20a5bb996e1f4',\n",
       "    'title': 'A unified approach for motion and force control of robot manipulators: The operational space formulation'}],\n",
       "  'citations': [{'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': 'a3711dbf296b5ddd97ba93826660cd3995611625',\n",
       "    'title': 'Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation'},\n",
       "   {'paperId': '253b41369d003952874c6a47a6038277b165cfa0',\n",
       "    'title': 'Affordances from Human Videos as a Versatile Representation for Robotics'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': '612c706a66fccd67255e469867962248725ca4a3',\n",
       "    'title': 'Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation'},\n",
       "   {'paperId': 'd914dc7f5d9291ee2127936e3206c90ca1fcea71',\n",
       "    'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation'},\n",
       "   {'paperId': '69951c75efb9ff615fb31eb4b6b42d4cd1ded58f',\n",
       "    'title': 'Reward Relabelling for combined Reinforcement and Imitation Learning on sparse-reward tasks'},\n",
       "   {'paperId': '93eb4901d10206cf39514b489813b733cf539b04',\n",
       "    'title': 'A thermodynamical model of non-deterministic computation in cortical neural networks'},\n",
       "   {'paperId': 'bf90ed116f407fb4cecc89f7763634c4028bdebb',\n",
       "    'title': 'Variable-Decision Frequency Option Critic'},\n",
       "   {'paperId': 'e83d5f67f173b83dfea284ce072ba16976d5573b',\n",
       "    'title': 'A Simluation Model for Robot System Selection using Task Primitives for Expressing Actions and Data Structures'},\n",
       "   {'paperId': 'e0eb9870a6c105cadd92cae8f5218b2a84955849',\n",
       "    'title': 'Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks'},\n",
       "   {'paperId': '3d51d8dcbdb33a3b74fbfc92680f1ded4121185b',\n",
       "    'title': 'Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks'},\n",
       "   {'paperId': '7b48db10b65d7bfbf59a74d9fc100f08159c3245',\n",
       "    'title': 'UAV-borne remote sensing for AI-assisted support of search and rescue missions'},\n",
       "   {'paperId': 'f66a13803d4c872a3dae6a41491818907e0c1dac',\n",
       "    'title': 'Planning with Spatial-Temporal Abstraction from Point Clouds for Deformable Object Manipulation'},\n",
       "   {'paperId': '5572521b26ed3efcaafe050975b404fc5e7c89a7',\n",
       "    'title': 'Guided Skill Learning and Abstraction for Long-Horizon Manipulation'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '699901877c89c9d29c88d5067559ba5d0bcf3e41',\n",
       "    'title': 'Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '78839ec995beab7f5fa8ce8d549fb4cf04b33d45',\n",
       "    'title': 'Meta-Learning Parameterized Skills'},\n",
       "   {'paperId': '4ca19525f996c9e60cca4f7e04ea76a4a1160a11',\n",
       "    'title': 'PLATO: Predicting Latent Affordances Through Object-Centric Play'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'},\n",
       "   {'paperId': '5ee839d965a1596298895ace7d003b98e165962c',\n",
       "    'title': 'Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '09c04311402819d6171cdb47b2a381f10d4f564e',\n",
       "    'title': 'Hierarchical Primitive Composition: Simultaneous Activation of Skills with Inconsistent Action Dimensions in Multiple Hierarchies'},\n",
       "   {'paperId': 'c9e602b5f1df92388e0b6755e7c851f2ec8d7495',\n",
       "    'title': 'Cooperative Multi-Robot Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f08b57993f63392379cb3ffa1145ae14ac8682c2',\n",
       "    'title': 'D EEP S EQUENCED L INEAR D YNAMICAL S YSTEMS FOR M ANIPULATION P OLICY L EARNING'}],\n",
       "  'citnuminlist': 3,\n",
       "  'refnuminlist': 11,\n",
       "  'isKeypaper': True},\n",
       " '372715a73955b7fbc1daf816bd52c0641b3ff5f2': {'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '0cd7b89035be937006c0130077c3e685d9fd4a4d',\n",
       "    'title': 'Not your grandmother’s toolbox – the Robotics Toolbox reinvented for Python'},\n",
       "   {'paperId': '400811ee31020a3f002551476dac25973e13035e',\n",
       "    'title': 'How to train your robot with deep reinforcement learning: lessons we have learned'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'd052f58cfa47ff972280063682a78516aa500353',\n",
       "    'title': 'Learning Dexterous Manipulation from Suboptimal Experts'},\n",
       "   {'paperId': 'b4873e3a17058c81a8d2bba838cdd0c415ee80e7',\n",
       "    'title': 'Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning'},\n",
       "   {'paperId': '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0',\n",
       "    'title': 'Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer'},\n",
       "   {'paperId': '6e05b33face941e235439ef75cadb56dad5c1746',\n",
       "    'title': 'A Purely-Reactive Manipulability-Maximising Motion Controller.'},\n",
       "   {'paperId': 'f0a53f2516ef6c62277fcea3b25e4dbbdd8a0aa2',\n",
       "    'title': 'Reinforcement Learning from Imperfect Demonstrations under Soft Expert Guidance'},\n",
       "   {'paperId': 'fc1655d4ee203857fb0c839667d00288209753d9',\n",
       "    'title': 'Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments'},\n",
       "   {'paperId': '92e0c1697bc4630903501185d11676f9542c9c80',\n",
       "    'title': 'Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards'},\n",
       "   {'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': 'fd91275dea1ed75d2021599f9e2ff6d0eff62103',\n",
       "    'title': '2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)'},\n",
       "   {'paperId': '6ecc2f4cf7dddea9a9466a473173492fdab2eee6',\n",
       "    'title': 'PyRep: Bringing V-REP to Deep Robot Learning'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': 'a62158491e3c88712277a8947a736ed8f17bbd4c',\n",
       "    'title': 'Control Regularization for Reduced Variance Reinforcement Learning'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': 'ae4d32f05cf40e4cc01c69d7787149a258c95eda',\n",
       "    'title': 'Residual Reinforcement Learning for Robot Control'},\n",
       "   {'paperId': '2ed619fbc7902155d54f6f21da16ad6c120eac63',\n",
       "    'title': 'Learning to Walk via Deep Reinforcement Learning'},\n",
       "   {'paperId': 'd4b34b12d3515680837c024bfa53fd370628aa51',\n",
       "    'title': 'Composing Entropic Policies using Divergence Correction'},\n",
       "   {'paperId': 'a441728f9fd6af1946368240162a72c2028c8cb1',\n",
       "    'title': 'Deep Exploration via Randomized Value Functions'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': 'c78c8a921a4171b373e7a298d2803940c935ee34',\n",
       "    'title': 'Policies Modulating Trajectory Generators'},\n",
       "   {'paperId': '1a1a22a3281e00d4ffd69b6490abc266d4eed0b3',\n",
       "    'title': 'Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning'},\n",
       "   {'paperId': '6654ba1d3e61cdf5f4decc7464436046cf602ed1',\n",
       "    'title': 'On Evaluation of Embodied Navigation Agents'},\n",
       "   {'paperId': 'c979efe1f0a8b0b343ea332368e5b51dc153c522',\n",
       "    'title': 'Policy Optimization with Demonstrations'},\n",
       "   {'paperId': '0c287aad52920c208df393b8e7dee7781a965ae8',\n",
       "    'title': 'Using Finite State Machines in Introductory Robotics'},\n",
       "   {'paperId': '4debb99c0c63bfaa97dd433bc2828e4dac81c48b',\n",
       "    'title': 'Addressing Function Approximation Error in Actor-Critic Methods'},\n",
       "   {'paperId': 'a6a16d202b0c904ce0cd5b7d208e3d2bc6a42eda',\n",
       "    'title': 'Structured Control Nets for Deep Reinforcement Learning'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': '9830c9d16293f8f87f998aa449143f0ed1554d1a',\n",
       "    'title': 'Behavior Trees in Robotics and AI: An Introduction'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': '4f4556e75ff5a1aaad4123a46b1010eb09bfdf19',\n",
       "    'title': 'Neuroscience-Inspired Artificial Intelligence'},\n",
       "   {'paperId': 'cf90552b5d2e992e93ab838fd615e1c36618e31c',\n",
       "    'title': 'Distral: Robust multitask reinforcement learning'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '34dc259e2d08ee0b6b8455d6a0d68a59eaf9d9d1',\n",
       "    'title': 'How Behavior Trees Modularize Hybrid Control Systems and Generalize Sequential Behavior Compositions, the Subsumption Architecture, and Decision Trees'},\n",
       "   {'paperId': 'dcdb8fab7b127eea0198b0622dda05557c8b4262',\n",
       "    'title': 'Deep predictive policy training using reinforcement learning'},\n",
       "   {'paperId': '802168a81571dde28f5ddb94d84677bc007afa7b',\n",
       "    'title': 'Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles'},\n",
       "   {'paperId': '3b9732bb07dc99bde5e1f9f75251c6ea5039373e',\n",
       "    'title': 'Deep Reinforcement Learning with Double Q-Learning'},\n",
       "   {'paperId': 'e6394850b166fa8d98439ed7dcd6d99bc3ad53c7',\n",
       "    'title': 'Springer Handbook of Robotics'},\n",
       "   {'paperId': '7b92a9121ba996c41144b65df8f5d0d758e47ab7',\n",
       "    'title': 'Manipulator Performance Measures - A Comprehensive Literature Survey'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': '066637b8401d4ad24b89f8daed7224683d18ca92',\n",
       "    'title': 'Manipulator Performance Measures - A Comprehensive Literature Survey'},\n",
       "   {'paperId': 'd911976f1fced41125164250f9279edf1e672871',\n",
       "    'title': 'Neural Computations Underlying Arbitration between Model-Based and Model-free Learning'},\n",
       "   {'paperId': 'c268b5d0ba92192597b82c7166eaf40d0a2f40c5',\n",
       "    'title': 'Speed/Accuracy Trade-Off between the Habitual and the Goal-Directed Processes'},\n",
       "   {'paperId': '94ab352de84d86f854a79f6e7cf8f5a2a57f852f',\n",
       "    'title': 'An Introduction to Reinforcement Learning'},\n",
       "   {'paperId': '340225326a604bffd4b4c15f14ace8bdc24789df',\n",
       "    'title': 'Control Systems Engineering'},\n",
       "   {'paperId': 'fc305a94648ce083799ff1e5e1e9e339b2ae316b',\n",
       "    'title': 'Theoretical and Empirical Analysis of Reward Shaping in Reinforcement Learning'},\n",
       "   {'paperId': '8de174ab5419b9d3127695405efd079808e956e8',\n",
       "    'title': 'Curriculum learning'},\n",
       "   {'paperId': '5fff3cc67bc1e27fca6f92aba00056d17f52e5a7',\n",
       "    'title': 'Decision theory, reinforcement learning, and the brain'},\n",
       "   {'paperId': '07e880c468301a9d5b85718eee029a3cba21e5e0',\n",
       "    'title': 'Central pattern generators for locomotion control in animals and robots: A review'},\n",
       "   {'paperId': '9a0cb6ea07d772f26bdfea3515f1f1e38f32ee5f',\n",
       "    'title': 'Probabilistic policy reuse in a reinforcement learning agent'},\n",
       "   {'paperId': '767ed9b4c974411b46c445bd3e235515760ecc65',\n",
       "    'title': 'Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': '23b367e9e3945455216f1f48d86a98bfa8d4ec3b',\n",
       "    'title': 'Model predictive control'},\n",
       "   {'paperId': 'ff8b697fd9b43297c0f24757c9d2915c324a8ed9',\n",
       "    'title': 'The Role of Learning in the Operation of Motivational Systems'},\n",
       "   {'paperId': 'f61117966b1ef9cf9f016e62bca19509dae33a9b',\n",
       "    'title': 'Essentials of Robust Control'},\n",
       "   {'paperId': '59b50a775542e87f078db35b868ac10ab43d4c75',\n",
       "    'title': 'Learning from delayed rewards'},\n",
       "   {'paperId': '07cc4371c8d3ddf7de0189f73227c3f0d896d66d',\n",
       "    'title': 'Potential field methods and their inherent limitations for mobile robot navigation'},\n",
       "   {'paperId': 'f35cd5246e0ca7bc493c91ae61a78d1589670367',\n",
       "    'title': 'Global path planning using artificial potential fields'},\n",
       "   {'paperId': '9d5085f3008a60abd7e53bc7f16963c4db32e342',\n",
       "    'title': 'Manipulability of Robotic Mechanisms'},\n",
       "   {'paperId': 'c8a04d0cbb9f70e86800b11b594c9a05d7b6bac0',\n",
       "    'title': 'Real-Time Obstacle Avoidance for Manipulators and Mobile Robots'},\n",
       "   {'paperId': '977706d1dc022280f47a2c67c646e85f38d88fe2',\n",
       "    'title': 'Optimal control theory : an introduction'},\n",
       "   {'paperId': '6e0149c673e73929e6e324e3269f1b33659eb196',\n",
       "    'title': 'Resolved Motion Rate Control of Manipulators and Human Prostheses'},\n",
       "   {'paperId': '28ae247c873440cba0f331cb8c2ca887403dc630',\n",
       "    'title': 'I. On governors'},\n",
       "   {'paperId': None,\n",
       "    'title': 'posing entropic policies using divergence correction , ” in International Conference on Machine Learning'}],\n",
       "  'citations': [{'paperId': 'bd0f444346bb98c5df1e7cf8b3437094676a9d58',\n",
       "    'title': 'Robotic Packaging Optimization with Reinforcement Learning'},\n",
       "   {'paperId': '0b64a139f6f38105f253e85ae88a9885c38ca063',\n",
       "    'title': 'Energy Optimization of Wind Turbines via a Neural Control Policy Based on Reinforcement Learning Markov Chain Monte Carlo Algorithm'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'e20f254ff106c7dd800a7019263d1370329d80e7',\n",
       "    'title': 'Renaissance Robot: Optimal Transport Policy Fusion for Learning Diverse Skills'},\n",
       "   {'paperId': '6d62781c7ebc9269d6ff27b0d8011a837238f3d6',\n",
       "    'title': 'Learning to Drive Using Sparse Imitation Reinforcement Learning'},\n",
       "   {'paperId': 'b34e47bde3f49b42bf08956f9f323324c26cf5a7',\n",
       "    'title': 'Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning'},\n",
       "   {'paperId': '7719b94fa8f2b27926dfbb1a44fac909d469d26c',\n",
       "    'title': 'Verification of safety critical control policies using kernel methods'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'}],\n",
       "  'citnuminlist': 3,\n",
       "  'refnuminlist': 3,\n",
       "  'isKeypaper': True},\n",
       " '105f44c9d445de2b93d1297c2d5ac10cc776d654': {'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': 'f204041dd567025217adc8070ca292e89cc80488',\n",
       "    'title': 'COG: Connecting New Skills to Past Experience with Offline Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'de46f4e4613364792bbd13f185c381ab656a27ef',\n",
       "    'title': 'RL Unplugged: Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '121cca1bb7cec48fa9080801927f50d99193eae6',\n",
       "    'title': 'Learning to Coordinate Manipulation Skills via Skill Behavior Diversification'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': '5759a53418ae3fe74ce96c531617914e7656e45e',\n",
       "    'title': 'On the Variance of the Adaptive Learning Rate and Beyond'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '4f0c4189d9a82f94dcd84fe879cfbe124aaf270b',\n",
       "    'title': 'Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'The starting position is sampled uniformly from a start region and the agent receives a one-time sparse reward of 100 when reaching the fixed goal'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '1de1e749668a65cf6b88b8138389581108bb129a',\n",
       "    'title': 'Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': 'c01566fa915db4a4d2636bf6b03a36e203e93845',\n",
       "    'title': 'ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'd356a5603f14c7a6873272774782d7812871f952',\n",
       "    'title': 'Reinforcement and Imitation Learning for Diverse Visuomotor Skills'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '5e2c4e7b3302549b3718601c44d9af6c7554efef',\n",
       "    'title': 'Learning Robust Rewards with Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': None, 'title': 'and M'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': 'a31d0e5668b311b1ec74c5607a9f96c35b395fa8',\n",
       "    'title': 'A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'bc6dff14a130c57a91d5a21339c23471faf1d46f', 'title': 'Et al'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': '99cdaebac3e3f28e3fabfa4bbfec111e59dea2a0',\n",
       "    'title': 'What is modelled during observational learning?'},\n",
       "   {'paperId': 'f65020fc3b1692d7989e099d6b6e698be5a50a93',\n",
       "    'title': 'Apprenticeship learning via inverse reinforcement learning'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': '8f1db4925c3ac6adfbc3e5c1972cf2a7b6c2d7b2',\n",
       "    'title': \"Specificity of Task Constraints and Effects of Visual Demonstrations and Verbal Instructions in Directing Learners' Search During Skill Acquisition\"},\n",
       "   {'paperId': 'd113305e1e3eb1359627209157c2b974330c5bd0',\n",
       "    'title': 'Imitation of gestures in children is goal-directed.'},\n",
       "   {'paperId': '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3',\n",
       "    'title': 'ALVINN: An Autonomous Land Vehicle in a Neural Network'}],\n",
       "  'citations': [{'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': '603543f649e283c54ba7d519cffb71754d21171e',\n",
       "    'title': 'Bridging Action Space Mismatch in Learning from Demonstrations'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': 'da592b4751859dd5d82bc833fecce00cb8b4315e',\n",
       "    'title': 'Robot programming by demonstration with a monocular RGB camera'},\n",
       "   {'paperId': 'ca137b4f6ba25ca2f26952044c6a2d06ae3e607f',\n",
       "    'title': 'Cross-Domain Transfer via Semantic Skill Imitation'},\n",
       "   {'paperId': 'db8d70d9da6b957a00ec7e8cc67493340c39aa29',\n",
       "    'title': 'Policy Transfer via Skill Adaptation and Composition'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '706f9da5bde61a24a98336dc1b9dce2e4f81a21c',\n",
       "    'title': 'H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': '22b816129ca770df3a88e76f754218b242df43f9',\n",
       "    'title': 'Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization'},\n",
       "   {'paperId': 'b9a3fb062ac940012f69e9cca5f34bb717370c3a',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '951da1a5a0c2bfb6f2d16dccfc488acb71f1447f',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Data for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': '5840bf765be8c3bcedab63f43f5982ddba26eaf9',\n",
       "    'title': 'SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'}],\n",
       "  'citnuminlist': 5,\n",
       "  'refnuminlist': 10,\n",
       "  'isKeypaper': True},\n",
       " '259b4f5ed43fda5dd3510821b40fac13021e7605': {'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4',\n",
       "    'title': 'Learning Transferable Visual Models From Natural Language Supervision'},\n",
       "   {'paperId': '0fd47bf484a05001ce787747cf5a879b9202ebfa',\n",
       "    'title': 'ViNG: Learning Open-World Navigation with Visual Goals'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '17985b57240bfaea02a6098a7a34e71e780180eb',\n",
       "    'title': 'Decoupling Representation Learning from Reinforcement Learning'},\n",
       "   {'paperId': '5fa8b76256a2125c7a72db372b6e0d6be90d3a54',\n",
       "    'title': 'Neural Dynamic Policies for End-to-End Sensorimotor Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '6b85b63579a916f705a8e10a49bd8d849d91b1fc',\n",
       "    'title': 'Language Models are Few-Shot Learners'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '79e14a09ff070e06ab9df598ccd885b929164ef9',\n",
       "    'title': 'CURL: Contrastive Unsupervised Representations for Reinforcement Learning'},\n",
       "   {'paperId': '429546ffa120a6c488167624ff561e685dcc63b3',\n",
       "    'title': 'Sparse Graphical Memory for Robust Planning'},\n",
       "   {'paperId': 'b8f0417844523d788630bb28a600dfeb74914e2d',\n",
       "    'title': 'Hallucinative Topological Memory for Zero-Shot Visual Planning'},\n",
       "   {'paperId': '34733eaf66007516347a40ad5d9bbe1cc9dacb6b',\n",
       "    'title': 'A Simple Framework for Contrastive Learning of Visual Representations'},\n",
       "   {'paperId': 'add2f205338d70e10ce5e686df4a690e2851bdfc',\n",
       "    'title': 'Momentum Contrast for Unsupervised Visual Representation Learning'},\n",
       "   {'paperId': '8c66f90f0ca7f12847a6152f4fe8d4e10eb34162',\n",
       "    'title': 'A Survey on Policy Search Algorithms for Learning Robot Controllers in a Handful of Trials'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'ad14227e4f51276892ffc37aa43fd8750bb5eba8',\n",
       "    'title': 'Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': 'c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3',\n",
       "    'title': 'Generalizing from a Few Examples: A Survey on Few-Shot Learning'},\n",
       "   {'paperId': '9405cc0d6169988371b2755e573cc28650d14dfe',\n",
       "    'title': 'Language Models are Unsupervised Multitask Learners'},\n",
       "   {'paperId': None,\n",
       "    'title': 'The pool of demonstrations are downloaded from the repository'},\n",
       "   {'paperId': 'b227f3e4c0dc96e5ac5426b85485a70f2175a205',\n",
       "    'title': 'Representation Learning with Contrastive Predictive Coding'},\n",
       "   {'paperId': '8d6adaa16ed0af9935a1130a305c85e8bdf8780d',\n",
       "    'title': 'An Algorithmic Perspective on Imitation Learning'},\n",
       "   {'paperId': '1064bb7a5e1cfe0004192b0db300d1c5f4a300c3',\n",
       "    'title': 'Semi-parametric Topological Memory for Navigation'},\n",
       "   {'paperId': 'b864f89eaa91120e04e8c62eb0b36568ab4244a8',\n",
       "    'title': 'Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation'},\n",
       "   {'paperId': 'ab68bd6f47bfa8744f0f39be8c163d28203eefa2',\n",
       "    'title': 'Bayesian Optimization with Automatic Prior Selection for Data-Efficient Direct Policy Search'},\n",
       "   {'paperId': '482c0cbfffa77154e3c879c497f50b605297d5bc',\n",
       "    'title': 'One-Shot Visual Imitation Learning via Meta-Learning'},\n",
       "   {'paperId': '5c57bb5630835a05eb1c3d0df3e12d6180d75de2',\n",
       "    'title': 'One-Shot Imitation Learning'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '954b01151ff13aef416d27adc60cd9a076753b1a',\n",
       "    'title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning'},\n",
       "   {'paperId': 'be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6',\n",
       "    'title': 'Matching Networks for One Shot Learning'},\n",
       "   {'paperId': '1a632fb89b6b05dc16fbc026d86e390e22ca6ac3',\n",
       "    'title': 'Robots that can adapt like animals'},\n",
       "   {'paperId': 'f216444d4f2959b4520c61d20003fa30a199670a',\n",
       "    'title': 'Siamese Neural Networks for One-Shot Image Recognition'},\n",
       "   {'paperId': '58a7acac2a4ac240e293752be4ffd46f786e5293',\n",
       "    'title': 'Robot Skill Learning: From Reinforcement Learning to Evolution Strategies'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': None, 'title': 'Survey: Robot programming by demonstration'},\n",
       "   {'paperId': '3bb5a439a0d610a7eac68f73068cdd278b8c9775',\n",
       "    'title': 'Pattern Recognition and Machine Learning'},\n",
       "   {'paperId': 'b05b67aca720d0bc39bc9afad02a19f522c7a1bc',\n",
       "    'title': 'Pharmacokinetics of a novel formulation of ivermectin after administration to goats'},\n",
       "   {'paperId': '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3',\n",
       "    'title': 'ALVINN: An Autonomous Land Vehicle in a Neural Network'},\n",
       "   {'paperId': None, 'title': 'Bottom Burner, Slide Cabinet, Hinge Cabinet 4'},\n",
       "   {'paperId': None,\n",
       "    'title': 'With all subtasks seen in the skill dataset, FIST is able to imitate a long-horizon task in the kitchen environment'}],\n",
       "  'citations': [{'paperId': 'fdffeff76d839dbd92088f59ebf70207a54274f0',\n",
       "    'title': 'PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining'},\n",
       "   {'paperId': '03c7f9a6b743eb1c9127b8d4552147b2b1a0c1e1',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Efficient Exploration for Task Automation of Surgical Robot'},\n",
       "   {'paperId': '7ccfd68dc071d6060962b17dd25b0c7fbfd58a9e',\n",
       "    'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures'},\n",
       "   {'paperId': 'ca137b4f6ba25ca2f26952044c6a2d06ae3e607f',\n",
       "    'title': 'Cross-Domain Transfer via Semantic Skill Imitation'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'df83d232cecb1c9977af7fd1e3e34fcececcada0',\n",
       "    'title': 'Hands-on Reinforcement Learning for Recommender Systems - From Bandits to SlateQ to Offline RL with Ray RLlib'},\n",
       "   {'paperId': '48b01b01d340efd148444f42aeb38a5b3ae1bdfd',\n",
       "    'title': 'Towards Informed Design and Validation Assistance in Computer Games Using Imitation Learning'},\n",
       "   {'paperId': '59f91478c1f63b0fb5628bbd1af8267792708443',\n",
       "    'title': 'Intra-agent speech permits zero-shot task acquisition'},\n",
       "   {'paperId': '4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "    'title': 'Chain of Thought Imitation with Procedure Cloning'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'}],\n",
       "  'citnuminlist': 2,\n",
       "  'refnuminlist': 3,\n",
       "  'isKeypaper': True},\n",
       " '45afe2d85f2896ce569be0d27678edcff68017e2': {'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': 'd85d16b003955c6996fafacea7f3c075c531225f',\n",
       "    'title': 'Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'ff0282b34d758a4aaad524ea554f6545852e3c68',\n",
       "    'title': 'ACRONYM: A Large-Scale Grasp Dataset Based on Simulation'},\n",
       "   {'paperId': '091810ee7c0a2a550f84c16dfef796d736298454',\n",
       "    'title': 'Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds'},\n",
       "   {'paperId': '67ecbbefe1a18a52f4f3e71f8bd2dbee552548e6',\n",
       "    'title': 'A Long Horizon Planning Framework for Manipulating Rigid Pointcloud Objects'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '70bc865301a87cba2ddd892f6a5ee88368c1b271',\n",
       "    'title': 'Perceive, Predict, and Plan: Safe Motion Planning Through Interpretable Semantic Representations'},\n",
       "   {'paperId': '2cd29e4bb050769dc07b30a3cdfb956656110004',\n",
       "    'title': 'CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations'},\n",
       "   {'paperId': '2d1a894f4ba4090c993acb76ed333846c686345a',\n",
       "    'title': 'Learning RGB-D Feature Embeddings for Unseen Object Instance Segmentation'},\n",
       "   {'paperId': 'b40f74087b7e069327ca1d29fd20d7065647ee64',\n",
       "    'title': 'Grasping in the Wild: Learning 6DoF Closed-Loop Grasping From Low-Cost Demonstrations'},\n",
       "   {'paperId': 'c513fdc39f0a563a643e0e5c5ebfb1a1d30d161f',\n",
       "    'title': '6-DOF Grasping for Target-driven Object Manipulation in Clutter'},\n",
       "   {'paperId': '1e5c72f9c43d37cb9fbabf192bbccf140bd904ab',\n",
       "    'title': 'Manipulation Trajectory Optimization with Online Grasp Synthesis and Selection'},\n",
       "   {'paperId': '5e9764f45e7ea6206594deb94753a5cad4e31a1a',\n",
       "    'title': 'IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data'},\n",
       "   {'paperId': '56e33fa93a16e5fe1d0b54a3bb51a1548812a6f2',\n",
       "    'title': 'Toward Sim-to-Real Directional Semantic Grasping'},\n",
       "   {'paperId': 'e5fecefe78932654e4cbdfe828bb45cd1e09009e',\n",
       "    'title': '6-DOF GraspNet: Variational Grasp Generation for Object Manipulation'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '8c21a1d8844e6b58fd74b2b94c512a23497029c1',\n",
       "    'title': 'Residual Policy Learning'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '06788f2b13aa1673707780d9531b8318a3942cde',\n",
       "    'title': 'Closing the Loop for Robotic Grasping: A Real-time, Generative Grasp Synthesis Approach'},\n",
       "   {'paperId': '55e5368377e07038c062e088e074e59453cd43d9',\n",
       "    'title': 'Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods'},\n",
       "   {'paperId': '4debb99c0c63bfaa97dd433bc2828e4dac81c48b',\n",
       "    'title': 'Addressing Function Approximation Error in Actor-Critic Methods'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': '3f0e3c09eeab2a66201cb2819740871fd7e4bd54',\n",
       "    'title': 'Real-Time Perception Meets Reactive Motion Generation'},\n",
       "   {'paperId': '494e2d5b40dcebde349f9872c7317e5003f9c5d2',\n",
       "    'title': 'Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection'},\n",
       "   {'paperId': '249080163f1a7a5fc24e7e19b815d8a5db95805d',\n",
       "    'title': 'The complexities of grasping in the wild'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': '8674494bd7a076286b905912d26d47f7501c4046',\n",
       "    'title': 'PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space'},\n",
       "   {'paperId': 'a59658d7b74f63a34e7182addbba1214775f49f5',\n",
       "    'title': 'DART: Noise Injection for Robust Imitation Learning'},\n",
       "   {'paperId': '41f2a087031944f9b990eb102f59b4ff58d6b5ef',\n",
       "    'title': 'Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics'},\n",
       "   {'paperId': '4135004c75a361c91311314fc588d229a7107526',\n",
       "    'title': 'InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations'},\n",
       "   {'paperId': 'd997beefc0922d97202789d2ac307c55c2c52fba',\n",
       "    'title': 'PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': 'f03b4ff1b4943691cec703b508c0a91f2d97a881',\n",
       "    'title': 'Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours'},\n",
       "   {'paperId': '3b9732bb07dc99bde5e1f9f75251c6ea5039373e',\n",
       "    'title': 'Deep Reinforcement Learning with Double Q-Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'PyBullet, a python module for physics simulation for games, robotics and machine learning'},\n",
       "   {'paperId': '9b686d76914befea66377ec79c1f9258d70ea7e3',\n",
       "    'title': 'ShapeNet: An Information-Rich 3D Model Repository'},\n",
       "   {'paperId': '265ba57e5423c93bc30842a5a41c3b06393d6c5e',\n",
       "    'title': 'Physics-based trajectory optimization for grasping in cluttered environments'},\n",
       "   {'paperId': '0f899b92b7fb03b609fee887e4b6f3b633eaf30d',\n",
       "    'title': 'Variational Inference with Normalizing Flows'},\n",
       "   {'paperId': 'fa88d45d86bb280c177037f9a111ad594fae43ed',\n",
       "    'title': 'Benchmarking in Manipulation Research: The YCB Object and Model Set and Benchmarking Protocols'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'e402c388af4dccf616a7adf7bf334c0d2e6ff948',\n",
       "    'title': 'Real-time grasp detection using convolutional neural networks'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': '06d7d9636ef700874359003d1046eb271ce2ed95',\n",
       "    'title': 'Data-Driven Grasp Synthesis—A Survey'},\n",
       "   {'paperId': '9b9cf335ff2f0260aa003185841d090ef9475f30',\n",
       "    'title': 'A Framework for Push-Grasping in Clutter'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '1c46943103bd7b7a2c7be86859995a4144d1938b',\n",
       "    'title': 'Visualizing Data using t-SNE'},\n",
       "   {'paperId': 'e420d30555e0f751fdff3735bc7be3d98021aae2',\n",
       "    'title': 'Graspit! A versatile simulator for robotic grasping'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': 'fe9fe9f15f24fbbb19b62bcd9a3418511a699b84',\n",
       "    'title': 'Policy Representation via Diffusion Probability Model for Reinforcement Learning'},\n",
       "   {'paperId': '3de1b80bb4cde51e50e7cb39979cf6697c25e940',\n",
       "    'title': 'A Fast 6DOF Visual Selective Grasping System Using Point Clouds'},\n",
       "   {'paperId': '9a821d85b9e97466dc0ba19d4087cc7114b555f3',\n",
       "    'title': 'Learning-based robotic grasping: A review'},\n",
       "   {'paperId': '4788b4041596262b13b7c95c085692520db49976',\n",
       "    'title': 'Recent trends in task and motion planning for robotics: A Survey'},\n",
       "   {'paperId': '86b2eb59f89adcdfb8a5ea6968564182dac3ca4b',\n",
       "    'title': 'NeRF in the Palm of Your Hand: Corrective Augmentation for Robotics via Novel-View Synthesis'},\n",
       "   {'paperId': '0e93f2df7d5a0fb58bad051da38e673da7c7b75f',\n",
       "    'title': 'Where To Start? Transferring Simple Skills to Complex Environments'},\n",
       "   {'paperId': 'afd85d75d9ed910f185573a90f74b69379d5f05b',\n",
       "    'title': 'A Brief Review of Recent Hierarchical Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': 'c9e7e2d0c9c0522a2438b6ddafbd55c93f610587',\n",
       "    'title': 'Neural Grasp Distance Fields for Robot Manipulation'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': '15ac70d077bb735eed4a8502ce49aa7782c803fd',\n",
       "    'title': 'What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': '0791a0441e1f672c43aecb2d6708fbc8725c8cad',\n",
       "    'title': '\"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations'},\n",
       "   {'paperId': 'd94bbe9e4fb0a90565537c47159ddc9806e844bb',\n",
       "    'title': 'Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection'},\n",
       "   {'paperId': 'baeacaf85d2a1c23cc58d0436440e25f7c20c546',\n",
       "    'title': 'CS 6301 Introduction to Robot Manipulation and Navigation Project Proposal Description'},\n",
       "   {'paperId': '9bc048a72d072212f1a3498d91fa64d98cfef3b4',\n",
       "    'title': 'Learning Generalizable Vision-Tactile Robotic Grasping Strategy for Deformable Objects via Transformer'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " '13dfb80b184a6568485fbfd11e5b24d51b0f503f': {'title': 'Hierarchical Skills for Efficient Exploration',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '6be24dd6896353368e9d01418090dbe5211402de',\n",
       "    'title': 'Planning in Learned Latent Action Spaces for Generalizable Legged Locomotion'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'af6de8ee0c8bf4639763b73f205b51dacd490627',\n",
       "    'title': 'D2RL: Deep Dense Architectures in Reinforcement Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '48c64d04c56b14f4b113b8cb85107d8f05a9102a',\n",
       "    'title': 'dm_control: Software and Tasks for Continuous Control'},\n",
       "   {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "    'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '61e57667cea382b72fbc9b1131520cef74bf8624',\n",
       "    'title': 'Discrete and Continuous Action Representation for Practical RL in Video Games'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '622cd0cc96d437481d8ca1de5ea5400e655efcc3',\n",
       "    'title': 'Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '1f9160fb21d80498842f137181ab12f1beeaa1f4',\n",
       "    'title': 'Hierarchical Approaches for Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '904307cb58795241b22cfaa34f560e610997f5c1',\n",
       "    'title': 'Divide-and-Conquer Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Adaptive Computation and Machine Learning Series'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8423cc50c18d68f797adaa4f571f5e4efbe325a5',\n",
       "    'title': 'A Laplacian Framework for Option Discovery in Reinforcement Learning'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf',\n",
       "    'title': '#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': '1464776f20e2bccb6182f183b5ff2e15b0ae5e56',\n",
       "    'title': 'Benchmarking Deep Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': '3f59bce00434b432dfd0b9ab20903acadaefd456',\n",
       "    'title': 'Reinforcement Learning with Parameterized Actions'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': '268f272a34b6d30526d08c31a76c5a0e73523f7d',\n",
       "    'title': 'Swing-twist decomposition in Clifford algebra'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '01c758f27adcb21dde477c384dc88896cbad4f3c',\n",
       "    'title': 'The utility of temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "    'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': '42bb0ac384fb87933be67f63b98d90a45d2fe6e9',\n",
       "    'title': 'Similarity estimation techniques from rounding algorithms'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': '8a7acaf6469c06ae5876d92f013184db5897bb13',\n",
       "    'title': 'Neuronlike adaptive elements that can solve difficult learning control problems'},\n",
       "   {'paperId': '4e02c6d40e24192f2bf6d945bfd1a76ab29367cb',\n",
       "    'title': 'Planning in a Hierarchy of Abstraction Spaces'},\n",
       "   {'paperId': '351bdc21bd5e67e8d41549f9d89e4fcd84438f0f',\n",
       "    'title': 'Learning and Executing Generalized Robot Plans'}],\n",
       "  'citations': [{'paperId': '84c50dfd732b6f4ff5d79d94bb4a905b007ba7c1',\n",
       "    'title': 'Distributional and hierarchical reinforcement learning for physical systems with noisy state observations and exogenous perturbations'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '96f5d475aaf9f812254a39807ef8577f7f6490da',\n",
       "    'title': 'Online Damage Recovery for Physical Robots with Hierarchical Quality-Diversity'},\n",
       "   {'paperId': '4ba1477f9d164efacde197341789b48d92447610',\n",
       "    'title': 'Hierarchical Strategies for Cooperative Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '41e41f650fc1926eda019be894ad96ab56315860',\n",
       "    'title': 'CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control'},\n",
       "   {'paperId': 'cfd232ade1fdee8f00d90a0c1e6148b8ee530e29',\n",
       "    'title': 'Choreographer: Learning and Adapting Skills in Imagination'},\n",
       "   {'paperId': 'ce0e769936453f827aee367e3463bb9915c6d78b',\n",
       "    'title': 'Emergency action termination for immediate reaction in hierarchical reinforcement learning'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '0c39887cf3a153efb3fd2ed0f7e2b9a6dccc973b',\n",
       "    'title': 'Hierarchical quality-diversity for online damage recovery'},\n",
       "   {'paperId': '5762e9c654ae9230db5936f21780ae794a838533',\n",
       "    'title': 'Perceiving the World: Question-guided Reinforcement Learning for Text-based Games'},\n",
       "   {'paperId': '53ae8b06d3f1e93b211724204cfe713fa965659f',\n",
       "    'title': 'Do You Need the Entropy Reward (in Practice)?'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '775f42ed458b8c5b0f2094ea4ff5b64c557b1a34',\n",
       "    'title': 'A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'}],\n",
       "  'citnuminlist': 2,\n",
       "  'refnuminlist': 11,\n",
       "  'isKeypaper': True},\n",
       " '119639e61c1f88c3d675dac2d3cf47530969276d': {'title': 'Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '57d5339c41981347b16bba3718c4cde04899842c',\n",
       "    'title': 'Jerk-limited Real-time Trajectory Generation with Arbitrary Target States'},\n",
       "   {'paperId': '4e93cea327e1420078f09d3377e4ff3e51eade5a',\n",
       "    'title': 'LASER: Learning a Latent Action Space for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '278f8d36029b17bde70fa459cd71fd01879794d6',\n",
       "    'title': 'Learning rich touch representations through cross-modal self-supervision'},\n",
       "   {'paperId': 'cf34efc663284da131e747407ac3f389f898e471',\n",
       "    'title': 'robosuite: A Modular Simulation Framework and Benchmark for Robot Learning'},\n",
       "   {'paperId': 'af233b7ee088557292aaa5bc9d656d604cab79d1',\n",
       "    'title': 'Hierarchical motor adaptations negotiate failures during force field learning'},\n",
       "   {'paperId': '84aa55e2969d5e1fcae9622a1ab1100e1da9205b',\n",
       "    'title': 'Learning Force Control for Contact-Rich Manipulation Tasks With Rigid Position-Controlled Robots'},\n",
       "   {'paperId': 'c86b142d6dccb2e3da2f448f79370f89cc7d6636',\n",
       "    'title': 'Learning Variable Impedance Control for Contact Sensitive Tasks'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Kostrikov. Soft actor-critic (sac) implementation in pytorch'},\n",
       "   {'paperId': 'e1335187f1a8ca1c42b45772c2d193b68944d04e',\n",
       "    'title': 'A Comparison of Action Spaces for Learning Manipulation Tasks'},\n",
       "   {'paperId': '9e0e385662961562324482860dca531413d5c4af',\n",
       "    'title': 'Variable Impedance Control in End-Effector Space: An Action Space for Reinforcement Learning in Contact-Rich Tasks'},\n",
       "   {'paperId': '0d6c52ac0424321e08cee82dd2ccb1fe0e826c01',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': 'c6a15f69cd91e422fcd2701f73ac921ed0a8a81b',\n",
       "    'title': 'Evolving Rewards to Automate Reinforcement Learning'},\n",
       "   {'paperId': 'ddea0e5c27ff91cda7ba50e1231aa7f7d076e58b',\n",
       "    'title': 'Reinforcement Learning, Fast and Slow'},\n",
       "   {'paperId': 'adc4ddeb10dd8da115d4bde9569794ff409fcd40',\n",
       "    'title': 'Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': 'e8e7b0b23e70624fbc3bc5ab74ae01e39c6750a5',\n",
       "    'title': 'Making Sense of Vision and Touch: Self-Supervised Learning of Multimodal Representations for Contact-Rich Tasks'},\n",
       "   {'paperId': 'ef3740d47a3424cccbe77ff94ae8ed47d3341fd3',\n",
       "    'title': 'A Framework for Robot Manipulation: Skill Formalism, Meta Learning and Adaptive Control'},\n",
       "   {'paperId': '2d9820a047976c94875c4f112905ddc802fd9a19',\n",
       "    'title': 'Force, Impedance, and Trajectory Learning for Contact Tooling and Haptic Identification'},\n",
       "   {'paperId': 'd355e339298fc2ab920688c1709d4ba6476a2bc6',\n",
       "    'title': 'Distributed Distributional Deterministic Policy Gradients'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '8165d6217a2f623f7d9e613c791e94102921cd3b',\n",
       "    'title': 'Thinking Fast and Slow'},\n",
       "   {'paperId': 'f219202ef23d61fd4c79c51b07a6042038f3d71a',\n",
       "    'title': 'Layered direct policy search for learning hierarchical skills'},\n",
       "   {'paperId': '0ea3bf6d45d05fe28c7ae683e0f92a9923e0e2bb',\n",
       "    'title': 'Learning locomotion skills using DeepRL: does the choice of action space matter?'},\n",
       "   {'paperId': '1dfa492d26fe379a6d5810dcff5acc2cd4d5eb6b',\n",
       "    'title': 'Unified passivity-based Cartesian force/impedance control for rigid and flexible joint robots via task-energy tanks'},\n",
       "   {'paperId': 'e5e2d2228a01b889cc836bd153cde3e933fc2b68',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Robot Navigation'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': '65438e0ba226c1f97bd8a36333ebc3297b1a32fd',\n",
       "    'title': 'Reinforcement learning in robotics: A survey'},\n",
       "   {'paperId': 'd1d828ff8b9ed26e4e64c8ae89ac2b98d683a576',\n",
       "    'title': 'Hierarchical reinforcement learning and decision making'},\n",
       "   {'paperId': 'aaf42313fb89aa731f538c0066f34e8cac4dd878',\n",
       "    'title': 'Learning force control policies for compliant manipulation'},\n",
       "   {'paperId': '463b2dbf62913d04cc28e48e9cbc357fa2371922',\n",
       "    'title': 'Human-Like Adaptation of Force and Impedance in Stable and Unstable Interactions'},\n",
       "   {'paperId': '62c9ae261c14877d62156ede956db723ebff8a5a',\n",
       "    'title': 'Learning variable impedance control'},\n",
       "   {'paperId': 'a04528c6b94052354653daa659dea3096ea3dce4',\n",
       "    'title': 'Biomimetic motor behavior for simultaneous adaptation of force, impedance and trajectory in interaction tasks'},\n",
       "   {'paperId': '4d6a87d76ec0c0379f0afbf24f84bba848c6246e',\n",
       "    'title': 'Policy search for motor primitives in robotics'},\n",
       "   {'paperId': '3402002efd65b8002d03d1d455b196087abe7fc8',\n",
       "    'title': 'Hierarchically organized behavior and its neural foundations: A reinforcement learning perspective'},\n",
       "   {'paperId': 'be96ba0a036b815a21fed6d8fdee1bfbbebb3b9e',\n",
       "    'title': 'CNS Learns Stable, Accurate, and Efficient Movements Using a Simple Algorithm'},\n",
       "   {'paperId': '1040e8e463c62f9dd45f57f4c9ef901f450511e6',\n",
       "    'title': 'Collision detection and reaction: A contribution to safe physical Human-Robot Interaction'},\n",
       "   {'paperId': '4cdd052446cd8fb0abe09ebc054394cad895c719',\n",
       "    'title': 'Cartesian Impedance Control of Redundant and Flexible-Joint Robots'},\n",
       "   {'paperId': '3d2417e2382fd2f9d25c6a67bcbd470eb0d2f533',\n",
       "    'title': 'Endpoint Stiffness of the Arm Is Directionally Tuned to Instability in the Environment'},\n",
       "   {'paperId': '1f869232f148ec52066fab06a49855937f84098b',\n",
       "    'title': 'Reinforcement learning by reward-weighted regression for operational space control'},\n",
       "   {'paperId': '2065d9eb28be0700a235afb78e4a073845bfb67d',\n",
       "    'title': 'Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': '3e15f5b5e3c3631eab9b46aa92e47b3d706554df',\n",
       "    'title': 'Stability properties of human reaching movements'},\n",
       "   {'paperId': '9aff035de60fa73a167189743e0a179741b44aa9',\n",
       "    'title': 'Hierarchical Policy Gradient Algorithms'},\n",
       "   {'paperId': 'a8bd98a38339685d39241de7d4053c47ebc96775',\n",
       "    'title': 'The central nervous system stabilizes unstable dynamics by learning optimal impedance'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '0caf766f329fd37a699a5c3ec62aebb1c9d44f1a',\n",
       "    'title': 'Internal models for motor control and trajectory planning'},\n",
       "   {'paperId': 'bcbbdf7a4c6be26172057b1f9790b7d9027680f2',\n",
       "    'title': 'Task-Dependent Viscoelasticity of Human Multijoint Arm and Its Spatial Characteristics for Interaction with Environments'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': '82479b544924ee734fb22f9dce78aace0f90cd3c',\n",
       "    'title': 'Robot juggling: implementation of memory-based learning'},\n",
       "   {'paperId': '33576c0fc316c45c3672523114b20a5bb996e1f4',\n",
       "    'title': 'A unified approach for motion and force control of robot manipulators: The operational space formulation'},\n",
       "   {'paperId': 'f5f8a6aa4adc13070224c2bd43a255c4e0844c15',\n",
       "    'title': 'Impedance Control: An Approach to Manipulation'}],\n",
       "  'citations': [{'paperId': 'aec15205a9d048b031488dd18d8c86b883dd5fa5',\n",
       "    'title': 'Model Predictive Impedance Control with Gaussian Processes for Human and Environment Interaction'},\n",
       "   {'paperId': 'b8a3a030a36fbb74deea909aec5a959c94477fd2',\n",
       "    'title': 'CLAS: Coordinating Multi-Robot Manipulation with Central Latent Action Spaces'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'c85662dcd17eed4452019b640a30a323970472ef': {'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '39cc9292cb602970453c7677ada5d575d03d1d77',\n",
       "    'title': 'SKID RAW: Skill Discovery From Raw Trajectories'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '3494d5d6d71bbf8a6467c5184c557b5044c53e78',\n",
       "    'title': 'Contextual Latent-Movements Off-Policy Optimization for Robotic Manipulation Skills'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'The five object sets (triplets) used in the paper. This image has been taken directly from (Lee et al., 2021) for clarity'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '7acbdb961f67d50fef359066f2a1d7755cf16ee2',\n",
       "    'title': 'Critic Regularized Regression'},\n",
       "   {'paperId': '8dd3ec3ca1b7400d998e747356d07763a7ac1fb0',\n",
       "    'title': 'Long-Horizon Visual Planning with Goal-Conditioned Hierarchical Predictors'},\n",
       "   {'paperId': '28db20a81eec74a50204686c3cf796c42a020d2e',\n",
       "    'title': 'Conservative Q-Learning for Offline Reinforcement Learning'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '7d85e83ae00f2d19b3cdb01f5feeb92a2102104f',\n",
       "    'title': 'Task-Embedded Control Networks for Few-Shot Imitation Learning'},\n",
       "   {'paperId': '23138df043cc0a637aadc20fdd5468fed304c07b',\n",
       "    'title': 'Dimensionality Reduction in Learning Gaussian Mixture Models of Movement Primitives for Contextualized Action Selection and Adaptation'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'df73c1505f49dcc354fb2b97e3b06bf06e20f3f7',\n",
       "    'title': 'Using probabilistic movement primitives in robotics'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'cee949487d13d0b64c4ef21b66ece96eb08472b3',\n",
       "    'title': 'Asymmetric Actor Critic for Image-Based Robot Learning'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': '23ca54529dd6cf208e3484a3d02a35860a1470af',\n",
       "    'title': 'Learning task-parameterized dynamic movement primitives using mixture of GMMs'},\n",
       "   {'paperId': 'd6ef620ed29b94edfc28c8ec263b29ffb4f4b89d',\n",
       "    'title': 'Learning movement primitive libraries through probabilistic segmentation'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'b941e362eaaed23293c7ec602ac842bc5253f1fd',\n",
       "    'title': 'Efficient Unsupervised Temporal Segmentation of Motion Data'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "    'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       "   {'paperId': '43fc6ab25ba5236688a87a591ed58c778bdcd417',\n",
       "    'title': 'Probabilistic segmentation applied to an assembly task'},\n",
       "   {'paperId': '59b5fcd44ebec6cf4a53a0c627bd4a1bc6f34c4f',\n",
       "    'title': 'Extracting low-dimensional control variables for movement primitives'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': '1695dbabf8e905db0b391ff522c323db5fc8b958',\n",
       "    'title': 'Learning to select and generalize striking movements in robot table tennis'},\n",
       "   {'paperId': '21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8',\n",
       "    'title': 'Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery'},\n",
       "   {'paperId': 'ffa89e2d70c7b12e42b12923ebc45a46fb7798a9',\n",
       "    'title': 'Learning table tennis with a Mixture of Motor Primitives'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Reach and grasp the red object. • lift(red) AND grasp(): Lift the red object. • hover(red,blue): Hover with the red object above the blue object'}],\n",
       "  'citations': [{'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': '57a6d470eacba2233caf811dfe036bba145fa292',\n",
       "    'title': 'DMAP: a Distributed Morphological Attention Policy for Learning to Locomote with a Changing Body'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '78839ec995beab7f5fa8ce8d549fb4cf04b33d45',\n",
       "    'title': 'Meta-Learning Parameterized Skills'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'}],\n",
       "  'citnuminlist': 3,\n",
       "  'refnuminlist': 14,\n",
       "  'isKeypaper': True},\n",
       " '23bb22710f7be585305bf01841b74ed167a706ce': {'title': 'Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '54a0425edc7be604f907f29e951140259825b367',\n",
       "    'title': 'Towards Generalized Manipulation Learning Through Grasp Mechanics-Based Features and Self-Supervision'},\n",
       "   {'paperId': '4c8407e06ccce55824a9faba2148f103869baf52',\n",
       "    'title': 'Object-Agnostic Dexterous Manipulation of Partially Constrained Trajectories'},\n",
       "   {'paperId': 'c37c2ded09f0e5ac181cdeebb141ec7c8b4641d6',\n",
       "    'title': 'Model-Augmented Actor-Critic: Backpropagating through Paths'},\n",
       "   {'paperId': '13288982406f55226d6a6380ca4c7f42238dfc3b',\n",
       "    'title': 'Information Theoretic Model Predictive Q-Learning'},\n",
       "   {'paperId': 'db1614b97aec1e0ead9554a038aa0f8dd9a26e30',\n",
       "    'title': 'Exploring Model-based Planning with Policy Networks'},\n",
       "   {'paperId': '16504bf0357d6a17bce49aa28bdaf7b8531156ed',\n",
       "    'title': 'Deep Value Model Predictive Control'},\n",
       "   {'paperId': '7a450675968d31b8363e21fb5d5b72474c128076',\n",
       "    'title': 'Deep Dynamics Models for Learning Dexterous Manipulation'},\n",
       "   {'paperId': '55d90d27237295da9eea7340c2b6ca8903463b94',\n",
       "    'title': 'Learn to Adapt to Human Walking: A Model-Based Reinforcement Learning Approach for a Robotic Assistant Rollator'},\n",
       "   {'paperId': '9001698e033524864d4d45f051a5ba362d4afd9e',\n",
       "    'title': 'When to Trust Your Model: Model-Based Policy Optimization'},\n",
       "   {'paperId': '6a9013a8cdd84e423223f76a903028011c84c4ab',\n",
       "    'title': 'Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control'},\n",
       "   {'paperId': 'd333f99881b09426283a9c7a1d25f7ac30d63062',\n",
       "    'title': 'Algorithmic Framework for Model-based Reinforcement Learning with Theoretical Guarantees'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '68df76350dffba2e5f5f965df57c3747c66bb4d0',\n",
       "    'title': 'Differentiable MPC for End-to-end Planning and Control'},\n",
       "   {'paperId': 'a7e07e0ecd1727778ade42d2e1df856171ec0898',\n",
       "    'title': 'Model-Based Reinforcement Learning via Meta-Policy Optimization'},\n",
       "   {'paperId': '56136aa0b2c347cbcf3d50821f310c4253155026',\n",
       "    'title': 'Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models'},\n",
       "   {'paperId': 'dee85f0ed3571d7b591e23000848c584242186ef',\n",
       "    'title': 'Composable Deep Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': '4debb99c0c63bfaa97dd433bc2828e4dac81c48b',\n",
       "    'title': 'Addressing Function Approximation Error in Actor-Critic Methods'},\n",
       "   {'paperId': '56f1ffacf2ceb93f160c7edaf2a322f69322dab6',\n",
       "    'title': 'MPC-Inspired Neural Network Policies for Sequential Decision Making'},\n",
       "   {'paperId': '852c931b5d9f9d4256befd725ee4185945c4964c',\n",
       "    'title': 'Temporal Difference Models: Model-Free Deep RL for Model-Based Control'},\n",
       "   {'paperId': '27dfecb6bb0308c7484e13dcaefd5eeebba677d3',\n",
       "    'title': 'Model-Ensemble Trust-Region Policy Optimization'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'cce22bf6405042a965a86557684c46a441f2a736',\n",
       "    'title': 'Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '869f4fc59d74a96098ed46935cb6fd1a537c38ce',\n",
       "    'title': 'Information theoretic MPC for model-based reinforcement learning'},\n",
       "   {'paperId': '360cf15dcba643b04f9028bca25396b3beb73f2d',\n",
       "    'title': 'Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning'},\n",
       "   {'paperId': 'e37b999f0c96d7136db07b0185b837d5decd599a',\n",
       "    'title': 'Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates'},\n",
       "   {'paperId': 'a0cdee6eabd3b0e179ce60b9639411ca82256423',\n",
       "    'title': 'AprilTag 2: Efficient and robust fiducial detection'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': '528a8ef7277d81d337c8b4c4fe0a9df483031773',\n",
       "    'title': 'Aggressive driving with model predictive path integral control'},\n",
       "   {'paperId': '4c09757f31f66e483c61266a458f5aaaf8723895',\n",
       "    'title': 'Optimal control with learned local models: Application to dexterous manipulation'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': 'e0ce0e87edb08038c1432755909bb7270026ec50',\n",
       "    'title': 'Benchmarking in Manipulation Research: Using the Yale-CMU-Berkeley Object and Model Set'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'd0c61536927c2f5dc2ddb74664268a3623580b9c',\n",
       "    'title': 'Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics'},\n",
       "   {'paperId': 'ee353502de5f49c597487dde826e4e3c375c0a3a',\n",
       "    'title': 'An underactuated hand for efficient finger-gaiting-based dexterous manipulation'},\n",
       "   {'paperId': '244539f454800697ed663326b7cfba337ca0c2ec',\n",
       "    'title': 'Guided Policy Search'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '60b7d47758a71978e74edff6dd8dea4d9c791d7a',\n",
       "    'title': 'PILCO: A Model-Based and Data-Efficient Approach to Policy Search'},\n",
       "   {'paperId': 'a1497bb0123a065a2a879c6de84dd03e16b1094d',\n",
       "    'title': 'Receding Horizon Differential Dynamic Programming'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': '6658cfd6c0b841f027f377019dd39d882ffaf276',\n",
       "    'title': 'Using Local Trajectory Optimizers to Speed Up Global Optimization in Dynamic Programming'},\n",
       "   {'paperId': None, 'title': 'Yale openhand project'}],\n",
       "  'citations': [{'paperId': '1484cc1fc1d7ed1b9e77b230aaee01f4b29e9327',\n",
       "    'title': 'Diminishing Return of Value Expansion Methods in Model-Based Reinforcement Learning'},\n",
       "   {'paperId': '4eda9565531a1f6b2c320f3a3a6ca4a8a32cf722',\n",
       "    'title': 'Sampling-based Exploration for Reinforcement Learning of Dexterous Manipulation'},\n",
       "   {'paperId': '87510a31fa6cc40793c33a2031a0ae1ab1aae0b5',\n",
       "    'title': 'A fault-tolerant and robust controller using model predictive path integral control for free-flying space robots'},\n",
       "   {'paperId': '6183b0803d49ed43cd6ded5ca41255a9646be9b7',\n",
       "    'title': 'Active Exploration for Robotic Manipulation'},\n",
       "   {'paperId': '84565f87768f607afc2f0ee726b865568c48d300',\n",
       "    'title': 'Mjolnir: A framework agnostic auto-tuning system with deep reinforcement learning'},\n",
       "   {'paperId': '3d125b664d4e2989bc4e6cbc1936d1e87b177569',\n",
       "    'title': 'Simplifying Model-based RL: Learning Representations, Latent-space Models, and Policies with One Objective'},\n",
       "   {'paperId': 'a00dcdf75819778e8235b94c1a9c88e8ecb3b678',\n",
       "    'title': 'Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '1d7ad567a22603cb5fb1130b9e8c155b4534e3e3',\n",
       "    'title': 'Backward Imitation and Forward Reinforcement Learning via Bi-directional Model Rollouts'},\n",
       "   {'paperId': '05cd4badb91cdb9d042742fe6093e58e9065dc82',\n",
       "    'title': 'Scalable Model-based Policy Optimization for Decentralized Networked Systems'},\n",
       "   {'paperId': '18c295cc2552e269201d774e71a5cf91a2ec557a',\n",
       "    'title': 'Force-Based Simultaneous Mapping and Object Reconstruction for Robotic Manipulation'},\n",
       "   {'paperId': 'd46eef8b0d46cfdebefe9941a0f60aeeed31ded0',\n",
       "    'title': 'Temporal Difference Learning for Model Predictive Control'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'},\n",
       "   {'paperId': 'e06b2fc2fe5bf9e2d2ece2201bd84d18cb82be1a',\n",
       "    'title': 'Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning'},\n",
       "   {'paperId': 'c425b527bbeffc2e4b5bc7a42649cd16a3fa216f',\n",
       "    'title': 'On-Policy Model Errors in Reinforcement Learning'},\n",
       "   {'paperId': '94fa6221b00568d04b4ede4c5f6efc90577b5d66',\n",
       "    'title': 'On the Feasibility of Learning Finger-gaiting In-hand Manipulation with Intrinsic Sensing'},\n",
       "   {'paperId': '02438e5f9c2d7035c62e9d10bcfe31afe1780631',\n",
       "    'title': 'GRiD: GPU-Accelerated Rigid Body Dynamics with Analytical Gradients'},\n",
       "   {'paperId': '18b7e69633ba83d2256448c8f5cfdea748bf5891',\n",
       "    'title': 'Simulation and control of bistable flows with AI'},\n",
       "   {'paperId': '1ac0eb332064248dea2f5afbe03020028fb91d5b',\n",
       "    'title': 'Learning to Visually Observe, Plan, and Control Compliant Robot In-Hand Manipulation'},\n",
       "   {'paperId': '9cb2027e76315afc4b17618f5ab29c9a8d96b75f',\n",
       "    'title': 'Data-Driven Risk-Sensitive Control for Personalized Lane Change Maneuvers'},\n",
       "   {'paperId': '71140a8b477a7344907891b1822a708df46246c2',\n",
       "    'title': 'Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy RL'},\n",
       "   {'paperId': 'fcd1d1f393025ae7d7ce11b960421d38fbdf319b',\n",
       "    'title': 'Learning Dynamics Models for Model Predictive Agents'},\n",
       "   {'paperId': '3c4cf069c30523987f312aaf9ed8ffc865cdc286',\n",
       "    'title': 'Navigational Behavior of Humans and Deep Reinforcement Learning Agents'},\n",
       "   {'paperId': 'bbe600b4d672f1fdf9bf5e8fd760d5a0f1e50fa5',\n",
       "    'title': 'Towards a Sample Efficient Reinforcement Learning Pipeline for Vision Based Robotics'},\n",
       "   {'paperId': '8c41cf8ff2005d99ed9f440548685a1be7c56ca2',\n",
       "    'title': 'D YNAMIC M IRROR D ESCENT BASED M ODEL P REDICTIVE C ONTROL FOR A CCELERATING R OBOT L EARNING'},\n",
       "   {'paperId': '5183d9d4d004f121007e1ce6c6d66a8e4e010d30',\n",
       "    'title': 'Learn2Assemble with Structured Representations and Search for Robotic Architectural Construction'}],\n",
       "  'citnuminlist': 1,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'd3c6e0b80c36c14f7d1761fb881f20c35165f507': {'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '5685abf9e7bb2c16449ae1eb181051e503602a55',\n",
       "    'title': 'Reinforcement Learning based Recommender Systems: A Survey'},\n",
       "   {'paperId': '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "    'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models'},\n",
       "   {'paperId': 'ae25c0c7d3e2de1c90346079e53b9e3e836c7de4',\n",
       "    'title': 'Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269',\n",
       "    'title': 'Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '105f2a77af4934b315ec92e1f65451e6f53e9c01',\n",
       "    'title': 'Visual Adversarial Imitation Learning using Variational Models'},\n",
       "   {'paperId': 'bb18b3e20ba195397a4d34c663fed485dce5fbee',\n",
       "    'title': 'Provable Representation Learning for Imitation with Contrastive Fourier Features'},\n",
       "   {'paperId': '56586eb758d7fabdbbd5ee4c5ed7e30c6d457d30',\n",
       "    'title': 'Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '131007c8d7f79004f52599e9279936c333b3c083',\n",
       "    'title': 'Learning Invariant Representations for Reinforcement Learning without Reconstruction'},\n",
       "   {'paperId': 'ec72a190006441883a049611e624e992953a2bcc',\n",
       "    'title': 'Mitigating Covariate Shift in Imitation Learning via Offline Data With Partial Coverage'},\n",
       "   {'paperId': '0237b930c018c774e5bcdc725b6369e721913c35',\n",
       "    'title': 'Closing the Closed-Loop Distribution Shift in Safe Imitation Learning'},\n",
       "   {'paperId': '23472dde7735dbe88b300662a0421aca52692075',\n",
       "    'title': 'Offline Learning from Demonstrations and Unlabeled Experience'},\n",
       "   {'paperId': 'cff6566e92e71c8fcc5cfa5d16eef34e95b1a1f3',\n",
       "    'title': 'PLAS: Latent Action Space for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '7acbdb961f67d50fef359066f2a1d7755cf16ee2',\n",
       "    'title': 'Critic Regularized Regression'},\n",
       "   {'paperId': 'de46f4e4613364792bbd13f185c381ab656a27ef',\n",
       "    'title': 'RL Unplugged: Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': '5e7bc93622416f14e6948a500278bfbe58cd3890',\n",
       "    'title': 'Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '14a4bd317880f6763bd2b53de28d93a37eabd10d',\n",
       "    'title': 'Provable Representation Learning for Imitation Learning via Bi-level Optimization'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Unplugged datasets followed by behavioral cloning in the latent action space. Baseline BC achieves low rewards due to the small expert sample size. Dotted lines denote the performance of CRR'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': 'a18334c63972f1d81ca1e7b74fa833e38aaad708',\n",
       "    'title': 'Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '188dac491f04c56e1eb7d7b33ac6aa0b87303232',\n",
       "    'title': 'DeepMDP: Learning Continuous Latent Space Models for Representation Learning'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': None, 'title': 'Diego de Las Casas'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': '7a4193d0b042643a8bb9ec262ed7f9d509bdb12e',\n",
       "    'title': 'Constrained Policy Optimization'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None, 'title': 'Searching for activation functions, 2017'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': 'f8196851e18e75ca7883dd18b09531e0f25c539f',\n",
       "    'title': 'On the Convergence of the Empirical Distribution'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': 'b91bd74cce7fb7eb2407ff5620a4569868e89ec9',\n",
       "    'title': 'Using Bisimulation for Policy Transfer in MDPs'},\n",
       "   {'paperId': '70e10a5459c6f1aaf346ee4f2dcc837151fbe75c',\n",
       "    'title': 'Efficient Reductions for Imitation Learning'},\n",
       "   {'paperId': '7a59fde27461a3ef4a21a249cc403d0d96e4a0d7',\n",
       "    'title': 'Random Features for Large-Scale Kernel Machines'},\n",
       "   {'paperId': 'f65020fc3b1692d7989e099d6b6e698be5a50a93',\n",
       "    'title': 'Apprenticeship learning via inverse reinforcement learning'},\n",
       "   {'paperId': '0fbb7d6e6e26748ef7adeb548dcf54e4d02864e4',\n",
       "    'title': 'State aggregation in Markov decision processes'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'f69e05a32fd1541bb41d981bdb013366c9150a85',\n",
       "    'title': 'Is imitation learning the route to humanoid robots?'},\n",
       "   {'paperId': '9aa1d909544fd9ffe061b84a90eb344ac303e6d9',\n",
       "    'title': 'The MAXQ Method for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': '8090121ad488b4af27bc59bf91b62e9c6a6f49c6',\n",
       "    'title': 'Markov Decision Processes: Discrete Stochastic Dynamic Programming'},\n",
       "   {'paperId': 'ea899c8a3806a02a225061a35f802b00d90a0a20',\n",
       "    'title': 'Reinforcement Learning with Soft State Aggregation'},\n",
       "   {'paperId': '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3',\n",
       "    'title': 'ALVINN: An Autonomous Land Vehicle in a Neural Network'}],\n",
       "  'citations': [{'paperId': 'ad84b7b1e94f2a88458deedc3b1972018a24c640',\n",
       "    'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': '8e16d556afd6fe7656a6acb991e5b7b47b21e074',\n",
       "    'title': 'Eliciting Compatible Demonstrations for Multi-Human Imitation Learning'},\n",
       "   {'paperId': '0e34addae55a571d7efd3a5e2543e86dd7d41a83',\n",
       "    'title': 'Interactive Language: Talking to Robots in Real Time'},\n",
       "   {'paperId': '2fd248460c4f44e5135b6a8446efbb56c989a187',\n",
       "    'title': 'Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': 'ff4011e2796d7efb3a60a301e89590efa8fe284a',\n",
       "    'title': 'Generative Adversarial Imitation Learning from Human Behavior with Reward Shaping'},\n",
       "   {'paperId': '8fa6e5720bba4175443a7aaba107c1e4b2408934',\n",
       "    'title': 'PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations'},\n",
       "   {'paperId': '10b28bd8e9a7c54d063a1647dd8f38dae48cea22',\n",
       "    'title': 'Making Linear MDPs Practical via Contrastive Representation Learning'},\n",
       "   {'paperId': '4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "    'title': 'Chain of Thought Imitation with Procedure Cloning'},\n",
       "   {'paperId': '68110c86044b3a649c9331a9ffaa4fc8d7e67d21',\n",
       "    'title': 'Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations'},\n",
       "   {'paperId': '5b37ecab1039b50102fac9e11dd02b0158ef742c',\n",
       "    'title': 'How to Leverage Unlabeled Data in Offline Reinforcement Learning'},\n",
       "   {'paperId': '1467cffb0bad7163412b03ab473f44a54c5c40ac',\n",
       "    'title': 'Imitation Learning by Estimating Expertise of Demonstrators'},\n",
       "   {'paperId': '2d39ce0421b85f0b5525ac0f282ea2a6599553f3',\n",
       "    'title': 'P RE -T RAINING FOR R OBOTS : L EVERAGING D IVERSE M ULTITASK D ATA VIA O FFLINE RL'}],\n",
       "  'citnuminlist': 1,\n",
       "  'refnuminlist': 14,\n",
       "  'isKeypaper': True},\n",
       " 'b68b8b980db62308864b2a7d33718182c5f8335b': {'title': 'Accelerating Reinforcement Learning with Learned Skill Priors',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'de46f4e4613364792bbd13f185c381ab656a27ef',\n",
       "    'title': 'RL Unplugged: Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': '5e7bc93622416f14e6948a500278bfbe58cd3890',\n",
       "    'title': 'Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': '5e9764f45e7ea6206594deb94753a5cad4e31a1a',\n",
       "    'title': 'IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data'},\n",
       "   {'paperId': '59f606de7c073b4179f627325702d242eea51ba1',\n",
       "    'title': 'Scaling data-driven robotics with reward sketching and batch reinforcement learning'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '4f0c4189d9a82f94dcd84fe879cfbe124aaf270b',\n",
       "    'title': 'Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning'},\n",
       "   {'paperId': '9e475a514f54665478aac6038c262e5a6bac5e64',\n",
       "    'title': 'nuScenes: A Multimodal Dataset for Autonomous Driving'},\n",
       "   {'paperId': None, 'title': 'Keyin: Keyframing for visual planning'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '1674008abd47f1ce1e894c672074a47ee6c3288c',\n",
       "    'title': 'Dynamics Learning with Cascaded Variational Inference for Multi-Step Manipulation'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '3e519d85cdcefdd1d2ad89829d6ad445695d8c58',\n",
       "    'title': 'RoboNet: Large-Scale Multi-Robot Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': '57daffd65a5d73a439903f3e50950c21c9eba687',\n",
       "    'title': 'Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '4625628163a2ee0e6cd320cd7a14b4ccded2a631',\n",
       "    'title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '5285cb8faada5de8a92a47622950f6cfd476ac1d',\n",
       "    'title': 'Off-Policy Deep Reinforcement Learning without Exploration'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "    'title': 'Composing Complex Skills by Learning Transition Policies'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': 'c01566fa915db4a4d2636bf6b03a36e203e93845',\n",
       "    'title': 'ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation'},\n",
       "   {'paperId': '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba',\n",
       "    'title': 'Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review'},\n",
       "   {'paperId': '9ff74f68b0e82bed43c86882aebed1e5d5ac5da5',\n",
       "    'title': 'The AdobeIndoorNav Dataset: Towards Deep Reinforcement Learning based Real-world Indoor Robot Visual Navigation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': '484ad17c926292fbe0d5211540832a8c8a8e958b',\n",
       "    'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': '2a65434d43ffa6554eaf14b728780919ad4f33eb',\n",
       "    'title': 'Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy'},\n",
       "   {'paperId': '467568f1777bc51a15a5100516cd4fe8de62b9ab',\n",
       "    'title': 'Transfer Learning for Reinforcement Learning Domains: A Survey'},\n",
       "   {'paperId': '01c758f27adcb21dde477c384dc88896cbad4f3c',\n",
       "    'title': 'The utility of temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '668b1277fbece28c4841eeab1c97e4ebd0079700',\n",
       "    'title': 'Pattern Recognition and Machine Learning'},\n",
       "   {'paperId': '2065d9eb28be0700a235afb78e4a073845bfb67d',\n",
       "    'title': 'Dynamic Movement Primitives -A Framework for Motor Control in Humans and Humanoid Robotics'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': '3307e0ff313497b8a1dc3358982cc679125d14cd',\n",
       "    'title': 'PolicyBlocks: An Algorithm for Creating Useful Macro-Actions in Reinforcement Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'e41cda7b81cc49640210173fd45eb06cdbd6e824',\n",
       "    'title': 'Finding Structure in Reinforcement Learning'},\n",
       "   {'paperId': '82346e1a97778bc0f36ab9d8e435c02b1db7a1f7',\n",
       "    'title': 'The influence of improvement in one mental function upon the efficiency of other functions. (I).'}],\n",
       "  'citations': [{'paperId': '43806158cabbcbca4efd58f75a82c882c085d60d',\n",
       "    'title': 'IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control'},\n",
       "   {'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': 'efabf7727dc3212676f03f1384809ce019db109d',\n",
       "    'title': 'Future-conditioned Unsupervised Pretraining for Decision Transformer'},\n",
       "   {'paperId': '1cdc6d284ea9bc7019490428949c619b26504f82',\n",
       "    'title': 'Beyond Reward: Offline Preference-guided Policy Optimization'},\n",
       "   {'paperId': 'f80de0e8d343166f8e9b2cf844253d7b24b443d4',\n",
       "    'title': 'FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation'},\n",
       "   {'paperId': 'fe9fe9f15f24fbbb19b62bcd9a3418511a699b84',\n",
       "    'title': 'Policy Representation via Diffusion Probability Model for Reinforcement Learning'},\n",
       "   {'paperId': 'c7dea47e008a439e11439dfe6a8c1b08357fad65',\n",
       "    'title': 'Distance Weighted Supervised Learning for Offline Interaction Data'},\n",
       "   {'paperId': '8fce3142bc144bdc08bf0cab1db908c7ad3f8454',\n",
       "    'title': 'Contrastive Language, Action, and State Pre-training for Robot Learning'},\n",
       "   {'paperId': '01706dd038b959e92a93f3141bb98be8f1f048f0',\n",
       "    'title': 'Hyper-Decision Transformer for Efficient Online Policy Adaptation'},\n",
       "   {'paperId': '1334a47e8f4e4ffd04ff534329d76a5e5cc16f46',\n",
       "    'title': 'Goal-Conditioned Imitation Learning using Score-based Diffusion Policies'},\n",
       "   {'paperId': 'ceaedeaabc9a8be28f9f4f9cf277c26dd47b6744',\n",
       "    'title': 'Constrained Exploration in Reinforcement Learning with Optimality Preservation'},\n",
       "   {'paperId': 'e1bd151a3f670fd0f77580702fe7a85dc78a41cb',\n",
       "    'title': 'Chain-of-Thought Predictive Control'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e',\n",
       "    'title': 'A Survey of Demonstration Learning'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': '612c706a66fccd67255e469867962248725ca4a3',\n",
       "    'title': 'Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation'},\n",
       "   {'paperId': '4c3dc49abcdb1122472d15f1f8fde4bbabc3859f',\n",
       "    'title': 'Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation'},\n",
       "   {'paperId': '06ad7a1b42b4a6a395ac01091e5cafeea23918ab',\n",
       "    'title': 'Meta-Reinforcement Learning via Exploratory Task Clustering'},\n",
       "   {'paperId': 'b43330013a5abcccd366d71f2f66c493c790abc6',\n",
       "    'title': 'Imitating Human Behaviour with Diffusion Models'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': 'ddb7ff9808d8a383590e63e56d24082e565342d3',\n",
       "    'title': 'Robot Target Location Based on the Difference in Monocular Vision Projection'},\n",
       "   {'paperId': '8fe2151725b6d5c886f2b6bd673246d7fba2cc6d',\n",
       "    'title': 'Towards Physically Adversarial Intelligent Networks (PAINs) for Safer Self-Driving'},\n",
       "   {'paperId': 'ca137b4f6ba25ca2f26952044c6a2d06ae3e607f',\n",
       "    'title': 'Cross-Domain Transfer via Semantic Skill Imitation'},\n",
       "   {'paperId': 'db8d70d9da6b957a00ec7e8cc67493340c39aa29',\n",
       "    'title': 'Policy Transfer via Skill Adaptation and Composition'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '5cef819106a386fe876a868b52d933e98e5c32e4',\n",
       "    'title': 'Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment'},\n",
       "   {'paperId': '9d1445f1845a2880ff9c752845660e9c294aa7b5',\n",
       "    'title': 'Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '8e2daf1f06d2d75bff9519c86382bf50fff6ae0f',\n",
       "    'title': 'Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '5572521b26ed3efcaafe050975b404fc5e7c89a7',\n",
       "    'title': 'Guided Skill Learning and Abstraction for Long-Horizon Manipulation'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': '9b5f4aab169fba588e214c010345232053f8ae76',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'ad4707bb87c6fc087e09d9f6609665b53835c899',\n",
       "    'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction'},\n",
       "   {'paperId': '22b816129ca770df3a88e76f754218b242df43f9',\n",
       "    'title': 'Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization'},\n",
       "   {'paperId': '30d0a5a5f8e776c844a37afe3b1ace0b21b24859',\n",
       "    'title': 'Retrospectives on the Embodied AI Workshop'},\n",
       "   {'paperId': '8cb359a03b499319c05eb7d36726341579dbe56f',\n",
       "    'title': 'EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '57901846a7aed271e90775dbbc1411ac0c077928',\n",
       "    'title': 'Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels'},\n",
       "   {'paperId': 'b9a3fb062ac940012f69e9cca5f34bb717370c3a',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '951da1a5a0c2bfb6f2d16dccfc488acb71f1447f',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Data for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '7b50825e62a207ac85e3d568730436d972f31597',\n",
       "    'title': 'Hierarchical Decision Transformer'},\n",
       "   {'paperId': '8d79b7d0d1c8e1bfc5ef20428e4566116e8eb862',\n",
       "    'title': 'Transferring Knowledge for Reinforcement Learning in Contact-Rich Manipulation'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': '1c33f76c6f182023a2eefb3bb46cba13fb2345a4',\n",
       "    'title': 'Learning Dynamic Manipulation Skills from Haptic-Play'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': 'c2366e759a97c2b21e9fc35c5e2f6b377eca38ab',\n",
       "    'title': 'Transformers are Adaptable Task Planners'},\n",
       "   {'paperId': '01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8',\n",
       "    'title': 'Behavior Transformers: Cloning k modes with one stone'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '15ddeb9b812e4063a8b907d50c720e01c753b2b4',\n",
       "    'title': 'Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '6d62781c7ebc9269d6ff27b0d8011a837238f3d6',\n",
       "    'title': 'Learning to Drive Using Sparse Imitation Reinforcement Learning'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'b34e47bde3f49b42bf08956f9f323324c26cf5a7',\n",
       "    'title': 'Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': 'a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef',\n",
       "    'title': 'A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization'},\n",
       "   {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': '71a3d26326e73489b8756bc73e93dd312eeb4d35',\n",
       "    'title': 'Towards Deployment-Efficient Reinforcement Learning: Lower Bound and Optimality'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '33e3f13087abd5241d55523140720f5e684b7bee',\n",
       "    'title': 'Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '51517d5d900a7c0cd33e210c48cb27ef3b96e5a9',\n",
       "    'title': 'JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "    'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models'},\n",
       "   {'paperId': 'a164aed0523d4f8e9375002fb2738e74d638b9eb',\n",
       "    'title': 'Learning latent actions to control assistive robots'},\n",
       "   {'paperId': '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "    'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans'},\n",
       "   {'paperId': '01bc9970cdceafe7a81fcd9acc95b82d6666c502',\n",
       "    'title': 'Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning'},\n",
       "   {'paperId': 'aedd968f09752786785d2de91151d22a7dc36117',\n",
       "    'title': 'Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': '5840bf765be8c3bcedab63f43f5982ddba26eaf9',\n",
       "    'title': 'SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': 'a176b0de62840f7118006277d94bbc1547162a4d',\n",
       "    'title': 'Learning to Synthesize Programs as Interpretable and Generalizable Policies'},\n",
       "   {'paperId': '6f4ca4a3fd6071787083d067cf420e468e930d62',\n",
       "    'title': 'SurRoL: An Open-source Reinforcement Learning Centered and dVRK Compatible Platform for Surgical Robot Learning'},\n",
       "   {'paperId': 'f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269',\n",
       "    'title': 'Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback'},\n",
       "   {'paperId': '3032844d6ac6882ccb03e7a2c22a0026b210ac05',\n",
       "    'title': 'What Matters in Learning from Offline Human Demonstrations for Robot Manipulation'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': 'c66ce9e730aabac10b2bfb03bd4a4b487e4b4f43',\n",
       "    'title': 'Learning to Share Autonomy Across Repeated Interaction'},\n",
       "   {'paperId': 'a59c1c387234ef010ab830d5400606c4f89f64d4',\n",
       "    'title': 'Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data'},\n",
       "   {'paperId': '33b456eb43e5391761540f17a29e598d7595565b',\n",
       "    'title': 'Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble'},\n",
       "   {'paperId': 'fb95d6e6e5f78f6e5c339e2058ce9ae9e803182b',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning with Imagined Subgoals'},\n",
       "   {'paperId': '4ff9a9248dceb00a7d02073815c085911d5c0a59',\n",
       "    'title': 'Discovering Generalizable Skills via Automated Generation of Diverse Tasks'},\n",
       "   {'paperId': 'f71da178cd63958fe659ad613d474b67c5615bd3',\n",
       "    'title': 'Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL'},\n",
       "   {'paperId': 'c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500',\n",
       "    'title': 'Decision Transformer: Reinforcement Learning via Sequence Modeling'},\n",
       "   {'paperId': '5d0143965645f2dfdc4b217560071f355957f2f4',\n",
       "    'title': 'Action Priors for Large Action Spaces in Robotics'},\n",
       "   {'paperId': 'da18109d2725757842c5b5bca85025f23677f323',\n",
       "    'title': 'IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '2128c5dc3537d423910387963b64c69aafcf8635',\n",
       "    'title': 'Stochastic Policy Optimization with Heuristic Information for Robot Learning'},\n",
       "   {'paperId': '261ada5d3d3a7653faf91194c10f61a098908171',\n",
       "    'title': 'Replay Buffer start Strategy a ) Strategy b ) EncoderEncoder Encoder Encoder EncoderGoal Goal DBTaskDemonstrationsif successful Online Goal Selection'},\n",
       "   {'paperId': '1b305b762bd6d9c929090631234b8a0721cc0ec7',\n",
       "    'title': 'GSC: Graph-based Skill Composition for Deep Reinforcement Learning'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'},\n",
       "   {'paperId': '0a02607e1774b50ae68835b632f6ab6275fbc098',\n",
       "    'title': 'Robot Learning from Observations'},\n",
       "   {'paperId': '0091527711433c314d9556e1a92ea90349568b7a',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       "   {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'}],\n",
       "  'citnuminlist': 15,\n",
       "  'refnuminlist': 8,\n",
       "  'isKeypaper': True},\n",
       " 'd669358916608af804c20329b7287d02c75b1311': {'title': 'Behavior Priors for Efficient Reinforcement Learning',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '0881655dcdf891f529ebe7ac18301e138a5e265b',\n",
       "    'title': 'Keep Doing What Worked: Behavioral Modelling Priors for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression'},\n",
       "   {'paperId': None, 'title': 'Critic regularized regression, 2020'},\n",
       "   {'paperId': '361c00b22e29d0816ca896513d2c165e26399821',\n",
       "    'title': 'Grandmaster level in StarCraft II using multi-agent reinforcement learning'},\n",
       "   {'paperId': 'ad14227e4f51276892ffc37aa43fd8750bb5eba8',\n",
       "    'title': 'Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '9be492858863c8c7c24be1ecb75724de5086bd8e',\n",
       "    'title': 'Behavior Regularized Offline Reinforcement Learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '57daffd65a5d73a439903f3e50950c21c9eba687',\n",
       "    'title': 'Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '9a3c9a0ac460c7891d03d56146f2d566c7e0fb08',\n",
       "    'title': 'Meta reinforcement learning as task inference'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': '4625628163a2ee0e6cd320cd7a14b4ccded2a631',\n",
       "    'title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables'},\n",
       "   {'paperId': 'bf7f1ada5feecc0992f71b39c1ebeccb19ae631b',\n",
       "    'title': 'InfoBot: Transfer and Exploration via the Information Bottleneck'},\n",
       "   {'paperId': '5285cb8faada5de8a92a47622950f6cfd476ac1d',\n",
       "    'title': 'Off-Policy Deep Reinforcement Learning without Exploration'},\n",
       "   {'paperId': 'c4955faa27e082a80504285c28324c58eb52250c',\n",
       "    'title': 'Understanding the impact of entropy on policy optimization'},\n",
       "   {'paperId': 'e513f5c1a77fb28a94bca31d7a44e58dbaf5e8cc',\n",
       "    'title': 'Meta-Learning'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '655cfe96e20675183dc8c2acbab659bce54fd6f5',\n",
       "    'title': 'Relative Entropy Regularized Policy Iteration'},\n",
       "   {'paperId': '1924e637531f3c8fe4f63b90e87f5587e7b90d75',\n",
       "    'title': 'Mental labour'},\n",
       "   {'paperId': '70f4a0478f2979005b3628491c8b95335834cb64',\n",
       "    'title': 'Learning to Share and Hide Intentions using Information Regularization'},\n",
       "   {'paperId': 'f650f1fd44ab0778d30577f8c2077b2ff58830da',\n",
       "    'title': 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "    'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b',\n",
       "    'title': 'DeepMind Control Suite'},\n",
       "   {'paperId': '31c8082ac852693431b53afcdc3ea97ed7974e9a',\n",
       "    'title': 'Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning'},\n",
       "   {'paperId': '8976e91ccae57a20c29f3c9d88bf45b19973c952',\n",
       "    'title': 'Fixing a Broken ELBO'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '33690ff21ef1efb576410e656f2e60c89d0307d6',\n",
       "    'title': 'Deep Reinforcement Learning that Matters'},\n",
       "   {'paperId': '96e81cabed55630f2ad3e1346300bd7a7a17f060',\n",
       "    'title': 'When Waiting is not an Option : Learning Options with a Deliberation Cost'},\n",
       "   {'paperId': '7e9c1e0d247b20a0683f4797d9ea248c3b53d424',\n",
       "    'title': 'A Simple Neural Attentive Meta-Learner'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'd2373c66d84be650608ae7134725cdc98f78ac06',\n",
       "    'title': 'Policy transfer via modularity and reward guiding'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'cf90552b5d2e992e93ab838fd615e1c36618e31c',\n",
       "    'title': 'Distral: Robust multitask reinforcement learning'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': 'd6ef620ed29b94edfc28c8ec263b29ffb4f4b89d',\n",
       "    'title': 'Learning movement primitive libraries through probabilistic segmentation'},\n",
       "   {'paperId': 'd0352057e2b99f65f8b5244a0b912026c86d7b21',\n",
       "    'title': 'Equivalence Between Policy Gradients and Soft Q-Learning'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'},\n",
       "   {'paperId': 'f5f235579f02d9fad0d18cd19795de7e45c2f8eb',\n",
       "    'title': 'A Unified Bellman Equation for Causal Information and Value in Markov Decision Processes'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '96a067e188f1c89db9faea1fea2314a15ae51bbc',\n",
       "    'title': 'Bridging the Gap Between Value and Policy Based Reinforcement Learning'},\n",
       "   {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "    'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       "   {'paperId': 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c',\n",
       "    'title': 'Deep Variational Information Bottleneck'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '282a380fb5ac26d99667224cef8c630f6882704f',\n",
       "    'title': 'Learning to reinforcement learn'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6a43d91c8d883e3463b358571125fa0ec7298b3a',\n",
       "    'title': 'Sample Efficient Actor-Critic with Experience Replay'},\n",
       "   {'paperId': '4eb5347043422f852376624ae4649768ba509288',\n",
       "    'title': 'Path integral guided policy search'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Safe policy improvement with baseline bootstrapping, 2017'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Nando de Freitas, and Nicolas Heess. Robust imitation of diverse behaviors'},\n",
       "   {'paperId': '954b01151ff13aef416d27adc60cd9a076753b1a',\n",
       "    'title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '3ed67ded2b4d3614b38798b3f17a8e69803d0980',\n",
       "    'title': 'Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': 'e2f9e943ae296c6cbd098e911116383f3a668e87',\n",
       "    'title': 'Guided Policy Search via Approximate Mirror Descent'},\n",
       "   {'paperId': 'dc3e905bfb27d21675ee1720413e007b014b37d3',\n",
       "    'title': 'Safe and Efficient Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '9ca3af4440eb4aa4fd0a65dfa559685b2c39cd42',\n",
       "    'title': 'Composing graphical models with neural networks for structured representations and fast inference'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '4a026fd65af4ba3575e64174de56fee093fa3330',\n",
       "    'title': 'Taming the Noise in Reinforcement Learning via Soft Updates'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': None, 'title': 'Intelligence, pages 202–211'},\n",
       "   {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "    'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       "   {'paperId': 'ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060',\n",
       "    'title': 'Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning'},\n",
       "   {'paperId': '0c3b69b5247ef18fd5bab1109d87a04184ea8f4b',\n",
       "    'title': 'A Recurrent Latent Variable Model for Sequential Data'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': '7198597c62f6b6433d9ed6d5b44f887bd05d3c56',\n",
       "    'title': 'Markov Chain Monte Carlo and Variational Inference: Bridging the Gap'},\n",
       "   {'paperId': '687d0e59d5c35f022ce4638b3e3a6142068efc94',\n",
       "    'title': 'Deterministic Policy Gradient Algorithms'},\n",
       "   {'paperId': '484ad17c926292fbe0d5211540832a8c8a8e958b',\n",
       "    'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'ca7f25d5b139c684a8d477e954380138dcba3a73',\n",
       "    'title': 'Variational Policy Search via Trajectory Optimization'},\n",
       "   {'paperId': '3a81cfb4a7a880b7cf8979f6067732e961aceb7c',\n",
       "    'title': 'Probabilistic Movement Primitives'},\n",
       "   {'paperId': 'd01e3414ca706eda917576d947ece811b5cbcdde',\n",
       "    'title': 'Empowerment - an Introduction'},\n",
       "   {'paperId': '9ee2c730c765d6429c3fa7770d1b5f9a2e74535c',\n",
       "    'title': 'On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference (Extended Abstract)'},\n",
       "   {'paperId': '234d71c4daf3f4c39bce840b349463070e86bf27',\n",
       "    'title': 'Thermodynamics as a theory of decision-making with information-processing costs'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '25badc676197a70aaf9911865eb03469e402ba57',\n",
       "    'title': 'Machine learning - a probabilistic perspective'},\n",
       "   {'paperId': '9a25c8d370d5bae018706e92cd3b975e7ec737fd',\n",
       "    'title': 'Optimal control as a graphical model inference problem'},\n",
       "   {'paperId': '1d75b5275f24c422b6f73dd77750b402d997d060',\n",
       "    'title': 'Trading Value and Information in MDPs'},\n",
       "   {'paperId': '66b35f2a58d9cf2804828185c33857a7d46e6424',\n",
       "    'title': 'An information-theoretic approach to curiosity-driven reinforcement learning'},\n",
       "   {'paperId': '103e676aec18872622fb660357134da546200da3',\n",
       "    'title': 'Decision Making with Imperfect Decision Makers'},\n",
       "   {'paperId': 'cd98c06f98a098c618b43e89c2b681a5f3903127',\n",
       "    'title': 'Information, Utility and Bounded Rationality'},\n",
       "   {'paperId': '42fd4c00b5b0a1fee99ddaa0b0e0ae5a2b6e204e',\n",
       "    'title': 'Information Theory of Decisions and Actions'},\n",
       "   {'paperId': '5cbfbbca3a1ea8ee39254dd4ef07b3d67761c39a',\n",
       "    'title': 'Relative Entropy Policy Search'},\n",
       "   {'paperId': '4d6a87d76ec0c0379f0afbf24f84bba848c6246e',\n",
       "    'title': 'Policy search for motor primitives in robotics'},\n",
       "   {'paperId': '2a65434d43ffa6554eaf14b728780919ad4f33eb',\n",
       "    'title': 'Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy'},\n",
       "   {'paperId': 'd0a9b181fc252108de45720d4645ac245e1ba463',\n",
       "    'title': 'Probabilistic Graphical Models - Principles and Techniques'},\n",
       "   {'paperId': '7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c',\n",
       "    'title': 'Robot trajectory optimization using approximate inference'},\n",
       "   {'paperId': '3bb5a439a0d610a7eac68f73068cdd278b8c9775',\n",
       "    'title': 'Pattern Recognition and Machine Learning'},\n",
       "   {'paperId': '668b1277fbece28c4841eeab1c97e4ebd0079700',\n",
       "    'title': 'Pattern Recognition and Machine Learning'},\n",
       "   {'paperId': '8570302f7b63e8fcf87030f556b065fd8c260021',\n",
       "    'title': 'Linearly-solvable Markov decision problems'},\n",
       "   {'paperId': '65f048e0420d11d099d7130fc16e4bd6e3ad88b1',\n",
       "    'title': 'Probabilistic inference for solving discrete and continuous state Markov Decision Processes'},\n",
       "   {'paperId': '996b3bfd8c85d5a7499cf9fbf6b137850c1a73b0',\n",
       "    'title': 'Stochastic neural networks'},\n",
       "   {'paperId': '6cc0699a4a99132b7aecddb7f2e37d731e36bb43',\n",
       "    'title': 'Empowerment: a universal agent-centric measure of control'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': 'fbcd7ecb6743e8a210feea539c0667ecb0ea9f48',\n",
       "    'title': 'An Auxiliary Variational Method'},\n",
       "   {'paperId': '0a8149fb5aa8a5684e7d530c264451a5cb9250f5',\n",
       "    'title': 'Recent Advances in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9b927ab0b897297c8843ebea98bc324f8d89cec6',\n",
       "    'title': 'Discrete event dynamic systems: Theory and applications.'},\n",
       "   {'paperId': '38688edefc7591ea2fc7d4294070e8bfe9d9ac3d',\n",
       "    'title': 'Learning Attractor Landscapes for Learning Motor Primitives'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '985f2c1baba284e9b7b604b7169a2e2778540fe6',\n",
       "    'title': 'Temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': '44d2abe2175df8153f465f6c39b68b76a0d40ab9',\n",
       "    'title': 'Long Short-Term Memory'},\n",
       "   {'paperId': '68da5e8c6678048469da5e9308fd340840e5f34e',\n",
       "    'title': 'HQ-Learning'},\n",
       "   {'paperId': '2024107fd768c22e2fd396179d17e5164c4bf7cd',\n",
       "    'title': 'Prioritized Goal Decomposition of Markov Decision Processes: Toward a Synthesis of Classical and Decision Theoretic Planning'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': '6bc8db0c7444d9c07aad440393b2fd300fb3595c',\n",
       "    'title': 'Function Optimization using Connectionist Reinforcement Learning Algorithms'},\n",
       "   {'paperId': 'c69201d091dd92699fd90a17b9e3407319726791',\n",
       "    'title': 'Neural Sequence Chunkers'},\n",
       "   {'paperId': '8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5',\n",
       "    'title': 'A tutorial on hidden Markov models and selected applications in speech recognition'},\n",
       "   {'paperId': 'd36efb9ad91e00faa334b549ce989bfae7e2907a',\n",
       "    'title': 'Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper'},\n",
       "   {'paperId': '23a94ce42fe0d50f5c993f34d4c9602f8aeac507',\n",
       "    'title': 'Rational choice and the structure of the environment.'},\n",
       "   {'paperId': None, 'title': 'Comparable MLP Prior torso'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Locomotion and Manipulation MLP Actor torso: (200, 100) Encoder for proprioception: (200, 100) Encoder for target: (50) Encoder for box: (50) Encoder for task encoding'},\n",
       "   {'paperId': None,\n",
       "    'title': '64) MLP/LSTM Prior torso: (64, 64) Critic torso: LSTM with size (128, 64) Entropy bonus'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Best target network update period (proprio): (50) Best target network update period (full)'},\n",
       "   {'paperId': None, 'title': 'Best HL Policy torso for AR-1 prior'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Nicolas Heess, and Martin Riedmiller. Compositional transfer in hierarchical reinforcement learning. Robotics: Science and Systems XVI'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Soft actor-critic (sac) implementation in pytorch'},\n",
       "   {'paperId': None, 'title': 'Parameter for AR-1 process'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Best target update period for proprio and full'},\n",
       "   {'paperId': None, 'title': 'Actor/Critic/Prior learning rate'},\n",
       "   {'paperId': None, 'title': 'LL Policy torso'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Best learning rate for MLP Proprio prior: 5e-5 HL Policy torso'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Manipulation (1 box, 1 target) HL Policy torso'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Best distillation cost (proprio): (0.1) Best distillation cost (full): (0.01) Actor learning rate'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Manipulation (2 box gather) Distillation cost: [1e-2, 1e-3, 1e-1] Best distillation cost for MLP prior'}],\n",
       "  'citations': [{'paperId': '98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2',\n",
       "    'title': 'Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '89590dddbd1e8711802d4ff585ef1817293787e0',\n",
       "    'title': 'Learning Complex Teamwork Tasks using a Sub-task Curriculum'},\n",
       "   {'paperId': '7f270c9b098727675c9d8b893e362b561d61f27e',\n",
       "    'title': 'Policy Expansion for Bridging Offline-to-Online Reinforcement Learning'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'a139053780742708bbba78201d47a32867fbc731',\n",
       "    'title': 'Humans account for cognitive costs when finding shortcuts: An information-theoretic analysis of navigation'},\n",
       "   {'paperId': 'd876e13c79c81b035752aa951ea2295218445af5',\n",
       "    'title': 'A Unified Theory of Dual-Process Control'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '5cc8cd8f13ea88b265713384906e64a7f455f61c',\n",
       "    'title': 'Coordinating Policies Among Multiple Agents via an Intelligent Communication Channel'},\n",
       "   {'paperId': '1a0dccf0c831655bd970bb8ba01fe8b93c08512e',\n",
       "    'title': 'How to Spend Your Robot Time: Bridging Kickstarting and Offline Reinforcement Learning for Vision-based Robotic Manipulation'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': '266cc7ff4856b6a2ce9cc0a3e5f6c155ecc448a2',\n",
       "    'title': 'Can Wikipedia Help Offline Reinforcement Learning?'},\n",
       "   {'paperId': '33e3f13087abd5241d55523140720f5e684b7bee',\n",
       "    'title': 'Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': '680bf160a6b6ba5929c27df4d3a291ced471ab1b',\n",
       "    'title': 'Reinforcement Learning for Vision-based Object Manipulation with Non-parametric Policy and Action Primitives'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': 'be793883b04967307b4c59764c0199d65bec5972',\n",
       "    'title': 'Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning'},\n",
       "   {'paperId': '5c37023c35fc1c95565d56b4fc4821fcf768651a',\n",
       "    'title': 'Reward is enough for convex MDPs'},\n",
       "   {'paperId': '319cc416ee18c6b85f55c5244d72a72f0bf11470',\n",
       "    'title': 'Neural Production Systems'}],\n",
       "  'citnuminlist': 2,\n",
       "  'refnuminlist': 9,\n",
       "  'isKeypaper': True},\n",
       " '1d6d157f4586ee5fffa172b7198ecb8f7101f921': {'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': 'ed580995e4a51424d8f1a20f5b64200fe227c2cd',\n",
       "    'title': 'Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control'},\n",
       "   {'paperId': 'c4141164bd0b9a0a9bdeb4ea793fc297c796d0d9',\n",
       "    'title': 'Understanding intelligence'},\n",
       "   {'paperId': 'cdc3815faee5a45a2e31ecf8ce4ea5bde5e23172',\n",
       "    'title': 'Model Predictive Control with a Visuomotor System for Physics-based Character Animation'},\n",
       "   {'paperId': 'e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379',\n",
       "    'title': 'V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control'},\n",
       "   {'paperId': '42bfe1fd848644059061ec4ee47f96af14ec6184',\n",
       "    'title': 'TossingBot: Learning to Throw Arbitrary Objects With Residual Physics'},\n",
       "   {'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': 'b65decc03155f2e88984e4fa16493f70e5413e4d',\n",
       "    'title': 'Hierarchical motor control in mammals and machines'},\n",
       "   {'paperId': '585b6022fee837281c5c668a44abaa7188b62562',\n",
       "    'title': 'DReCon: data-driven responsive control of physics-based characters'},\n",
       "   {'paperId': 'f91c670b4125fd9a18bb34d7bd9eaa38542bb9bb',\n",
       "    'title': 'Learning predict-and-simulate policies from unorganized human motion data'},\n",
       "   {'paperId': '68f50216374c50382e17890aa25871230517af13',\n",
       "    'title': 'Neural state machine for character-scene interactions'},\n",
       "   {'paperId': '2d864fe3d8145f32a23e4f5bafb2c8e2af67c062',\n",
       "    'title': 'Data-driven Gaze Animation using Recurrent Neural Networks'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'e9b3c086bdd4135453394e8c61d21e7f7841a31e',\n",
       "    'title': 'Iterative Reinforcement Learning Based Design of Dynamic Locomotion Skills for Cassie'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '40c61fd7a11926dbac9896ba657d3ef480869180',\n",
       "    'title': 'Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bb0ee42d406f2361fee89cf1274073185a0e9eec',\n",
       "    'title': 'Learning agile and dynamic motor skills for legged robots'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': 'f9717d29840f4d8f1cc19d1b1e80c5d12ec40608',\n",
       "    'title': 'A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play'},\n",
       "   {'paperId': '482376177d6ed10aa2975f9858a91e49ec121b00',\n",
       "    'title': 'Physics-based motion capture imitation with deep reinforcement learning'},\n",
       "   {'paperId': 'b47512c1444374add267ed7232b689037972da01',\n",
       "    'title': 'Learning Dexterous In-Hand Manipulation'},\n",
       "   {'paperId': 'f0525cac2a40e11cf2c315e29afecabbe842ec25',\n",
       "    'title': 'Deep learning of biomimetic sensorimotor control for biomechanical human animation'},\n",
       "   {'paperId': '18d4f415b39650006d92e42345264c33750273d0',\n",
       "    'title': 'Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning'},\n",
       "   {'paperId': '4d3b69bdcd1d325d29badc6a38f2d6cc504fe7d1',\n",
       "    'title': 'Sim-to-Real: Learning Agile Locomotion For Quadruped Robots'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'd356a5603f14c7a6873272774782d7812871f952',\n",
       "    'title': 'Reinforcement and Imitation Learning for Diverse Visuomotor Skills'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '80196cdfcd0c6ce2953bf65a7f019971e2026386',\n",
       "    'title': 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures'},\n",
       "   {'paperId': 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b',\n",
       "    'title': 'DeepMind Control Suite'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': '494e2d5b40dcebde349f9872c7317e5003f9c5d2',\n",
       "    'title': 'Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection'},\n",
       "   {'paperId': 'dc693d626059ac032cdaa85d0872e480945f3e21',\n",
       "    'title': 'Learning manipulation skills from a single demonstration'},\n",
       "   {'paperId': '0832a32e2f093c0d326cc2de9651cb2bf53ca80b',\n",
       "    'title': 'Adaptive whole-body manipulation in human-to-humanoid multi-contact motion retargeting'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': '9917363277c783a01bff32af1c27fc9b373ad55d',\n",
       "    'title': 'DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning'},\n",
       "   {'paperId': '6079719b677d0abda12abcd5cd46582ca91585ad',\n",
       "    'title': 'Phase-functioned neural networks for character control'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': '41f2a087031944f9b990eb102f59b4ff58d6b5ef',\n",
       "    'title': 'Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics'},\n",
       "   {'paperId': '5d1864d759d272c5dc928b641d113527a3e81f99',\n",
       "    'title': 'Domain randomization for transferring deep neural networks from simulation to the real world'},\n",
       "   {'paperId': 'f7ac2479e686eb2a7a8afc23f99f213fcd3c5292',\n",
       "    'title': '(CAD)$^2$RL: Real Single-Image Flight without a Single Real Image'},\n",
       "   {'paperId': '505808c55b2d96ad72f4b7bca04572655742b87d',\n",
       "    'title': 'Sim-to-Real Robot Learning from Pixels with Progressive Nets'},\n",
       "   {'paperId': 'aaafe319cc539b24057d6e92d5392fe41c3085b0',\n",
       "    'title': 'Learning Dexterous Manipulation Policies from Experience and Imitation'},\n",
       "   {'paperId': '91f18a1a819ff255833cc16077d38f09be4c991e',\n",
       "    'title': 'Authoring directed gaze for full-body motion capture'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'bc69708aeaae562ab1406ca7dd0e50c1ec247635',\n",
       "    'title': 'Terrain-adaptive locomotion skills using deep reinforcement learning'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': '2c03df8b48bf3fa39054345bafabfeff15bfd11d',\n",
       "    'title': 'Deep Residual Learning for Image Recognition'},\n",
       "   {'paperId': 'f1ef8b6b5fc64a0387be4514c45a15a7f02bb306',\n",
       "    'title': 'A Review of Eye Gaze in Virtual Agents, Social Robotics and HCI: Behaviour Generation, User Interaction and Perception'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': 'd12e9aa253e8dfc2c62033ae135e2ef7d4ee1a65',\n",
       "    'title': 'Catching Objects in Flight'},\n",
       "   {'paperId': '65caf5b7c3fd265b8cf6b3fe736e51f55e29acc8',\n",
       "    'title': 'STAC: Simultaneous tracking and calibration'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '89b9928df443e4e686a4f82b9bd8d67dc23cfa05',\n",
       "    'title': 'Learning and generalization of complex tasks from unstructured demonstrations'},\n",
       "   {'paperId': '98ae87637727d82b181202b7776bbb03f49c9441',\n",
       "    'title': 'Terrain runner'},\n",
       "   {'paperId': '18ddbeaf7a5608e92c56d349121dd0bd29f9410d',\n",
       "    'title': 'Eyecatch: simulating visuomotor coordination for object interception'},\n",
       "   {'paperId': 'b759fe1487c9191d687856c44e1ff9cffa7f2ddb',\n",
       "    'title': 'Discovery of complex behaviors through contact-invariant optimization'},\n",
       "   {'paperId': '806ba8f47af971ab10bfc3cd8d425f1633d8cc1d',\n",
       "    'title': 'Skill learning and task outcome prediction for manipulation'},\n",
       "   {'paperId': '431688ee595f07f7127265634c255a2c93e1b8f9',\n",
       "    'title': 'Space-time planning with parameterized locomotion controllers'},\n",
       "   {'paperId': '79ca373e522fb3f368f864c113269baf85596611',\n",
       "    'title': 'Sampling-based contact-rich motion control'},\n",
       "   {'paperId': '111507f37e92627c78567333620d0dc1b1ff5107',\n",
       "    'title': 'Generalized biped walking control'},\n",
       "   {'paperId': '4d6a87d76ec0c0379f0afbf24f84bba848c6246e',\n",
       "    'title': 'Policy search for motor primitives in robotics'},\n",
       "   {'paperId': 'f31592bf0aa8f8d24ea52576db878a3d557aa8e1',\n",
       "    'title': 'Learning for control from multiple demonstrations'},\n",
       "   {'paperId': 'eb5b459c8a3e56064158fb3514eeab763486e437',\n",
       "    'title': 'Reinforcement learning of motor skills with policy gradients'},\n",
       "   {'paperId': 'f98feb36fdd28d2ec743a6d4dfee9f9a0957aa85',\n",
       "    'title': 'Motion graphs'},\n",
       "   {'paperId': '221472c2fa00a5f7ff5400c60ffddc928e50cab8',\n",
       "    'title': 'SIMBICON: simple biped locomotion control'},\n",
       "   {'paperId': '6d690814ced7170d044f6446341b7b7969dcf21c',\n",
       "    'title': 'Modeling embodied visual behaviors'},\n",
       "   {'paperId': '0f4b7a4a4b37f33d80b146e5222c21a79dc45e91',\n",
       "    'title': 'Reinforcement learning for imitating constrained reaching movements'},\n",
       "   {'paperId': '5813c242f52c424d960ecbc0fd2923ba8539c6ef',\n",
       "    'title': 'Synthesis of Whole-Body Behaviors through Hierarchical Control of Behavioral Primitives'},\n",
       "   {'paperId': '76312b97d154ac37fb855ca0a4f3be423043039a',\n",
       "    'title': 'A Model of the Upper Extremity for Simulating Musculoskeletal Surgery and Analyzing Neuromuscular Control'},\n",
       "   {'paperId': '47b1b7cb41f5ebd327ad351e2c24d4f96cafb327',\n",
       "    'title': 'Automated derivation of behavior vocabularies for autonomous humanoid motion'},\n",
       "   {'paperId': '651bb2de1d6c1e938878cd8fcf042ccf559e0a99',\n",
       "    'title': 'Computational approaches to motor learning by imitation.'},\n",
       "   {'paperId': 'd00595481313734057f8c5205cee3e79a11ebe10',\n",
       "    'title': 'Effective reinforcement learning for mobile robots'},\n",
       "   {'paperId': '212a6148b6d8a48e3e4d935df98f65e51bd534de',\n",
       "    'title': 'Interactive control of avatars animated with human motion data'},\n",
       "   {'paperId': '77a17c7e8f7341cfde92f63a04ea484d2b85881b',\n",
       "    'title': 'Interactive motion generation from examples'},\n",
       "   {'paperId': 'c9a54c513552c88ea815232a4f377662c8e24ad8',\n",
       "    'title': 'Composable controllers for physics-based character animation'},\n",
       "   {'paperId': 'b05b67aca720d0bc39bc9afad02a19f522c7a1bc',\n",
       "    'title': 'Pharmacokinetics of a novel formulation of ivermectin after administration to goats'},\n",
       "   {'paperId': '5cb7b3330d959b170f5d935580ffe2200bbae07a',\n",
       "    'title': 'An Behavior-based Robotics'},\n",
       "   {'paperId': '44d2abe2175df8153f465f6c39b68b76a0d40ab9',\n",
       "    'title': 'Long Short-Term Memory'},\n",
       "   {'paperId': '6ce0468a0827ec3ce9b53a45150e40a46a22cc94',\n",
       "    'title': 'Robot Learning From Demonstration'},\n",
       "   {'paperId': 'a9da09d1e63686706d64782e654d69f13fd292ad',\n",
       "    'title': 'Learning by Demonstration'},\n",
       "   {'paperId': 'd147dfa8f1417f3d7519929d9ae1051f91c98d0a',\n",
       "    'title': 'Animat vision: Active vision in artificial animals'},\n",
       "   {'paperId': '75d540a212127020aed9a0c16d92cc280560f56e',\n",
       "    'title': 'Sensor-actuator networks'},\n",
       "   {'paperId': '16f5015bf514c95be1ec87b9ee098dd227cfc0ae',\n",
       "    'title': 'Animation of dynamic legged locomotion'}],\n",
       "  'citations': [{'paperId': '67722409a74bf8a61b370a6fd5a524b279e2b6f4',\n",
       "    'title': 'Simulation and Retargeting of Complex Multi-Character Interactions'},\n",
       "   {'paperId': '7a993a0aa4929754bc80070e4e1f96d56fd2543d',\n",
       "    'title': 'Perpetual Humanoid Control for Real-time Simulated Avatars'},\n",
       "   {'paperId': 'ca6c9837ef0e3728e72fa6a5257880e2dd042a3c',\n",
       "    'title': 'Composite Motion Learning with Task Control'},\n",
       "   {'paperId': '41ba5dffee04facc136caa54d95a3ab19e131a7d',\n",
       "    'title': 'PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors'},\n",
       "   {'paperId': '98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2',\n",
       "    'title': 'Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ae75404a597b573edd05a129819da8c2385af709',\n",
       "    'title': 'RoboPianist: A Benchmark for High-Dimensional Robot Control'},\n",
       "   {'paperId': 'ce211df49cb2fc836d7d49c628c63df291f96a6b',\n",
       "    'title': 'Learning to Transfer In‐Hand Manipulations Using a Greedy Shape Curriculum'},\n",
       "   {'paperId': 'a38d81be1d373b7f90e6e0128afc33ddbefc5cc7',\n",
       "    'title': 'Through Hawks’ Eyes: Synthetically Reconstructing the Visual Field of a Bird in Flight'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '244aecf3afb80c3ad936c1a6d8d7f3677bb18f33',\n",
       "    'title': 'Synthesizing Physical Character-Scene Interactions'},\n",
       "   {'paperId': '2029600e6c6a6535a48ab84ee0adc33f1234cd83',\n",
       "    'title': 'Efficient Trust Region-Based Safe Reinforcement Learning with Low-Bias Distributional Actor-Critic'},\n",
       "   {'paperId': 'fa52690108fd5d24e0226ecfbdcc967fe418fb71',\n",
       "    'title': 'Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments'},\n",
       "   {'paperId': '39f4e1d98864dc551eb16b2d8e134141a26f28fc',\n",
       "    'title': 'Synthesizing Get‐Up Motions for Physics‐based Characters'},\n",
       "   {'paperId': 'a5978dc8e57c27966d218a06e4ccc8aa3a507507',\n",
       "    'title': 'ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters'},\n",
       "   {'paperId': '9b744d237d3d002d2f90ecfbe037a371c6766f4c',\n",
       "    'title': 'Learning Virtual Chimeras by Dynamic Motion Reassembly'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '3e437b8b131fca7e3fa64a305e1bb43087ccaf9a',\n",
       "    'title': 'Learning High-Risk High-Precision Motion Control'},\n",
       "   {'paperId': '65441ccf98e2055264b1e30e5ab11fbf99a49705',\n",
       "    'title': 'Efficient Multi-Task Learning via Iterated Single-Task Transfer'},\n",
       "   {'paperId': 'a5978dc8e57c27966d218a06e4ccc8aa3a507507',\n",
       "    'title': 'ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '6239a83689d30f769c09c874003cd843750343b1',\n",
       "    'title': 'QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars'},\n",
       "   {'paperId': 'ddb0acda05adf6ae3bc4763575dde39423e8ef6c',\n",
       "    'title': 'Neural3Points: Learning to Generate Physically Realistic Full‐body Motion for Virtual Reality Users'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '3e38ccc5ba9a4b4290060716614e02b7ba54df79',\n",
       "    'title': 'MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control'},\n",
       "   {'paperId': '6a88b9241025357154479687a94d791a3204c958',\n",
       "    'title': 'Physics-based character controllers using conditional VAEs'},\n",
       "   {'paperId': '680a374378f2c1a3f40a58bc3fa4da1a1b678924',\n",
       "    'title': 'Learning to Use Chopsticks in Diverse Styles'},\n",
       "   {'paperId': '0417d220540173987693e6d9707d34cf204bc025',\n",
       "    'title': 'Learning to use chopsticks in diverse gripping styles'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '3463c53a49f428b9898de60861c57a9f82215b9e',\n",
       "    'title': 'Generative GaitNet'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '82207d9860e516022c38ceaad8a9e88a15b39c98',\n",
       "    'title': 'GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'eb49a25eda2e131ac8f95e95aeddf38ac7e9df6f',\n",
       "    'title': 'Pose2Room: Understanding 3D Scenes from Human Activities'},\n",
       "   {'paperId': '4424652c21574544a177d5879a9ad95d3c6d2fdc',\n",
       "    'title': 'A Survey on Deep Learning for Skeleton‐Based Human Animation'},\n",
       "   {'paperId': '66d6cf5181ca209bd4bc0350961c14a03ee8fbd6',\n",
       "    'title': 'Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization'},\n",
       "   {'paperId': '065dc8e953d5e5da138c33a8c3f4f7181b19cd54',\n",
       "    'title': 'Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '29be0eddfff83aab67a2edab40e04d97a226c0c5',\n",
       "    'title': 'Learning Task-Agnostic Action Spaces for Movement Optimization'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'f4e57e9baa8aaf1bc3a5ad1ead42a134a04c2bf8',\n",
       "    'title': 'DeepPhase: periodic autoencoders for learning motion phase manifolds'},\n",
       "   {'paperId': '49e2e6f316cadeba3d2013fb60584fad2121f106',\n",
       "    'title': 'Human dynamics from monocular video with dynamic camera movements'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': '14c57993e50ee37f9d13b2eca332f3265bd91c99',\n",
       "    'title': 'Flexible Motion Optimization with Modulated Assistive Forces'},\n",
       "   {'paperId': '4132db5416d9ac9addffd5e50c2df7ac3b691810',\n",
       "    'title': 'DASH: Modularized Human Manipulation Simulation with Vision and Language for Embodied AI'},\n",
       "   {'paperId': '7c5144952e771330c10d5facd49115c5ddead0d0',\n",
       "    'title': 'Stochastic Scene-Aware Motion Prediction'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '77da995d98cb4a4bd154f54553cd60bf7686c281',\n",
       "    'title': 'Learning a family of motor skills from a single motion clip'},\n",
       "   {'paperId': 'f197ba9e30df222d6676e7055f41d818acdd09b1',\n",
       "    'title': 'ManipNet'},\n",
       "   {'paperId': '90287fb351bbdefa85b25c19e01545effa8243f9',\n",
       "    'title': 'Learning a family of motor skills from a single motion clip'},\n",
       "   {'paperId': '477eee0c0180785b8281c3789b8a4c39c12725c3',\n",
       "    'title': 'Interactive Characters for Virtual Reality Stories'},\n",
       "   {'paperId': '69d8171bb34b62b4b895863c67816429f5802c27',\n",
       "    'title': 'Efficient Hyperparameter Optimization for Physics-based Character Animation'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'd9e8ac993cd75c1ab9364ba30a86444f2537a9c4',\n",
       "    'title': 'SimPoE: Simulated Character Control for 3D Human Pose Estimation'},\n",
       "   {'paperId': '13f6165633f67604ecaa4662448a0cc62d1674a5',\n",
       "    'title': 'Learning Human Search Behavior from Egocentric Visual Inputs'},\n",
       "   {'paperId': '8c28132e7bca149b9f0c5f7c443716b0abe193a5',\n",
       "    'title': 'Wide Obstacle & Sphere MountainThin Obstacle & Sphere Moving Obstacle'},\n",
       "   {'paperId': 'c54050891234d4b4422df5d63ddefc76b5edd60c',\n",
       "    'title': 'Creating Deep Learning-based Acrobatic Videos Using Imitation Videos'},\n",
       "   {'paperId': '9c5d83974888b30f842a6b3d7e335f7cfd1c10c2',\n",
       "    'title': 'ManipNet: neural manipulation synthesis with a hand-object spatial representation'},\n",
       "   {'paperId': 'd72136467b952f2ec855558936ec871a5c342fa2',\n",
       "    'title': 'Directable Physics-Based Character Animation'},\n",
       "   {'paperId': 'b6f35630a16c22c9e9a36a86a82525f5d8034306',\n",
       "    'title': 'Deep Reinforcement Learning for Task Planning of Virtual Characters'},\n",
       "   {'paperId': '8b62d928a7be4a6f408cc7a433c215a749604a95',\n",
       "    'title': 'UniCon: Universal Neural Controller For Physics-based Character Motion'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '3c279a4760315ca9ab29255ff7ff0a9c1717948b',\n",
       "    'title': 'Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for Physically Embedded 3D Sokoban'},\n",
       "   {'paperId': '5dab673e999a2ea7be23d57467fab4276c6e7ede',\n",
       "    'title': 'Never Stop Learning: The Effectiveness of Fine-Tuning in Robotic Reinforcement Learning'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'}],\n",
       "  'citnuminlist': 8,\n",
       "  'refnuminlist': 4,\n",
       "  'isKeypaper': True},\n",
       " 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc': {'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': '63196e0189bf710abcacaf418edbdc29e7750b94',\n",
       "    'title': 'A Distributional View on Multi-Objective Policy Optimization'},\n",
       "   {'paperId': 'b68a38f6fb6061d641f85b0de5daf7372eb29da2',\n",
       "    'title': 'The Variational Bandwidth Bottleneck: Stochastic Evaluation on an Information Budget'},\n",
       "   {'paperId': 'e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379',\n",
       "    'title': 'V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': '585b6022fee837281c5c668a44abaa7188b62562',\n",
       "    'title': 'DReCon: data-driven responsive control of physics-based characters'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'bf7f1ada5feecc0992f71b39c1ebeccb19ae631b',\n",
       "    'title': 'InfoBot: Transfer and Exploration via the Information Bottleneck'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '482376177d6ed10aa2975f9858a91e49ec121b00',\n",
       "    'title': 'Physics-based motion capture imitation with deep reinforcement learning'},\n",
       "   {'paperId': '395c976f517c84e204c7320a289b2b0c60d1fb66',\n",
       "    'title': 'Learning Whole-Body Motor Skills for Humanoids'},\n",
       "   {'paperId': '7d85e83ae00f2d19b3cdb01f5feeb92a2102104f',\n",
       "    'title': 'Task-Embedded Control Networks for Few-Shot Imitation Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': None,\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills -Supplementary Material Peng'},\n",
       "   {'paperId': None, 'title': 'URL https://cloud'},\n",
       "   {'paperId': 'cddb1f7f9f004396a2efef285caf29d7780a8e21',\n",
       "    'title': 'Robust Imitation of Diverse Behaviors'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': 'e822b8ea156649133b0a9ae3670535f49bd53605',\n",
       "    'title': 'Learning to Schedule Control Fragments for Physics-Based Characters Using Deep Q-Learning'},\n",
       "   {'paperId': '3c63f8b8263cd6cc4c8c7429d46bb656accddc49',\n",
       "    'title': 'Hybrid Reward Architecture for Reinforcement Learning'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4603f26093d64fc4107d3ec49667003f60210654',\n",
       "    'title': 'Generalizing Skills with Semi-Supervised Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'dc3e905bfb27d21675ee1720413e007b014b37d3',\n",
       "    'title': 'Safe and Efficient Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "    'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       "   {'paperId': '2e36ea91a3c8fbff92be2989325531b4002e2afc',\n",
       "    'title': 'Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models'},\n",
       "   {'paperId': '1616fcc95f5183209b80f71272d6649527dbccf4',\n",
       "    'title': 'Learning parameterized motor skills on a humanoid robot'},\n",
       "   {'paperId': '66ad2fbc8b73242a889699868611fcf239e3435d',\n",
       "    'title': 'Semi-supervised Learning with Deep Generative Models'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Semi - supervised learning with deep generative mod'},\n",
       "   {'paperId': '4aa4069693bee00d1b0759ca3df35e59284e9845',\n",
       "    'title': 'DeViSE: A Deep Visual-Semantic Embedding Model'},\n",
       "   {'paperId': '3a81cfb4a7a880b7cf8979f6067732e961aceb7c',\n",
       "    'title': 'Probabilistic Movement Primitives'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '1695dbabf8e905db0b391ff522c323db5fc8b958',\n",
       "    'title': 'Learning to select and generalize striking movements in robot table tennis'},\n",
       "   {'paperId': '79ca373e522fb3f368f864c113269baf85596611',\n",
       "    'title': 'Sampling-based contact-rich motion control'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': 'c9a54c513552c88ea815232a4f377662c8e24ad8',\n",
       "    'title': 'Composable controllers for physics-based character animation'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': '3fe2d23b3061500bec6623c4b299aed9e258a953',\n",
       "    'title': 'Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control'},\n",
       "   {'paperId': '7a993a0aa4929754bc80070e4e1f96d56fd2543d',\n",
       "    'title': 'Perpetual Humanoid Control for Real-time Simulated Avatars'},\n",
       "   {'paperId': '41ba5dffee04facc136caa54d95a3ab19e131a7d',\n",
       "    'title': 'PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors'},\n",
       "   {'paperId': '9c6f0ca7ff6555f308fdb6235bf0dd11ec091929',\n",
       "    'title': 'DiffMimic: Efficient Motion Mimicking with Differentiable Physics'},\n",
       "   {'paperId': '8d32b150ce19200a2d2d71f68b63972080ef99ad',\n",
       "    'title': 'Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '996a91acf87a2ef80f3ea624c1cd4bf0fc5ce4ba',\n",
       "    'title': 'User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '65441ccf98e2055264b1e30e5ab11fbf99a49705',\n",
       "    'title': 'Efficient Multi-Task Learning via Iterated Single-Task Transfer'},\n",
       "   {'paperId': '3e38ccc5ba9a4b4290060716614e02b7ba54df79',\n",
       "    'title': 'MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control'},\n",
       "   {'paperId': '6a88b9241025357154479687a94d791a3204c958',\n",
       "    'title': 'Physics-based character controllers using conditional VAEs'},\n",
       "   {'paperId': 'f7c1433665ec94eb9092d22e3042311d9c754615',\n",
       "    'title': 'Visual Pre-training for Navigation: What Can We Learn from Noise?'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': '2ce42614bbffe69dfbcda4d4b13ffaac87213412',\n",
       "    'title': 'Transferable and Adaptable Driving Behavior Prediction'},\n",
       "   {'paperId': '6d6e58607cf068273a2da5c66513b4cd4f110b2e',\n",
       "    'title': 'Learning Coordinated Terrain-Adaptive Locomotion by Imitating a Centroidal Dynamics Planner'},\n",
       "   {'paperId': '09c04311402819d6171cdb47b2a381f10d4f564e',\n",
       "    'title': 'Hierarchical Primitive Composition: Simultaneous Activation of Skills with Inconsistent Action Dimensions in Multiple Hierarchies'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '29be0eddfff83aab67a2edab40e04d97a226c0c5',\n",
       "    'title': 'Learning Task-Agnostic Action Spaces for Movement Optimization'},\n",
       "   {'paperId': 'bafbb3c535d9ee0fbffaad266f732a3892f53b4e',\n",
       "    'title': 'Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review'},\n",
       "   {'paperId': '1e0f91b88b60b3bb7f35824d0526263bb6e39781',\n",
       "    'title': 'OstrichRL: A Musculoskeletal Ostrich Simulation to Study Bio-mechanical Locomotion'},\n",
       "   {'paperId': 'b69a7afd74c6a70fa3a1bf2934143afcd1f54dc3',\n",
       "    'title': 'Hierarchical Adaptable and Transferable Networks (HATN) for Driving Behavior Prediction'},\n",
       "   {'paperId': 'f5aeaba6a0d4df824d752bceb583883c10d2ade9',\n",
       "    'title': 'Unsupervised Domain Adaptation with Dynamics-Aware Rewards in Reinforcement Learning'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': '26cf79bce38d3981707aaa94ea8291ade77dedf9',\n",
       "    'title': 'Manipulator-Independent Representations for Visual Imitation'},\n",
       "   {'paperId': '63196e0189bf710abcacaf418edbdc29e7750b94',\n",
       "    'title': 'A Distributional View on Multi-Objective Policy Optimization'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 9,\n",
       "  'isKeypaper': True},\n",
       " 'e90323d515a024be8a6d0465dd90eefd681f9245': {'title': 'Discovering Motor Programs by Recomposing Demonstrations',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '5df5561b55ab872f2b3df559ddd475299f660b42',\n",
       "    'title': 'Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration'},\n",
       "   {'paperId': 'df350766b53d4db23790a0408b0d2c7a185cff74',\n",
       "    'title': 'Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data for Imitation'},\n",
       "   {'paperId': 'c35f8f48f748c040c33bbe1f21c794e209341987',\n",
       "    'title': 'Neural Program Synthesis from Diverse Demonstration Videos'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': '809f951c77b5a39e2a9d556e9cf9938de87f2393',\n",
       "    'title': 'An Inference-Based Policy Gradient Method for Learning Options'},\n",
       "   {'paperId': 'bf48f1d556fdb85d5dbe8cfd93ef13c212635bcf',\n",
       "    'title': 'Neural Task Programming: Learning to Generalize Across Hierarchical Tasks'},\n",
       "   {'paperId': '494e2d5b40dcebde349f9872c7317e5003f9c5d2',\n",
       "    'title': 'Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection'},\n",
       "   {'paperId': '694d284d7c63c7f29c48b540bc4ece9f63b79160',\n",
       "    'title': 'Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "    'title': 'Attention is All you Need'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None,\n",
       "    'title': 'OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': '8cf83c619423a1504f26495d5f6a495054c46462',\n",
       "    'title': 'Learning to Poke by Poking: Experiential Learning of Intuitive Physics'},\n",
       "   {'paperId': '90188082284ef4fe0d23a9216fec919a964520ff',\n",
       "    'title': 'TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning'},\n",
       "   {'paperId': 'f03b4ff1b4943691cec703b508c0a91f2d97a881',\n",
       "    'title': 'Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': 'dce1da37953f6f54abaf5e34d1096dbbb822f416',\n",
       "    'title': 'Towards learning hierarchical skills for multi-phase manipulation tasks'},\n",
       "   {'paperId': 'a103a6ca72f50f0830638ef4a4fb0d8c0ffba456',\n",
       "    'title': 'Movement primitives via optimization'},\n",
       "   {'paperId': '9d242175cbe2f082da78e469bc9b23144c33b320',\n",
       "    'title': 'Learning modular policies for robotics'},\n",
       "   {'paperId': 'f258a0540505b0057b3f5ffc084cebad1554b03c',\n",
       "    'title': 'Towards Robot Skill Learning: From Simple Skills to Table Tennis'},\n",
       "   {'paperId': '4177ec52d1b80ed57f2e72b0f9a42365f1a8598d',\n",
       "    'title': 'Speech recognition with deep recurrent neural networks'},\n",
       "   {'paperId': '89b9928df443e4e686a4f82b9bd8d67dc23cfa05',\n",
       "    'title': 'Learning and generalization of complex tasks from unstructured demonstrations'},\n",
       "   {'paperId': '6b92156c9e6ea53dec9e4941b62c11a1fbda485e',\n",
       "    'title': 'Robot learning from demonstration by constructing skill trees'},\n",
       "   {'paperId': '271a4077c037b86fb7daf6bff3e66682322ff7d7',\n",
       "    'title': 'Movement segmentation using a primitive library'},\n",
       "   {'paperId': '0645c8792d5095f0de45e95c5fd9de5238f426e6',\n",
       "    'title': 'Learning motor primitives for robotics'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': '84b8551bb1f80f2bf61ea3ef1599f4dc99e7c91c',\n",
       "    'title': 'Skill Chaining : Skill Discovery in Continuous Domains'},\n",
       "   {'paperId': '1c46943103bd7b7a2c7be86859995a4144d1938b',\n",
       "    'title': 'Visualizing Data using t-SNE'},\n",
       "   {'paperId': None, 'title': 'Visualizing high-dimensional data using t-sne'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': 'e7dbbbd570006bb6255667f87780a325277ca591',\n",
       "    'title': 'Natural methods for robot task learning: instructive demonstrations, generalization and practice'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'dc19d325198236660f80311b27fb8b9db76ce623',\n",
       "    'title': 'Multiple paired forward and inverse models for motor control'},\n",
       "   {'paperId': 'db4849a2e61ae47fbc52efe1c855c618bba0e7b8',\n",
       "    'title': 'Behavioural cloning in control of a dynamic system'},\n",
       "   {'paperId': '1ac57524ba2d2a69c1bb6defed7352a06fd7050d',\n",
       "    'title': 'Using Dynamic Time Warping to Find Patterns in Time Series'},\n",
       "   {'paperId': '2a1332efbef8d0a67fd78ce0cfa69fc5117a933a',\n",
       "    'title': 'A schema theory of discrete motor skill learning.'},\n",
       "   {'paperId': 'c547e1f79e6039d05c5ae433a36612d7f8e4d3f5',\n",
       "    'title': 'STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving'},\n",
       "   {'paperId': '886adc4f19c043bce3dd964e7f4b649cd9db4e1e',\n",
       "    'title': 'A closed-loop theory of motor learning.'},\n",
       "   {'paperId': '581df3c47ab6829097602956cf7205380aab1d3a',\n",
       "    'title': 'Movement control in skilled motor performance.'},\n",
       "   {'paperId': 'a6452cb80b5f36b8ca298f3ee7d40a6378210c3e',\n",
       "    'title': 'The problem of serial order in behavior'}],\n",
       "  'citations': [{'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "    'title': 'NetHack is Hard to Hack'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'd914dc7f5d9291ee2127936e3206c90ca1fcea71',\n",
       "    'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "    'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': 'ad4707bb87c6fc087e09d9f6609665b53835c899',\n",
       "    'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '77d3d69f1c4c160e3765c416bc13aed863176197',\n",
       "    'title': 'One After Another: Learning Incremental Skills for a Changing World'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': '69f657fbafa51a4411dd6aba63091d616114959d',\n",
       "    'title': 'SHERLock: Self-Supervised Hierarchical Event Representation Learning'},\n",
       "   {'paperId': '193dadd9de36ef8e6883088fbd35d38fa7ce590e',\n",
       "    'title': 'Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '864a04cf024a0fd04f0e80af6273f997543ef7db',\n",
       "    'title': 'Rapid adaptation of brain–computer interfaces to new neuronal ensembles or participants via generative modelling'},\n",
       "   {'paperId': '0f929e132ccc363a2d707c24319c2894935d93c8',\n",
       "    'title': 'Video2Skill: Adapting Events in Demonstration Videos to Skills in an Environment using Cyclic MDP Homomorphisms'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '3e85d208b1b927fdb69ecf8336c70995818aaebd',\n",
       "    'title': 'MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale'},\n",
       "   {'paperId': '9956e3ea2b894f45ca9070ee1984caadb74edbf7',\n",
       "    'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': None, 'title': 'TRANSLATING ROBOT SKILLS: LEARNING UNSUPER-'},\n",
       "   {'paperId': 'fb2769a88b3eb152779694768038d3715a9274ba',\n",
       "    'title': 'Broad Generalization in Robotics Will Require Broad Datasets'},\n",
       "   {'paperId': '0a02607e1774b50ae68835b632f6ab6275fbc098',\n",
       "    'title': 'Robot Learning from Observations'},\n",
       "   {'paperId': 'c9af30358358b15d05ce72a86ec5f0ce883afdc6',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Single Demonstration'},\n",
       "   {'paperId': 'ec8eee61f42e07228bc78faac9817cff3e000ebf',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Demonstration'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '3d176aa8e0f93a2c1c6ca59fbd3da4b36c7f5940',\n",
       "    'title': 'Unsupervised Hierarchical Concept Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'}],\n",
       "  'citnuminlist': 9,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " '6ec8797952213227eea2e63620f4d7c060d598d5': {'title': 'Hierarchical reinforcement learning for efficient exploration and transfer',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'be0d8e13a8aa83ee214292fe79ad86ef2a5055e5',\n",
       "    'title': 'Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': '66605b6ceae9847156526e46ca9fe467804fca54',\n",
       "    'title': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e0889fcee1acd985af76a3907d5d0029bf260be9',\n",
       "    'title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning'},\n",
       "   {'paperId': 'c520bf47db3360ae3a52219771390a354ed8a91f',\n",
       "    'title': 'Go-Explore: a New Approach for Hard-Exploration Problems'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd397f4cf400f6ffcb1b8e3db27bb75966a0513cf',\n",
       "    'title': 'Self-Imitation Learning'},\n",
       "   {'paperId': '90dbf33d452de57c5a4a55631d05df9509f721e9',\n",
       "    'title': 'Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning'},\n",
       "   {'paperId': 'a8bf5d45bd97972251ff2258b15cfefca1f196be',\n",
       "    'title': 'Strategic Object Oriented Reinforcement Learning'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': '2dad7e558a1e2982d0d42042021f4cde4af04abf',\n",
       "    'title': 'Dilated Recurrent Neural Networks'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': 'e6c82a3e91ef732b96012fd08a634a9b0e23d765',\n",
       "    'title': 'Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret'},\n",
       "   {'paperId': '50e9a441f56124b7b969e6537b66469a0e1aa707',\n",
       "    'title': 'Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction'},\n",
       "   {'paperId': '237a1cf18ed83bb3ad852b34f443c6c1ff3336c1',\n",
       "    'title': 'An analysis of model-based Interval Estimation for Markov Decision Processes'},\n",
       "   {'paperId': None,\n",
       "    'title': 'An analysis of modelbased interval estimation for Markov decision processes'},\n",
       "   {'paperId': '42af0ed020c2caecafb7dbe826064d7f9ba2022b',\n",
       "    'title': 'Dynamic abstraction in reinforcement learning via clustering'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '8090121ad488b4af27bc59bf91b62e9c6a6f49c6',\n",
       "    'title': 'Markov Decision Processes: Discrete Stochastic Dynamic Programming'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'fc3b5dc5528e24dbbc8f4ec273e622ce40eec855',\n",
       "    'title': 'Disentangling Transfer in Continual Reinforcement Learning'},\n",
       "   {'paperId': '589047ec02f30a81c5057de30ef9a84a92fe0928',\n",
       "    'title': 'Efficient search of active inference policy spaces using k-means'},\n",
       "   {'paperId': '263f352d9d2071680429bef9bdee0a1098692578',\n",
       "    'title': 'Efficient Exploration for Multi-Agent Reinforcement Learning via Transferable Successor Features'},\n",
       "   {'paperId': '1bb82660573bbaf01572041da16842ee2398ae39',\n",
       "    'title': 'Learning Robust Real-Time Cultural Transmission without Human Data'},\n",
       "   {'paperId': '1eb34aba4d1f8744958811f0197adb9286f4fee2',\n",
       "    'title': 'Challenging social media threats using collective well-being-aware recommendation algorithms and an educational virtual companion'},\n",
       "   {'paperId': '72052073bbcf9a7ae1782790a8b07b4309b36d96',\n",
       "    'title': 'Toward an Adaptive Threshold on Cooperative Bandwidth Management Based on Hierarchical Reinforcement Learning'}],\n",
       "  'citnuminlist': 1,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " 'eadbe2e4f9de47dd357589cf59e3d1f0199e5075': {'title': 'Learning quadrupedal locomotion over challenging terrain',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': '1803722f786b901a744bc363c0ebdc51902ceceb',\n",
       "    'title': 'Learning Agile Robotic Locomotion Skills by Imitating Animals'},\n",
       "   {'paperId': '003987bfff295e76946bf430376af4fe3d466cb4',\n",
       "    'title': 'Learning to Walk in the Real World with Minimal Human Effort'},\n",
       "   {'paperId': '291f68313aed46698da268aa4d455753fa0a0d66',\n",
       "    'title': 'Heuristic Planning for Rough Terrain Locomotion in Presence of External Disturbances and Variable Perception Quality'},\n",
       "   {'paperId': 'fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258',\n",
       "    'title': 'Teacher–Student Curriculum Learning'},\n",
       "   {'paperId': 'ca5045c9d9e0bf2e95f6694dff657e28ffcd4f07',\n",
       "    'title': 'Learning by Cheating'},\n",
       "   {'paperId': '320b227027030fc291de2896fc3c6da49d7614be',\n",
       "    'title': \"Solving Rubik's Cube with a Robot Hand\"},\n",
       "   {'paperId': '13c1eba20d8e2ec3e636c83b37fffba38fb0be72',\n",
       "    'title': 'Dynamic Locomotion on Slippery Ground'},\n",
       "   {'paperId': 'd078f720a7fb0e1961a17ea967332599e6d2b692',\n",
       "    'title': 'Data Efficient Reinforcement Learning for Legged Robots'},\n",
       "   {'paperId': 'f54fa72dd88b85c633e0bbebbc106e4c7209cf59',\n",
       "    'title': 'Dynamic Walking with Compliance on a Cassie Bipedal Robot'},\n",
       "   {'paperId': '40c61fd7a11926dbac9896ba657d3ef480869180',\n",
       "    'title': 'Robust Recovery Controller for a Quadrupedal Robot using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bb0ee42d406f2361fee89cf1274073185a0e9eec',\n",
       "    'title': 'Learning agile and dynamic motor skills for legged robots'},\n",
       "   {'paperId': 'c48ca266c1e16f9adc5fb7770afd95a0feec8753',\n",
       "    'title': 'Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions'},\n",
       "   {'paperId': '2ed619fbc7902155d54f6f21da16ad6c120eac63',\n",
       "    'title': 'Learning to Walk via Deep Reinforcement Learning'},\n",
       "   {'paperId': '76a89e10661d5ccb96af2c76625c24e909ab3b84',\n",
       "    'title': 'Feedback Control of a Cassie Bipedal Robot: Walking, Standing, and Riding a Segway'},\n",
       "   {'paperId': '719068eb8b8c9ab8552ec3e82c1b1088a9eacdce',\n",
       "    'title': 'Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Vision 60: Latest blind-mode stress testing of V60 legged robot'},\n",
       "   {'paperId': 'c78c8a921a4171b373e7a298d2803940c935ee34',\n",
       "    'title': 'Policies Modulating Trajectory Generators'},\n",
       "   {'paperId': '28a8270e0e9195172bd102263e4eb98332d674d7',\n",
       "    'title': 'Contact Model Fusion for Event-Based Locomotion in Unstructured Terrains'},\n",
       "   {'paperId': '4d3b69bdcd1d325d29badc6a38f2d6cc504fe7d1',\n",
       "    'title': 'Sim-to-Real: Learning Agile Locomotion For Quadruped Robots'},\n",
       "   {'paperId': '921196c32213a229245a9705ee4768bc941e7a26',\n",
       "    'title': 'An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling'},\n",
       "   {'paperId': 'a2c5a02559fe47aa3508ed6df8e739b63f3a20bf',\n",
       "    'title': 'Learning symmetric and low-energy locomotion'},\n",
       "   {'paperId': '8357a92444f0707c55e3b90721108e4c918338b7',\n",
       "    'title': 'Dynamic Locomotion Through Online Nonlinear Motion Optimization for Quadrupedal Robots'},\n",
       "   {'paperId': '7c2823787817ec2012a0f846a1beea3ed872a16f',\n",
       "    'title': 'Per-Contact Iteration Method for Solving Contact Dynamics'},\n",
       "   {'paperId': '072efec6c62b50a50e4a15318e84561d240e9958',\n",
       "    'title': 'Legged Robot State-Estimation Through Combined Forward Kinematic and Preintegrated Contact Factors'},\n",
       "   {'paperId': '0af8cdb71ce9e5bf37ad2a11f05af293cfe62172',\n",
       "    'title': 'Sim-to-Real Transfer of Robotic Control with Dynamics Randomization'},\n",
       "   {'paperId': 'ae6ac70f3282e37f5c6ffcbf9405d50af1e1448b',\n",
       "    'title': 'Learning Ground Traversability From Simulations'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': 'af70f66f056cd50d17bad36787b6ca56fabbfe5f',\n",
       "    'title': 'Minimal criterion coevolution: a new approach to open-ended search'},\n",
       "   {'paperId': 'ff7bcaa4556cb13fc7bf03e477172493546172cd',\n",
       "    'title': 'What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?'},\n",
       "   {'paperId': '50048e4cc92808c58fee29d5be1ab0b8e86cb1fe',\n",
       "    'title': 'Probabilistic Contact Estimation and Impact Detection for State Estimation of Quadruped Robots'},\n",
       "   {'paperId': 'a854720600da774f110f6db00650e6e38969e063',\n",
       "    'title': 'Probabilistic foot contact estimation by fusing information from dynamics and differential/forward kinematics'},\n",
       "   {'paperId': 'bfba319e021ec261ad7cad164749cc5eb85951f5',\n",
       "    'title': 'ANYmal - a highly mobile and dynamic quadrupedal robot'},\n",
       "   {'paperId': '69c61df39fe836b7d1a843ab2fbb07862e04b4db',\n",
       "    'title': 'Dynamic trotting on slopes for quadrupedal robots'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'd185c6739e123b2273edfe3f86f06fe73b82a13f',\n",
       "    'title': 'Slip Detection and Recovery for Quadruped Robots'},\n",
       "   {'paperId': 'adfcf065e15fd3bc9badf6145034c84dfb08f204',\n",
       "    'title': 'Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling'},\n",
       "   {'paperId': '13983afc29c66f09123b0f685a53c4f9d1b10a01',\n",
       "    'title': 'ROBOT-CENTRIC ELEVATION MAPPING WITH UNCERTAINTY ESTIMATES'},\n",
       "   {'paperId': 'dc6ac3437f0a6e64e4404b1b9d188394f8a3bf71',\n",
       "    'title': 'Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps'},\n",
       "   {'paperId': '6e4f257c88961d79f5cf3a9d97a39c6d838ebf18',\n",
       "    'title': 'State estimation for legged robots on unstable and slippery terrain'},\n",
       "   {'paperId': 'a9ff7e3eb883cf033bcd87a90c70194aab52e808',\n",
       "    'title': 'A reactive controller framework for quadrupedal locomotion on challenging terrain'},\n",
       "   {'paperId': None, 'title': 'bulletphysics'},\n",
       "   {'paperId': None, 'title': 'Bullet physics library (2013); pybullet.org'},\n",
       "   {'paperId': '332d222681f6457fddef92340375b98178e5e7a8',\n",
       "    'title': 'State Estimation for Legged Robots - Consistent Fusion of Leg Kinematics and IMU'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '30dbd52334d49d95fb0d0cb48b95e9588bceca0b',\n",
       "    'title': 'A Survey of Procedural Noise Functions'},\n",
       "   {'paperId': '9143fffbcc2212e4a8b4421c926248035e6a0b70',\n",
       "    'title': 'Revising the evolutionary computation abstraction: minimal criteria novelty search'},\n",
       "   {'paperId': '785b1f1baf862ebb7255a747695ced7f87f97112',\n",
       "    'title': 'A Survey of Procedural Methods for Terrain Modelling'},\n",
       "   {'paperId': 'ddafc0ef153683d56b7cd08b59d02f576e1c0a3f',\n",
       "    'title': 'Efficient Bipedal Robots Based on Passive-Dynamic Walkers'},\n",
       "   {'paperId': None, 'title': 'Open dynamics engine (2005); ode.org'},\n",
       "   {'paperId': '95ca81e5ac7ef8b983098e23353af13a51e2b5c2',\n",
       "    'title': 'Principles of Animal Locomotion'},\n",
       "   {'paperId': 'e99eda93c1090c606800064f7d904fa006e64332',\n",
       "    'title': 'Series elastic actuators'},\n",
       "   {'paperId': '26bc0449360d7016f684eafae5b5d2feded32041',\n",
       "    'title': 'An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories'},\n",
       "   {'paperId': None,\n",
       "    'title': 'This article cites 8 articles, 1 of which you can access for free'}],\n",
       "  'citations': [{'paperId': '8dcdd8a94493412ca72f4e74379540797b4731d8',\n",
       "    'title': 'Game changers in science and technology - now and beyond'},\n",
       "   {'paperId': '9386f62ce3725b58d26ee07a164de71035be3a37',\n",
       "    'title': 'Maneuvering on non-Newtonian fluidic terrain: a survey of animal and bio-inspired robot locomotion techniques on soft yielding grounds'},\n",
       "   {'paperId': 'e1ec853d49c36417b2fa0a1c9e07057fd66a0b8c',\n",
       "    'title': 'Synaptic motor adaptation: A three-factor learning rule for adaptive robotic control in spiking neural networks'},\n",
       "   {'paperId': '30b6356e9856433fce36e17457c10df6e0c3b332',\n",
       "    'title': 'Design of a novel side-mounted leg mechanism with high flexibility for a multi-mission quadruped earth rover BJTUBOT'},\n",
       "   {'paperId': 'f3c0e8a4693a69836e93e3ad313863886fb53a6c',\n",
       "    'title': 'LeggedWalking on Inclined Surfaces'},\n",
       "   {'paperId': '7cd083071f70cce2278a3a62305ab747dad41aba',\n",
       "    'title': 'The neuromechanics of animal locomotion: From biology to robotics and back'},\n",
       "   {'paperId': 'd102a6b4ebf6848095c7a44117e6a1ee0fd8a1f7',\n",
       "    'title': 'On the Linear Convergence of Policy Gradient under Hadamard Parameterization'},\n",
       "   {'paperId': '6de25ae066cd85e1e1417f587c38afd5cbf2233a',\n",
       "    'title': 'Hierarchical Vision Navigation System for Quadruped Robots with Foothold Adaptation Learning'},\n",
       "   {'paperId': '1b62e1d84164cefc61d502f1d35d7cf740b4dba0',\n",
       "    'title': 'Privileged Knowledge Distillation for Sim-to-Real Policy Generalization'},\n",
       "   {'paperId': '3f65c577888ea5933ce3e193b3e3b25c547d812f',\n",
       "    'title': 'RL + Model-based Control: Using On-demand Optimal Control to Learn Versatile Legged Locomotion'},\n",
       "   {'paperId': 'bcf4b63c32f20aab48355276fa90782db3b6321a',\n",
       "    'title': 'Cross-Domain Policy Adaptation via Value-Guided Data Filtering'},\n",
       "   {'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': '47f3828e8d93fb0b74ebf636148cf943f8dcc4d9',\n",
       "    'title': 'Maneuverable and Efficient Locomotion of a Myriapod Robot with Variable Body-Axis Flexibility via Instability and Bifurcation.'},\n",
       "   {'paperId': 'dd50f8e92ba42f93a20463c4f4edf6a48340af4c',\n",
       "    'title': 'Barkour: Benchmarking Animal-level Agility with Quadruped Robots'},\n",
       "   {'paperId': '8269c424aa7bfc6ac216e291f4e5e985b9727031',\n",
       "    'title': 'M-EMBER: Tackling Long-Horizon Mobile Manipulation via Factorized Domain Transfer'},\n",
       "   {'paperId': '132e507c80e1e77f31b6fa41c1c7ca5863844bfb',\n",
       "    'title': 'DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training'},\n",
       "   {'paperId': '2211948812d471a8367cfabc75eb692745d23b66',\n",
       "    'title': 'From Data-Fitting to Discovery: Interpreting the Neural Dynamics of Motor Control through Reinforcement Learning'},\n",
       "   {'paperId': '3fe2d23b3061500bec6623c4b299aed9e258a953',\n",
       "    'title': 'Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control'},\n",
       "   {'paperId': '86abde61814c147e6b126fa35b5da381eced7d01',\n",
       "    'title': 'Learning Quadruped Locomotion using Bio-Inspired Neural Networks with Intrinsic Rhythmicity'},\n",
       "   {'paperId': '1ead198b54feb7229de81d1dbda51094b0eabb65',\n",
       "    'title': 'A Survey on Population-Based Deep Reinforcement Learning'},\n",
       "   {'paperId': 'dda84c5a64267826769e36b86ba64a2780c0d651',\n",
       "    'title': 'Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement'},\n",
       "   {'paperId': 'ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7',\n",
       "    'title': 'Behavior Contrastive Learning for Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'd69aaf1e8e955fe37a264ae97a5e1963ae118650',\n",
       "    'title': 'Multilegged matter transport: A framework for locomotion on noisy landscapes'},\n",
       "   {'paperId': 'eac18b558dc2a7fae4ce6b77b999b3a624ccb8d0',\n",
       "    'title': 'Enhancing Efficiency of Quadrupedal Locomotion over Challenging Terrains with Extensible Feet'},\n",
       "   {'paperId': '25084def2daa34b27112f5d258f56c8ea25ed439',\n",
       "    'title': 'More Than an Arm: Using a Manipulator as a Tail for Enhanced Stability in Legged Locomotion'},\n",
       "   {'paperId': '1857dc2570eb0da12b710da8eed195ce3643625d',\n",
       "    'title': 'IndoorSim-to-OutdoorReal: Learning to Navigate Outdoors without any Outdoor Experience'},\n",
       "   {'paperId': '98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2',\n",
       "    'title': 'Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning'},\n",
       "   {'paperId': '9913bb081f27873ea1ae88dcca464b6d5561f95e',\n",
       "    'title': 'Surrogate Assisted Generation of Human-Robot Interaction Scenarios'},\n",
       "   {'paperId': '05b9ed9302c4716a5d6800f660da927e48f82233',\n",
       "    'title': 'Roll-Drop: accounting for observation noise with a single parameter'},\n",
       "   {'paperId': '9e179bba2f2ff863b7d9e4f6497408e81f943c12',\n",
       "    'title': 'AMP in the wild: Learning robust, agile, natural legged locomotion skills'},\n",
       "   {'paperId': 'fc7beef76d20d2d7dff2b074db92cf0ae19372cb',\n",
       "    'title': 'A Survey on Reinforcement Learning Methods in Bionic Underwater Robots'},\n",
       "   {'paperId': '05e3dd7135500150a38a362bece1d42c5b4d5740',\n",
       "    'title': 'Learning and Adapting Agile Locomotion Skills by Transferring Experience'},\n",
       "   {'paperId': '041b9d1edfe621980da89a34142868a37c381921',\n",
       "    'title': 'Torque-based Deep Reinforcement Learning for Task-and-Robot Agnostic Learning on Bipedal Robots Using Sim-to-Real Transfer'},\n",
       "   {'paperId': '4fe7d9d635073390fcc7ff3bf2e2e4ed62b8dcad',\n",
       "    'title': 'Learning Perceptive Bipedal Locomotion over Irregular Terrain'},\n",
       "   {'paperId': 'c8a73f86e228a6f91b7ace3d6f5d97db4fa76b9d',\n",
       "    'title': 'Force Map: Learning to Predict Contact Force Distribution from Vision'},\n",
       "   {'paperId': '1705babb9bb2eeadb3729cb4a0ccae8832c7d25b',\n",
       "    'title': 'AutoRL Hyperparameter Landscapes'},\n",
       "   {'paperId': 'f914607f7684548fb0bbe0bac7e7ad46821f994f',\n",
       "    'title': 'Neural Volumetric Memory for Visual Locomotion Control'},\n",
       "   {'paperId': '3e2cb09e9bd733e9268696e216d8d7b56ebc5853',\n",
       "    'title': 'DribbleBot: Dynamic Legged Manipulation in the Wild'},\n",
       "   {'paperId': '87f15a6a46bd6f1db548509d0ae1dd3c1702312e',\n",
       "    'title': 'Managing power grids through topology actions: A comparative study between advanced rule-based and reinforcement learning agents'},\n",
       "   {'paperId': '23a0a415b9fb30a22b18035959990df40fd0e5b9',\n",
       "    'title': 'Two-layer adaptive trajectory tracking controller for quadruped robots on slippery terrains'},\n",
       "   {'paperId': '5c0b3b80cbb4d3068a5f2029354b408fb0d02187',\n",
       "    'title': 'MarsSim: A High-Fidelity Physical and Visual Simulation for Mars Rovers'},\n",
       "   {'paperId': 'd122b96dc292672541f58ec32a083b60502fbaba',\n",
       "    'title': 'Adaptive Model Prediction Control-Based Multi-Terrain Trajectory Tracking Framework for Mobile Spherical Robots'},\n",
       "   {'paperId': '972a2d8f9395e1be0738bfc72fc3473bfb816d83',\n",
       "    'title': 'Event-based Agile Object Catching with a Quadrupedal Robot'},\n",
       "   {'paperId': '055a1f9c5254235fd764db875db783532bd5706b',\n",
       "    'title': 'Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot using Scalable Motion Imitation'},\n",
       "   {'paperId': '7c53b2eca7c3ed1c10e4f7f0183354ce7744d536',\n",
       "    'title': 'Posture Estimation and Trajectory Tracking for SQuRo: a Small-sized Quadruped Robotic Rat'},\n",
       "   {'paperId': 'b171fe18ce0d6902af724d12a4fc3e8cbdcadb9b',\n",
       "    'title': 'Bionic Multi-legged Robot Based on End-to-end Artificial Neural Network Control'},\n",
       "   {'paperId': '452ae64d4457492d28a03ad613d47b6d98cf8157',\n",
       "    'title': 'Swift progress for robots over complex terrain'},\n",
       "   {'paperId': '21a8833ae27188db9cb01eda56ad12d4fa6ea1e3',\n",
       "    'title': 'Legs as Manipulator: Pushing Quadrupedal Agility Beyond Locomotion'},\n",
       "   {'paperId': '327485eb56631c851028cc31a1eff7d8eaf1ff1b',\n",
       "    'title': 'Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations'},\n",
       "   {'paperId': 'ebe1a1abe24f89a2943adb1ee382eb247cc1f5c3',\n",
       "    'title': 'Design a Hybrid Energy-Supply for the Electrically Driven Heavy-Duty Hexapod Vehicle'},\n",
       "   {'paperId': '94d315d22a6e1847cc9a83718c8278486aa4179f',\n",
       "    'title': 'Visual-Policy Learning through Multi-Camera View to Single-Camera View Knowledge Distillation for Robot Manipulation Tasks'},\n",
       "   {'paperId': '04da578636e7eacaeb7fb3b3303ba8bdbc8ebd22',\n",
       "    'title': 'Understanding the Synergies between Quality-Diversity and Deep Reinforcement Learning'},\n",
       "   {'paperId': '8d32b150ce19200a2d2d71f68b63972080ef99ad',\n",
       "    'title': 'Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach'},\n",
       "   {'paperId': '776a4d87b01131dc63c9195a0453c30f08b39a76',\n",
       "    'title': 'AptSim2Real: Approximately-Paired Sim-to-Real Image Translation'},\n",
       "   {'paperId': '2a54afef4449b177aec6afeea4f3859f372cb88d',\n",
       "    'title': 'Learning Arm-Assisted Fall Damage Reduction and Recovery for Legged Mobile Manipulators'},\n",
       "   {'paperId': 'ee342958e3e659df33792d9eac5613e7c3aac713',\n",
       "    'title': 'Learning Bipedal Walking for Humanoids with Current Feedback'},\n",
       "   {'paperId': 'd0727f2acf4718a84bdb496f4845b6fcb238d02f',\n",
       "    'title': 'Learning Humanoid Locomotion with Transformers'},\n",
       "   {'paperId': '8b79fc443482797a537083ae87af394334c18aef',\n",
       "    'title': 'Modular Safety-Critical Control of Legged Robots'},\n",
       "   {'paperId': 'abe003a6731a7caf803d148cb477b120e27e53be',\n",
       "    'title': 'Underwater legged robotics: review and perspectives'},\n",
       "   {'paperId': '4a2184f5c6260cfbfc01e23a9649ee69cc165769',\n",
       "    'title': 'Parallel Learning: Overview and Perspective for Computational Learning Across Syn2Real and Sim2Real'},\n",
       "   {'paperId': '7484ed52baeb76b523afba843d7798b3a28e7f60',\n",
       "    'title': 'A Simple Controller for Omnidirectional Trotting of Quadrupedal Robots: Command Following and Waypoint Tracking'},\n",
       "   {'paperId': '66bdc48793b0421d0ad802637ed0ce7e9d1ca9e8',\n",
       "    'title': 'Machine Learning in Unmanned Systems for Chemical Synthesis'},\n",
       "   {'paperId': 'd45cf038afdcb6f4148056a2de637910258e0ab3',\n",
       "    'title': 'Puppeteer and Marionette: Learning Anticipatory Quadrupedal Locomotion Based on Interactions of a Central Pattern Generator and Supraspinal Drive'},\n",
       "   {'paperId': 'b0da07a544c506176979e5290abcd63306373a98',\n",
       "    'title': 'Learning physical characteristics like animals for legged robots'},\n",
       "   {'paperId': '240db2d4e5903e891559066ea6f5735c987cd9f0',\n",
       "    'title': 'Editorial: Multimodal behavior from animals to bio-inspired robots'},\n",
       "   {'paperId': '9fda8dbbc030dbf9dae798b051505756be6ffd3a',\n",
       "    'title': 'Robust and Versatile Bipedal Jumping Control through Reinforcement Learning'},\n",
       "   {'paperId': '2029349c55c1dba3493c5b3bd25152f18ba21ae2',\n",
       "    'title': 'Augmented Language Models: a Survey'},\n",
       "   {'paperId': '6320111cbceb891e1b32369137d355a39e03387a',\n",
       "    'title': 'Agile and Versatile Robot Locomotion via Kernel-based Residual Learning'},\n",
       "   {'paperId': 'a876ccf832a4e3cde125d2788108dcb0a0f68681',\n",
       "    'title': 'Time-attenuating Twin Delayed DDPG Reinforcement Learning for Trajectory Tracking Control of Quadrotors'},\n",
       "   {'paperId': '313ad5902b2220a87b4f7033ff1a739bde06020c',\n",
       "    'title': 'Variational Integrators and Graph-Based Solvers for Multibody Dynamics in Maximal Coordinates'},\n",
       "   {'paperId': '405c77c56e34535b5bb8f68e93bb6e2465d525d3',\n",
       "    'title': 'Adaptive Gait Generation for Hexapod Robots Based on Reinforcement Learning and Hierarchical Framework'},\n",
       "   {'paperId': '6fb70221cebdce9fd31a54f3ad111c94b9976246',\n",
       "    'title': 'A Real-Time Planning and Control Framework for Robust and Dynamic Quadrupedal Locomotion'},\n",
       "   {'paperId': 'e9379bf7d22926346a8004833aad986a4ef3742c',\n",
       "    'title': 'Soft-body dynamics induces energy efficiency in undulatory swimming: A deep learning study'},\n",
       "   {'paperId': '879b4debf3233fccb17aa943d283c1d0cf8232ea',\n",
       "    'title': 'NeuronsGym: A Hybrid Framework and Benchmark for Robot Tasks with Sim2Real Policy Learning'},\n",
       "   {'paperId': '51742dadd56e4703a2dce60add6c3181f2e7644d',\n",
       "    'title': 'DITTO: Offline Imitation Learning with World Models'},\n",
       "   {'paperId': 'a408d9e6e36070ee63422806711b4d2ca70c2cd6',\n",
       "    'title': 'Geometry of contact: contact planning for multi-legged robots via spin models duality'},\n",
       "   {'paperId': 'fef05bb6dea1aeb65926db98f9434811202997bb',\n",
       "    'title': 'Hydrodynamic modeling of a robotic surface vehicle using representation learning for long-term prediction'},\n",
       "   {'paperId': '2029600e6c6a6535a48ab84ee0adc33f1234cd83',\n",
       "    'title': 'Efficient Trust Region-Based Safe Reinforcement Learning with Low-Bias Distributional Actor-Critic'},\n",
       "   {'paperId': '1499579caf04760d58c650c20e2f4129e49a4806',\n",
       "    'title': 'Learning quadrupedal locomotion on deformable terrain'},\n",
       "   {'paperId': '980b07368e08932b8d3e089c7509039cb30edce6',\n",
       "    'title': 'DreamWaQ: Learning Robust Quadrupedal Locomotion With Implicit Terrain Imagination via Deep Reinforcement Learning'},\n",
       "   {'paperId': '40f9c7b75f5b66437ddfc2ebc70e4c9f32f7a811',\n",
       "    'title': 'Directionally Compliant Legs Enabling Crevasse Traversal in Small Ground‐Based Robots'},\n",
       "   {'paperId': '66d2e5aaac2dad2213409adc4ac12fe7d5f53bd3',\n",
       "    'title': 'A Neural Coordination Strategy for Attachment and Detachment of a Climbing Robot Inspired by Gecko Locomotion'},\n",
       "   {'paperId': '9c09e4cd77a35f8e94469a52b4b8da6322c92e52',\n",
       "    'title': 'Learning-Based Design and Control for Quadrupedal Robots With Parallel-Elastic Actuators'},\n",
       "   {'paperId': '65686bed452b981129b8384bad502041181f5917',\n",
       "    'title': 'Reinforced Meta-Learning Method for Shape-Dependent Regulation of Cutting Force in Pork Carcass Operation Robots'},\n",
       "   {'paperId': 'c835c9196e3c11a420138f6eaca3abae92dd1495',\n",
       "    'title': 'Single-Level Differentiable Contact Simulation'},\n",
       "   {'paperId': 'c645cf02eba92f7d2064885a0093c1859aa09a32',\n",
       "    'title': 'ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots'},\n",
       "   {'paperId': '479059a41b614ea6f44912e3e4a34c3a07001601',\n",
       "    'title': 'Backward Reachability Analysis of Neural Feedback Loops: Techniques for Linear and Nonlinear Systems'},\n",
       "   {'paperId': '9f8b75e4df1912160addd24b89016d8f4de933a7',\n",
       "    'title': 'Perceptive Locomotion through Nonlinear Model Predictive Control'},\n",
       "   {'paperId': 'bbff7a772490879f3fb491eea4dec67deaedc1f2',\n",
       "    'title': 'AdaptiveON: Adaptive Outdoor Local Navigation Method for Stable and Reliable Actions'},\n",
       "   {'paperId': '174bf19aa934e9ba2016a1837cfe667b13a05953',\n",
       "    'title': 'Adaptive Actuation of Magnetic Soft Robots Using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ca4b0fe3abcacbea88de59c703f6fb7fcc261a59',\n",
       "    'title': 'Quadruped Capturability and Push Recovery via a Switched-Systems Characterization of Dynamic Balance'},\n",
       "   {'paperId': '0b07f27e61ebfcc3beabc4ba4dd3855bb8fb71db',\n",
       "    'title': 'The Need for and Feasibility of Alternative Ground Robots to Traverse Sandy and Rocky Extraterrestrial Terrain'},\n",
       "   {'paperId': '0f206e1a51dee24b99cb1010d451e4c0df207138',\n",
       "    'title': 'Scalable Safe Exploration for Global Optimization of Dynamical Systems'},\n",
       "   {'paperId': '9684a9ddc740631be5dd15816af282b91de98ddd',\n",
       "    'title': 'BiConMP: A Nonlinear Model Predictive Control Framework for Whole Body Motion Planning'},\n",
       "   {'paperId': '228382ac9f728874a17c1182d1e660dcb00403ac',\n",
       "    'title': 'Robust and Versatile Bipedal Jumping Control through Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'da10b38d33091fa11af28a8738fa3bf4aee2bc04',\n",
       "    'title': 'Sensitivity Adaptation of Lower-Limb Exoskeleton for Human Performance Augmentation Based on Deep Reinforcement Learning'},\n",
       "   {'paperId': '046960aedec5a95c83baa0e933ccb003fee410e0',\n",
       "    'title': 'Solving Challenging Control Problems via Learning-based Motion Planning and Imitation'},\n",
       "   {'paperId': 'd45722cb40520169e290b4c37b525fde44171207',\n",
       "    'title': 'Visual CPG-RL: Learning Central Pattern Generators for Visually-Guided Quadruped Navigation'},\n",
       "   {'paperId': 'd04c8caaddd159ff6696babec6f09ec13b4e81b5',\n",
       "    'title': 'Hyperparameters in Contextual RL are Highly Situational'},\n",
       "   {'paperId': '00a6ea1e57ae3b7e93af2d3848ed5fa28b15f49c',\n",
       "    'title': 'Multi-embodiment Legged Robot Control as a Sequence Modeling Problem'},\n",
       "   {'paperId': 'd4bcc5c1bb5ae63878b7801f7f3170fd8b22a347',\n",
       "    'title': 'Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer'},\n",
       "   {'paperId': '039cac821166586d4e9bebf1070941f3939e0d71',\n",
       "    'title': 'Walk These Ways: Tuning Robot Control for Generalization with Multiplicity of Behavior'},\n",
       "   {'paperId': 'b084cb218984f5123f87dc354a601a1fd7ddd265',\n",
       "    'title': 'A Motion Planning and Control Method of Quadruped Robot Based on Deep Reinforcement Learning'},\n",
       "   {'paperId': '225bbf4c8eb27e946661fd7e38790be77e1d079f',\n",
       "    'title': 'Multi-sensor Fusion for Stiffness Estimation to Assist Legged Robot Control in Unstructured Environment'},\n",
       "   {'paperId': '5397190644ee73cc31e75c207530a058cf8ec544',\n",
       "    'title': 'CapPlanner: Adaptable to Various Topology and Locomotion Capability for Hexapod Robots'},\n",
       "   {'paperId': 'd32d7214e5eea6218a53121d499416cc674a1079',\n",
       "    'title': 'Ground Contact Parameter Estimation Guided Gait Planning for Hexapod Robots'},\n",
       "   {'paperId': '20d760821b179bfacea4f39998496708b9799446',\n",
       "    'title': 'Quadruped Reinforcement Learning without Explicit State Estimation'},\n",
       "   {'paperId': '9b0042ac06288cb40ad6a6430fc006d12b680581',\n",
       "    'title': 'Sim-to-real: Quadruped Robot Control with Deep Reinforcement Learning and Parallel Training'},\n",
       "   {'paperId': '03e22185dea8337cde261f04cf27259d21ffa80e',\n",
       "    'title': 'Learning-Based Model Predictive Control for Quadruped Locomotion on Slippery Ground'},\n",
       "   {'paperId': '840fc5f7afb0a27ac54f09c6e3c8b8a775f4cfd1',\n",
       "    'title': 'A Reinforcement Learning-Based Strategy of Path Following for Snake Robots with an Onboard Camera'},\n",
       "   {'paperId': '1111b2581d759cc90bc45ea9c3d8a1f9700bed4e',\n",
       "    'title': 'High-speed quadrupedal locomotion by imitation-relaxation reinforcement learning'},\n",
       "   {'paperId': '68409226cd46b26f5d84691e97a7cfbfc59d1e79',\n",
       "    'title': 'Search and tracking strategy of autonomous surface underwater vehicle in oceanic eddies based on deep reinforcement learning'},\n",
       "   {'paperId': 'a42f67594450ce014e9873bd105ea68f466f88e5',\n",
       "    'title': 'Prismatic Quasi-Direct-Drives for dynamic quadruped locomotion with high payload capacity'},\n",
       "   {'paperId': '2974c6fa5e4c74e6c1ce20739b10f717a388fa14',\n",
       "    'title': 'Learning Agile Paths from Optimal Control'},\n",
       "   {'paperId': 'd8753089a4a57cc5b147d461d8e2c3585f0e8a38',\n",
       "    'title': 'Dynamic Bipedal Turning through Sim-to-Real Reinforcement Learning'},\n",
       "   {'paperId': '1e769aae9ad1460f9921cd15756d1ec962e034e3',\n",
       "    'title': 'Practice Makes Perfect: an iterative approach to achieve precise tracking for legged robots'},\n",
       "   {'paperId': '2ace3e0cf0e302a09a2801d244ddd434f5a38d2e',\n",
       "    'title': 'User-Conditioned Neural Control Policies for Mobile Robotics'},\n",
       "   {'paperId': 'ca0663a1b5b3e6ad316315817aaa331800cfcd2b',\n",
       "    'title': 'Optimization-Based Control for Dynamic Legged Robots'},\n",
       "   {'paperId': '8a7dbe1a807c948eb1b6496c22e12f8ba03c94ff',\n",
       "    'title': 'Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment'},\n",
       "   {'paperId': '077fcca7b074061cd04c034137c96fbc9041d9d6',\n",
       "    'title': 'Distributed Data-Driven Predictive Control for Multi-Agent Collaborative Legged Locomotion'},\n",
       "   {'paperId': 'e0eb9870a6c105cadd92cae8f5218b2a84955849',\n",
       "    'title': 'Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks'},\n",
       "   {'paperId': '3d51d8dcbdb33a3b74fbfc92680f1ded4121185b',\n",
       "    'title': 'Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks'},\n",
       "   {'paperId': '19057bcc4176590b3c2d031dd60333c19f49aa5a',\n",
       "    'title': 'Localizing Complex Terrains through Adaptive Submodularity'},\n",
       "   {'paperId': 'ce91da230a2ac174307463632b731f74decf6860',\n",
       "    'title': 'Design and Control of a Muscle-skeleton Robot Elbow based on Reinforcement Learning'},\n",
       "   {'paperId': '410caec38e2be5228d2116b22ed7872d77305cc8',\n",
       "    'title': 'Learning Visual Locomotion with Cross-Modal Supervision'},\n",
       "   {'paperId': 'ba258f3da9209188e3e7be16ebd500a270490e34',\n",
       "    'title': 'Adapt to Non-stationary Environments via Multi-teacher and Single-Student Process'},\n",
       "   {'paperId': 'c939731e22e4c71cc6abd33fe4472b8667545e69',\n",
       "    'title': 'Bilateral Constrained Control for Prosthesis Walking on Stochastically Uneven Terrain'},\n",
       "   {'paperId': 'a6c6e464b2ab15bb04176c26da3816d4038d5176',\n",
       "    'title': 'Dungeons and Data: A Large-Scale NetHack Dataset'},\n",
       "   {'paperId': '0a7446207ae24a7ff81a5f31ecd32838e3d74e1c',\n",
       "    'title': 'Learning Modular Robot Locomotion from Demonstrations'},\n",
       "   {'paperId': 'fb090d312f6f33b7f9bb44f04f81304fb7a11bac',\n",
       "    'title': 'Learning Deep Sensorimotor Policies for Vision-based Autonomous Drone Racing'},\n",
       "   {'paperId': '6a2e91285b6e7cac66726d85192f5a1ffa7f8b97',\n",
       "    'title': 'ViNL: Visual Navigation and Locomotion Over Obstacles'},\n",
       "   {'paperId': 'c14a5239ab4e8dd2964169450e727a6b089671a2',\n",
       "    'title': 'DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality'},\n",
       "   {'paperId': 'a7120b4499de497f55b6ec1d194f73c165ea17cf',\n",
       "    'title': 'A passive, asymmetrically-compliant knee joint improves obstacle traversal in an insect-scale legged robot'},\n",
       "   {'paperId': '8faf8a1f9f00eafbd4f4e4bab2b82332a6b84c3f',\n",
       "    'title': 'SafeTAC: Safe Tsallis Actor-Critic Reinforcement Learning for Safer Exploration'},\n",
       "   {'paperId': '33f8a81898a4dc40d32b24a5baf8e79c0f05bd77',\n",
       "    'title': 'Learning Agile Hybrid Whole-body Motor Skills for Thruster-Aided Humanoid Robots'},\n",
       "   {'paperId': '9c6ee29930206b508d69799cea3bf7091912fc11',\n",
       "    'title': 'Non-blocking Asynchronous Training for Reinforcement Learning in Real-World Environments'},\n",
       "   {'paperId': 'c70052c6c59b4ba9d61fc8df5239d9500d7bcf8f',\n",
       "    'title': 'Improved Performance of CPG Parameter Inference for Path-following Control of Legged Robots'},\n",
       "   {'paperId': '1d6a531d18fcda469a0ef9b2e864828a21083f1f',\n",
       "    'title': 'A passive, asymmetrically-compliant knee joint improves obstacle traversal in an insect-scale legged robot'},\n",
       "   {'paperId': '91b2d1844ee01dc0ddad524e9a872222e2f9ef1f',\n",
       "    'title': 'Learn from Interaction: Learning to Pick via Reinforcement Learning in Challenging Clutter'},\n",
       "   {'paperId': 'ae4f973555b98d8a8797f35d4fbaa7f5e6a306ce',\n",
       "    'title': 'Weighted Maximum Likelihood for Controller Tuning'},\n",
       "   {'paperId': '99be2f7ac7a026a039b16762a6e22634a038fbbb',\n",
       "    'title': 'Robotic Table Wiping via Reinforcement Learning and Whole-body Trajectory Optimization'},\n",
       "   {'paperId': '31fefff364f37cc1066598d865ca3fb6b785ed32',\n",
       "    'title': 'TOIST: Task Oriented Instance Segmentation Transformer with Noun-Pronoun Distillation'},\n",
       "   {'paperId': '8a79ac36b26348c1293433b68bc5e538706f321b',\n",
       "    'title': 'Deep Whole-Body Control: Learning a Unified Policy for Manipulation and Locomotion'},\n",
       "   {'paperId': '5d191acd4af42a5b49ad6cf6a54ce7003a626cc1',\n",
       "    'title': 'Sample Efficient Dynamics Learning for Symmetrical Legged Robots: Leveraging Physics Invariance and Geometric Symmetries'},\n",
       "   {'paperId': '82b1865d820516014ff19e917d17e3e8e05597b9',\n",
       "    'title': 'Cooperative control strategy of wheel-legged robot based on attitude balance'},\n",
       "   {'paperId': 'd27be7f60b256d5dc8facb1337aede953b8ca9ef',\n",
       "    'title': 'Behavior policy learning: Learning multi-stage tasks via solution sketches and model-based controllers'},\n",
       "   {'paperId': '4bbbb6ee03d35a7c032a21ed98d9c53609f8a348',\n",
       "    'title': 'In-Hand Object Rotation via Rapid Motor Adaptation'},\n",
       "   {'paperId': '784ddf99c2d65f507817fa63d3d0300d161a29f0',\n",
       "    'title': 'Efficient Learning of Locomotion Skills through the Discovery of Diverse Environmental Trajectory Generator Priors'},\n",
       "   {'paperId': '9406e95bcafaf5dbce5629467a5e4282d40b667d',\n",
       "    'title': 'NeRF2Real: Sim2real Transfer of Vision-guided Bipedal Motion Skills using Neural Radiance Fields'},\n",
       "   {'paperId': 'e367fba2a2227dccedba0d2874ba30112c216ee4',\n",
       "    'title': 'OPT-Mimic: Imitation of Optimized Trajectories for Dynamic Quadruped Behaviors'},\n",
       "   {'paperId': '937fe158b2bebfb5a3fb0df6c47f89f8f4ce0b0f',\n",
       "    'title': 'Accelerate Reinforcement Learning with PID Controllers in the Pendulum Simulations'},\n",
       "   {'paperId': '44b6fc338220fcd83448f7093c586c18be299236',\n",
       "    'title': 'Saving the Limping: Fault-tolerant Quadruped Locomotion via Reinforcement Learning'},\n",
       "   {'paperId': 'fdeba3a6ba0a56204141df154d429dbebd363da3',\n",
       "    'title': 'Robust Adaptive Ensemble Adversary Reinforcement Learning'},\n",
       "   {'paperId': '1dc0ef7402adc45b73bcac31b60d044d6810f072',\n",
       "    'title': 'CPG-RL: Learning Central Pattern Generators for Quadruped Locomotion'},\n",
       "   {'paperId': '5aa9b72f773ea65c0d2253ca4188a6c5d7e5cae1',\n",
       "    'title': 'Hybrid Bipedal Locomotion Based on Reinforcement Learning and Heuristics'},\n",
       "   {'paperId': '4e48d349574cac944fc854ee2b4ae598dd150246',\n",
       "    'title': 'Meta Reinforcement Learning for Optimal Design of Legged Robots'},\n",
       "   {'paperId': 'd435a0357ea6b4f99ee03ef246b89130d34c3820',\n",
       "    'title': 'An Insight on Mud Behavior Upon Stepping'},\n",
       "   {'paperId': 'f62ebb92aaaa7098908fcb476d1abfee45853029',\n",
       "    'title': 'Development of a Small-Sized Quadruped Robotic Rat Capable of Multimodal Motions'},\n",
       "   {'paperId': '00dc442fe036e3732c8066cbeb44df77d495dec9',\n",
       "    'title': 'Learning an Efficient Terrain Representation for Haptic Localization of a Legged Robot'},\n",
       "   {'paperId': 'd4d1a492c37a7d2c0dcbc626e6150ec42e1a191e',\n",
       "    'title': 'Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion'},\n",
       "   {'paperId': 'c358b9f1aee25507317b97f7ffb2117c6c7c4c20',\n",
       "    'title': 'Zero-Shot Retargeting of Learned Quadruped Locomotion Policies Using Hybrid Kinodynamic Model Predictive Control'},\n",
       "   {'paperId': '4e3fe9deba7b6434718901bd4648d760f9ae559a',\n",
       "    'title': 'MultiRoboLearn: An open-source Framework for Multi-robot Deep Reinforcement Learning'},\n",
       "   {'paperId': 'd8323ccdcd36a3e2697745614d7a6db8c8fad14c',\n",
       "    'title': 'Safe Balancing Control of a Soft Legged Robot'},\n",
       "   {'paperId': 'd96f4277342a3b8c2dc180edfa20385e1dd3c57e',\n",
       "    'title': 'A Needs Learning Algorithm Applied to Stable Gait Generation of Quadruped Robot'},\n",
       "   {'paperId': '34469e6383765a8d36b96c915298c38b79c2b141',\n",
       "    'title': 'Advanced Skills by Learning Locomotion and Local Navigation End-to-End'},\n",
       "   {'paperId': '7aa615799a2b06c15d994ec9f85be4904b271a1c',\n",
       "    'title': 'Learning and Deploying Robust Locomotion Policies with Minimal Dynamics Randomization'},\n",
       "   {'paperId': '88d988ab69c0dc100a6d51cde62ff8384e095f8f',\n",
       "    'title': 'Efficient learning of robust quadruped bounding using pretrained neural networks'},\n",
       "   {'paperId': '1acfe52998410df75dfb047a002904eda5cd038c',\n",
       "    'title': 'Real‐Time Remodeling of Granular Terrain for Robot Locomotion'},\n",
       "   {'paperId': '03446d8730c4bc5cb3abc9d8c13e8e6d3898e2ae',\n",
       "    'title': 'Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation'},\n",
       "   {'paperId': 'df684d5854895e73e40c16d2ad91f05c2bfbb76c',\n",
       "    'title': 'A heuristic control framework for heavy‐duty hexapod robot over complex terrain'},\n",
       "   {'paperId': '42930355e2a4bd834508015d91eb66e78381a4ad',\n",
       "    'title': 'Recent Approaches for Perceptive Legged Locomotion'},\n",
       "   {'paperId': 'fcd36ca206b3584fbd96d5f7b606241c1b04ec97',\n",
       "    'title': 'HyperGuider: Virtual Reality Framework for Interactive Path Planning of Quadruped Robot in Cluttered and Multi-Terrain Environments'},\n",
       "   {'paperId': 'f840668624d353d1b54c17e05589d268c80ecaea',\n",
       "    'title': 'Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments'},\n",
       "   {'paperId': '70b4d1fa5d8f63c500bdad5e308991cd24261e86',\n",
       "    'title': 'Multi-segmented Adaptive Feet for Versatile Legged Locomotion in Natural Terrain'},\n",
       "   {'paperId': 'f6d713fd34e34000b73e8c6852286567ac8b70cd',\n",
       "    'title': 'Legged locomotion over irregular terrains: state of the art of human and robot performance'},\n",
       "   {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "    'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       "   {'paperId': '411145a65dac7989f51a4aaed11485f7dedfc328',\n",
       "    'title': 'Learning to Exploit Elastic Actuators for Quadruped Locomotion'},\n",
       "   {'paperId': '2123301d17d5032143fa7538495090ba1a5e57e1',\n",
       "    'title': 'GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots'},\n",
       "   {'paperId': '6aa3b0795e1500e3b4ffcf4503acd9a63b716386',\n",
       "    'title': 'Variable stiffness locomotion with guaranteed stability for quadruped robots traversing uneven terrains'},\n",
       "   {'paperId': 'b4888a20a9910e292448a51bf4248cc5b60e2af3',\n",
       "    'title': 'A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning'},\n",
       "   {'paperId': '0e3d081b1a5c2d53a415e29c038901f867e62c4b',\n",
       "    'title': 'Generating a Terrain-Robustness Benchmark for Legged Locomotion: A Prototype via Terrain Authoring and Active Learning'},\n",
       "   {'paperId': '90d913ef1e67cd5e7ef322d006bc322618126f28',\n",
       "    'title': 'Lyapunov Design for Robust and Efficient Robotic Reinforcement Learning'},\n",
       "   {'paperId': 'd30b069c54b06c7749b83fbdf050c17fdea374d9',\n",
       "    'title': 'AACC: Asymmetric Actor-Critic in Contextual Reinforcement Learning'},\n",
       "   {'paperId': '825cb0a820dd8f2c51a78520c30b660dfcfec30a',\n",
       "    'title': 'Sequence Model Imitation Learning with Unobserved Contexts'},\n",
       "   {'paperId': '2a02b0d8c1d2fb44ab8e4dfa60b95c1090e45b6e',\n",
       "    'title': 'Mechanism design and workspace analysis of a hexapod robot'},\n",
       "   {'paperId': 'c61bed5cc7d4a7d31b673460e32ff5c0cc2550d5',\n",
       "    'title': 'PrePARE: Predictive Proprioception for Agile Failure Event Detection in Robotic Exploration of Extreme Terrains'},\n",
       "   {'paperId': '22e8e99f14a9d6156ce4ed8471b8b12a65e67ece',\n",
       "    'title': 'RCA: Ride Comfort-Aware Visual Navigation via Self-Supervised Learning'},\n",
       "   {'paperId': 'ee4bd997237ef6cb9f66823411b75aada1d9af34',\n",
       "    'title': 'Fine-tuning Deep Reinforcement Learning Policies with r-STDP for Domain Adaptation'},\n",
       "   {'paperId': '7bf68f963352ed25cadfc8eb102539d242ab300f',\n",
       "    'title': 'Learning Bipedal Walking On Planned Footsteps For Humanoid Robots'},\n",
       "   {'paperId': 'e297c00c77419703d42de737d8b4546b42190c07',\n",
       "    'title': 'An Optimal Motion Planning Framework for Quadruped Jumping'},\n",
       "   {'paperId': '23c9cac7f58297cb2bad7be5804d0fc277199113',\n",
       "    'title': 'Rethinking Sim2Real: Lower Fidelity Simulation Leads to Higher Sim2Real Transfer in Navigation'},\n",
       "   {'paperId': 'cf76906fc3de50ce69666349761d69a07dcad104',\n",
       "    'title': 'World robot challenge 2020 – partner robot: a data-driven approach for room tidying with mobile manipulator'},\n",
       "   {'paperId': '7017cab1f0068c29b38d75662501520feca34b11',\n",
       "    'title': 'Curriculum Adversarial Training for Robust Reinforcement Learning'},\n",
       "   {'paperId': 'f8662489d245e19a52b194bf1415505a534bbf07',\n",
       "    'title': 'Ground Reaction Force Estimation in a Quadruped Robot via Liquid State Networks'},\n",
       "   {'paperId': '78773424cf3103a8f9f6504732c2e1c63e75ece8',\n",
       "    'title': 'Model-free End-to-end Learning of Agile Quadrupedal Locomotion over Challenging Terrain'},\n",
       "   {'paperId': 'e1773f449c760a931dda794453e4c3d01aae1735',\n",
       "    'title': 'Dynamic Bipedal Maneuvers through Sim-to-Real Reinforcement Learning'},\n",
       "   {'paperId': '7d50844028b3d04a9479b6b8df06a97deb5f2057',\n",
       "    'title': 'The Roles and Comparison of Rigid and Soft Tails in Gecko-Inspired Climbing Robots: A Mini-Review'},\n",
       "   {'paperId': '79e007e57ccaf0d5616456603c815ad7b22c17d1',\n",
       "    'title': 'i-Sim2Real: Reinforcement Learning of Robotic Policies in Tight Human-Robot Interaction Loops'},\n",
       "   {'paperId': 'f30162253c625e298a98253adedb2fceb2066707',\n",
       "    'title': 'Optimal Modulation of Joint Stiffness with Guaranteed Stability for Quadruped Robots'},\n",
       "   {'paperId': 'd6f014cbee0679272c27667b8d8bc4b26b5976ef',\n",
       "    'title': 'Learning Agile, Robust Locomotion Skills for Quadruped Robot'},\n",
       "   {'paperId': '9bc63eee5cd984b4290504a224624e857611edf6',\n",
       "    'title': 'Optimizing Bipedal Maneuvers of Single Rigid-Body Models for Reinforcement Learning'},\n",
       "   {'paperId': 'f0abfaf5f0d88a3548779fd98bbfa25611483f2c',\n",
       "    'title': 'Egocentric Visual Self-Modeling for Legged Robot Locomotion'},\n",
       "   {'paperId': 'c4535e525af3f6e57133c5729a445f1eb1792db0',\n",
       "    'title': 'A Learning System for Motion Planning of Free-Float Dual-Arm Space Manipulator towards Non-Cooperative Object'},\n",
       "   {'paperId': '577be8d620ec7bcd080d4001f2511245c58e9639',\n",
       "    'title': 'Learning fast and agile quadrupedal locomotion over complex terrain'},\n",
       "   {'paperId': '9e9bce8675bd54ec2faa2b0bf393e0d3924c5185',\n",
       "    'title': 'How to pick a mobile robot simulator: A quantitative comparison of CoppeliaSim, Gazebo, MORSE and Webots with a focus on accuracy of motion'},\n",
       "   {'paperId': '02e7e348def95bfe702cd6c20bbce3b4aea29877',\n",
       "    'title': 'Auto-Tuning of Controller and Online Trajectory Planner for Legged Robots'},\n",
       "   {'paperId': 'd6ebaa2daebcb509250f633badc3fb154ef53f72',\n",
       "    'title': 'Robust Adversarial Reinforcement Learning with Dissipation Inequation Constraint'},\n",
       "   {'paperId': '25bc06b508b2c63b9faf77881e528530b147b988',\n",
       "    'title': 'DayDreamer: World Models for Physical Robot Learning'},\n",
       "   {'paperId': '059450e61c20ee3ffae38a8c2d9e5eb2df119818',\n",
       "    'title': 'Learning Variable Impedance Control for Aerial Sliding on Uneven Heterogeneous Surfaces by Proprioceptive and Tactile Sensing'},\n",
       "   {'paperId': '015e9fc95d834bdaf75227c716bde993c47f1e9c',\n",
       "    'title': 'DiPCAN: Distilling Privileged Information for Crowd-Aware Navigation'},\n",
       "   {'paperId': '971ed60f58d40ceedeec90d894a6d655787c6633',\n",
       "    'title': 'A Learning-based Iterative Control Framework for Controlling a Robot Arm with Pneumatic Artificial Muscles'},\n",
       "   {'paperId': '06f73c93d7902355752abbc1de330436caa9e35f',\n",
       "    'title': 'Learning Agile Skills via Adversarial Imitation of Rough Partial Demonstrations'},\n",
       "   {'paperId': 'f58c32713a6a52faa32552f59f509410462e2191',\n",
       "    'title': 'Imitate then Transcend: Multi-Agent Optimal Execution with Dual-Window Denoise PPO'},\n",
       "   {'paperId': '899a7e69fd0e4ab2018d0060f53781b13310c432',\n",
       "    'title': 'Neural Scene Representation for Locomotion on Structured Terrain'},\n",
       "   {'paperId': '208d1f69567d2380e7292a8c2e6ef1aa2e22a649',\n",
       "    'title': 'Learning Fast and Precise Pixel-to-Torque Control: A Platform for Reproducible Research of Learning on Hardware'},\n",
       "   {'paperId': '6f837af0fb2c4e88b747f2b8517f349c7e5379b0',\n",
       "    'title': 'Human-AI Shared Control via Policy Dissection'},\n",
       "   {'paperId': 'b9ff9a07e1736c03177c9bc0a415ea44619ed357',\n",
       "    'title': 'Adapting Rapid Motor Adaptation for Bipedal Robots'},\n",
       "   {'paperId': '31b345a5bc3c8c01646e6414ca1318c2e8315443',\n",
       "    'title': 'Automated Dynamic Algorithm Configuration'},\n",
       "   {'paperId': '94c198c21e6e9e81b11c79fb4187e530f4428ac4',\n",
       "    'title': 'Online Optimal Landing Control of the MIT Mini Cheetah'},\n",
       "   {'paperId': 'b2060bb520989be53cc7419b36b0c5510525ba43',\n",
       "    'title': 'Learning Efficient and Robust Multi-Modal Quadruped Locomotion: A Hierarchical Approach'},\n",
       "   {'paperId': 'f60ae545d982a7e27cb6f259b09f3a87637601c8',\n",
       "    'title': 'Unsupervised Learning of Terrain Representations for Haptic Monte Carlo Localization'},\n",
       "   {'paperId': 'd24adf3894c3ec076e0d072546e348a09b9df12c',\n",
       "    'title': 'A Novel Model of Interaction Dynamics between Legged Robots and Deformable Terrain'},\n",
       "   {'paperId': '4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "    'title': 'Chain of Thought Imitation with Procedure Cloning'},\n",
       "   {'paperId': 'e648c452664236c70ffa4ed85db51dc7621e20c6',\n",
       "    'title': 'Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models'},\n",
       "   {'paperId': '0c845688166c07cb095ed0dbd55da817f34a4cfb',\n",
       "    'title': 'Learning to guide multiple heterogeneous actors from a single human demonstration via automatic curriculum learning in StarCraft II'},\n",
       "   {'paperId': '14cea4bbbe417ecf00bfe49c33955fa1facabda8',\n",
       "    'title': 'Tiny Robot Learning: Challenges and Directions for Machine Learning in Resource-Constrained Robots'},\n",
       "   {'paperId': '366ca45102a6111dbb256a908aa3f2bdb6a707dc',\n",
       "    'title': 'Factory: Fast Contact for Robotic Assembly'},\n",
       "   {'paperId': 'b7cbf03974511d7855b4c01e3005f3e38bd1a136',\n",
       "    'title': 'Rapid Locomotion via Reinforcement Learning'},\n",
       "   {'paperId': '91e6d31e3bb634007dbc3abc3d84da01412fea17',\n",
       "    'title': 'Neural-Fly enables rapid learning for agile flight in strong winds'},\n",
       "   {'paperId': '8ece304c7f3843f62b9874bf1f9daff48a367909',\n",
       "    'title': 'Model-free dynamic control of robotic joints with integrated elastic ligaments'},\n",
       "   {'paperId': '96c9b1f1da5368f5f900e2091633b1139d92ecc8',\n",
       "    'title': 'Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning'},\n",
       "   {'paperId': '702dafe2b3916b3f85b4fc4d413046d97221ddc0',\n",
       "    'title': 'A Survey of Traversability Estimation for Mobile Robots'},\n",
       "   {'paperId': 'c3ea1481cab4e3c0085f9a0046e467a36feec49a',\n",
       "    'title': 'Sideways crab-walking is faster and more efficient than forward walking for a hexapod robot'},\n",
       "   {'paperId': '5e87f2dddd49c3c0d440b4ee3153f3e771688664',\n",
       "    'title': 'Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe Quadruped Navigation'},\n",
       "   {'paperId': 'f4d756e140653932b159c54b96b551da24264a6a',\n",
       "    'title': 'Neural Gaits: Learning Bipedal Locomotion via Control Barrier Functions and Zero Dynamics Policies'},\n",
       "   {'paperId': 'efbc2c6306ff1f3bfa282fc62f8467764fd41c25',\n",
       "    'title': 'Accelerated Policy Learning with Parallel Differentiable Simulation'},\n",
       "   {'paperId': 'ce1b6ba0d639348dd31844ceb4d2965927cef123',\n",
       "    'title': 'Trajectory Optimization Using Neural Network Gradients of Learned Dynamics'},\n",
       "   {'paperId': '111aed3cd77e1d73e2b3457d6d61a7fa33751b1c',\n",
       "    'title': 'Gradient-Based Trajectory Optimization With Learned Dynamics'},\n",
       "   {'paperId': '9cc7dca7c1a10b36ae7cbd8b311690dab9278dd7',\n",
       "    'title': 'Sim-to-Real Transfer of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid'},\n",
       "   {'paperId': '1630c0818963a7e68a870ce13694f7da2fbf94fc',\n",
       "    'title': 'Learning to walk autonomously via reset-free quality-diversity'},\n",
       "   {'paperId': 'b34e47bde3f49b42bf08956f9f323324c26cf5a7',\n",
       "    'title': 'Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning'},\n",
       "   {'paperId': '287b52febaedd0aef461b90c63899490a7c6275a',\n",
       "    'title': 'On Slip Detection for Quadruped Robots'},\n",
       "   {'paperId': 'cc0a106772fd8ad21d9b314194ce487758ecf692',\n",
       "    'title': 'Autoencoder for Synthetic to Real Generalization: From Simple to More Complex Scenes'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': 'bee725759dc222f2469577617d18c793606f4c33',\n",
       "    'title': 'Assessing evolutionary terrain generation methods for curriculum reinforcement learning'},\n",
       "   {'paperId': 'c14605e00dc35886a5767ccc15a5882209a9d1f9',\n",
       "    'title': 'Adversarial Motion Priors Make Good Substitutes for Complex Reward Functions'},\n",
       "   {'paperId': '0f836b3ce6623539aceb05827c51f69bcae60e52',\n",
       "    'title': 'Learning Minimum-Time Flight in Cluttered Environments'},\n",
       "   {'paperId': '03eebc19358c4bddb4a987b9e94ecbcb7e58b5d2',\n",
       "    'title': 'Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning'},\n",
       "   {'paperId': 'e97bcb1695b586fdd7b76893bcd57d493f339ca6',\n",
       "    'title': 'MetaMorph: Learning Universal Controllers with Transformers'},\n",
       "   {'paperId': 'e24c2c88979024d45ffe62530c1119efd7692a7c',\n",
       "    'title': 'Meta-reinforcement learning for the tuning of PI controllers: An offline approach'},\n",
       "   {'paperId': '33fae337de4cbf73dcf55acac1a2605fb727eadb',\n",
       "    'title': 'DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning'},\n",
       "   {'paperId': '247a775cf515b330e6100c48eb40d2d0c61df490',\n",
       "    'title': 'Learning Torque Control for Quadrupedal Locomotion'},\n",
       "   {'paperId': 'b5cb6ff28247957fa44c1073e3d53cca4702bbcc',\n",
       "    'title': 'Context is Everything: Implicit Identification for Dynamics Adaptation'},\n",
       "   {'paperId': 'c833d28f18646bb9a90b9e9c60f5279d1f360b89',\n",
       "    'title': 'Adaptive Motion Skill Learning of Quadruped Robot on Slopes Based on Augmented Random Search Algorithm'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'},\n",
       "   {'paperId': '6ab3f4a06efeaa6d402b3a39e39609efaf9759eb',\n",
       "    'title': 'Machine intelligence for chemical reaction space'},\n",
       "   {'paperId': 'a90b9c22950e03f4f38ed42d7ca2422ca565cc73',\n",
       "    'title': 'Safe Reinforcement Learning for Legged Locomotion'},\n",
       "   {'paperId': '8ed3b0f925164bfd3a9984668756143ee2a499bc',\n",
       "    'title': 'Imitation and Adaptation Based on Consistency: A Quadruped Robot Imitates Animals from Videos Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '8aef93f752e5777bde4fab109cc6a135d0093271',\n",
       "    'title': 'A Transferable Legged Mobile Manipulation Framework Based on Disturbance Predictive Control'},\n",
       "   {'paperId': '4eed1b909d8eec87dd73cb52cd53b97c62c5d6fb',\n",
       "    'title': 'Dojo: A Differentiable Physics Engine for Robotics'},\n",
       "   {'paperId': '1bb82660573bbaf01572041da16842ee2398ae39',\n",
       "    'title': 'Learning Robust Real-Time Cultural Transmission without Human Data'},\n",
       "   {'paperId': '6d0adac188152fbaa45a88ba4da788926ed8144a',\n",
       "    'title': 'Reinforcement Learning in Practice: Opportunities and Challenges'},\n",
       "   {'paperId': '229bfba8186ba337412283cc469da123d97a5c76',\n",
       "    'title': 'Kirin: A Quadruped Robot with High Payload Carrying Capability'},\n",
       "   {'paperId': '5d5812c5236c5fc576ba308dd9ccbd573f5dd7ef',\n",
       "    'title': 'Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion'},\n",
       "   {'paperId': 'df8e5f2e19b696fc5ed4bec9b61835943c8e8a8f',\n",
       "    'title': 'Contextualize Me - The Case for Context in Reinforcement Learning'},\n",
       "   {'paperId': '06033eb03186179627c33bf7f5562a494a13fc80',\n",
       "    'title': 'Versatile modular neural locomotion control with fast learning'},\n",
       "   {'paperId': '4abb9301a657c69390963487b20d8c9bf521c6f6',\n",
       "    'title': 'AutoFac: The Perpetual Robot Machine'},\n",
       "   {'paperId': '5da5c2167a85ecb5d1ea22656ae36fdf995df0f2',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': '6c201113de5df03aceaf7170835db6e91eb1cb90',\n",
       "    'title': 'Quadruped robots venture into the wild with open eyes'},\n",
       "   {'paperId': '6d27025c4ec92be4b34b4afe8bd0421cd8a3bdda',\n",
       "    'title': 'CERBERUS: Autonomous Legged and Aerial Robotic Exploration in the Tunnel and Urban Circuits of the DARPA Subterranean Challenge'},\n",
       "   {'paperId': '8e3132fa3344f493c9ccf9b186d86ee62b4a518b',\n",
       "    'title': 'Smart Magnetic Microrobots Learn to Swim with Deep Reinforcement Learning'},\n",
       "   {'paperId': '19c55f823f074faf0ba0756c7a5d03844bdf9d5f',\n",
       "    'title': 'Combining Learning-based Locomotion Policy with Model-based Manipulation for Legged Mobile Manipulators'},\n",
       "   {'paperId': 'c922de3571481c29a04633f38f7a449e6307bd9a',\n",
       "    'title': 'An Efficient Locally Reactive Controller for Safe Navigation in Visual Teach and Repeat Missions'},\n",
       "   {'paperId': 'c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1',\n",
       "    'title': 'Automated Reinforcement Learning (AutoRL): A Survey and Open Problems'},\n",
       "   {'paperId': '14596fc6de764ef4cf98f2f2a6c8eb521cca2e5d',\n",
       "    'title': 'ValueNetQP: Learned one-step optimal control for legged locomotion'},\n",
       "   {'paperId': 'fd6552df6fded8dc833d1d188cb3b6a398ae15b3',\n",
       "    'title': 'Dynamic and Robust CubeSat Control with Machine Learning'},\n",
       "   {'paperId': '7267d12df350d88dd181199ab881590a85182bcc',\n",
       "    'title': 'Learning Free Gait Transition for Quadruped Robots Via Phase-Guided Controller'},\n",
       "   {'paperId': 'bbaffbe6b99a6677be4ff072c3ba187f79538de7',\n",
       "    'title': 'Invariance Through Latent Alignment'},\n",
       "   {'paperId': 'aee3fbc8f90e2b11a38e75508ac7b248787a53e6',\n",
       "    'title': 'Policy Search for Model Predictive Control With Application to Agile Drone Flight'},\n",
       "   {'paperId': 'd325aa9305ec92a75511dff3c5b2cdcc8b6c3953',\n",
       "    'title': 'Deep Reinforcement Learning with Shallow Controllers: An Experimental Application to PID Tuning'},\n",
       "   {'paperId': 'e06b2fc2fe5bf9e2d2ece2201bd84d18cb82be1a',\n",
       "    'title': 'Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning'},\n",
       "   {'paperId': '6d6e58607cf068273a2da5c66513b4cd4f110b2e',\n",
       "    'title': 'Learning Coordinated Terrain-Adaptive Locomotion by Imitating a Centroidal Dynamics Planner'},\n",
       "   {'paperId': '5de01b66bbc938fdd9385137727dfde9fa8748b9',\n",
       "    'title': 'An Adaptable Approach to Learn Realistic Legged Locomotion without Examples'},\n",
       "   {'paperId': 'd9b34c6b616f75485856794478bfbeab1ea93b81',\n",
       "    'title': 'Perspectives in machine learning for wildlife conservation'},\n",
       "   {'paperId': '35efc3a4c5f64d96ded6daea692f3935c96f0415',\n",
       "    'title': 'Legged Robots that Keep on Learning: Fine-Tuning Locomotion Policies in the Real World'},\n",
       "   {'paperId': '09c04311402819d6171cdb47b2a381f10d4f564e',\n",
       "    'title': 'Hierarchical Primitive Composition: Simultaneous Activation of Skills with Inconsistent Action Dimensions in Multiple Hierarchies'},\n",
       "   {'paperId': '66d6cf5181ca209bd4bc0350961c14a03ee8fbd6',\n",
       "    'title': 'Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization'},\n",
       "   {'paperId': '8aea987f44f52473fd2b3048ecdb788ad2471329',\n",
       "    'title': 'Learning Perceptual Locomotion on Uneven Terrains Using Sparse Visual Observations'},\n",
       "   {'paperId': 'c1fa87ddb3194431c1bf005a466fac092ba05fc0',\n",
       "    'title': 'PM-FSM: Policies Modulating Finite State Machine for Robust Quadrupedal Locomotion'},\n",
       "   {'paperId': '5c85edd4b333e78b2c42bdde6f3eec5f911bdbdc',\n",
       "    'title': 'Lifelong Robotic Reinforcement Learning by Retaining Experiences'},\n",
       "   {'paperId': 'b4ec35c1b60eb30370601052507b49d90715c013',\n",
       "    'title': 'Real-Time Multi-Contact Model Predictive Control via ADMM'},\n",
       "   {'paperId': 'e6c3937c8876a6ab5c01dca117a81b4f7c029fc5',\n",
       "    'title': 'Reinforcement Learning with Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion'},\n",
       "   {'paperId': '6d7b6096e945c2449f43073725390eb396e51fd5',\n",
       "    'title': 'Learning to Navigate Sidewalks in Outdoor Environments'},\n",
       "   {'paperId': '732beb269fd4fec6a95f023ef766a2b6f62abe11',\n",
       "    'title': 'Jammkle: Fibre jamming 3D printed multi-material tendons and their application in a robotic ankle'},\n",
       "   {'paperId': '856a6642e9b228523a690bbfd34a26c0cc4ef497',\n",
       "    'title': 'Versatile modular neural locomotion control with fast learning'},\n",
       "   {'paperId': '74840f753eaf6aa7ce1bbe246cd20e02e645b828',\n",
       "    'title': 'Model-free reinforcement learning for robust locomotion using demonstrations from trajectory optimization'},\n",
       "   {'paperId': '065dc8e953d5e5da138c33a8c3f4f7181b19cd54',\n",
       "    'title': 'Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers'},\n",
       "   {'paperId': '8afc06ff5d46a3317ec994bce25a530cb12580a0',\n",
       "    'title': 'Control of Rough Terrain Vehicles Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '90110e0b6bab9fca7caeeea41853c66a273094f9',\n",
       "    'title': 'GLiDE: Generalizable Quadrupedal Locomotion in Diverse Environments with a Centroidal Model'},\n",
       "   {'paperId': '3d61d92f1c85e9d4810242ff50f4f74d64a537a5',\n",
       "    'title': 'Robust High-Speed Running for Quadruped Robots via Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': 'cc9b539f89d3745e374d4459ee73034f570b1cd9',\n",
       "    'title': 'RLOC: Terrain-Aware Legged Locomotion Using Reinforcement Learning and Optimal Control'},\n",
       "   {'paperId': 'ef91495dffa49d8b2fb94205814b242b5efb8eb6',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': '29c3df9e21ffde43f0940c2e761302faa5738795',\n",
       "    'title': 'ValueNetQP: Learned one-step optimal control for legged locomotion'},\n",
       "   {'paperId': '1425a60910e4e7b676d9b6cbf8ec0942d4c77635',\n",
       "    'title': 'Deep Predictive Model Learning with Parametric Bias'},\n",
       "   {'paperId': 'a8e00e5493241e267b7468e849af0e6b6670a032',\n",
       "    'title': 'Sim-to-Real Learning of Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid'},\n",
       "   {'paperId': '0bfc092d7ddac5349e8dc9d7dd57d5189f9cad6d',\n",
       "    'title': 'Optimal planar leg geometry in robots and crabs for idealized rocky terrain'},\n",
       "   {'paperId': '12574375432c29f54b59b910fd14fbc31a06b4aa',\n",
       "    'title': 'SIMPLIFYING ROBOTIC LOCOMOTION BY ESCAPING TRAPS VIA AN ACTIVE TAIL'},\n",
       "   {'paperId': 'bafbb3c535d9ee0fbffaad266f732a3892f53b4e',\n",
       "    'title': 'Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review'},\n",
       "   {'paperId': 'c3f91ad72a53ef0b48958c56c454a66f9405c594',\n",
       "    'title': 'Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning'},\n",
       "   {'paperId': '891ccc729b192f20f28ba83ef6b9e66829d41086',\n",
       "    'title': 'Transfer Learning of Ground Robot Terrain Experience'},\n",
       "   {'paperId': '4a1afbbab7518a942db11a9243781297ba8764aa',\n",
       "    'title': 'A L EARNING S YSTEM FOR M OTION P LANNING OF F REE -F LOAT D UAL -A RM S PACE M ANIPULATOR TOWARDS N ON -C OOPERATIVE O BJECT'},\n",
       "   {'paperId': 'bb713d183f3927a4680dcbdd359cc43ad2905c98',\n",
       "    'title': 'Learning to Get Up with Deep Reinforcement Learning'},\n",
       "   {'paperId': '8789e934e485ff4bdd4cec67158fd9170945cd24',\n",
       "    'title': 'RCA: Ride Comfort-Aware First-Person Navigation via Self-Supervised Learning'},\n",
       "   {'paperId': 'a7e3d0968b7e6cd996ca2c9fa31f6b504448c8f6',\n",
       "    'title': 'Human-AI Shared Control via Frequency-based Policy Dissection'},\n",
       "   {'paperId': '2991e54854781fad5d3dfd0afd63a4b3ef0c1a6d',\n",
       "    'title': 'L EARNING TO W ALK A UTONOMOUSLY VIA R ESET F REE Q UALITY -D IVERSITY'},\n",
       "   {'paperId': '8b257879ad4a836d6b0096b564633037d362aada',\n",
       "    'title': 'AdaptiveON: Adaptive Outdoor Navigation Method For Stable and Reliable Actions'},\n",
       "   {'paperId': '12229099d938f0d17c899fb483dccfaf7434f5ae',\n",
       "    'title': 'Sim-to-Real Learning of Robust Compliant Bipedal Locomotion on Torque Sensor-Less Gear-Driven Humanoid'},\n",
       "   {'paperId': '7a8c6eda548ba449743b1eada06854cf8e2a0bc2',\n",
       "    'title': 'Meta Reinforcement Learning for Adaptive Control: An Offline Approach'},\n",
       "   {'paperId': '8c8868d75f5fc7a055fdbc8610ab20b0a4304829',\n",
       "    'title': 'Deep Reinforcement Learning: Opportunities and Challenges'},\n",
       "   {'paperId': '4955fd3f669325eca81ec8dec5e1c8f07152f727',\n",
       "    'title': 'A learning-based control approach for blind quadrupedal locomotion with guided-DRL and hierarchical-DRL'},\n",
       "   {'paperId': '54f8a1a0bfed162a202e03f5297fe8a2c98015d6',\n",
       "    'title': 'Motion Acquisition of Vertical Jumping by a Bio-inspired Legged Robot via Deep Reinforcement Learning'},\n",
       "   {'paperId': '34208f572540670d59048e22215709c94302bb85',\n",
       "    'title': 'Adaptive Locomotion Control of Sixteen-legged Robot based on Deep Reinforcement Learning'},\n",
       "   {'paperId': '614999ad5f6f4927506f12bbe6e07ca49cc2a122',\n",
       "    'title': 'Learning multiple gaits of quadruped robot using hierarchical reinforcement learning'},\n",
       "   {'paperId': '07d0a0f864988e1ebef96d2e7e1ff2a4f2ad7ab6',\n",
       "    'title': 'Adaptive Mimic: Deep Reinforcement Learning of Parameterized Bipedal Walking from Infeasible References'},\n",
       "   {'paperId': '2677d57eb6c632b7b13d369d7b736cccd99e4a47',\n",
       "    'title': 'Accelerated Robot Skill Acquisition by Reinforcement Learning-Aided Sim-to-Real Domain Adaptation'},\n",
       "   {'paperId': '8f95a14f41291fd976a3c0aed645be869fc97157',\n",
       "    'title': 'An insect-scale robot reveals the effects of different body dynamics regimes during open-loop running in feature-laden terrain'},\n",
       "   {'paperId': '3126881244a9aadb90585565566bc5258e60dfd9',\n",
       "    'title': 'Explosive Electric Actuator and Control for Legged Robots'},\n",
       "   {'paperId': '38c7e26a48b8699a25dd3fa3e05c800cd407ec19',\n",
       "    'title': 'Locomotion Control With Frequency and Motor Pattern Adaptations'},\n",
       "   {'paperId': '79ed87b1080dee97cf064a863e1a9cf4eece2f35',\n",
       "    'title': 'What is an artificial muscle? A comparison of soft actuators to biological muscles'},\n",
       "   {'paperId': '6245ffb6adab0a461b9d823aabe9da5ce8929aaa',\n",
       "    'title': 'd3rlpy: An Offline Deep Reinforcement Learning Library'},\n",
       "   {'paperId': '0ee9b633a0914b51f1eec3ad434752aa58e10149',\n",
       "    'title': 'A System for General In-Hand Object Re-Orientation'},\n",
       "   {'paperId': '946af0e7efcc025c5dd2f652ce453528771411b6',\n",
       "    'title': 'Is Bang-Bang Control All You Need? Solving Continuous Control with Bernoulli Policies'},\n",
       "   {'paperId': '3e5958680040b1b545d06a634a3025953cdb0486',\n",
       "    'title': 'Deep Reinforcement Learning with Gait Mode Specification for Quadrupedal Trot-Gallop Energetic Analysis'},\n",
       "   {'paperId': 'd6c0d931c822725596cf624709942a38571ec5e9',\n",
       "    'title': 'Learning Inertial Odometry for Dynamic Legged Robot State Estimation'},\n",
       "   {'paperId': '8af5ae20eed248c3eee7b2086a8724ad11b078a1',\n",
       "    'title': 'Learning to Jump from Pixels'},\n",
       "   {'paperId': '6414e4ceef2e3d7280c394be944f434100a16f1f',\n",
       "    'title': 'A Locally Actuatable Soft Robotic Film for Actively Reconfiguring Shapes of Flexible Electronics.'},\n",
       "   {'paperId': 'c263702a419d7119ca9b9804e0b44e59b03511d2',\n",
       "    'title': 'Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots'},\n",
       "   {'paperId': '71140a8b477a7344907891b1822a708df46246c2',\n",
       "    'title': 'Policy Search using Dynamic Mirror Descent MPC for Model Free Off Policy RL'},\n",
       "   {'paperId': '55566461ceb1f1617ac6479cdee9636c463e682b',\n",
       "    'title': 'Real-time Optimal Landing Control of the MIT Mini Cheetah'},\n",
       "   {'paperId': '71a0f9210665370045a82addeaa77402cf95e8be',\n",
       "    'title': 'CARL: A Benchmark for Contextual and Adaptive Reinforcement Learning'},\n",
       "   {'paperId': '2aa7411f63649776471f7192979bcee6a51c7c27',\n",
       "    'title': 'Linear Policies are Sufficient to Enable Low-Cost Quadrupedal Robots to Traverse Rough Terrain'},\n",
       "   {'paperId': '236b681cbb67c155574eb912282810f9eee1d36c',\n",
       "    'title': 'Coupling-dependent convergence behavior of phase oscillators with tegotae-control'},\n",
       "   {'paperId': '30b3ead1dc164212c36663de2bfbc059bc16e19d',\n",
       "    'title': 'Terrain-Aware Risk-Assessment-Network-Aided Deep Reinforcement Learning for Quadrupedal Locomotion in Tough Terrain'},\n",
       "   {'paperId': '238213c56f154e1f590dc05e6186df419fc55479',\n",
       "    'title': 'Run Like a Dog: Learning Based Whole-Body Control Framework for Quadruped Gait Style Transfer'},\n",
       "   {'paperId': '3c130487bd01d54e08507f100b4e231d1f072c12',\n",
       "    'title': 'Computational Design of Reconfigurable Underactuated Linkages for Adaptive Grippers*'},\n",
       "   {'paperId': '0f5f4a05a59d26f167e51f99269e313f43fe7cc4',\n",
       "    'title': 'Hierarchical Terrain-Aware Control for Quadrupedal Locomotion by Combining Deep Reinforcement Learning and Optimal Control'},\n",
       "   {'paperId': '4bffd08662984e29ce39d4a625cb6e5ad91f767f',\n",
       "    'title': 'Environmentally Adaptive Control Including Variance Minimization Using Stochastic Predictive Network with Parametric Bias: Application to Mobile Robots'},\n",
       "   {'paperId': '10e178bdce27f99da857fbb4a7537c5b5c829370',\n",
       "    'title': 'Quadruped Robot Hopping on Two Legs'},\n",
       "   {'paperId': '99d561082755f4d287a46f353554548475164c9f',\n",
       "    'title': 'Design of galloping robots with elastic spine: tracking relations between dynamic model parameters based on motion analysis of a real cheetah*'},\n",
       "   {'paperId': '9ea6970ca4f2b80ae552ef6727b594449753e16f',\n",
       "    'title': 'Rapid Stability Margin Estimation for Contact-Rich Locomotion'},\n",
       "   {'paperId': '8b4121c8440069ebe436d7bdfd428a894beb2b23',\n",
       "    'title': 'A Hierarchical Framework for Quadruped Locomotion Based on Reinforcement Learning'},\n",
       "   {'paperId': 'f06cc5b04f4eb0cb1ac8303062ad7869a8e26cd7',\n",
       "    'title': 'Rough Terrain Navigation for Legged Robots using Reachability Planning and Template Learning'},\n",
       "   {'paperId': '0b0105a4da7dd2a5c79b578b2d7dc78776d59a91',\n",
       "    'title': 'Solving Challenging Control Problems Using Two-Staged Deep Reinforcement Learning'},\n",
       "   {'paperId': '36f64ff576a594d05f92b42a9f2788afd9137ccd',\n",
       "    'title': 'Animal Gaits on Quadrupedal Robots Using Motion Matching and Model-Based Control'},\n",
       "   {'paperId': 'ca6096142016a2ba8133f6cb2c04ad30f5eae730',\n",
       "    'title': 'Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ef4da0ec68858646c770ae98cc7eba6bd696e986',\n",
       "    'title': 'End-to-End Deep Reinforcement Learning for Image-Based UAV Autonomous Control'},\n",
       "   {'paperId': 'd6300149735fd9d02c4a88c774537034165a82ee',\n",
       "    'title': 'Learning Vision-Guided Dynamic Locomotion Over Challenging Terrains'},\n",
       "   {'paperId': '310ef093e432bb89817b4a8f0840ead3f54c6366',\n",
       "    'title': 'Adaptive Quadruped Balance Control for Dynamic Environments Using Maximum-Entropy Reinforcement Learning'},\n",
       "   {'paperId': '3cb945ea2a6eb7a42b05afb6c29f6731bed88bfd',\n",
       "    'title': 'Modeling and Trajectory Optimization for Standing Long Jumping of a Quadruped with A Preloaded Elastic Prismatic Spine'},\n",
       "   {'paperId': '3d2aa736a1c84b9b88f848cf1a5e146aba2894d6',\n",
       "    'title': 'Reinforcement Learning for Collaborative Quadrupedal Manipulation of a Payload over Challenging Terrain'},\n",
       "   {'paperId': '1868e9d4c3da2d91fd0b1ccc75aeaa56860f8d97',\n",
       "    'title': 'Navigating by touch: haptic Monte Carlo localization via geometric sensing and terrain classification'},\n",
       "   {'paperId': '3032844d6ac6882ccb03e7a2c22a0026b210ac05',\n",
       "    'title': 'What Matters in Learning from Offline Human Demonstrations for Robot Manipulation'},\n",
       "   {'paperId': '6d8119305b42418d261a10a048936e2f97a63c1d',\n",
       "    'title': 'Fast and Slow Adaptations of Interlimb Coordination via Reflex and Learning During Split-Belt Treadmill Walking of a Quadruped Robot'},\n",
       "   {'paperId': 'de563e2d2540fd59250cb8c20a910012ac32cc41',\n",
       "    'title': 'Achieving natural behavior in a robot using neurally inspired hierarchical perceptual control'},\n",
       "   {'paperId': 'e9bc8f28961aea18eb226cf404ea8cbb7a670e24',\n",
       "    'title': 'Deep Reinforcement Learning for Multi-contact Motion Planning of Hexapod Robots'},\n",
       "   {'paperId': 'bc7c3e5cad38f54171c3ecd3a8ec2f3dd3e78b12',\n",
       "    'title': 'Design, modeling and analysis of a novel self-crossing mechanism'},\n",
       "   {'paperId': '1cf6f6693472189e8947d4278f567810b739a745',\n",
       "    'title': 'Self-reconfigurable multilegged robot swarms collectively accomplish challenging terradynamic tasks'},\n",
       "   {'paperId': '7cccef8922f8f5a16d532c883951715555fc3e5a',\n",
       "    'title': 'Explaining fast improvement in online imitation learning'},\n",
       "   {'paperId': 'd67ccf10f6da4e56960be99887cae4a3c7e1cb39',\n",
       "    'title': 'High-Payload Online Identification and Adaptive Control for an Electrically-actuated Quadruped Robot'},\n",
       "   {'paperId': 'be1cc68d185b5e82843928f366587268e3a771d9',\n",
       "    'title': 'An Adaptive Control Algorithm for Quadruped Locomotion with Proprioceptive Linear Legs'},\n",
       "   {'paperId': 'd109007a6a86f84b51d2d4560dca328598a0c554',\n",
       "    'title': 'Learning Quadruped Locomotion Policies with Reward Machines'},\n",
       "   {'paperId': '1613af8bd03688a9d3214c3e1c1876b6d7088805',\n",
       "    'title': 'Leveraging Human Knowledge to Learn Quadruped Locomotion Policies'},\n",
       "   {'paperId': 'f57ca18e5c0a09c7d5afb4dd1993846b4635e70e',\n",
       "    'title': 'Pareto gamuts'},\n",
       "   {'paperId': 'a2804c01da7c1d508f21d9c28ee1e276e5202c1c',\n",
       "    'title': 'Model-free Reinforcement Learning for Robust Locomotion Using Trajectory Optimization for Exploration'},\n",
       "   {'paperId': '5c7c878b300c1e8ef1b42e3a3e35a1a93877df0c',\n",
       "    'title': 'Fast Contact-Implicit Model-Predictive Control'},\n",
       "   {'paperId': '41104cfa3b7901a94efa8ba8763f0d8989f6b108',\n",
       "    'title': 'Lyapunov-stable neural-network control'},\n",
       "   {'paperId': '1ca5ff6555d9fc634d3858d1fda9b3de2a91b13a',\n",
       "    'title': 'RMA: Rapid Motor Adaptation for Legged Robots'},\n",
       "   {'paperId': '24195cc3feef7fba08723adf3dd5ef894461fc45',\n",
       "    'title': 'Adaptation of Quadruped Robot Locomotion with Meta-Learning'},\n",
       "   {'paperId': 'f7d246d3d5b4afbd66febf7e54ab36f0e3be4022',\n",
       "    'title': 'Quadruped Locomotion on Non-Rigid Terrain using Reinforcement Learning'},\n",
       "   {'paperId': 'ffaef4f37f19cee889b1c2528122f8d0a32cd8d8',\n",
       "    'title': 'Action Set Based Policy Optimization for Safe Power Grid Management'},\n",
       "   {'paperId': '320959db43db08185cfbabe655e044c2a378ed39',\n",
       "    'title': 'Virtual to Real-World Transfer Learning: A Systematic Review'},\n",
       "   {'paperId': '7ad1b82507b61c7113c4bde17fa3d89bb256cff3',\n",
       "    'title': 'SECANT: Self-Expert Cloning for Zero-Shot Generalization of Visual Policies'},\n",
       "   {'paperId': '2df2601a161b665928714b5e67a2a651adf0a8dd',\n",
       "    'title': 'Recent Progress in Legged Robots Locomotion Control'},\n",
       "   {'paperId': '0fa5926f8a60e0475c63d5e3281f9d101a9eb544',\n",
       "    'title': 'Optical Tactile Sim-to-Real Policy Transfer via Real-to-Sim Tactile Image Translation'},\n",
       "   {'paperId': '3a47c09379952ff8c6ab2549acb806a3d2582aa0',\n",
       "    'title': 'Tactile Sim-to-Real Policy Transfer via Real-to-Sim Image Translation'},\n",
       "   {'paperId': 'fdfd09d6b95e3c5e1719cddd66bc332d2745b2ec',\n",
       "    'title': 'Causal Navigation by Continuous-time Neural Networks'},\n",
       "   {'paperId': '27cc8023a9671f30cf0264d4c9cc8120594b2008',\n",
       "    'title': 'Meta-Adaptive Nonlinear Control: Theory and Algorithms'},\n",
       "   {'paperId': 'd769ca62d90adc7e7869849a421426bdc54a32fb',\n",
       "    'title': 'Policy Finetuning: Bridging Sample-Efficient Offline and Online Reinforcement Learning'},\n",
       "   {'paperId': '0c580a3ebb5679cb22eb07f623e228cf025a0ecc',\n",
       "    'title': 'Traversing Steep and Granular Martian Analog Slopes With a Dynamic Quadrupedal Robot'},\n",
       "   {'paperId': '9805daa77812336cebd7bdd43392537f7793154b',\n",
       "    'title': 'A Novel Torsional Actuator Augmenting Twisting Skeleton and Artificial Muscle for Robots in Extreme Environments'},\n",
       "   {'paperId': '131887aafe640345bad33695035cf4a8c511116d',\n",
       "    'title': 'Real-Time Trajectory Adaptation for Quadrupedal Locomotion using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f14bf51cff2674b8de304791519f4c3e4131cd2b',\n",
       "    'title': 'Real-time Optimal Navigation Planning Using Learned Motion Costs'},\n",
       "   {'paperId': '41389aceadd2ea4dfb7df4f3b9ce3afd7d942e25',\n",
       "    'title': 'Learning Visible Connectivity Dynamics for Cloth Smoothing'},\n",
       "   {'paperId': 'b19501ef422698ed5d57085b111ecc03621dfd25',\n",
       "    'title': 'Blind Bipedal Stair Traversal via Sim-to-Real Reinforcement Learning'},\n",
       "   {'paperId': '23299871f55f974d979ab4c3fdddda73a31e3ed7',\n",
       "    'title': 'Learning where to trust unreliable models in an unstructured world for deformable object manipulation'},\n",
       "   {'paperId': 'ccd751c5456d36ecc5fbd9c5b9b2b3074fd7de44',\n",
       "    'title': 'Learning Contact-aware CPG-based Locomotion in a Soft Snake Robot'},\n",
       "   {'paperId': '2d02d63321c66a4de3078321c69c3f01e45768f8',\n",
       "    'title': 'Episodic Learning for Safe Bipedal Locomotion with Control Barrier Functions and Projection-to-State Safety'},\n",
       "   {'paperId': '0614d0da5f49ff902ffd9209482f29b902a1a824',\n",
       "    'title': 'Learning to drive from a world on rails'},\n",
       "   {'paperId': 'a394106353d67842bbe618eea370d903fbe1688f',\n",
       "    'title': 'Locomotor transitions in the potential energy landscape-dominated regime'},\n",
       "   {'paperId': 'bf0140fe2726fb3ab4cd4ec2b93256534d409839',\n",
       "    'title': 'Auto-Tuned Sim-to-Real Transfer'},\n",
       "   {'paperId': '6f3b5e142ebe7d90e3042392919d8597a07bffa1',\n",
       "    'title': 'Fast and Efficient Locomotion via Learned Gait Transitions'},\n",
       "   {'paperId': '5bcf8ff7d038884d07126a74e89d2b6ffefa63d0',\n",
       "    'title': 'Learning Linear Policies for Robust Bipedal Locomotion on Terrains with Varying Slopes'},\n",
       "   {'paperId': '6ab8a6a69ef7e129fd0b90ef3f592becdc917a0f',\n",
       "    'title': 'How Are Learned Perception-Based Controllers Impacted by the Limits of Robust Control?'},\n",
       "   {'paperId': '7a2fc4dd06477e70095ce8873e5278eb1487bee6',\n",
       "    'title': 'Natural Walking With Musculoskeletal Models Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '7c21d8b66d70ef1de3bb96107536055494e3cd23',\n",
       "    'title': 'Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot'},\n",
       "   {'paperId': '72568f836477ee3e55d0e4bf6fa98f0aa374b32f',\n",
       "    'title': 'A survey of the development of biomimetic intelligence and robotics'},\n",
       "   {'paperId': 'a9fac48484f1d6f114adf937aa221326fb494f20',\n",
       "    'title': 'Autonomous Overtaking in Gran Turismo Sport Using Curriculum Reinforcement Learning'},\n",
       "   {'paperId': '9789dd9408a4a5116b40ab99603d9143b15a26db',\n",
       "    'title': 'Reinforcement Learning for Robust Parameterized Locomotion Control of Bipedal Robots'},\n",
       "   {'paperId': '2dab6f49707f58e4e1b04e4690c4f3fa46064a13',\n",
       "    'title': 'Measuring and modeling the motor system with machine learning'},\n",
       "   {'paperId': 'bd5cd9d680bd4bea13c7d1e0433e862f3bfb8411',\n",
       "    'title': 'Real-world embodied AI through a morphologically adaptive quadruped robot'},\n",
       "   {'paperId': '4e4c487fe6904ca0bf817b831acead220d2113e5',\n",
       "    'title': 'Autonomous Drone Racing with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e7eafa8e1947feaf4c212bed5d54d48aa117f1f2',\n",
       "    'title': 'A CPG-Based Agile and Versatile Locomotion Framework Using Proximal Symmetry Loss'},\n",
       "   {'paperId': 'abf37c94d572b0a31f838f3b7ea543a890ccb5ca',\n",
       "    'title': 'CPG-ACTOR: Reinforcement Learning for Central Pattern Generators'},\n",
       "   {'paperId': '38fa4bd7e944b73d128b93f2d279416f93074222',\n",
       "    'title': 'Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems'},\n",
       "   {'paperId': '400811ee31020a3f002551476dac25973e13035e',\n",
       "    'title': 'How to train your robot with deep reinforcement learning: lessons we have learned'},\n",
       "   {'paperId': '9f87d5d57a6636d7a6e092052c8854570f49c453',\n",
       "    'title': 'Achieving natural behavior in a robot using neurally inspired hierarchical control'},\n",
       "   {'paperId': 'd7094e7a652a2910a85606c4509c326d32adc0da',\n",
       "    'title': 'Trajectory Optimization of Contact-Rich Motions Using Implicit Differential Dynamic Programming'},\n",
       "   {'paperId': '30b28c0a7611b85b2c93637955a096069edbaab9',\n",
       "    'title': 'Zero-Shot Sim-to-Real Transfer of Tactile Control Policies for Aggressive Swing-Up Manipulation'},\n",
       "   {'paperId': '670abbfc876d8a08d21c1d2e5f4805dd40c997c2',\n",
       "    'title': 'Learning-Based Methods of Perception and Navigation for Ground Vehicles in Unstructured Environments: A Review'},\n",
       "   {'paperId': '6338e211bedbc02ddd329ec25b8e11063ddb9b58',\n",
       "    'title': 'Structured learning of rigid‐body dynamics: A survey and unified view from a robotics perspective'},\n",
       "   {'paperId': 'a3e650964ead5af2fbb91a4480eb9af2e34c79fd',\n",
       "    'title': 'Learning Navigation Skills for Legged Robots with Learned Robot Embeddings'},\n",
       "   {'paperId': '9afe7671176c721cbd55a6d009ecc8d837e49518',\n",
       "    'title': 'COCOI: Contact-aware Online Context Inference for Generalizable Non-planar Pushing'},\n",
       "   {'paperId': '8e5143c11f3c9dee4f42a297fad45705af28f890',\n",
       "    'title': 'Towards Robust Data-Driven Control Synthesis for Nonlinear Systems with Actuation Uncertainty'},\n",
       "   {'paperId': 'cfa478d8e8f01f7aa6feebe6a8a491b63a179101',\n",
       "    'title': 'Circus ANYmal: A Quadruped Learning Dexterous Manipulation with Its Limbs'},\n",
       "   {'paperId': '39168507d220851d5e2919e17a1779a832520051',\n",
       "    'title': 'A Legged Soft Robot Platform for Dynamic Locomotion'},\n",
       "   {'paperId': 'c7bba8e052291c21d98959d5e0714814d1225323',\n",
       "    'title': 'Learning Agile Locomotion Skills with a Mentor'},\n",
       "   {'paperId': '85dc944592de00b16c81be5acee7a71a0aea929e',\n",
       "    'title': 'Self-supervised Learning of LiDAR Odometry for Robotic Applications'},\n",
       "   {'paperId': '903f344a59bf6f738368b15c0c19912ac171085b',\n",
       "    'title': 'Dynamics Randomization Revisited: A Case Study for Quadrupedal Locomotion'},\n",
       "   {'paperId': '5ff1612a04acd3c86d408bb00e90982c20f46333',\n",
       "    'title': 'Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '8c28132e7bca149b9f0c5f7c443716b0abe193a5',\n",
       "    'title': 'Wide Obstacle & Sphere MountainThin Obstacle & Sphere Moving Obstacle'},\n",
       "   {'paperId': '7d52161336907a873c71c015550d43240565250f',\n",
       "    'title': 'BOX LCD: A S IMPLE T ESTBED FOR L EARNED S IMULATOR R ESEARCH'},\n",
       "   {'paperId': '8c41cf8ff2005d99ed9f440548685a1be7c56ca2',\n",
       "    'title': 'D YNAMIC M IRROR D ESCENT BASED M ODEL P REDICTIVE C ONTROL FOR A CCELERATING R OBOT L EARNING'},\n",
       "   {'paperId': '7cc2d0a806864a30a9559fda30f859b891691621',\n",
       "    'title': 'Minimizing Energy Consumption Leads to the Emergence of Gaits in Legged Robots Studies of motor control in both and'},\n",
       "   {'paperId': 'd8d4d03d7af2b4c3ac5b9bef4dad390f3d1e6fc2',\n",
       "    'title': 'A Novel Approach to an Autonomous and Dynamic Satellite Control System Using On-Orbit Machine Learning'},\n",
       "   {'paperId': '949e86002ec59aa94292d86580874ce5efa6431a',\n",
       "    'title': 'Using Physics Knowledge for Learning Rigid-body Forward Dynamics with Gaussian Process Force Priors'},\n",
       "   {'paperId': '1d01143f83dc85743f396adebc43ce93d3e5eed4',\n",
       "    'title': 'Learning to Walk over Structured Terrains by Imitating MPC'},\n",
       "   {'paperId': '2430902ddd00bd3b366c92b58924c3a5c737e57c',\n",
       "    'title': 'Biologically Inspired Planning and Optimization of Foot Trajectory of a Quadruped Robot'},\n",
       "   {'paperId': '866b946c01618a724f7832e5ddeb24818079a51d',\n",
       "    'title': 'Research on Foot Slippage Estimation of Mammal Type Legged Robot'},\n",
       "   {'paperId': 'e9810e7ae50715760eaa66f3fcfc1c8d0fc2e26a',\n",
       "    'title': 'Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition'},\n",
       "   {'paperId': 'e5ab04396a0b29efae5ef8b1ce258644322359ea',\n",
       "    'title': 'Overleaf Example'},\n",
       "   {'paperId': '516a540b0737c886d514f200ed84d6c77d0c93bb',\n",
       "    'title': 'Towards Real Robot Learning in the Wild: A Case Study in Bipedal Locomotion'},\n",
       "   {'paperId': 'e4315bc0ec3975e33d7c15a27d233d597ccbcf4e',\n",
       "    'title': 'Visual-Locomotion: Learning to Walk on Complex Terrains with Vision'},\n",
       "   {'paperId': 'e28edc7c5b0e529d272649108276c381312a01a5',\n",
       "    'title': 'Linear Contact-Implicit Model-Predictive Control'},\n",
       "   {'paperId': '4de41b9951ee6642b14ebb5660f7ba676c26d248',\n",
       "    'title': 'Accelerated Policy Search'},\n",
       "   {'paperId': 'c9c3791856f068ab49c037f3f197f8c7c9cb198f',\n",
       "    'title': 'Combat Efficiency and Effectiveness of AI-driven Multisensory Search-and-Destroy Agent'},\n",
       "   {'paperId': 'e313d5dfda4ab359039d77ef59adc71b22d001dc',\n",
       "    'title': 'Pareto Gamuts: Exploring Optimal Designs Across Varying Contexts'},\n",
       "   {'paperId': '029128fc05ef1a1b6a4094703ab87071edbe7b0b',\n",
       "    'title': 'Sample-Efficient Learning-Based Controller For Bipedal Walking In Robotic Systems'},\n",
       "   {'paperId': '5dbeb9dc4812f88779070cf5a8ea8c4bbef9c62c',\n",
       "    'title': 'Robust Quadruped Jumping via Deep Reinforcement Learning'},\n",
       "   {'paperId': '3df0d5a525674a7ea1ae8362f4498b8240bc74c0',\n",
       "    'title': 'Zero-Shot Terrain Generalization for Visual Locomotion Policies'},\n",
       "   {'paperId': '7c2afaf2764ee9699736f60952c4024e64da81dd',\n",
       "    'title': 'Bound Controller for a Quadruped Robot using Pre-Fitting Deep Reinforcement Learning'},\n",
       "   {'paperId': '0a3645b9fcdf9ff99b4c6ef410bbd1192592e325',\n",
       "    'title': 'Efficient Learning of Control Policies for Robust Quadruped Bounding using Pretrained Neural Networks'},\n",
       "   {'paperId': 'e5d65718a3a830caa2bc2813d04ce71c5b7046d8',\n",
       "    'title': 'Quadrupedal robots trot into the wild'}],\n",
       "  'citnuminlist': 2,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'd242950c9d4903d078055b3f5bbbad1b5e626e74': {'title': 'Learning Robot Skills with Temporal Variational Inference',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': '9624331101e13f42da55ab06c0f6036b78471181',\n",
       "    'title': 'Learning attribute grammars for movement primitive sequencing'},\n",
       "   {'paperId': '443ed79616c240cd9c3bc1fbe363550b6b76fe29',\n",
       "    'title': 'Variational Temporal Abstraction'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '5df5561b55ab872f2b3df559ddd475299f660b42',\n",
       "    'title': 'Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration'},\n",
       "   {'paperId': '9d671a4de50b98c3f00623ee597e37c9f00ba0cc',\n",
       "    'title': 'Temporal Difference Variational Auto-Encoder'},\n",
       "   {'paperId': 'c01566fa915db4a4d2636bf6b03a36e203e93845',\n",
       "    'title': 'ROBOTURK: A Crowdsourcing Platform for Robotic Skill Learning through Imitation'},\n",
       "   {'paperId': 'df350766b53d4db23790a0408b0d2c7a185cff74',\n",
       "    'title': 'Multiple Interactions Made Easy (MIME): Large Scale Demonstrations Data for Imitation'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': '809f951c77b5a39e2a9d556e9cf9938de87f2393',\n",
       "    'title': 'An Inference-Based Policy Gradient Method for Learning Options'},\n",
       "   {'paperId': 'bf48f1d556fdb85d5dbe8cfd93ef13c212635bcf',\n",
       "    'title': 'Neural Task Programming: Learning to Generalize Across Hierarchical Tasks'},\n",
       "   {'paperId': '694d284d7c63c7f29c48b540bc4ece9f63b79160',\n",
       "    'title': 'Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'd6ef620ed29b94edfc28c8ec263b29ffb4f4b89d',\n",
       "    'title': 'Learning movement primitive libraries through probabilistic segmentation'},\n",
       "   {'paperId': '28d68b2064607c02dd1e549e870a894b11674f48',\n",
       "    'title': 'Learning composable models of parameterized skills'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '90188082284ef4fe0d23a9216fec919a964520ff',\n",
       "    'title': 'TSC-DL: Unsupervised trajectory segmentation of multi-modal surgical demonstrations with Deep Learning'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Ornstein Uhlenbeck Noise Parameters: Our DDPG implementation for the RL training uses the Ornstein Uhlenbeck noise process, with parameters identical to those used in the DDPG paper'},\n",
       "   {'paperId': '9d242175cbe2f082da78e469bc9b23144c33b320',\n",
       "    'title': 'Learning modular policies for robotics'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Learning modular policies for robotics. Frontiers in computational neuroscience'},\n",
       "   {'paperId': 'f258a0540505b0057b3f5ffc084cebad1554b03c',\n",
       "    'title': 'Towards Robot Skill Learning: From Simple Skills to Table Tennis'},\n",
       "   {'paperId': 'b7782af5fb454ed719f506462d38c7dc80e844a5',\n",
       "    'title': 'The Principle of Maximum Causal Entropy for Estimating Interacting Processes'},\n",
       "   {'paperId': 'afa621e449527ae3306286d0fe6cd443fe38d722',\n",
       "    'title': 'Dynamical Movement Primitives: Learning Attractor Models for Motor Behaviors'},\n",
       "   {'paperId': '89b9928df443e4e686a4f82b9bd8d67dc23cfa05',\n",
       "    'title': 'Learning and generalization of complex tasks from unstructured demonstrations'},\n",
       "   {'paperId': '1695dbabf8e905db0b391ff522c323db5fc8b958',\n",
       "    'title': 'Learning to select and generalize striking movements in robot table tennis'},\n",
       "   {'paperId': '6b92156c9e6ea53dec9e4941b62c11a1fbda485e',\n",
       "    'title': 'Robot learning from demonstration by constructing skill trees'},\n",
       "   {'paperId': '271a4077c037b86fb7daf6bff3e66682322ff7d7',\n",
       "    'title': 'Movement segmentation using a primitive library'},\n",
       "   {'paperId': 'ffa89e2d70c7b12e42b12923ebc45a46fb7798a9',\n",
       "    'title': 'Learning table tennis with a Mixture of Motor Primitives'},\n",
       "   {'paperId': '0645c8792d5095f0de45e95c5fd9de5238f426e6',\n",
       "    'title': 'Learning motor primitives for robotics'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': '84b8551bb1f80f2bf61ea3ef1599f4dc99e7c91c',\n",
       "    'title': 'Skill Chaining : Skill Discovery in Continuous Domains'},\n",
       "   {'paperId': None, 'title': 'Visualizing highdimensional data using t-sne'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': None, 'title': 'Cmu graphics lab motion capture database'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '32c28efca0ccfeeed03d0283e00e3acc05675d19',\n",
       "    'title': 'Directed information for channels with feedback'},\n",
       "   {'paperId': '44d2abe2175df8153f465f6c39b68b76a0d40ab9',\n",
       "    'title': 'Long Short-Term Memory'},\n",
       "   {'paperId': 'a9da09d1e63686706d64782e654d69f13fd292ad',\n",
       "    'title': 'Learning by Demonstration'},\n",
       "   {'paperId': 'db4849a2e61ae47fbc52efe1c855c618bba0e7b8',\n",
       "    'title': 'Behavioural cloning in control of a dynamic system'},\n",
       "   {'paperId': 'c547e1f79e6039d05c5ae433a36612d7f8e4d3f5',\n",
       "    'title': 'STRIPS: A New Approach to the Application of Theorem Proving to Problem Solving'},\n",
       "   {'paperId': None,\n",
       "    'title': 'DATASET DETAILS: Regarding the CMU Mocap dataset, the data used in this project was obtained from mocap'},\n",
       "   {'paperId': None,\n",
       "    'title': '2018) environments to evaluate our approach: • SawyerPickPlace -An environment where a sawyer robot grasps and places objects in specific positions in respective bins. We use 4 variants of this task'}],\n",
       "  'citations': [{'paperId': '211041e7a7086c96583f5312ade0b7866e3f1dc9',\n",
       "    'title': 'Learning disentangled skills for hierarchical reinforcement learning through trajectory autoencoder with weak labels'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'd914dc7f5d9291ee2127936e3206c90ca1fcea71',\n",
       "    'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation'},\n",
       "   {'paperId': 'ef213d6543cd995ac6b1dfde7d08a7a120232391',\n",
       "    'title': 'Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '9d1445f1845a2880ff9c752845660e9c294aa7b5',\n",
       "    'title': 'Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery'},\n",
       "   {'paperId': 'ce0e769936453f827aee367e3463bb9915c6d78b',\n",
       "    'title': 'Emergency action termination for immediate reaction in hierarchical reinforcement learning'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': '453e2920d36900068c7069f7ee6ff64744920b38',\n",
       "    'title': 'AtHom: Two Divergent Attentions Stimulated By Homomorphic Training in Text-to-Image Synthesis'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': '49d710921b04885470e17d28f8c46fb309bdf9ac',\n",
       "    'title': 'DIDER: Discovering Interpretable Dynamically Evolving Relations'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '3682d32d7761c2f81cdb07ca9c75363d83681e3e',\n",
       "    'title': 'Fairness and Bias in Robot Learning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '77d3d69f1c4c160e3765c416bc13aed863176197',\n",
       "    'title': 'One After Another: Learning Incremental Skills for a Changing World'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': '67ef4de78111f5bba87472ea2a783f5463bf9924',\n",
       "    'title': 'Study of Variational Inference for Flexible Distributed Probabilistic Robotics'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': 'ee21c47254d1bcf33e13bf746218021816443745',\n",
       "    'title': 'Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '193dadd9de36ef8e6883088fbd35d38fa7ce590e',\n",
       "    'title': 'Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '5e522a45507a97d4738d4d81980793859eb5fb50',\n",
       "    'title': 'Variational Inference MPC using Tsallis Divergence'},\n",
       "   {'paperId': '39cc9292cb602970453c7677ada5d575d03d1d77',\n",
       "    'title': 'SKID RAW: Skill Discovery From Raw Trajectories'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '5764095b0186a3fc3832c1052aa14996a5927edc',\n",
       "    'title': 'RODE: Learning Roles to Decompose Multi-Agent Tasks'},\n",
       "   {'paperId': '615addcfcb3bd22bd17821fb70da1af8f1e04bdd',\n",
       "    'title': 'Causal Mechanism Transfer Network for Time Series Domain Adaptation in Mechanical Systems'},\n",
       "   {'paperId': None, 'title': 'TRANSLATING ROBOT SKILLS: LEARNING UNSUPER-'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '1b305b762bd6d9c929090631234b8a0721cc0ec7',\n",
       "    'title': 'GSC: Graph-based Skill Composition for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'fb2769a88b3eb152779694768038d3715a9274ba',\n",
       "    'title': 'Broad Generalization in Robotics Will Require Broad Datasets'},\n",
       "   {'paperId': 'ede66a0627ee774052c4234311be1966cf927634',\n",
       "    'title': 'Bottom-up Discovery of Reusable Sensorimotor Skills from Unstructured Demonstrations'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '6be61525ee8b21c3bef6564df17b435fc4f84282',\n",
       "    'title': 'Action and Perception as Divergence Minimization'},\n",
       "   {'paperId': 'ec684b9cf2433680f6bd70779186f34bcd5b4f06',\n",
       "    'title': 'MaxEnt Reward Expected Reward Latent Representations Missing Data Controllable Future Factorized Target Perception Action Both Low Entropy Preferences Empowerment Skill Discovery Amortized Inference Maximum Likelihood Variational Inference Input Density Exploration Information GainFiltering Latent S'},\n",
       "   {'paperId': 'a16ce6c25509bc0113fb4d4a73cd55fb5ea73d94',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference Supplement'}],\n",
       "  'citnuminlist': 4,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " '872edada2165ad65c1664b813efdb92e3bec1b36': {'title': 'Multi-expert learning of adaptive legged locomotion',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'c4f3ab1a23e3edbb6877c7956863833c5232d492',\n",
       "    'title': 'Contact-Implicit Trajectory Optimization Using an Analytically Solvable Contact Model for Locomotion on Variable Ground'},\n",
       "   {'paperId': '4968dd3c2570d41dc301e54ddc669187e822e0bf',\n",
       "    'title': 'Learning Natural Locomotion Behaviors for Humanoid Robots Using Human Bias'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Iterative design and sim-to-real, in Proceedings of the Conference on Robot Learning (CoRL) (PMLR, 2020), pp'},\n",
       "   {'paperId': None,\n",
       "    'title': 'PyBullet, a Python module for physics simulation for games, robotics and machine learning; http://pybullet.org'},\n",
       "   {'paperId': 'd34c156f2771dd99a671fb5e81c7463351d29a98',\n",
       "    'title': 'Sensory cortical control of movement'},\n",
       "   {'paperId': '68f50216374c50382e17890aa25871230517af13',\n",
       "    'title': 'Neural state machine for character-scene interactions'},\n",
       "   {'paperId': 'b50e2b988fd390266edd5f90dc5ea565a8461b00',\n",
       "    'title': 'Dynamic locomotion synchronization of bipedal robot and human operator via bilateral feedback teleoperation'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '951af7222535d934ca2b401ca0cd2181b28284f9',\n",
       "    'title': 'Reinforcement learning in artificial and biological systems'},\n",
       "   {'paperId': 'bb0ee42d406f2361fee89cf1274073185a0e9eec',\n",
       "    'title': 'Learning agile and dynamic motor skills for legged robots'},\n",
       "   {'paperId': 'd5372a947f67be78fa2a8d74d6e2816d6bb574f8',\n",
       "    'title': 'Using Deep Reinforcement Learning to Learn High-Level Policies on the ATRIAS Biped'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': None,\n",
       "    'title': 'MCP: Learning composable hierarchical SCIENCE ROBOTICS | RESEARCH ARTICLE Science Robotics, Vol 5, Issue 49. 26 control with multiplicative compositional policies, in Advances in Neural Information'},\n",
       "   {'paperId': '719068eb8b8c9ab8552ec3e82c1b1088a9eacdce',\n",
       "    'title': 'Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real'},\n",
       "   {'paperId': '395c976f517c84e204c7320a289b2b0c60d1fb66',\n",
       "    'title': 'Learning Whole-Body Motor Skills for Humanoids'},\n",
       "   {'paperId': 'f75752646c0a4206f9b06608248a9e84d949970e',\n",
       "    'title': 'Multi-contact Motion Planning and Control'},\n",
       "   {'paperId': '608d53fcd69173d30914e29d9b8ca4b37efe9ac4',\n",
       "    'title': 'Dynamic Locomotion in the MIT Cheetah 3 Through Convex Model-Predictive Control'},\n",
       "   {'paperId': 'c1a262a126e6fe8f9989a5b8c0d3116b68fbbfb6',\n",
       "    'title': 'Mode-adaptive neural networks for quadruped motion control'},\n",
       "   {'paperId': '322581d242a0903ed4100eac6e969c977bbf785f',\n",
       "    'title': 'Blue Brain'},\n",
       "   {'paperId': '4d3b69bdcd1d325d29badc6a38f2d6cc504fe7d1',\n",
       "    'title': 'Sim-to-Real: Learning Agile Locomotion For Quadruped Robots'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': '137b81f817c8bb6143e6554fe6b711d76736c6de',\n",
       "    'title': 'Setting up a Reinforcement Learning Task with a Real-World Robot'},\n",
       "   {'paperId': 'd5aca18de1158e9b103504c2fbb548777509d521',\n",
       "    'title': 'Impedance Control and Performance Measure of Series Elastic Actuators'},\n",
       "   {'paperId': '004acfec16c36649408c561faa102dd9de76f085',\n",
       "    'title': 'Multi-level Factorisation Net for Person Re-identification'},\n",
       "   {'paperId': '07e2a6eee7a333e57a08aef3e8628e61527007b4',\n",
       "    'title': 'Gait and Trajectory Optimization for Legged Systems Through Phase-Based End-Effector Parameterization'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': '772e90438a12f9abbdb364ea18775d7890d179aa',\n",
       "    'title': 'Whole-Body Nonlinear Model Predictive Control Through Contacts for Quadrupeds'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '03ee8367a3cf8dde2f91d7a49322dda31af3e54e',\n",
       "    'title': 'A whole-body support pose taxonomy for multi-contact humanoid robot motions'},\n",
       "   {'paperId': '8d482146e741976b45c21d39269d76207adb2372',\n",
       "    'title': 'DeepLoco'},\n",
       "   {'paperId': '6079719b677d0abda12abcd5cd46582ca91585ad',\n",
       "    'title': 'Phase-functioned neural networks for character control'},\n",
       "   {'paperId': '0ea3bf6d45d05fe28c7ae683e0f92a9923e0e2bb',\n",
       "    'title': 'Learning locomotion skills using DeepRL: does the choice of action space matter?'},\n",
       "   {'paperId': '1078898179128b9e36a2f7713ad15c4833d36a5f',\n",
       "    'title': 'Multi-Contact Planning and Control'},\n",
       "   {'paperId': 'bfba319e021ec261ad7cad164749cc5eb85951f5',\n",
       "    'title': 'ANYmal - a highly mobile and dynamic quadrupedal robot'},\n",
       "   {'paperId': '3b9732bb07dc99bde5e1f9f75251c6ea5039373e',\n",
       "    'title': 'Deep Reinforcement Learning with Double Q-Learning'},\n",
       "   {'paperId': 'e6394850b166fa8d98439ed7dcd6d99bc3ad53c7',\n",
       "    'title': 'Springer Handbook of Robotics'},\n",
       "   {'paperId': 'd8c242fd6ccf50eac25bc94984ad673b804f8ae8',\n",
       "    'title': 'Online impedance parameter tuning for compliant biped balancing'},\n",
       "   {'paperId': '348c711f19a70591e3f634efa1df27831362ea1a',\n",
       "    'title': 'No falls, no resets: Reliable humanoid behavior in the DARPA robotics challenge'},\n",
       "   {'paperId': 'd6f26270c808531a80827b9c1b4b899c23d26ae5',\n",
       "    'title': 'Online Planning for Autonomous Running Jumps Over Obstacles in High-Speed Quadrupeds'},\n",
       "   {'paperId': '1c2c51dbf1d4a0388ca53006ecbff71b6e96683f',\n",
       "    'title': 'Optimization‐based Full Body Control for the DARPA Robotics Challenge'},\n",
       "   {'paperId': 'f304aaef4d8c7d6d7ae4ef0e8a0ac72a70260b2a',\n",
       "    'title': 'Internal models direct dragonfly interception steering'},\n",
       "   {'paperId': '1a632fb89b6b05dc16fbc026d86e390e22ca6ac3',\n",
       "    'title': 'Robots that can adapt like animals'},\n",
       "   {'paperId': '39768dc29f81d6ed9e2fdfd36ab73c5ec2622d92',\n",
       "    'title': 'Whole-body motion planning with centroidal dynamics and full kinematics'},\n",
       "   {'paperId': '4f2acb96d3df9dbafc1f9ff929089b419026dae8',\n",
       "    'title': 'Kinematic primitives for walking and trotting gaits of a quadruped robot with compliant legs'},\n",
       "   {'paperId': '80634726fc1d0e929c6ad59ccb3d053a504e2d87',\n",
       "    'title': 'Learning robot gait stability using neural networks as sensory feedback function for Central Pattern Generators'},\n",
       "   {'paperId': '3c51ecb94bd0c6c83c8fbc090a0499f0d6f2b071',\n",
       "    'title': 'Dynamic primitives of motor behavior'},\n",
       "   {'paperId': '1695dbabf8e905db0b391ff522c323db5fc8b958',\n",
       "    'title': 'Learning to select and generalize striking movements in robot table tennis'},\n",
       "   {'paperId': 'bf92aa92df91782b0fef7d23ba24ac4269dcfbaf',\n",
       "    'title': 'A human-like walking for the COmpliant huMANoid COMAN based on CoM trajectory reconstruction from kinematic Motion Primitives'},\n",
       "   {'paperId': '28d3d4d71dbda46a71bef7c0c6477c71bb547db0',\n",
       "    'title': 'A sparse model predictive control formulation for walking motion generation'},\n",
       "   {'paperId': '644a079073969a92674f69483c4a85679d066545',\n",
       "    'title': 'Double Q-learning'},\n",
       "   {'paperId': '2950be1e21d41c7b0924615f3cdd189df87f6002',\n",
       "    'title': 'Stages in learning motor synergies: a view based on the equilibrium-point hypothesis.'},\n",
       "   {'paperId': '5b8e3f71d49ad1a196d098bd994a5131d1522442',\n",
       "    'title': 'Series compliance for an efficient running gait'},\n",
       "   {'paperId': '764a3bbcede985f91b56ff6acd2c09f8284b56db',\n",
       "    'title': 'Muscle synergies during locomotion in the cat: a model for motor cortex control'},\n",
       "   {'paperId': '41146983cdbd273e3d5dc7e734f53e2b847a5477',\n",
       "    'title': 'From Swimming to Walking with a Salamander Robot Driven by a Spinal Cord Model'},\n",
       "   {'paperId': '14dfedd6c8013ca4e23cb4bcfb7e1c00bddb43af',\n",
       "    'title': 'Threshold position control and the principle of minimal interaction in motor actions.'},\n",
       "   {'paperId': 'fd399c0dd19b587e1b4e76fefa854c90dde64172',\n",
       "    'title': 'The Blue Brain Project'},\n",
       "   {'paperId': '0a8149fb5aa8a5684e7d530c264451a5cb9250f5',\n",
       "    'title': 'Recent Advances in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': 'Theory Appl'},\n",
       "   {'paperId': '67275cc9674c837b1009f2d72fe3e9c4bf36394d',\n",
       "    'title': 'The development of a practical dexterous assembly robot system without the use of force sensor'},\n",
       "   {'paperId': 'dabcc82c0d437e10c9dc2365698007a26c5f2bab',\n",
       "    'title': 'High-frequency steering maneuvers mediated by tactile cues: antennal wall-following in the cockroach.'},\n",
       "   {'paperId': '6ac4b79008819fee5078a20280b493be0e936ae3',\n",
       "    'title': 'Visual control of cursorial prey pursuit by tiger beetles (Cicindelidae)'},\n",
       "   {'paperId': '09e46595df2a652e595051c18954303293147822',\n",
       "    'title': 'Does the nervous system use equilibrium-point control to guide single and multiple joint movements?'},\n",
       "   {'paperId': 'c8d90974c3f3b40fa05e322df2905fc16204aa56',\n",
       "    'title': 'Adaptive Mixtures of Local Experts'},\n",
       "   {'paperId': '32580f01d9b499f26876370b851bf0012992752a',\n",
       "    'title': 'The co-ordination and regulation of movements'},\n",
       "   {'paperId': 'a4361a4bd93e207fb4cf263a63c24ead39cc2076',\n",
       "    'title': 'Dynamic Programming'},\n",
       "   {'paperId': None,\n",
       "    'title': 'PyBullet, a Python module for physics simulation for games, robotics and machine learning'}],\n",
       "  'citations': [{'paperId': '6de25ae066cd85e1e1417f587c38afd5cbf2233a',\n",
       "    'title': 'Hierarchical Vision Navigation System for Quadruped Robots with Foothold Adaptation Learning'},\n",
       "   {'paperId': '3fe2d23b3061500bec6623c4b299aed9e258a953',\n",
       "    'title': 'Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control'},\n",
       "   {'paperId': '86abde61814c147e6b126fa35b5da381eced7d01',\n",
       "    'title': 'Learning Quadruped Locomotion using Bio-Inspired Neural Networks with Intrinsic Rhythmicity'},\n",
       "   {'paperId': '49b6b4f0a180b2bf4816cdb89ca76d4d961e2ae3',\n",
       "    'title': 'A survey on control of humanoid fall over'},\n",
       "   {'paperId': '05b9ed9302c4716a5d6800f660da927e48f82233',\n",
       "    'title': 'Roll-Drop: accounting for observation noise with a single parameter'},\n",
       "   {'paperId': '35670428cea27f63de637f6f179be3be9dbd0289',\n",
       "    'title': 'Computational Models That Use a Quantitative Structure–Activity Relationship Approach Based on Deep Learning'},\n",
       "   {'paperId': 'ab3e0702bf2244031bb16c5976fb98be108f210b',\n",
       "    'title': 'A unified modeling and trajectory planning method based on system manipulability for the operation process of the legged locomotion manipulation system'},\n",
       "   {'paperId': '85a3b0052318c5a85c112a8c48137c77b100572d',\n",
       "    'title': 'Learning hybrid locomotion skills—Learn to exploit residual actions and modulate model-based gait control'},\n",
       "   {'paperId': '055a1f9c5254235fd764db875db783532bd5706b',\n",
       "    'title': 'Learning a Single Policy for Diverse Behaviors on a Quadrupedal Robot using Scalable Motion Imitation'},\n",
       "   {'paperId': 'ebe1a1abe24f89a2943adb1ee382eb247cc1f5c3',\n",
       "    'title': 'Design a Hybrid Energy-Supply for the Electrically Driven Heavy-Duty Hexapod Vehicle'},\n",
       "   {'paperId': 'ee342958e3e659df33792d9eac5613e7c3aac713',\n",
       "    'title': 'Learning Bipedal Walking for Humanoids with Current Feedback'},\n",
       "   {'paperId': '38b1e61c6274630c8a78b0470a9636335ed7b542',\n",
       "    'title': 'Stability Control of Quadruped Robot Based on Active State Adjustment'},\n",
       "   {'paperId': '6320111cbceb891e1b32369137d355a39e03387a',\n",
       "    'title': 'Agile and Versatile Robot Locomotion via Kernel-based Residual Learning'},\n",
       "   {'paperId': '879b4debf3233fccb17aa943d283c1d0cf8232ea',\n",
       "    'title': 'NeuronsGym: A Hybrid Framework and Benchmark for Robot Tasks with Sim2Real Policy Learning'},\n",
       "   {'paperId': '25353bd484d3b0af23c8011a3e37b329e6681754',\n",
       "    'title': 'Controlling a One-Legged Robot to Clear Obstacles by Combining the SLIP Model with Air Trajectory Planning'},\n",
       "   {'paperId': 'efd6eaac7887eebb5be38f38043915809a787804',\n",
       "    'title': 'Perceptive Locomotion with Controllable Pace and Natural Gait Transitions Over Uneven Terrains'},\n",
       "   {'paperId': 'e055c25020b4c688c4946bf7d211585db3e30ba9',\n",
       "    'title': 'Unknown Slope-Oriented Research on Model Predictive Control for Quadruped Robot'},\n",
       "   {'paperId': 'c645cf02eba92f7d2064885a0093c1859aa09a32',\n",
       "    'title': 'ViTAL: Vision-Based Terrain-Aware Locomotion for Legged Robots'},\n",
       "   {'paperId': '174bf19aa934e9ba2016a1837cfe667b13a05953',\n",
       "    'title': 'Adaptive Actuation of Magnetic Soft Robots Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       "    'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors'},\n",
       "   {'paperId': 'b0e7406fa4bbbcee16ecb87cc4bea3c0584ba67e',\n",
       "    'title': 'Whole Body Collaborative Planning Method for Legged Locomotion Manipulation System in Operation Process'},\n",
       "   {'paperId': '19c915b29ebcfbe86534ccbfd20f6504d96ab3de',\n",
       "    'title': 'Sampling visual SLAM with a wide‐angle camera for legged mobile robots'},\n",
       "   {'paperId': '1111b2581d759cc90bc45ea9c3d8a1f9700bed4e',\n",
       "    'title': 'High-speed quadrupedal locomotion by imitation-relaxation reinforcement learning'},\n",
       "   {'paperId': '2d06606c92d86932a9587ef84e8858d659481c7b',\n",
       "    'title': 'An Adaptive Approach to Whole-Body Balance Control of Wheel-Bipedal Robot Ollie'},\n",
       "   {'paperId': '5aa9b72f773ea65c0d2253ca4188a6c5d7e5cae1',\n",
       "    'title': 'Hybrid Bipedal Locomotion Based on Reinforcement Learning and Heuristics'},\n",
       "   {'paperId': '8abea1b488813901470a0b2c9bb35eb13f57b407',\n",
       "    'title': 'Residual Policy Learning Facilitates Efficient Model-Free Autonomous Racing'},\n",
       "   {'paperId': 'd4d1a492c37a7d2c0dcbc626e6150ec42e1a191e',\n",
       "    'title': 'Learning Low-Frequency Motion Control for Robust and Dynamic Robot Locomotion'},\n",
       "   {'paperId': 'd9c0c3eea7a2581e434b4c9d58ce405638f7dbd5',\n",
       "    'title': 'Multi-expert synthesis for versatile locomotion and manipulation skills'},\n",
       "   {'paperId': '7aa615799a2b06c15d994ec9f85be4904b271a1c',\n",
       "    'title': 'Learning and Deploying Robust Locomotion Policies with Minimal Dynamics Randomization'},\n",
       "   {'paperId': '88d988ab69c0dc100a6d51cde62ff8384e095f8f',\n",
       "    'title': 'Efficient learning of robust quadruped bounding using pretrained neural networks'},\n",
       "   {'paperId': 'df684d5854895e73e40c16d2ad91f05c2bfbb76c',\n",
       "    'title': 'A heuristic control framework for heavy‐duty hexapod robot over complex terrain'},\n",
       "   {'paperId': '8900f198c1779ccc69b57bc954a6451a6c732e56',\n",
       "    'title': 'Real-time Digital Double Framework to Predict Collapsible Terrains for Legged Robots'},\n",
       "   {'paperId': '2123301d17d5032143fa7538495090ba1a5e57e1',\n",
       "    'title': 'GenLoco: Generalized Locomotion Controllers for Quadrupedal Robots'},\n",
       "   {'paperId': '0e3d081b1a5c2d53a415e29c038901f867e62c4b',\n",
       "    'title': 'Generating a Terrain-Robustness Benchmark for Legged Locomotion: A Prototype via Terrain Authoring and Active Learning'},\n",
       "   {'paperId': '2a02b0d8c1d2fb44ab8e4dfa60b95c1090e45b6e',\n",
       "    'title': 'Mechanism design and workspace analysis of a hexapod robot'},\n",
       "   {'paperId': 'e51f66ac9f7225baf701ea682a9796f23e53876c',\n",
       "    'title': 'Gait Generation for the Multi-legged Robot with Injured Legs'},\n",
       "   {'paperId': '577be8d620ec7bcd080d4001f2511245c58e9639',\n",
       "    'title': 'Learning fast and agile quadrupedal locomotion over complex terrain'},\n",
       "   {'paperId': '98604cd6f1e3c08ce9c947c1d3cda746e7ffddf4',\n",
       "    'title': 'Run-and-jump Planning and Control of a Compact Two-wheeled Legged Robot'},\n",
       "   {'paperId': 'fe7ed4acbb7bd02d5b03f9c91f028e8cf56fff77',\n",
       "    'title': 'A Closed-Loop Perception, Decision-Making and Reasoning Mechanism for Human-Like Navigation'},\n",
       "   {'paperId': 'b2060bb520989be53cc7419b36b0c5510525ba43',\n",
       "    'title': 'Learning Efficient and Robust Multi-Modal Quadruped Locomotion: A Hierarchical Approach'},\n",
       "   {'paperId': '96c9b1f1da5368f5f900e2091633b1139d92ecc8',\n",
       "    'title': 'Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning'},\n",
       "   {'paperId': '57d7c68e5463b9eadd57ec6a2305a9782b6400ab',\n",
       "    'title': 'Wineinformatics: Comparing and Combining SVM Models Built by Wine Reviews from Robert Parker and Wine Spectator for 95 + Point Wine Prediction'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': '247a775cf515b330e6100c48eb40d2d0c61df490',\n",
       "    'title': 'Learning Torque Control for Quadrupedal Locomotion'},\n",
       "   {'paperId': '8ed3b0f925164bfd3a9984668756143ee2a499bc',\n",
       "    'title': 'Imitation and Adaptation Based on Consistency: A Quadruped Robot Imitates Animals from Videos Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '8aef93f752e5777bde4fab109cc6a135d0093271',\n",
       "    'title': 'A Transferable Legged Mobile Manipulation Framework Based on Disturbance Predictive Control'},\n",
       "   {'paperId': '5ee839d965a1596298895ace7d003b98e165962c',\n",
       "    'title': 'Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates'},\n",
       "   {'paperId': '4d04fc236ecc18b60b9ea10626d9e337d15fc13e',\n",
       "    'title': 'Adaptive 3D descattering with a dynamic synthesis network'},\n",
       "   {'paperId': '559b6662ef199b17d093a0942278b568781729f6',\n",
       "    'title': 'Design and Dynamic Locomotion Control of Quadruped Robot with Perception-Less Terrain Adaptation'},\n",
       "   {'paperId': '5d5812c5236c5fc576ba308dd9ccbd573f5dd7ef',\n",
       "    'title': 'Concurrent Training of a Control Policy and a State Estimator for Dynamic and Robust Legged Locomotion'},\n",
       "   {'paperId': '06033eb03186179627c33bf7f5562a494a13fc80',\n",
       "    'title': 'Versatile modular neural locomotion control with fast learning'},\n",
       "   {'paperId': '5da5c2167a85ecb5d1ea22656ae36fdf995df0f2',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': '7267d12df350d88dd181199ab881590a85182bcc',\n",
       "    'title': 'Learning Free Gait Transition for Quadruped Robots Via Phase-Guided Controller'},\n",
       "   {'paperId': '8aea987f44f52473fd2b3048ecdb788ad2471329',\n",
       "    'title': 'Learning Perceptual Locomotion on Uneven Terrains Using Sparse Visual Observations'},\n",
       "   {'paperId': '78ef69a26bb02e3350bbbe0f795a40169bb728b4',\n",
       "    'title': 'Accessibility-Based Clustering for Efficient Learning of Locomotion Skills'},\n",
       "   {'paperId': 'e6c3937c8876a6ab5c01dca117a81b4f7c029fc5',\n",
       "    'title': 'Reinforcement Learning with Evolutionary Trajectory Generator: A General Approach for Quadrupedal Locomotion'},\n",
       "   {'paperId': '1a18fc6c0a4602659460b0d53ee432c9d60f1442',\n",
       "    'title': 'An Improved Dyna-Q Algorithm for Mobile Robot Path Planning in Unknown Dynamic Environment'},\n",
       "   {'paperId': '856a6642e9b228523a690bbfd34a26c0cc4ef497',\n",
       "    'title': 'Versatile modular neural locomotion control with fast learning'},\n",
       "   {'paperId': 'd071bd584b4ad29977c166a0f290b54a1c4bcd9d',\n",
       "    'title': 'Adaptive 3D descattering with a dynamic synthesis network'},\n",
       "   {'paperId': '90110e0b6bab9fca7caeeea41853c66a273094f9',\n",
       "    'title': 'GLiDE: Generalizable Quadrupedal Locomotion in Diverse Environments with a Centroidal Model'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': 'bafbb3c535d9ee0fbffaad266f732a3892f53b4e',\n",
       "    'title': 'Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review'},\n",
       "   {'paperId': 'ef91495dffa49d8b2fb94205814b242b5efb8eb6',\n",
       "    'title': 'Learning robust perceptive locomotion for quadrupedal robots in the wild'},\n",
       "   {'paperId': '2067d4822757de4d84d87c1bc8d3f156d31658cd',\n",
       "    'title': 'Bionic Control Method for a Multi-Motor Joint Based on the Physiological Muscle Model'},\n",
       "   {'paperId': '48e40436aaf6db8b8b2766c161f04e4db33ce971',\n",
       "    'title': 'Learning Motor Skills of Reactive Reaching and Grasping of Objects'},\n",
       "   {'paperId': '38c7e26a48b8699a25dd3fa3e05c800cd407ec19',\n",
       "    'title': 'Locomotion Control With Frequency and Motor Pattern Adaptations'},\n",
       "   {'paperId': '79ed87b1080dee97cf064a863e1a9cf4eece2f35',\n",
       "    'title': 'What is an artificial muscle? A comparison of soft actuators to biological muscles'},\n",
       "   {'paperId': 'd121a64dd4914168ca94636efb22168ff066baf9',\n",
       "    'title': 'Distributed Multi-Agent Learning is More Effectively than Single-Agent'},\n",
       "   {'paperId': 'dcf2f8c8766069c17e876538e8218814945a851f',\n",
       "    'title': 'What Robot do I Need? Fast Co-Adaptation of Morphology and Control using Graph Neural Networks'},\n",
       "   {'paperId': 'c88050a44081796e22283bf281689725660efebb',\n",
       "    'title': 'Where are you heading? Dynamic Trajectory Prediction with Expert Goal Examples'},\n",
       "   {'paperId': '29705f5cc06f5b7c748b0e36cade6f597648f667',\n",
       "    'title': 'Design and Implementation of Symmetric Legged Robot for Highly Dynamic Jumping and Impact Mitigation'},\n",
       "   {'paperId': '451133fccc014e8e887194cb475eb0840612f8b5',\n",
       "    'title': 'Staircase-climbing capability-based dimension design of a hexapod robot'},\n",
       "   {'paperId': '238213c56f154e1f590dc05e6186df419fc55479',\n",
       "    'title': 'Run Like a Dog: Learning Based Whole-Body Control Framework for Quadruped Gait Style Transfer'},\n",
       "   {'paperId': '8536106c8d87b7f4997f03851765e3f395345003',\n",
       "    'title': 'Meta-Learning for Fast Adaptive Locomotion with Uncertainties in Environments and Robot Dynamics'},\n",
       "   {'paperId': 'd4da158a6883d5c56824de3bbf9ec060591b7008',\n",
       "    'title': 'Learning to Navigate in a VUCA Environment: Hierarchical Multi-expert Approach'},\n",
       "   {'paperId': '1572d45b029fbb98d8ac4692a0f081e0dac29284',\n",
       "    'title': 'Accessibility-Based Clustering for Efficient Learning of Robot Fall Recovery'},\n",
       "   {'paperId': 'd6300149735fd9d02c4a88c774537034165a82ee',\n",
       "    'title': 'Learning Vision-Guided Dynamic Locomotion Over Challenging Terrains'},\n",
       "   {'paperId': '783355e8eb76f8d1f58a694853824f7827a770e0',\n",
       "    'title': 'Gait Self-learning for Damaged Robots Combining Bionic Inspiration and Deep Reinforcement Learning'},\n",
       "   {'paperId': '9f7688e9a30a5a7b4b56cddc81ded79210c2a2f7',\n",
       "    'title': 'Terrain-Perception-Free Quadrupedal Spinning Locomotion on Versatile Terrains: Modeling, Analysis, and Experimental Validation'},\n",
       "   {'paperId': 'e90caa8e6e4566daa0d257f2d768d97e4870c360',\n",
       "    'title': 'A Bio-Inspired Compliance Planning and Implementation Method for Hydraulically Actuated Quadruped Robots with Consideration of Ground Stiffness'},\n",
       "   {'paperId': '72568f836477ee3e55d0e4bf6fa98f0aa374b32f',\n",
       "    'title': 'A survey of the development of biomimetic intelligence and robotics'},\n",
       "   {'paperId': '2f2e2ac0832d1ceab3b8d09a05099db3528a18d8',\n",
       "    'title': 'Towards Hierarchical Task Decomposition using Deep Reinforcement Learning for Pick and Place Subtasks'},\n",
       "   {'paperId': 'd7094e7a652a2910a85606c4509c326d32adc0da',\n",
       "    'title': 'Trajectory Optimization of Contact-Rich Motions Using Implicit Differential Dynamic Programming'},\n",
       "   {'paperId': '67edfceea062bd52a3e43608aad8d9e8a81b35e6',\n",
       "    'title': 'Meta-Reinforcement Learning for Adaptive Motor Control in Changing Robot Dynamics and Environments'},\n",
       "   {'paperId': '2430902ddd00bd3b366c92b58924c3a5c737e57c',\n",
       "    'title': 'Biologically Inspired Planning and Optimization of Foot Trajectory of a Quadruped Robot'},\n",
       "   {'paperId': 'f8b181b5bbbfe9eccd64717ea1eb4e1289d84152',\n",
       "    'title': 'Achieving Trust in Future Human Interactions with Omnipresent AI: Some Postulates'},\n",
       "   {'paperId': '0a3645b9fcdf9ff99b4c6ef410bbd1192592e325',\n",
       "    'title': 'Efficient Learning of Control Policies for Robust Quadruped Bounding using Pretrained Neural Networks'}],\n",
       "  'citnuminlist': 1,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " '3ecaf71cf1d3596dba52497a1a88541e0e53b4d0': {'title': 'Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': 'fc1655d4ee203857fb0c839667d00288209753d9',\n",
       "    'title': 'Residual Reactive Navigation: Combining Classical and Learned Navigation Strategies For Deployment in Unknown Environments'},\n",
       "   {'paperId': '7f1a309f6057a0a1e9d7643c1b14e28915d90aaf',\n",
       "    'title': 'Combining Optimal Control and Learning for Visual Navigation in Novel Environments'},\n",
       "   {'paperId': '655bd06acc2e32af0ac8e796743f64826b856308',\n",
       "    'title': 'Visual-based Autonomous Driving Deployment from a Stochastic and Uncertainty-aware Perspective'},\n",
       "   {'paperId': 'dd54595a62a57b23f1f5ba25066618dcd044153b',\n",
       "    'title': 'Safe Reinforcement Learning With Model Uncertainty Estimates'},\n",
       "   {'paperId': 'c78c8a921a4171b373e7a298d2803940c935ee34',\n",
       "    'title': 'Policies Modulating Trajectory Generators'},\n",
       "   {'paperId': '1a1a22a3281e00d4ffd69b6490abc266d4eed0b3',\n",
       "    'title': 'Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning'},\n",
       "   {'paperId': '6654ba1d3e61cdf5f4decc7464436046cf602ed1',\n",
       "    'title': 'On Evaluation of Embodied Navigation Agents'},\n",
       "   {'paperId': '56136aa0b2c347cbcf3d50821f310c4253155026',\n",
       "    'title': 'Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models'},\n",
       "   {'paperId': 'c37e0d93d19efbd8bb50d1a4d92979793f4341d2',\n",
       "    'title': 'Reinforcement Learning from Imperfect Demonstrations'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': 'e3b0ea7209731c47b582215c6c67f9c691ad9863',\n",
       "    'title': 'Deep Q-learning From Demonstrations'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': '799c0e461332570ecde97e13266fecde8476efe3',\n",
       "    'title': 'Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation'},\n",
       "   {'paperId': 'f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0',\n",
       "    'title': 'Uncertainty-Aware Reinforcement Learning for Collision Avoidance'},\n",
       "   {'paperId': 'aa0b2517c1555fc5b3885723959f7ac950ba1626',\n",
       "    'title': 'From perception to decision: A data-driven approach to end-to-end motion planning for autonomous ground robots'},\n",
       "   {'paperId': '7af7f2f539cd3479faae4c66bbef49b0f66202fa',\n",
       "    'title': 'Target-driven visual navigation in indoor scenes using deep reinforcement learning'},\n",
       "   {'paperId': 'e3672e19c47853ff95f0a0c05b362eddda6e6a0b',\n",
       "    'title': 'Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age'},\n",
       "   {'paperId': '98773f086ff15e13c6046d91d1ffaf067454f367',\n",
       "    'title': 'Deep Neural Network for Real-Time Autonomous Indoor Navigation'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '0f0d11429e5aaecbc9fce8445afaa3bad7a74888',\n",
       "    'title': 'Learning monocular reactive UAV control in cluttered natural environments'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': 'eb94f71a5cf0ea4116089e5d147c6a1e986a18c7',\n",
       "    'title': 'Performance Comparison of Bug Navigation Algorithms'},\n",
       "   {'paperId': '701f12a601a6f4059005af3dbe41163531bc793c',\n",
       "    'title': 'Extending the potential fields approach to avoid trapping situations'},\n",
       "   {'paperId': '1cf496e7db9712bfd6f9373b161cbc359031f568',\n",
       "    'title': 'A potential field approach to path planning'},\n",
       "   {'paperId': '07cc4371c8d3ddf7de0189f73227c3f0d896d66d',\n",
       "    'title': 'Potential field methods and their inherent limitations for mobile robot navigation'},\n",
       "   {'paperId': '529cf7a716e6c9da99c6a468730f22398f75c1a4',\n",
       "    'title': 'The vector field histogram-fast obstacle avoidance for mobile robots'},\n",
       "   {'paperId': 'f35cd5246e0ca7bc493c91ae61a78d1589670367',\n",
       "    'title': 'Global path planning using artificial potential fields'},\n",
       "   {'paperId': 'c8a04d0cbb9f70e86800b11b594c9a05d7b6bac0',\n",
       "    'title': 'Real-Time Obstacle Avoidance for Manipulators and Mobile Robots'}],\n",
       "  'citations': [{'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'b34e47bde3f49b42bf08956f9f323324c26cf5a7',\n",
       "    'title': 'Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning'},\n",
       "   {'paperId': '947070ff65dc9a0b0024d299acdcfa8251b5118b',\n",
       "    'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors'},\n",
       "   {'paperId': '601f6617bb5f3270cbfb1bd0ed1190147ba78dcc',\n",
       "    'title': 'IPAPRec: A Promising Tool for Learning High-Performance Mapless Navigation Skills With Deep Reinforcement Learning'},\n",
       "   {'paperId': '66ffe137060531c013d0cee1506b1cf294da9a4c',\n",
       "    'title': 'Overleaf Example'}],\n",
       "  'citnuminlist': 3,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac': {'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning',\n",
       "  'year': 2020,\n",
       "  'references': [{'paperId': '0655bd7f2722b9ffe69613ac2c11bcb9b5eb9aa3',\n",
       "    'title': 'Verifiably safe exploration for end-to-end reinforcement learning'},\n",
       "   {'paperId': '39a16b5d8421a2eca2ba2fa1b76029d5bcacd165',\n",
       "    'title': 'Data-efficient visuomotor policy training using reinforcement learning and generative models'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': '361e953f792a585496834ee14216b94d0ce9ae74',\n",
       "    'title': 'VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning'},\n",
       "   {'paperId': '405d2cc9cccc035b4c8d950d6cfab4cc1a5d0628',\n",
       "    'title': 'Meta-Q-Learning'},\n",
       "   {'paperId': '00753de4e5553de8a569e951f531bd683d8dcb16',\n",
       "    'title': 'Watch, Try, Learn: Meta-Learning from Demonstrations and Reward'},\n",
       "   {'paperId': '9b114351bf97a66ab8cfef4c3b04a0f70503295e',\n",
       "    'title': 'Deep Imitative Models for Flexible Inference, Planning, and Control'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'ccd0f4addd6bd4340b7de2c2ecc419af0fc9daac',\n",
       "    'title': 'Continuous Relaxation of Symbolic Planner for One-Shot Imitation Learning'},\n",
       "   {'paperId': '82b4b03a4659d6e04bd7cbf51d6e08fde1348dbd',\n",
       "    'title': 'Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'ec1204806b1e189587fb8d15edd7cafea6e32e36',\n",
       "    'title': 'Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual Foresight'},\n",
       "   {'paperId': '65b072f590433f2c09aa2a8d566bcaa9c3b5b9a0',\n",
       "    'title': 'Guided Meta-Policy Search'},\n",
       "   {'paperId': '4625628163a2ee0e6cd320cd7a14b4ccded2a631',\n",
       "    'title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables'},\n",
       "   {'paperId': '09fde98a2e7fe70ea6394b5c5bda3fb8638e92a7',\n",
       "    'title': 'Affordance Learning for End-to-End Visuomotor Robot Control'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '4c54efd3b9ce84f4268b165f5896c0692a50bcd1',\n",
       "    'title': 'Learning Action Representations for Reinforcement Learning'},\n",
       "   {'paperId': 'ae4d32f05cf40e4cc01c69d7787149a258c95eda',\n",
       "    'title': 'Residual Reinforcement Learning for Robot Control'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '25e433197844c239742f67fbb4171e913e0b9fe2',\n",
       "    'title': 'Analyzing Inverse Problems with Invertible Neural Networks'},\n",
       "   {'paperId': '5df5561b55ab872f2b3df559ddd475299f660b42',\n",
       "    'title': 'Neural Task Graphs: Generalizing to Unseen Tasks From a Single Video Demonstration'},\n",
       "   {'paperId': 'df2b0e26d0599ce3e70df8a9da02e51594e0e992',\n",
       "    'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '7d85e83ae00f2d19b3cdb01f5feeb92a2102104f',\n",
       "    'title': 'Task-Embedded Control Networks for Few-Shot Imitation Learning'},\n",
       "   {'paperId': '776f3d2250285ac03b2019ecf18668fcdd72a9ce',\n",
       "    'title': 'One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'f4eff7c0127a2ef92c441f028c3bb15b64cabcc8',\n",
       "    'title': 'One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning'},\n",
       "   {'paperId': 'b864f89eaa91120e04e8c62eb0b36568ab4244a8',\n",
       "    'title': 'Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': 'f466157848d1a7772fb6d02cdac9a7a5e7ef982e',\n",
       "    'title': 'Neural Discrete Representation Learning'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': '482c0cbfffa77154e3c879c497f50b605297d5bc',\n",
       "    'title': 'One-Shot Visual Imitation Learning via Meta-Learning'},\n",
       "   {'paperId': '1bead9000a719cb258bac7320228055aee650d2c',\n",
       "    'title': 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards'},\n",
       "   {'paperId': 'a2141a5ec0c65ea0a9861ae562f4c9fb8020d197',\n",
       "    'title': 'DARLA: Improving Zero-Shot Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '00357a417ce470a78f7a84d18ae2604330455d2a',\n",
       "    'title': 'Meta-Learning with Temporal Convolutions'},\n",
       "   {'paperId': 'a7fb199f85943b3fb6b5f7e9f1680b2e2a445cce',\n",
       "    'title': 'Learning from Demonstrations for Real World Reinforcement Learning'},\n",
       "   {'paperId': '4135004c75a361c91311314fc588d229a7107526',\n",
       "    'title': 'InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '5c57bb5630835a05eb1c3d0df3e12d6180d75de2',\n",
       "    'title': 'One-Shot Imitation Learning'},\n",
       "   {'paperId': 'c889d6f98e6d79b89c3a6adf8a921f88fa6ba518',\n",
       "    'title': 'Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks'},\n",
       "   {'paperId': '72a3da7491ebc09e319167dba4b2cdb1d285bcdc',\n",
       "    'title': 'Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction'},\n",
       "   {'paperId': 'dcdb8fab7b127eea0198b0622dda05557c8b4262',\n",
       "    'title': 'Deep predictive policy training using reinforcement learning'},\n",
       "   {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "    'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       "   {'paperId': '282a380fb5ac26d99667224cef8c630f6882704f',\n",
       "    'title': 'Learning to reinforcement learn'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Fakoor et al., 2020) and meta-imitation methods (Duan et al., 2017'},\n",
       "   {'paperId': '5ce030f1650145a103527e883e7a9d9a25c45547',\n",
       "    'title': 'A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots'},\n",
       "   {'paperId': '954b01151ff13aef416d27adc60cd9a076753b1a',\n",
       "    'title': 'RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '04162cb8cfaa0f7e37586823ff4ad0bff09ed21d',\n",
       "    'title': 'Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Pybullet, a python module for physics simulation for games, robotics and machine learning'},\n",
       "   {'paperId': '9b686d76914befea66377ec79c1f9258d70ea7e3',\n",
       "    'title': 'ShapeNet: An Information-Rich 3D Model Repository'},\n",
       "   {'paperId': '4d376d6978dad0374edfa6709c9556b42d3594d3',\n",
       "    'title': 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'b70dd1fccbba3466f22b592ca090836513ebf494',\n",
       "    'title': 'Data-Efficient Generalization of Robot Skills with Contextual Policy Search'},\n",
       "   {'paperId': '27f0504bb0ddb10249aeee4b0216cdb6f2a51dd8',\n",
       "    'title': 'Robot motor skill coordination with EM-based Reinforcement Learning'},\n",
       "   {'paperId': '220ad0189a4c9ee5b5c299f269a0e4bef290e8fd',\n",
       "    'title': 'Learning and generalization of motor skills by learning from demonstration'},\n",
       "   {'paperId': 'bcdace37da7ffad8d509991d64a8977882e766ce',\n",
       "    'title': 'Imitation learning for locomotion and manipulation'},\n",
       "   {'paperId': 'b9dfc5c3ceac9b2b2b74505517a3a3efaa864859',\n",
       "    'title': 'Policy Gradient Methods for Robotics'},\n",
       "   {'paperId': 'dc649486b881e672eea6546da48c46e1f98daf32',\n",
       "    'title': 'Near-Optimal Reinforcement Learning in Polynomial Time'},\n",
       "   {'paperId': '651bb2de1d6c1e938878cd8fcf042ccf559e0a99',\n",
       "    'title': 'Computational approaches to motor learning by imitation.'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '9aa1d909544fd9ffe061b84a90eb344ac303e6d9',\n",
       "    'title': 'The MAXQ Method for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': 'a9da09d1e63686706d64782e654d69f13fd292ad',\n",
       "    'title': 'Learning by Demonstration'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Related Work Combining RL with demonstrations. Our work is related to methods for learning from demonstrations (Pomerleau'},\n",
       "   {'paperId': '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3',\n",
       "    'title': 'ALVINN: An Autonomous Land Vehicle in a Neural Network'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Multimodal imitation learning from unstructured demonstrations using generative adversarial nets'}],\n",
       "  'citations': [{'paperId': 'd11ae7f22045a2217fb2ef169037fba216153c63',\n",
       "    'title': 'Stabilizing Contrastive RL: Techniques for Offline Goal Reaching'},\n",
       "   {'paperId': '1cdc6d284ea9bc7019490428949c619b26504f82',\n",
       "    'title': 'Beyond Reward: Offline Preference-guided Policy Optimization'},\n",
       "   {'paperId': 'fe9fe9f15f24fbbb19b62bcd9a3418511a699b84',\n",
       "    'title': 'Policy Representation via Diffusion Probability Model for Reinforcement Learning'},\n",
       "   {'paperId': '8fce3142bc144bdc08bf0cab1db908c7ad3f8454',\n",
       "    'title': 'Contrastive Language, Action, and State Pre-training for Robot Learning'},\n",
       "   {'paperId': '21d5d3aa466577ef6aac75d428ada4fdbf1cf035',\n",
       "    'title': 'Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets'},\n",
       "   {'paperId': '1334a47e8f4e4ffd04ff534329d76a5e5cc16f46',\n",
       "    'title': 'Goal-Conditioned Imitation Learning using Score-based Diffusion Policies'},\n",
       "   {'paperId': '311d53dcb0bfe50017959323393dd88ea2b451d8',\n",
       "    'title': 'Online augmentation of learned grasp sequence policies for more adaptable and data-efficient in-hand manipulation'},\n",
       "   {'paperId': 'e1bd151a3f670fd0f77580702fe7a85dc78a41cb',\n",
       "    'title': 'Chain-of-Thought Predictive Control'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': '2ebd5df74980a37370b0bcdf16deff958289c041',\n",
       "    'title': 'Foundation Models for Decision Making: Problems, Methods, and Opportunities'},\n",
       "   {'paperId': '53cf80e5eadc8e4b3df358ce856cf14cb71efc18',\n",
       "    'title': 'Accelerating Policy Gradient by Estimating Value Function from Prior Computation in Deep Reinforcement Learning'},\n",
       "   {'paperId': '7f270c9b098727675c9d8b893e362b561d61f27e',\n",
       "    'title': 'Policy Expansion for Bridging Offline-to-Online Reinforcement Learning'},\n",
       "   {'paperId': 'b43330013a5abcccd366d71f2f66c493c790abc6',\n",
       "    'title': 'Imitating Human Behaviour with Diffusion Models'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': 'da592b4751859dd5d82bc833fecce00cb8b4315e',\n",
       "    'title': 'Robot programming by demonstration with a monocular RGB camera'},\n",
       "   {'paperId': 'db8d70d9da6b957a00ec7e8cc67493340c39aa29',\n",
       "    'title': 'Policy Transfer via Skill Adaptation and Composition'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '9d1445f1845a2880ff9c752845660e9c294aa7b5',\n",
       "    'title': 'Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery'},\n",
       "   {'paperId': '8745c5b9522c11818418f64fdc880894faeaed16',\n",
       "    'title': 'A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '08640a3d174ef8a1c5fca2de3bd4969e592daca2',\n",
       "    'title': 'Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows'},\n",
       "   {'paperId': '160821fa1cfd66f90d145f144e0b81e3e62fbcb5',\n",
       "    'title': 'On the Effect of Pre-training for Transformer in Different Modality on Offline Reinforcement Learning'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '33777d1de46f275aa0911954abc483c249eda9d1',\n",
       "    'title': 'Learning on the Job: Self-Rewarding Offline-to-Online Finetuning for Industrial Insertion of Novel Connectors from Vision'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': '9b5f4aab169fba588e214c010345232053f8ae76',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0f1402c536cc3cbbcb73b06f96289e50a34ca3cf',\n",
       "    'title': 'Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling'},\n",
       "   {'paperId': 'b9a3fb062ac940012f69e9cca5f34bb717370c3a',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '951da1a5a0c2bfb6f2d16dccfc488acb71f1447f',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Data for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': '935464b62317df2a96a3cebd873d66f5320208e6',\n",
       "    'title': 'Sampling Through the Lens of Sequential Decision Making'},\n",
       "   {'paperId': 'c2366e759a97c2b21e9fc35c5e2f6b377eca38ab',\n",
       "    'title': 'Transformers are Adaptable Task Planners'},\n",
       "   {'paperId': '01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8',\n",
       "    'title': 'Behavior Transformers: Cloning k modes with one stone'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '04615a9955bce148aa7ba29e864389c26e10523a',\n",
       "    'title': 'DEP-RL: Embodied Exploration for Reinforcement Learning in Overactuated and Musculoskeletal Systems'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '7e781f77ba346662dcac2c7c1e976ed431a602c8',\n",
       "    'title': 'Efficient Reinforcement Learning from Demonstration Using Local Ensemble and Reparameterization with Split and Merge of Expert Policies'},\n",
       "   {'paperId': '9c5f056c4e7986064722bb522e46e3546be8da51',\n",
       "    'title': 'A Review of Safe Reinforcement Learning: Methods, Theory and Applications'},\n",
       "   {'paperId': '1f668eeb2579c4403fb43ff3ec08671f736dedef',\n",
       "    'title': 'Simultaneous Double Q-learning with Conservative Advantage Learning for Actor-Critic Methods'},\n",
       "   {'paperId': '70caab1d6cd0d8910f79d6635fbb7edbfeb1b45a',\n",
       "    'title': 'Training and Evaluation of Deep Policies using Reinforcement Learning and Generative Models'},\n",
       "   {'paperId': '32e6c81eeecea06d102f2dc8edcf8b5018ba1a80',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline RL'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '7206d5e465a889cea6aac8593f447d273f2d8efa',\n",
       "    'title': 'Towards Predicting Fine Finger Motions from Ultrasound Images via Kinematic Representation'},\n",
       "   {'paperId': '266cc7ff4856b6a2ce9cc0a3e5f6c155ecc448a2',\n",
       "    'title': 'Can Wikipedia Help Offline Reinforcement Learning?'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "    'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models'},\n",
       "   {'paperId': '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "    'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans'},\n",
       "   {'paperId': '01bc9970cdceafe7a81fcd9acc95b82d6666c502',\n",
       "    'title': 'Learning Neuro-Symbolic Relational Transition Models for Bilevel Planning'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': '99dff0f1591050f09fd471692b9b62a9b744508c',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning'},\n",
       "   {'paperId': '04d6ac9efe844244886e12736112ab26d3925a98',\n",
       "    'title': 'Learning Pseudometric-based Action Representations for Offline Reinforcement Learning'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': '6c1a13f1479227f921a20e92671407b195159dee',\n",
       "    'title': 'Conservative Data Sharing for Multi-Task Offline Reinforcement Learning'},\n",
       "   {'paperId': 'f70e82b8b7c792a3cdbf2c9bf2e7af06fd6a7269',\n",
       "    'title': 'Skill Preferences: Learning to Extract and Execute Robotic Skills from Human Feedback'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '75f425319694047f763f02f2a07912cd5621cfa4',\n",
       "    'title': 'Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research'},\n",
       "   {'paperId': '556d3f6d15acc3b37d102ccda65c4ee8f9eeb738',\n",
       "    'title': 'Optimality Inductive Biases and Agnostic Guidelines for Offline Reinforcement Learning'},\n",
       "   {'paperId': '33b456eb43e5391761540f17a29e598d7595565b',\n",
       "    'title': 'Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble'},\n",
       "   {'paperId': 'f71da178cd63958fe659ad613d474b67c5615bd3',\n",
       "    'title': 'Behavioral Priors and Dynamics Models: Improving Performance and Domain Transfer in Offline RL'},\n",
       "   {'paperId': 'b947a2fdbe9193c15a5b2b17e8a54bb348deafb4',\n",
       "    'title': 'Discovering Multi-Agent Auto-Curricula in Two-Player Zero-Sum Games'},\n",
       "   {'paperId': 'f10fa70d2757d8716565a006360158df0a311c05',\n",
       "    'title': 'Neural Auto-Curricula'},\n",
       "   {'paperId': 'c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500',\n",
       "    'title': 'Decision Transformer: Reinforcement Learning via Sequence Modeling'},\n",
       "   {'paperId': '4f9f09d1ab684b145627f5cbad5560f364e51559',\n",
       "    'title': 'Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning'},\n",
       "   {'paperId': '1e5c44c9f91ad6b1acbbdfe0c3c7f4e5200db39e',\n",
       "    'title': 'Contingencies from Observations: Tractable Contingency Planning with Learned Behavior Models'},\n",
       "   {'paperId': '7d79bac55be79502ab70e8fe46e92e649a6670da',\n",
       "    'title': 'Representation Matters: Offline Pretraining for Sequential Decision Making'},\n",
       "   {'paperId': '5d0143965645f2dfdc4b217560071f355957f2f4',\n",
       "    'title': 'Action Priors for Large Action Spaces in Robotics'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '59b934124c2bf4355dc2692898564428d5e5e13a',\n",
       "    'title': 'G ENERALISATION IN L IFELONG R EINFORCEMENT L EARNING THROUGH L OGICAL C OMPOSITION'},\n",
       "   {'paperId': 'a22a285d13806d456d3181995013fc70e90dc143',\n",
       "    'title': 'The Reﬂective Explorer: Online Meta-Exploration from Ofﬂine Data in Realistic Robotic Tasks'},\n",
       "   {'paperId': '70ed12638e3ac76597dfcf268e831798d697d8ab',\n",
       "    'title': 'Multi-Task Offline Reinforcement Learning with Conservative Data Sharing'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'}],\n",
       "  'citnuminlist': 11,\n",
       "  'refnuminlist': 11,\n",
       "  'isKeypaper': True},\n",
       " 'b43d8c8b25bc65cbf3097480e9000649c79b7a51': {'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information',\n",
       "  'year': 2018,\n",
       "  'references': [{'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '4135004c75a361c91311314fc588d229a7107526',\n",
       "    'title': 'InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "    'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': '35da0a2001eea88486a5de677ab97868c93d0824',\n",
       "    'title': 'InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets'},\n",
       "   {'paperId': '4ab53de69372ec2cd2d90c126b6a100165dc8ed1',\n",
       "    'title': 'Generative Adversarial Imitation Learning'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': '694d284d7c63c7f29c48b540bc4ece9f63b79160',\n",
       "    'title': 'Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning'},\n",
       "   {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "    'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': '54e325aee6b2d476bbbb88615ac15e251c6e8214',\n",
       "    'title': 'Generative Adversarial Nets'},\n",
       "   {'paperId': 'f6dc3727048d47b0ec034af7270fb7efa36cced7',\n",
       "    'title': 'Learning to predict phases of manipulation tasks as hidden states'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': '21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8',\n",
       "    'title': 'Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery'},\n",
       "   {'paperId': '215838674eb124c4482ba89dbda94293ae3bfae6',\n",
       "    'title': 'Expectation Maximization Algorithm'},\n",
       "   {'paperId': '96494e722f58705fa20302fe6179d483f52705b4',\n",
       "    'title': 'Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks'},\n",
       "   {'paperId': '1c59bfa0e8654ebea94277064f82062875cae8b6',\n",
       "    'title': 'Identifying useful subgoals in reinforcement learning by local graph partitioning'},\n",
       "   {'paperId': 'dca9444e1c69eee36c0be04703d71114a762c84a',\n",
       "    'title': 'Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning'},\n",
       "   {'paperId': 'a819d8b5763feb8db6f179ad87a2f6e4793fd5d6',\n",
       "    'title': 'Accelerating Reinforcement Learning through the Discovery of Useful Subgoals'},\n",
       "   {'paperId': '390ec126ebc0f7f2719e9b2598decc58294b4350',\n",
       "    'title': 'Intra-Option Learning about Temporally Abstract Actions'},\n",
       "   {'paperId': '32c28efca0ccfeeed03d0283e00e3acc05675d19',\n",
       "    'title': 'Directed information for channels with feedback'},\n",
       "   {'paperId': '557668619327081a6b77aa5b181fa84722a875a4',\n",
       "    'title': 'CAUSALITY, FEEDBACK AND DIRECTED INFORMATION'},\n",
       "   {'paperId': '7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3',\n",
       "    'title': 'ALVINN: An Autonomous Land Vehicle in a Neural Network'}],\n",
       "  'citations': [{'paperId': '85bc55ba9ab93c09713b0891bbcf0541b8f27ea9',\n",
       "    'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': '57611c1415d5d3b37ed2e244fc4df7ba591014e6',\n",
       "    'title': 'Interpretable Skill Learning for Dynamic Treatment Regimes through Imitation'},\n",
       "   {'paperId': '1c6fbf5c76aee77b539dc3f50991d7ac6c8356e8',\n",
       "    'title': 'Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives'},\n",
       "   {'paperId': '5a31c0706b7bb60487c0bbbf7cd30dc2bf92c2cb',\n",
       "    'title': 'Automated Task-Time Interventions to Improve Teamwork using Imitation Learning'},\n",
       "   {'paperId': '53d661d536965daf4ff6dcfd3b7e42ffa9061d78',\n",
       "    'title': 'Diverse Policy Optimization for Structured Action Space'},\n",
       "   {'paperId': '329d809a69eada0521da1b536bb9db9348d51868',\n",
       "    'title': 'Hierarchical Imitation Learning with Vector Quantized Models'},\n",
       "   {'paperId': 'b0092b450aaa3e5c15f0fc9949341f332759d418',\n",
       "    'title': 'Learning From Guided Play: Improving Exploration for Adversarial Imitation Learning With Simple Auxiliary Tasks'},\n",
       "   {'paperId': '6ace8342b5bb52e5db1bc7f6bc42b4c4c4d7b938',\n",
       "    'title': 'A Context-based Multi-task Hierarchical Inverse Reinforcement Learning Algorithm'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8',\n",
       "    'title': 'Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '381af7aa06e245b72d20d5cbcbed7438dbf62de9',\n",
       "    'title': 'Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control'},\n",
       "   {'paperId': 'b76d90ed06850af3833325aeea18609bd740536e',\n",
       "    'title': 'An Interrelated Imitation Learning Method for Heterogeneous Drone Swarm Coordination'},\n",
       "   {'paperId': '19af31fee863f254a16185e00b5fd6375445a9e1',\n",
       "    'title': 'Learning Multi-Task Transferable Rewards via Variational Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'e6b4865b24110a07bc5877727ab6971802476561',\n",
       "    'title': 'Data-driven prediction of Air Traffic Controllers reactions to resolving conflicts'},\n",
       "   {'paperId': '0422e037c7d47e1b123b6d6c1eb6334931f2086c',\n",
       "    'title': 'SHAIL: Safety-Aware Hierarchical Adversarial Imitation Learning for Autonomous Driving in Urban Environments'},\n",
       "   {'paperId': '1f9a4883a768157f5a98921c6c0f4126e0280759',\n",
       "    'title': 'Hierarchical Imitation Learning via Subgoal Representation Learning for Dynamic Treatment Recommendation'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '9e912ed660e30cd41541ada3e2b47c03b28040e7',\n",
       "    'title': 'MORAL: Aligning AI with Human Norms through Multi-Objective Reinforced Active Learning'},\n",
       "   {'paperId': '9b969e22973b4d3ead02d69a678c5dfd2e83b560',\n",
       "    'title': 'Inverse Contextual Bandits: Learning How Behavior Evolves over Time'},\n",
       "   {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "    'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       "   {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "    'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       "   {'paperId': '9d2038e233042790929123ff62354eb18ee52e47',\n",
       "    'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': 'acff9c4e2ad66dd785f75cf91dd0aa442c6cae14',\n",
       "    'title': 'Adversarial Option-Aware Hierarchical Imitation Learning'},\n",
       "   {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "    'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '886af81a035a0e3ef9c35181d2ed14e9f2c7d4be',\n",
       "    'title': 'INTERPRETABLE POLICY LEARNING'},\n",
       "   {'paperId': '0af794dfb8af1930bdca475be03e45c42db92a23',\n",
       "    'title': 'Drone Formation Control via Belief-Correlated Imitation Learning'},\n",
       "   {'paperId': 'd9ceb68a016be1dcd8d246497d7d964ed4b22751',\n",
       "    'title': 'Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation'},\n",
       "   {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "    'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       "   {'paperId': 'a75dace80c238148d8ca23279a6453a32aa83535',\n",
       "    'title': 'Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '20f3973efecab0ee66adcb07dc33b7cdcfbfecf7',\n",
       "    'title': 'Modeling Long-horizon Tasks as Sequential Interaction Landscapes'},\n",
       "   {'paperId': '45d7a12b198bdd74d91b8a69bc88b1d9bec2091f',\n",
       "    'title': 'Learning Situational Driving'},\n",
       "   {'paperId': '0d4936466857c43df99b4831789f3570b8c8a46f',\n",
       "    'title': 'oIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions'},\n",
       "   {'paperId': 'a71b917c48e892a2ee410870bbb2da488519e9aa',\n",
       "    'title': 'Domain-Adversarial and -Conditional State Space Model for Imitation Learning'},\n",
       "   {'paperId': '9d999574ad0eab7a70f96e7bdfc61648cf3ffc16',\n",
       "    'title': 'PODNet: A Neural Network for Discovery of Plannable Options'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'ac0f9a108a55781296f558efdd990ed199ca931c',\n",
       "    'title': 'Information Theoretic Causal Effect Quantification'},\n",
       "   {'paperId': '13b00c6c8e6fd35a540b08904824aff0d6b66897',\n",
       "    'title': 'Meta-Inverse Reinforcement Learning with Probabilistic Context Variables'},\n",
       "   {'paperId': 'fac2dddeba492c3dd440f051fde7f2756645bc2f',\n",
       "    'title': 'Mature GAIL: Imitation Learning for Low-level and High-dimensional Input using Global Encoder and Cost Transformation'},\n",
       "   {'paperId': '19248c1f25551937423074404d87ac06c7448ca1',\n",
       "    'title': 'GRP Model for Sensorimotor Learning'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'e3f719389c45969e2e19066c7b34faf4a93b1176',\n",
       "    'title': 'Jointly Forecasting and Controlling Behavior by Learning from High-Dimensional Data'},\n",
       "   {'paperId': 'cf75b24fe14a25b3093a6e72b534e36de22303ac',\n",
       "    'title': 'Multi-Modal Imitation Learning in Partially Observable Environments'}],\n",
       "  'citnuminlist': 6,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " 'ffb3886a253ff927bcc46b78e00409893865a68e': {'title': 'Dynamics-Aware Unsupervised Discovery of Skills',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': 'cae23343d2efddca3592b08a521a896af5098248',\n",
       "    'title': 'Recurrent World Models Facilitate Policy Evolution'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': 'af988ef489f25f42ea12dfdb4d8738eaff21f6d6',\n",
       "    'title': 'TherML: Thermodynamics of Machine Learning'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': '56136aa0b2c347cbcf3d50821f310c4253155026',\n",
       "    'title': 'Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba',\n",
       "    'title': 'Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review'},\n",
       "   {'paperId': '5c55162deeea9870f765e0a9cd3e8387b05a0ea2',\n",
       "    'title': 'Disentangling the independently controllable factors of variation by interacting with the world'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '27dfecb6bb0308c7484e13dcaefd5eeebba677d3',\n",
       "    'title': 'Model-Ensemble Trust-Region Policy Optimization'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': 'cce22bf6405042a965a86557684c46a441f2a736',\n",
       "    'title': 'Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning'},\n",
       "   {'paperId': '09cd5d3634d84d528d16c4bcc89c6fd5b883c7cc',\n",
       "    'title': 'Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control'},\n",
       "   {'paperId': None,\n",
       "    'title': 'TF-Agents: A library for reinforcement learning in tensorflow'},\n",
       "   {'paperId': '9917363277c783a01bff32af1c27fc9b373ad55d',\n",
       "    'title': 'DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '225ab689f41cef1dc18237ef5dab059a49950abf',\n",
       "    'title': 'Curiosity-Driven Exploration by Self-Supervised Prediction'},\n",
       "   {'paperId': '360cf15dcba643b04f9028bca25396b3beb73f2d',\n",
       "    'title': 'Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '850d78496304829d16d14701e4d81692f088f47d',\n",
       "    'title': 'EX2: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c',\n",
       "    'title': 'Deep Variational Information Bottleneck'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '0fcb2034e31a2bc2f12a2b1363d0d77baf445fdf',\n",
       "    'title': '#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e37b999f0c96d7136db07b0185b837d5decd599a',\n",
       "    'title': 'Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None,\n",
       "    'title': '2017) for discrete action skills, while we use SAC for continuous skills'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '6e90fd78e8a3b98af3954aae5209703aa966603e',\n",
       "    'title': 'Unifying Count-Based Exploration and Intrinsic Motivation'},\n",
       "   {'paperId': 'ff7f3277c6fa759e84e1ab7664efdac1c1cec76b',\n",
       "    'title': 'OpenAI Gym'},\n",
       "   {'paperId': 'fb3c6456708b0e143f545d77dc8ec804eb947395',\n",
       "    'title': 'Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks'},\n",
       "   {'paperId': '528a8ef7277d81d337c8b4c4fe0a9df483031773',\n",
       "    'title': 'Aggressive driving with model predictive path integral control'},\n",
       "   {'paperId': '4c09757f31f66e483c61266a458f5aaaf8723895',\n",
       "    'title': 'Optimal control with learned local models: Application to dexterous manipulation'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '62e11a7ad4096521b8ba1a30c1994648197330d1',\n",
       "    'title': 'One-shot learning of manipulation skills with online dynamics adaptation and neural network priors'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': '127d856c8b74d3e54a2f7da7b11b784014832ed9',\n",
       "    'title': 'Improving PILCO with Bayesian Neural Network Dynamics Models'},\n",
       "   {'paperId': None, 'title': 'URL http://arxiv.org/ abs/1606.01540'},\n",
       "   {'paperId': 'ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060',\n",
       "    'title': 'Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning'},\n",
       "   {'paperId': 'e4257bc131c36504a04382290cbc27ca8bb27813',\n",
       "    'title': 'Action-Conditional Video Prediction using Deep Networks in Atari Games'},\n",
       "   {'paperId': 'e89d656a39fc3b08af47ebb9a583e182a596dabe',\n",
       "    'title': 'DeepMPC: Learning Deep Latent Features for Model Predictive Control'},\n",
       "   {'paperId': '2470fcf0f89082de874ac9133ccb3a8667dd89a8',\n",
       "    'title': 'Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models'},\n",
       "   {'paperId': 'bb1a17010254abfa5e1f2a17553582ce449f8e16',\n",
       "    'title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': '718577588f38727ff28cf5321a5772a6fcdc1865',\n",
       "    'title': 'Gaussian Processes for Data-Efficient Learning in Robotics and Control'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': None,\n",
       "    'title': 'TensorFlow: Large-scale machine learning on heterogeneous systems'},\n",
       "   {'paperId': '2319a491378867c7049b3da055c5df60e1671158',\n",
       "    'title': 'Playing Atari with Deep Reinforcement Learning'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '60b7d47758a71978e74edff6dd8dea4d9c791d7a',\n",
       "    'title': 'PILCO: A Model-Based and Data-Efficient Approach to Policy Search'},\n",
       "   {'paperId': '33224ad0cdf6e2dc4893194dd587309c7887f0ba',\n",
       "    'title': 'Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)'},\n",
       "   {'paperId': '82266f6103bade9005ec555ed06ba20b5210ff22',\n",
       "    'title': 'Gaussian Processes for Machine Learning'},\n",
       "   {'paperId': '11b6bdfe36c48b11367b27187da11d95892f0361',\n",
       "    'title': 'Maximum Entropy Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'e8077729ef723d71a0b7492f2f271ae413b5a4d5',\n",
       "    'title': 'What is Intrinsic Motivation? A Typology of Computational Approaches'},\n",
       "   {'paperId': 'c4673454332a692b259840a3ef80fc51557ac85b',\n",
       "    'title': 'Gaussian Processes and Reinforcement Learning for Identification and Control of an Autonomous Blimp'},\n",
       "   {'paperId': 'c9ef00e1ce135cf6ae566c58e80a8a9f9f73e3cc',\n",
       "    'title': 'Intrinsic Motivation Systems for Autonomous Mental Development'},\n",
       "   {'paperId': 'e76e9bd14eef5cb8d7582f186d6dee97e8b4cf6d',\n",
       "    'title': 'Multivariate Information Bottleneck'},\n",
       "   {'paperId': '6cc0699a4a99132b7aecddb7f2e37d731e36bb43',\n",
       "    'title': 'Empowerment: a universal agent-centric measure of control'},\n",
       "   {'paperId': 'c233ffab4638e3cd3a2ba3a62dc844b488b38731',\n",
       "    'title': 'Estimating mutual information and multi-information in large networks'},\n",
       "   {'paperId': '8e42a85570d5d9e25656824c9b2a6b759425661a',\n",
       "    'title': 'Gaussian process model based predictive control'},\n",
       "   {'paperId': '48230ed0c3fa53ef1d43d79e1f6b113f13e83b9b',\n",
       "    'title': 'Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems'},\n",
       "   {'paperId': 'aae4efb3d412d585ea0dec03f933397c93caf989',\n",
       "    'title': 'The IM algorithm: a variational approach to Information Maximization'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': '021be2238245edf3f62befc5650a462d0e052fc4',\n",
       "    'title': 'Information projections revisited'},\n",
       "   {'paperId': 'c76c62c5ab6c076a80f925d277ef04dd36f6bf9c',\n",
       "    'title': 'The information bottleneck method'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '552f45f64e64486ad273126e4f0695a6c044a015',\n",
       "    'title': 'Using Options for Knowledge Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'c8d90974c3f3b40fa05e322df2905fc16204aa56',\n",
       "    'title': 'Adaptive Mixtures of Local Experts'},\n",
       "   {'paperId': '8c11def05d9701db9a78589e2289e12f8262b928',\n",
       "    'title': 'Model predictive control: Theory and practice - A survey'}],\n",
       "  'citations': [{'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "    'title': 'NetHack is Hard to Hack'},\n",
       "   {'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': '1cdc6d284ea9bc7019490428949c619b26504f82',\n",
       "    'title': 'Beyond Reward: Offline Preference-guided Policy Optimization'},\n",
       "   {'paperId': 'da728351fbb93512425fbf0e28a65c95bc19e109',\n",
       "    'title': 'Unsupervised Discovery of Continuous Skills on a Sphere'},\n",
       "   {'paperId': 'ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7',\n",
       "    'title': 'Behavior Contrastive Learning for Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'baed31b161cb5399e02eb31c7d2adb8147169a41',\n",
       "    'title': 'IMAP: Intrinsically Motivated Adversarial Policy'},\n",
       "   {'paperId': 'bae67cb4cda1d2a9b333baf9e15e25b3468b5b4e',\n",
       "    'title': 'Efficient Quality-Diversity Optimization through Diverse Quality Species'},\n",
       "   {'paperId': 'e383a0f98251b9eda0ea6408543b9194b6f60b9f',\n",
       "    'title': 'Habits and goals in synergy: a variational Bayesian framework for behavior'},\n",
       "   {'paperId': 'ae75404a597b573edd05a129819da8c2385af709',\n",
       "    'title': 'RoboPianist: A Benchmark for High-Dimensional Robot Control'},\n",
       "   {'paperId': '5045c2a64a4ea41d8da9ee8eb279498db1ff3d78',\n",
       "    'title': 'Planning Goals for Exploration'},\n",
       "   {'paperId': '8b494e93653215e0f21d658b773c7b4368c54d9d',\n",
       "    'title': 'Hierarchical Monte Carlo Tree Search for Latent Skill Planning'},\n",
       "   {'paperId': '5a2457f1cf0971c34208e695af6c1d39c7bb83a4',\n",
       "    'title': 'Latent-Conditioned Policy Gradient for Multi-Objective Deep Reinforcement Learning'},\n",
       "   {'paperId': '8d32b150ce19200a2d2d71f68b63972080ef99ad',\n",
       "    'title': 'Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach'},\n",
       "   {'paperId': '4d0e2aa1142aa323df862ebf56513adf04fcb5ae',\n",
       "    'title': 'Learning robotic manipulation skills with multiple semantic goals by conservative curiosity-motivated exploration'},\n",
       "   {'paperId': '28544cb4c409bde8239f03b595bb82219a35aecd',\n",
       "    'title': 'Controlled Diversity with Preference : Towards Learning a Diverse Set of Desired Skills'},\n",
       "   {'paperId': '9a2c0ce9368eb44a5724596e13b55c18842cf08a',\n",
       "    'title': 'MAP-Elites with Descriptor-Conditioned Gradients and Archive Distillation into a Single Policy'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '7fcc7c21023b3c8f95773e14db7cc8007c4c8d24',\n",
       "    'title': 'Neural Laplace Control for Continuous-time Delayed Systems'},\n",
       "   {'paperId': '53d661d536965daf4ff6dcfd3b7e42ffa9061d78',\n",
       "    'title': 'Diverse Policy Optimization for Structured Action Space'},\n",
       "   {'paperId': 'f63adcba79ab09c2eed7d22174661be98a018bf4',\n",
       "    'title': 'ALAN: Autonomously Exploring Robotic Agents in the Real World'},\n",
       "   {'paperId': 'e966cca871cef85f3bfb9a6c69cdcbec23357c1d',\n",
       "    'title': 'Controllability-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'aeaddc24fe0c4179105e483e83cbe1387515914b',\n",
       "    'title': 'Predictable MDP Abstraction for Unsupervised Model-Based RL'},\n",
       "   {'paperId': 'ef213d6543cd995ac6b1dfde7d08a7a120232391',\n",
       "    'title': 'Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '40bfcb5901a3c9cd9581433fd2eb1906abcfae6d',\n",
       "    'title': 'Hierarchical Learning with Unsupervised Skill Discovery for Highway Merging Applications'},\n",
       "   {'paperId': '27d2df5d338ac0c68b44d2bbe1694a71ce12eb95',\n",
       "    'title': 'Diversity Through Exclusion (DTE): Niche Identification for Reinforcement Learning through Value-Decomposition'},\n",
       "   {'paperId': '2926ca8a2afc8e32515613664f53e80e6d7172ee',\n",
       "    'title': 'Skill Decision Transformer'},\n",
       "   {'paperId': 'b093a3fa79512c48524f81c754bddec7b16afb17',\n",
       "    'title': 'Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation'},\n",
       "   {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "    'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       "   {'paperId': '08f2018ca23962ddabaeeb9dbf60ae99526e6741',\n",
       "    'title': 'Intrinsic Motivation in Dynamical Control Systems'},\n",
       "   {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "    'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '4d9c9d5e5afc3313bfbcf5e69e49ec32f4ec49d4',\n",
       "    'title': 'Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees'},\n",
       "   {'paperId': '12075ea34f5fbe32ec5582786761ab34d401209b',\n",
       "    'title': 'Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain.'},\n",
       "   {'paperId': '288e5a79e73742f9ead3bfa9463891717414f6fd',\n",
       "    'title': 'Addressing Hindsight Bias in Multigoal Reinforcement Learning'},\n",
       "   {'paperId': '988dae20df8d69869aa41097a05d821446cff621',\n",
       "    'title': 'DisTop: Discovering a Topological representation to learn diverse and rewarding skills'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'fbe1003ec391f6bcf4660f6ef81f1e6199849bfe',\n",
       "    'title': 'Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning'},\n",
       "   {'paperId': '4ba1477f9d164efacde197341789b48d92447610',\n",
       "    'title': 'Hierarchical Strategies for Cooperative Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '9d1445f1845a2880ff9c752845660e9c294aa7b5',\n",
       "    'title': 'Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery'},\n",
       "   {'paperId': '80518a0192cdb7b97c47a2248003c8ffa26afb24',\n",
       "    'title': 'ODPP: A Unified Algorithm Framework for Unsupervised Option Discovery based on Determinantal Point Process'},\n",
       "   {'paperId': '41e41f650fc1926eda019be894ad96ab56315860',\n",
       "    'title': 'CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control'},\n",
       "   {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "    'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       "   {'paperId': 'cfd232ade1fdee8f00d90a0c1e6148b8ee530e29',\n",
       "    'title': 'Choreographer: Learning and Adapting Skills in Imagination'},\n",
       "   {'paperId': 'd0d55472bc4ed3daedfde6ed3a535740bc53745c',\n",
       "    'title': 'Discovering Unsupervised Behaviours from Full-State Trajectories'},\n",
       "   {'paperId': '3d20a371c5b75574d38c6bf53eda2db3e0ddfa9b',\n",
       "    'title': 'Curiosity in hindsight'},\n",
       "   {'paperId': 'ce0e769936453f827aee367e3463bb9915c6d78b',\n",
       "    'title': 'Emergency action termination for immediate reaction in hierarchical reinforcement learning'},\n",
       "   {'paperId': 'fbc6a615c97ecc596d18388f137957852ed5c9fe',\n",
       "    'title': 'Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '594cc7ce6690ad6d3dd79ea57391ab1bd4d41119',\n",
       "    'title': 'Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '9e8b29d025cd2718ff61b363ce1c1f422d612303',\n",
       "    'title': 'Learning General World Models in a Handful of Reward-Free Deployments'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': '59ca92312aeb2f2f3fe8cc698cd3dccc52dc9964',\n",
       "    'title': 'Random Actions vs Random Policies: Bootstrapping Model-Based Direct Policy Search'},\n",
       "   {'paperId': '66972e283c4325c15d6243cd733e3500482195a6',\n",
       "    'title': 'Augmentative Topology Agents For Open-Ended Learning'},\n",
       "   {'paperId': 'a78248272e8b87e6f7650fdc29dbb77454c1a745',\n",
       "    'title': 'Simple Emergent Action Representations from Multi-Task Policy Training'},\n",
       "   {'paperId': 'b4f7501a773d0b0e48249bf8a0cea7619cd8013c',\n",
       "    'title': 'Diverse Effective Relationship Exploration for Cooperative Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '1cc13cff6f12d450457f51eb8d5d8e20bce47b56',\n",
       "    'title': 'Skill-Based Reinforcement Learning with Intrinsic Reward Matching'},\n",
       "   {'paperId': '475c83ef687067b108e2a23a1af0e95262d048ab',\n",
       "    'title': 'A Mixture of Surprises for Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': '5e5419b46a9449d23c1e223d067310e56192b5dc',\n",
       "    'title': 'Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery'},\n",
       "   {'paperId': 'd98b5bad8cdadfbfa3e657e645d7224046c95cc8',\n",
       "    'title': 'See and Copy: Generation of complex compositional movements from modular and geometric RNN representations'},\n",
       "   {'paperId': '03446d8730c4bc5cb3abc9d8c13e8e6d3898e2ae',\n",
       "    'title': 'Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation'},\n",
       "   {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "    'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       "   {'paperId': '007017ca4b9760bbd72f7ea1a4357c4d52dfd205',\n",
       "    'title': 'Exploiting Reward Shifting in Value-Based Deep RL'},\n",
       "   {'paperId': '79dbba6bfba5bf20bc564f895c2cc8c8bf6953b4',\n",
       "    'title': 'Optimistic Curiosity Exploration and Conservative Exploitation with Linear Reward Shaping'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': '1c33f76c6f182023a2eefb3bb46cba13fb2345a4',\n",
       "    'title': 'Learning Dynamic Manipulation Skills from Haptic-Play'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "    'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       "   {'paperId': '8e21576387f46f1b9090bdbff1ceadf187feeada',\n",
       "    'title': 'CompoSuite: A Compositional Reinforcement Learning Benchmark'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '7b9d0b289ece26d1cccc2ee389b881fda1a4cc62',\n",
       "    'title': 'Fast Population-Based Reinforcement Learning on a Single Machine'},\n",
       "   {'paperId': '4247b93593b6ecd70262a1b1c7021dcecc26c8e0',\n",
       "    'title': 'Contrastive Learning as Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '78839ec995beab7f5fa8ce8d549fb4cf04b33d45',\n",
       "    'title': 'Meta-Learning Parameterized Skills'},\n",
       "   {'paperId': '31e205fa16a2b94564e69291f9a32ea547773a63',\n",
       "    'title': 'Uniqueness and Complexity of Inverse MDP Models'},\n",
       "   {'paperId': '47d90b910756b6a173a97be75cc03a9dafbf3497',\n",
       "    'title': 'Discovering Policies with DOMiNO: Diversity Optimization Maintaining Near Optimality'},\n",
       "   {'paperId': '7db177c489890313d74b117233b2aec4b78b3778',\n",
       "    'title': 'First Contact: Unsupervised Human-Machine Co-Adaptation via Mutual Information Maximization'},\n",
       "   {'paperId': '53f97097b17c33c3c6c02819cc969432842547b8',\n",
       "    'title': 'Learning to Optimize in Model Predictive Control'},\n",
       "   {'paperId': '867d3905b7a527ba46e03c454db55fa55c709389',\n",
       "    'title': 'Cliff Diving: Exploring Reward Surfaces in Reinforcement Learning Environments'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '14903cf4df35936b8a5c1dfa638c59727be6112d',\n",
       "    'title': 'A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '46bd3c20e6f7a9b6e413ea9f3452965601fd6de9',\n",
       "    'title': 'Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '9229fe9049677b0d00a38713bf1642a1955a1f18',\n",
       "    'title': 'INFOrmation Prioritization through EmPOWERment in Visual Model-Based RL'},\n",
       "   {'paperId': '58750c7e5856c2f95f1f61b853c3d31768e7bc7e',\n",
       "    'title': 'Screening goals and selecting policies in hierarchical reinforcement learning'},\n",
       "   {'paperId': '77d3d69f1c4c160e3765c416bc13aed863176197',\n",
       "    'title': 'One After Another: Learning Incremental Skills for a Changing World'},\n",
       "   {'paperId': '5762e9c654ae9230db5936f21780ae794a838533',\n",
       "    'title': 'Perceiving the World: Question-guided Reinforcement Learning for Text-based Games'},\n",
       "   {'paperId': '1952fc35b919f149a4cc2647c114b7038593391c',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions from Language'},\n",
       "   {'paperId': '9fc6b6d3fead719cdc95263e62209548223576d1',\n",
       "    'title': 'It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum Generation'},\n",
       "   {'paperId': '6d8f5366a04ed1955bb62759369c938f05da7f27',\n",
       "    'title': 'Open-Ended Reinforcement Learning with Neural Reward Functions'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': 'fa21a215468e881820266d1df362340987bc3fa8',\n",
       "    'title': 'Diversify and Disambiguate: Learning From Underspecified Data'},\n",
       "   {'paperId': 'fa950a27da0281095f7be7d8a2224391dcbd247b',\n",
       "    'title': 'Challenging Common Assumptions in Convex Reinforcement Learning'},\n",
       "   {'paperId': 'bf5d2d302f046cda8f75ceb09c842109e09c5862',\n",
       "    'title': 'Lipschitz-constrained Unsupervised Skill Discovery'},\n",
       "   {'paperId': '9bf925ecb1e6c6bfeecfc15aec1d0c6d7c28e135',\n",
       "    'title': 'CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'd40ea2ab20249944a44125e78117452ef0bad752',\n",
       "    'title': 'Explaining Reinforcement Learning Policies through Counterfactual Trajectories'},\n",
       "   {'paperId': 'ab2542ae894b0834494c968e14f96bfd7908aa90',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Adversarially Guided Subgoals'},\n",
       "   {'paperId': '64290435a11eee0b4ea5c23cb9937d1e0af957e6',\n",
       "    'title': 'Adversarially Guided Subgoal Generation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c397a67aad5bd5d08364135ac01b028af930c604',\n",
       "    'title': 'State-Conditioned Adversarial Subgoal Generation'},\n",
       "   {'paperId': 'b1e30f99171e1727e34dc2e7625fd884aa5fcd29',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning: Problems and Solutions'},\n",
       "   {'paperId': '66cc87463c0f27256a18eb02915460ef0b510c0b',\n",
       "    'title': 'Prospective Learning: Back to the Future'},\n",
       "   {'paperId': '5070d9e5ed307c1f6a3c95eb27b4b4c1b88ea6fc',\n",
       "    'title': 'Data-Efficient Learning of High-Quality Controls for Kinodynamic Planning used in Vehicular Navigation'},\n",
       "   {'paperId': 'a24e9d58962e94b3391a4a132e0eb19b5c3abd75',\n",
       "    'title': 'Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': '5dd82eee3efefb96aeaaae8b817b6be2e204dc2f',\n",
       "    'title': 'Autonomous Reinforcement Learning: Formalism and Benchmarking'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '387a17823d7c47c0bd3390a124708933032989e0',\n",
       "    'title': 'Generalized Decision Transformer for Offline Hindsight Information Matching'},\n",
       "   {'paperId': '0213fa01c7b8aa7668b11fd9edf283fe10d5719e',\n",
       "    'title': 'Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning'},\n",
       "   {'paperId': 'a01fae01cd9a3067fa6b8a777e70efe86bdc4699',\n",
       "    'title': 'Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '65e36b8fc38819944528d232368d8669feb3b01a',\n",
       "    'title': 'Wasserstein Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '40888b859c5b40868943162e3c4769dae1aed716',\n",
       "    'title': 'The Information Geometry of Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': 'ee21c47254d1bcf33e13bf746218021816443745',\n",
       "    'title': 'Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation'},\n",
       "   {'paperId': '5d0c5b55db27d44e80406a825fb86ae3e1325a8b',\n",
       "    'title': 'A First-Occupancy Representation for Reinforcement Learning'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '064bbace1d1ce11787975c59f8209ea6306f261e',\n",
       "    'title': 'Search-Based Task Planning with Learned Skill Effect Models for Lifelong Robotic Manipulation'},\n",
       "   {'paperId': '1f312f9363eecaae22b65b1d9397cc2737168143',\n",
       "    'title': 'Dynamics-Aware Quality-Diversity for Efficient Learning of Skill Repertoires'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '167b5d14442024f3af48ae4e79c6fb41ac87b42b',\n",
       "    'title': 'Active Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': 'ae25c0c7d3e2de1c90346079e53b9e3e836c7de4',\n",
       "    'title': 'Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning'},\n",
       "   {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "    'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       "   {'paperId': '73e240cb95309142b61db3e9afd9282bf0b5464c',\n",
       "    'title': 'Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey'},\n",
       "   {'paperId': '69f657fbafa51a4411dd6aba63091d616114959d',\n",
       "    'title': 'SHERLock: Self-Supervised Hierarchical Event Representation Learning'},\n",
       "   {'paperId': 'c5d97d4c84d2bfb82c06a0dff76e192f51266000',\n",
       "    'title': 'Efficient Task Adaptation by Mixing Discovered Skills'},\n",
       "   {'paperId': 'a42744a7c8db25290254b2b60831972a3926ae02',\n",
       "    'title': 'S KILL D ECISION T RANSFORMER'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': '5840bf765be8c3bcedab63f43f5982ddba26eaf9',\n",
       "    'title': 'SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING'},\n",
       "   {'paperId': '15521311d6d47218e54c9b9ca0cda8283fb36c52',\n",
       "    'title': 'Generating Diverse Cooperative Agents by Learning Incompatible Policies'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '193dadd9de36ef8e6883088fbd35d38fa7ce590e',\n",
       "    'title': 'Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots'},\n",
       "   {'paperId': 'f7a50ac5b68f20c5c3c72f31327de4296af3e788',\n",
       "    'title': 'Behavior From the Void: Unsupervised Active Pre-Training'},\n",
       "   {'paperId': 'ff33db486b561f36617bfee1d55fbd42d3045969',\n",
       "    'title': 'O PEN -E NDED R EINFORCEMENT L EARNING WITH N EU RAL R EWARD F UNCTIONS'},\n",
       "   {'paperId': '4d4ea4b8cfcfcaee478a543298a090117daf5a60',\n",
       "    'title': 'G ENERALIZED D ECISION T RANSFORMER FOR O FFLINE H INDSIGHT I NFORMATION M ATCHING'},\n",
       "   {'paperId': '0e66997f28780f5883987dced13560034caa7afe',\n",
       "    'title': 'A UTONOMOUS R EINFORCEMENT L EARNING : F ORMALISM AND B ENCHMARKING'},\n",
       "   {'paperId': 'a75307ab1aec53a5071e677d2fbb780f87bd6220',\n",
       "    'title': 'INFO RMATION P RIORITIZATION THROUGH E M POWER MENT IN V ISUAL M ODEL -B ASED RL'},\n",
       "   {'paperId': 'a75307ab1aec53a5071e677d2fbb780f87bd6220',\n",
       "    'title': 'INFO RMATION P RIORITIZATION THROUGH E M POWER MENT IN V ISUAL M ODEL -B ASED RL'},\n",
       "   {'paperId': 'a2335853a24f48f3569344062b830f2d8d0dc6ad',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions'},\n",
       "   {'paperId': '7319249a853288dce45ca331a87dce052a2abfba',\n",
       "    'title': 'Disentangling Controlled Effects for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '82bf393bde761760cf0aab6bbe081408ebcb1958',\n",
       "    'title': 'Hierarchical reinforcement learning based energy management strategy for hybrid electric vehicle'},\n",
       "   {'paperId': 'ae6c5559fc682802e86343c48409c470719836a7',\n",
       "    'title': 'Learning Generalizable Behavior via Visual Rewrite Rules'},\n",
       "   {'paperId': 'faf8abf19a89fa0e40f3f50550816a3bdecb80f0',\n",
       "    'title': 'Information is Power: Intrinsic Control via Information Capture'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '9317736e9bb1b25c9d8e7325b7b364b7fbae0f3f',\n",
       "    'title': 'URLB: Unsupervised Reinforcement Learning Benchmark'},\n",
       "   {'paperId': 'f5aeaba6a0d4df824d752bceb583883c10d2ade9',\n",
       "    'title': 'Unsupervised Domain Adaptation with Dynamics-Aware Rewards in Reinforcement Learning'},\n",
       "   {'paperId': 'f0436c1ebb86ca0f576ed5cf94d74e2908fb02d5',\n",
       "    'title': 'Understanding the World Through Action'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': 'e643eb4ec5afd45fca31cb6d4259034042268064',\n",
       "    'title': 'Provable Hierarchy-Based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '98d0821e7165c1f5e08123e46efea804dddfb577',\n",
       "    'title': 'Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization'},\n",
       "   {'paperId': '06858604cc652722ca5092072c50a066000c565e',\n",
       "    'title': 'Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks'},\n",
       "   {'paperId': 'dbabe6ce982b0d8b3f3a842ec85ddc088733385e',\n",
       "    'title': 'Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration'},\n",
       "   {'paperId': '0f929e132ccc363a2d707c24319c2894935d93c8',\n",
       "    'title': 'Video2Skill: Adapting Events in Demonstration Videos to Skills in an Environment using Cyclic MDP Homomorphisms'},\n",
       "   {'paperId': '8a913111f23fbded7f2e9d2d6c9c4278e7c682c9',\n",
       "    'title': 'APS: Active Pretraining with Successor Features'},\n",
       "   {'paperId': 'e3f397673eba5b984e5443a1c635cbcbb7e5b959',\n",
       "    'title': 'SAUCE: Truncated Sparse Document Signature Bit-Vectors for Fast Web-Scale Corpus Expansion'},\n",
       "   {'paperId': '8ea35555abd28cca7b038a83a0f8c73dd9a46a66',\n",
       "    'title': 'Learning Task Agnostic Skills with Data-driven Guidance'},\n",
       "   {'paperId': '46515eb6eb66973db6bd9cef146853badb938b7e',\n",
       "    'title': 'Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '0ff1247950819a6beafa369178c6c9489de3ceaa',\n",
       "    'title': 'Unsupervised Discovery of Transitional Skills for Deep Reinforcement Learning'},\n",
       "   {'paperId': '517d655b16d1fbf3d36a3c5da8d1a211dfc42760',\n",
       "    'title': 'Experimental Evidence that Empowerment May Drive Exploration in Sparse-Reward Environments'},\n",
       "   {'paperId': '92edf4d699a139b757e6cfb5407872af5472a758',\n",
       "    'title': 'Explore and Control with Adversarial Surprise'},\n",
       "   {'paperId': '107e4ea37d2e5364893107a8ce072972c4a10dfb',\n",
       "    'title': 'Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '4ff9a9248dceb00a7d02073815c085911d5c0a59',\n",
       "    'title': 'Discovering Generalizable Skills via Automated Generation of Diverse Tasks'},\n",
       "   {'paperId': '5ed0e750ab088c3a1ffcfb168b6c36f08dffd031',\n",
       "    'title': 'Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation'},\n",
       "   {'paperId': 'd44c89071b087c318aa3bee56d9a5b71e11e5e7f',\n",
       "    'title': 'DAIR: Disentangled Attention Intrinsic Regularization for Safe and Efficient Bimanual Manipulation'},\n",
       "   {'paperId': '2446a59a1a673a7722c5b432d063a9045cf19902',\n",
       "    'title': 'Pretrained Encoders are All You Need'},\n",
       "   {'paperId': '45f573f302dc7e77cbc5d1a74ccbac3564bbebc8',\n",
       "    'title': 'PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training'},\n",
       "   {'paperId': '0f1382cb004b4834cc3ca7824a61d0d6b86a5763',\n",
       "    'title': 'Pretraining Representations for Data-Efficient Reinforcement Learning'},\n",
       "   {'paperId': '97c60d5111de6e454689f9d40b14518eb15969d8',\n",
       "    'title': 'Celebrating Diversity in Shared Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '26d601215e16b7b69e3ee2f88282312ba4577519',\n",
       "    'title': 'Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning'},\n",
       "   {'paperId': 'c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500',\n",
       "    'title': 'Decision Transformer: Reinforcement Learning via Sequence Modeling'},\n",
       "   {'paperId': 'e9094e2a33af929d67793d480fffa6836b1bfe07',\n",
       "    'title': 'Efficient Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': 'd93b090bd7eb49fac0c87ca1761077a86de059d4',\n",
       "    'title': 'Hierarchies of Planning and Reinforcement Learning for Robot Navigation'},\n",
       "   {'paperId': 'b1cbe4d5400d6ae593c7de69ec14f6d0a3a85159',\n",
       "    'title': 'Discovering diverse athletic jumping strategies'},\n",
       "   {'paperId': 'c1d7caee5cbd8bb4b5e85141400941dc48db71ac',\n",
       "    'title': 'Policy manifold search: exploring the manifold hypothesis for diversity-based neuroevolution'},\n",
       "   {'paperId': '4ac04e2267071df8c0d4503f1dec9a336f9fb108',\n",
       "    'title': 'Discriminator Augmented Model-Based Reinforcement Learning'},\n",
       "   {'paperId': '17051a6fb19dfa224bd4c4de5825ea15d765e723',\n",
       "    'title': 'Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning'},\n",
       "   {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "    'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '2badd9707ec9e6ae08daa6a4de2edb45a784ef67',\n",
       "    'title': 'Hard Attention Control By Mutual Information Maximization'},\n",
       "   {'paperId': '78d4285fbcb06fcf3289e099c19abae7e7ed95a3',\n",
       "    'title': 'Discovering Diverse Multi-Agent Strategic Behavior via Reward Randomization'},\n",
       "   {'paperId': '0295df1b9d11e6e49b20119410fd755a4d7781af',\n",
       "    'title': 'Behavior From the Void: Unsupervised Active Pre-Training'},\n",
       "   {'paperId': 'f89af36631d45126faf2d23b81c4a767d4f91d56',\n",
       "    'title': 'Task-Agnostic Morphology Evolution'},\n",
       "   {'paperId': '9956e3ea2b894f45ca9070ee1984caadb74edbf7',\n",
       "    'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '1a7471fb3d324a074bda8a7530e9e7749329cef9',\n",
       "    'title': 'Relative Variational Intrinsic Control'},\n",
       "   {'paperId': '25c310d4f7bc581a94455a1e96373d924ec736ae',\n",
       "    'title': 'Reset-Free Lifelong Learning with Skill-Space Planning'},\n",
       "   {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "    'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': 'b62f094179aa79015c57690c01e76e7d7d75f78c',\n",
       "    'title': 'Representation Matters: Improving Perception and Exploration for Robotics'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '5764095b0186a3fc3832c1052aa14996a5927edc',\n",
       "    'title': 'RODE: Learning Roles to Decompose Multi-Agent Tasks'},\n",
       "   {'paperId': '532b7a2cd054465f66ec582b55e128bdb842c28f',\n",
       "    'title': 'Efficient Empowerment Estimation for Unsupervised Stabilization'},\n",
       "   {'paperId': '362102cbbcf119680b90a761ee46ad93303396e7',\n",
       "    'title': 'Hybrid Control for Learning Motor Skills'},\n",
       "   {'paperId': '599fcb9ca476098f33334852cc360bae9f1f7ee2',\n",
       "    'title': '[Appendix] Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '0cff1642794d9628b6e5388a4af97bf9a77ccdba',\n",
       "    'title': 'L ATENT S KILL P LANNING FOR E XPLORATION AND T RANSFER'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': None, 'title': 'TRANSLATING ROBOT SKILLS: LEARNING UNSUPER-'},\n",
       "   {'paperId': '567c75aa0c146f92db19134806b5b41266b13f59',\n",
       "    'title': 'V ALUE F UNCTION S PACES : S KILL -C ENTRIC S TATE A BSTRACTIONS FOR L ONG -H ORIZON R EASONING'},\n",
       "   {'paperId': 'adabcfc03fa110d40d6ab2a613a36dda9c12dfff',\n",
       "    'title': 'Exploration via Empowerment Gain: Combining Novelty, Surprise and Learning Progress'},\n",
       "   {'paperId': 'd4c8ae091666cdc19aa3510aa0ce1c81963e8006',\n",
       "    'title': 'Data-Efficient Exploration with Self Play for Atari'},\n",
       "   {'paperId': '02d1ff7e59fd8a66d3515493b5e63cee0ab8168f',\n",
       "    'title': 'Intrinsic Control of Variational Beliefs in Dynamic Partially-Observed Visual Environments'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '061c37ed44945c98af68e9a3bc6a19e614c5981d',\n",
       "    'title': 'Behavior From the Void: Unsupervised Active Pre-Training'},\n",
       "   {'paperId': '2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68',\n",
       "    'title': 'Exploration in Deep Reinforcement Learning: A Comprehensive Survey'},\n",
       "   {'paperId': '1b305b762bd6d9c929090631234b8a0721cc0ec7',\n",
       "    'title': 'GSC: Graph-based Skill Composition for Deep Reinforcement Learning'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'},\n",
       "   {'paperId': 'ede66a0627ee774052c4234311be1966cf927634',\n",
       "    'title': 'Bottom-up Discovery of Reusable Sensorimotor Skills from Unstructured Demonstrations'},\n",
       "   {'paperId': 'd3a91fdc4ba400e71ea89103f4f2a1e276d52d41',\n",
       "    'title': 'Deep Reinforcement Learning with Hierarchical Structures'},\n",
       "   {'paperId': 'fcacc984afe8d65c3c3ca9c321e3cfa9321748ad',\n",
       "    'title': 'DATA-EFFICIENT REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '696f71ca179182b823af4b3559ca3fd2d6f6c2b7',\n",
       "    'title': 'Robustly Learning Composable Options in Deep Reinforcement Learning'},\n",
       "   {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "    'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       "   {'paperId': '463a75286a1246594a7cf8f6abfc8fe557bc7dde',\n",
       "    'title': 'Improved Sample Complexity for Incremental Autonomous Exploration in MDPs'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': '1760e16f7d6603c1f4981a3d7a9a188699ede2db',\n",
       "    'title': 'Policy Manifold Search for Improving Diversity-based Neuroevolution'},\n",
       "   {'paperId': '4d1537347d8f5c463188166ae96c3c0d7a3260fa',\n",
       "    'title': 'Skill Transfer via Partially Amortized Hierarchical Planning'},\n",
       "   {'paperId': 'b667641dc6acd7c0233503615942ea00ea9875f5',\n",
       "    'title': 'Continual Learning of Control Primitives: Skill Discovery via Reset-Games'},\n",
       "   {'paperId': 'e4a46c64aafbef0406e9cfa90dd9c43e3e07598c',\n",
       "    'title': 'One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'd0f68a31f9ecc3c51185fc7bae6b0629d3ef984c',\n",
       "    'title': 'Brief Survey of Model-Based Reinforcement Learning Techniques'},\n",
       "   {'paperId': '3d176aa8e0f93a2c1c6ca59fbd3da4b36c7f5940',\n",
       "    'title': 'Unsupervised Hierarchical Concept Learning'},\n",
       "   {'paperId': 'bf37352ed947f43e51f089de92addb377b7ad8e0',\n",
       "    'title': 'Diverse Exploration via InfoMax Options'},\n",
       "   {'paperId': '56775c1b1fdf8440913d22f8946f7fdb241bbad9',\n",
       "    'title': 'Learning Diverse Options via InfoMax Termination Critic'},\n",
       "   {'paperId': '560e9a8a1b027adbefb838ca337aa73af7998ca9',\n",
       "    'title': 'Disentangling causal effects for hierarchical reinforcement learning'},\n",
       "   {'paperId': '6be61525ee8b21c3bef6564df17b435fc4f84282',\n",
       "    'title': 'Action and Perception as Divergence Minimization'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '5f952b6c4fe9fa6d0fbe7122d9e4f40921118c87',\n",
       "    'title': 'Independent Skill Transfer for Deep Reinforcement Learning'},\n",
       "   {'paperId': '18939501fc35341ae409520f11e49ca2be1c6790',\n",
       "    'title': 'SOAC: The Soft Option Actor-Critic Architecture'},\n",
       "   {'paperId': '019820cbb73d0651a913bb74cbfb713c8ad772df',\n",
       "    'title': 'ELSIM: End-to-end learning of reusable skills through intrinsic motivation'},\n",
       "   {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "    'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       "   {'paperId': '5050392c82b2c59c0b469ff631102058e8f67693',\n",
       "    'title': 'PAC Bounds for Imitation and Model-based Batch Learning of Contextual Markov Decision Processes'},\n",
       "   {'paperId': 'f2cbb43dbfbeca86ee6541035099128f21a78f09',\n",
       "    'title': 'Skill Discovery of Coordination in Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45',\n",
       "    'title': 'Novel Policy Seeking with Constrained Optimization'},\n",
       "   {'paperId': '064c6f04352edd9dee1ba1124777b3dcc62a740a',\n",
       "    'title': 'Simple Sensor Intentions for Exploration'},\n",
       "   {'paperId': '3a71c306eb6232658c9e5fd48aed1ef3befe5fbe',\n",
       "    'title': 'Planning to Explore via Self-Supervised World Models'},\n",
       "   {'paperId': 'e1a39a6614503546bbb72a8c75aaf0ae93a3ac01',\n",
       "    'title': 'Option Discovery using Deep Skill Chaining'},\n",
       "   {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "    'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'f92ab050081c248207498691a4b7b681def471f7',\n",
       "    'title': 'Model-Predictive Control via Cross-Entropy and Gradient-Based Optimization'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "    'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       "   {'paperId': '7b0871c783e721bfbf9b5d16e575130a07a672cd',\n",
       "    'title': 'Generalized Hindsight for Reinforcement Learning'},\n",
       "   {'paperId': '286e5a0bba7fe88bdd87fa1e2637a8d2d223be11',\n",
       "    'title': 'Preventing Imitation Learning with Adversarial Policy Ensembles'},\n",
       "   {'paperId': '5a2769ec9ebbff4f5d19686498715f9b5ce29b10',\n",
       "    'title': 'Intelligence, physics and information - the tradeoff between accuracy and simplicity in machine learning'},\n",
       "   {'paperId': '27485fa549cd72dc73da6b187661f5b1f403d9c1',\n",
       "    'title': 'Phase Transitions for the Information Bottleneck in Representation Learning'},\n",
       "   {'paperId': '258864fd38248d5cf5d6bd10880cc72d3db51007',\n",
       "    'title': 'Learning Calibratable Policies using Programmatic Style-Consistency'},\n",
       "   {'paperId': 'ec684b9cf2433680f6bd70779186f34bcd5b4f06',\n",
       "    'title': 'MaxEnt Reward Expected Reward Latent Representations Missing Data Controllable Future Factorized Target Perception Action Both Low Entropy Preferences Empowerment Skill Discovery Amortized Inference Maximum Likelihood Variational Inference Input Density Exploration Information GainFiltering Latent S'},\n",
       "   {'paperId': '4959a0e042c62a9daa4c4e16f995f4b473aac06f',\n",
       "    'title': 'HARD ATTENTION CONTROL BY MUTUAL INFORMA-'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '80869d6f2373e95b479b4979e47b8e6e0231222b',\n",
       "    'title': 'PAC Imitation and Model-based Batch Learning of Contextual MDPs'},\n",
       "   {'paperId': '35b730007d99ca762060aa66d46aa04e7e806822',\n",
       "    'title': 'NECK IN REPRESENTATION LEARNING'},\n",
       "   {'paperId': '8079263658b6cdf88a9b9d818ff468091a7e1bf2',\n",
       "    'title': 'No Representation without Transformation'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': '1f4cb71b70a74ca37c55bae600ae827a21025a87',\n",
       "    'title': 'Adaptive Prior Selection for Repertoire-Based Online Adaptation in Robotics'},\n",
       "   {'paperId': 'b93317f61c6ed99542da9d1d691ded9732c16c1c',\n",
       "    'title': 'Unsupervised Meta-Learning for Reinforcement Learning'}],\n",
       "  'citnuminlist': 7,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " '99a7df93a2e16bd7ac3349d52cc34417cda7909d': {'title': 'Learning Latent Plans from Play',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': '1fa1f04b80f057e477549e6b9798fab7c7e57db5',\n",
       "    'title': 'Hindsight policy gradients'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Learning latent plans from play. CoRR, abs/1903.01973'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '54cd5a5ddd286442fa94da7ec344a7e76b9a6ccd',\n",
       "    'title': 'Visual Foresight: Model-Based Deep Reinforcement Learning for Vision-Based Robotic Control'},\n",
       "   {'paperId': '776f3d2250285ac03b2019ecf18668fcdd72a9ce',\n",
       "    'title': 'One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': 'cc2fb12eaa4dae74c5de0799b29624b5c585c43b',\n",
       "    'title': 'Behavioral Cloning from Observation'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '852c931b5d9f9d4256befd725ee4185945c4964c',\n",
       "    'title': 'Temporal Difference Models: Model-Free Deep RL for Model-Based Control'},\n",
       "   {'paperId': '9217c4aa5a95d3209d2071e6889c8dd4b7d9309e',\n",
       "    'title': 'Zero-Shot Visual Imitation'},\n",
       "   {'paperId': 'cee949487d13d0b64c4ef21b66ece96eb08472b3',\n",
       "    'title': 'Asymmetric Actor Critic for Image-Based Robot Learning'},\n",
       "   {'paperId': 'b864f89eaa91120e04e8c62eb0b36568ab4244a8',\n",
       "    'title': 'Deep Imitation Learning for Complex Manipulation Tasks from Virtual Reality Teleoperation'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': 'a7680e975d395891522d3c10e3bf892f9b618048',\n",
       "    'title': 'Vision-Based Multi-Task Manipulation for Inexpensive Robots Using End-to-End Learning from Demonstration'},\n",
       "   {'paperId': '2adae2da173b9dd720c8bcac0250a90a7f1ec697',\n",
       "    'title': 'Time-Contrastive Networks: Self-Supervised Learning from Video'},\n",
       "   {'paperId': '8499a250422a3c66357367c8d5fa504de5424c59',\n",
       "    'title': 'Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play'},\n",
       "   {'paperId': '494e2d5b40dcebde349f9872c7317e5003f9c5d2',\n",
       "    'title': 'Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection'},\n",
       "   {'paperId': None, 'title': 'and N'},\n",
       "   {'paperId': 'f466157848d1a7772fb6d02cdac9a7a5e7ef982e',\n",
       "    'title': 'Neural Discrete Representation Learning'},\n",
       "   {'paperId': 'cf18287e79b1fd73cd333fc914bb24c00a537f4c',\n",
       "    'title': 'Self-Supervised Visual Planning with Temporal Skip Connections'},\n",
       "   {'paperId': '482c0cbfffa77154e3c879c497f50b605297d5bc',\n",
       "    'title': 'One-Shot Visual Imitation Learning via Meta-Learning'},\n",
       "   {'paperId': '1418c9da011db25fa95a32989d5a578bc3bc4601',\n",
       "    'title': 'Intrinsically Motivated Goal Exploration Processes with Automatic Curriculum Learning'},\n",
       "   {'paperId': 'fbde1483960676744ef55243fd8122000049ddc1',\n",
       "    'title': 'The Intentional Unintentional Agent: Learning to Solve Many Continuous Control Tasks Simultaneously'},\n",
       "   {'paperId': 'cddb1f7f9f004396a2efef285caf29d7780a8e21',\n",
       "    'title': 'Robust Imitation of Diverse Behaviors'},\n",
       "   {'paperId': '82bb03fe3d3f2c3187ed7456f91dfcc91c6ea21d',\n",
       "    'title': 'Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '5c57bb5630835a05eb1c3d0df3e12d6180d75de2',\n",
       "    'title': 'One-Shot Imitation Learning'},\n",
       "   {'paperId': '67a462fd5c8f20846df0d1a8cfedac70d5678a50',\n",
       "    'title': 'Combining self-supervised learning and imitation for vision-based rope manipulation'},\n",
       "   {'paperId': '66386a946a04534275bd466862364d139790f41f', 'title': 'P IXEL'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': '8f92b4ea04758df2acfb49bd46a4cde923c3ddcb',\n",
       "    'title': 'Deep visual foresight for planning robot motion'},\n",
       "   {'paperId': None, 'title': 'Hierarchical actor-critic. CoRR'},\n",
       "   {'paperId': '3ed67ded2b4d3614b38798b3f17a8e69803d0980',\n",
       "    'title': 'Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model'},\n",
       "   {'paperId': '8cf83c619423a1504f26495d5f6a495054c46462',\n",
       "    'title': 'Learning to Poke by Poking: Experiential Learning of Intuitive Physics'},\n",
       "   {'paperId': '6d4e3616d0b27957c4107ae877dc0dd4504b69ab',\n",
       "    'title': 'Shuffle and Learn: Unsupervised Learning Using Temporal Order Verification'},\n",
       "   {'paperId': 'd82b55c35c8673774a708353838918346f6c006f',\n",
       "    'title': 'Generating Sentences from a Continuous Space'},\n",
       "   {'paperId': 'f03b4ff1b4943691cec703b508c0a91f2d97a881',\n",
       "    'title': 'Supersizing self-supervision: Learning to grasp from 50K tries and 700 robot hours'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Improving variational inference with inverse autoregressive flow.(nips), 2016'},\n",
       "   {'paperId': 'c7c19d99175f252ea6b9babe98673743b71ac49d',\n",
       "    'title': 'MuJoCo HAPTIX: A virtual reality system for hand manipulation'},\n",
       "   {'paperId': '3f25e17eb717e5894e0404ea634451332f85d287',\n",
       "    'title': 'Learning Structured Output Representation using Deep Conditional Generative Models'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': '6c11626ae08706e6185fceff0a6d05e4bfd6bd06',\n",
       "    'title': 'Unsupervised Learning of Visual Representations using Videos'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'b6bfae6efa1110a57a4d8362721d152d78aae358',\n",
       "    'title': 'A Survey on Policy Search for Robotics'},\n",
       "   {'paperId': '65438e0ba226c1f97bd8a36333ebc3297b1a32fd',\n",
       "    'title': 'Reinforcement learning in robotics: A survey'},\n",
       "   {'paperId': '52449f97e09c7465adbc1d4f16e063802d392530',\n",
       "    'title': 'Autonomous reinforcement learning on raw visual input data in a real world application'},\n",
       "   {'paperId': 'bc6dff14a130c57a91d5a21339c23471faf1d46f', 'title': 'Et al'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '220ad0189a4c9ee5b5c299f269a0e4bef290e8fd',\n",
       "    'title': 'Learning and generalization of motor skills by learning from demonstration'},\n",
       "   {'paperId': '4e5dfb0b1e54412e799eb0e86d552956cc3a5f54',\n",
       "    'title': 'A survey of robot learning from demonstration'},\n",
       "   {'paperId': 'b48d78ed73144d69f6239696e55ba9596fe7813b',\n",
       "    'title': 'A multirange architecture for collision‐free off‐road robot navigation'},\n",
       "   {'paperId': '43ecc5329bc232f3ed2444a302ac286e2c4338ad',\n",
       "    'title': 'Play in evolution and development'},\n",
       "   {'paperId': '16d3b1859b935d5ec36116f69b1bfce9df6bf8c4',\n",
       "    'title': 'Core knowledge.'},\n",
       "   {'paperId': 'edf098cd466afc722c856959235c2757c16419a1',\n",
       "    'title': 'The genesis of animal play: Testing the limits'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Childrens play: A developmental and evolutionary orientation'},\n",
       "   {'paperId': 'b28c13222051ae7f650eb3b73dad0c962d2e6afb',\n",
       "    'title': 'Intrinsic and extrinsic motivation : the search for optimal motivation and performance'},\n",
       "   {'paperId': '03e185e502c6c1996a516d124d497954fea7f8f2',\n",
       "    'title': 'The ambiguity of play'},\n",
       "   {'paperId': 'ee42e5da818f3b4955d4043a397dca8b49e15dff',\n",
       "    'title': 'Play, Learning and the Early Childhood Curriculum'},\n",
       "   {'paperId': '6df43f70f383007a946448122b75918e3a9d6682',\n",
       "    'title': 'Learning to Achieve Goals'},\n",
       "   {'paperId': '2980dfe5c99658dc3e508d9d6e1d7f26e6fc8934',\n",
       "    'title': 'A possibility for implementing curiosity and boredom in model-building neural controllers'},\n",
       "   {'paperId': None, 'title': 'Ethology and child development'},\n",
       "   {'paperId': '0c75ff8369feac7d1eedc18978d26192842d895b',\n",
       "    'title': 'From exploration to play: A cross-sectional study of infant free play behavior.'},\n",
       "   {'paperId': '3b2e8ee9f47bde02ceb635fd66f62ffccef03e99',\n",
       "    'title': 'Animal Play Behavior'},\n",
       "   {'paperId': None, 'title': 'Exploration and play in children'}],\n",
       "  'citations': [{'paperId': 'd11ae7f22045a2217fb2ef169037fba216153c63',\n",
       "    'title': 'Stabilizing Contrastive RL: Techniques for Offline Goal Reaching'},\n",
       "   {'paperId': 'a91b821a95ccd417e0f1315247732dd4bcf45991',\n",
       "    'title': 'Data Quality in Imitation Learning'},\n",
       "   {'paperId': '43806158cabbcbca4efd58f75a82c882c085d60d',\n",
       "    'title': 'IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control'},\n",
       "   {'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': 'ad84b7b1e94f2a88458deedc3b1972018a24c640',\n",
       "    'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation'},\n",
       "   {'paperId': 'd4a5f340648a1cfdc90ae13f284dcc037698f6c4',\n",
       "    'title': 'Imitating Task and Motion Planning with Visuomotor Transformers'},\n",
       "   {'paperId': '6eef081e4a2322ae9f96632032da6cbe147b9f66',\n",
       "    'title': 'Learning Video-Conditioned Policies for Unseen Manipulation Tasks'},\n",
       "   {'paperId': '6cec825e32b1790a69893a5b2506818241506217',\n",
       "    'title': 'A Glimpse in ChatGPT Capabilities and its impact for AI research'},\n",
       "   {'paperId': '79e1d443805db1e655daf55a231a180c3be383ae',\n",
       "    'title': 'Get Back Here: Robust Imitation by Return-to-Distribution Planning'},\n",
       "   {'paperId': 'c7dea47e008a439e11439dfe6a8c1b08357fad65',\n",
       "    'title': 'Distance Weighted Supervised Learning for Offline Interaction Data'},\n",
       "   {'paperId': '682346d2a78fa57e8e9649052f3fea01f5924f1e',\n",
       "    'title': 'Robust flight navigation out of distribution with liquid neural networks'},\n",
       "   {'paperId': '21d5d3aa466577ef6aac75d428ada4fdbf1cf035',\n",
       "    'title': 'Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets'},\n",
       "   {'paperId': '1334a47e8f4e4ffd04ff534329d76a5e5cc16f46',\n",
       "    'title': 'Goal-Conditioned Imitation Learning using Score-based Diffusion Policies'},\n",
       "   {'paperId': 'e1bd151a3f670fd0f77580702fe7a85dc78a41cb',\n",
       "    'title': 'Chain-of-Thought Predictive Control'},\n",
       "   {'paperId': '26063b5f5efa71e1c3b726f6f8c792368ba43ce4',\n",
       "    'title': 'DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics'},\n",
       "   {'paperId': 'dae9be0f0d815b53b46974377a0edf9169a99f3f',\n",
       "    'title': 'Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': 'fdffeff76d839dbd92088f59ebf70207a54274f0',\n",
       "    'title': 'PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining'},\n",
       "   {'paperId': '327485eb56631c851028cc31a1eff7d8eaf1ff1b',\n",
       "    'title': 'Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations'},\n",
       "   {'paperId': '2ebd5df74980a37370b0bcdf16deff958289c041',\n",
       "    'title': 'Foundation Models for Decision Making: Problems, Methods, and Opportunities'},\n",
       "   {'paperId': 'd70f4d07501b55e94e37d495e153d51053a0b7d3',\n",
       "    'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': 'a4701fadfd92683f2df4245d3ea873f1df61a71a',\n",
       "    'title': 'MimicPlay: Long-Horizon Imitation Learning by Watching Human Play'},\n",
       "   {'paperId': 'cf8626c23d1d22405b9cc8061044ce7d8f8adf77',\n",
       "    'title': 'Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability'},\n",
       "   {'paperId': 'db20bd3bb82d1011ce704d440d8c2578f665e6e1',\n",
       "    'title': 'Aligning Robot and Human Representations'},\n",
       "   {'paperId': 'b0092b450aaa3e5c15f0fc9949341f332759d418',\n",
       "    'title': 'Learning From Guided Play: Improving Exploration for Adversarial Imitation Learning With Simple Auxiliary Tasks'},\n",
       "   {'paperId': '4fd4e392fb39124744bdfbb6d71ae2030be5132e',\n",
       "    'title': 'DALL-E-Bot: Introducing Web-Scale Diffusion Models to Robotics'},\n",
       "   {'paperId': 'd4cc991c48c2a94d31ac69e245217bb96c3b028e',\n",
       "    'title': 'Enabling Visual Action Planning for Object Manipulation Through Latent Space Roadmap'},\n",
       "   {'paperId': '7ccfd68dc071d6060962b17dd25b0c7fbfd58a9e',\n",
       "    'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures'},\n",
       "   {'paperId': '267050502b6963328cccb2f8e4279f1f7265c465',\n",
       "    'title': 'Learning robotic navigation from experience: principles, methods and recent results'},\n",
       "   {'paperId': '9ffc8f7b3fbd5e609f609b1c20206129f22b4eb7',\n",
       "    'title': 'CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning'},\n",
       "   {'paperId': '3b873d954a0f0f8d50cdeb500c105e993cf2a424',\n",
       "    'title': 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection'},\n",
       "   {'paperId': '8745c5b9522c11818418f64fdc880894faeaed16',\n",
       "    'title': 'A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '91cac43160ca45e5a1a41e0c5b7e6ec5a74033b3',\n",
       "    'title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models'},\n",
       "   {'paperId': '0fd6c747b48526ba4abc05b4ae9260f93718ce8f',\n",
       "    'title': 'Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '11c59fcc2c928615faa9edff7d86906ee750f5d8',\n",
       "    'title': 'StructDiffusion: Object-Centric Diffusion for Semantic Rearrangement of Novel Objects'},\n",
       "   {'paperId': '03d6484cbe5cc6e8c15c9271bfb29211671a6f56',\n",
       "    'title': 'StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'fe123b27a3ecc63c7087cb0d3a04e43790661c2d',\n",
       "    'title': 'LAD: Language Augmented Diffusion for Reinforcement Learning'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '706f9da5bde61a24a98336dc1b9dce2e4f81a21c',\n",
       "    'title': 'H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': '9b5f4aab169fba588e214c010345232053f8ae76',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': '60380ee913d20e722368245f23e0d4baf52e139a',\n",
       "    'title': 'A Policy-Guided Imitation Approach for Offline Reinforcement Learning'},\n",
       "   {'paperId': '8e16d556afd6fe7656a6acb991e5b7b47b21e074',\n",
       "    'title': 'Eliciting Compatible Demonstrations for Multi-Human Imitation Learning'},\n",
       "   {'paperId': 'ad4707bb87c6fc087e09d9f6609665b53835c899',\n",
       "    'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction'},\n",
       "   {'paperId': 'c0ca3ec9cf42c067a0123990137f6ee57980ab85',\n",
       "    'title': 'AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments'},\n",
       "   {'paperId': '0e34addae55a571d7efd3a5e2543e86dd7d41a83',\n",
       "    'title': 'Interactive Language: Talking to Robots in Real Time'},\n",
       "   {'paperId': '2627a1b899c61d3fc3712361ddd509889c4dece6',\n",
       "    'title': 'Learning High Speed Precision Table Tennis on a Physical Robot'},\n",
       "   {'paperId': '3230715347856f6f9c14567fbf28539e539691a0',\n",
       "    'title': 'Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning'},\n",
       "   {'paperId': '8f84dcbad8cd3b5b4d9229c56bc95f24be859a35',\n",
       "    'title': 'Grounding Language with Visual Affordances over Unstructured Data'},\n",
       "   {'paperId': '35f8eb09776abebad8a963b59978d673ab97301a',\n",
       "    'title': 'Understanding Hindsight Goal Relabeling from a Divergence Minimization Perspective'},\n",
       "   {'paperId': '82022e51d007c953ba02e01070c5e7a165377b59',\n",
       "    'title': 'Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': '682649e54113007724168c3920efe9a919f2d3d8',\n",
       "    'title': 'Instruction-driven history-aware policies for robotic manipulations'},\n",
       "   {'paperId': 'd1ad1bfa0bb76002b10e7f211b937842baeb28d9',\n",
       "    'title': 'Meta-Reinforcement Learning via Language Instructions'},\n",
       "   {'paperId': '57e62822d1f7ebba53aca99fd9b8d646df88e8bf',\n",
       "    'title': 'Task-Agnostic Learning to Accomplish New Tasks'},\n",
       "   {'paperId': 'fbb15aa7303586d25dc73f84c23f9b5447b0c06b',\n",
       "    'title': 'Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL'},\n",
       "   {'paperId': 'b83a17f7bd638e6b8b696624718f4371e0981025',\n",
       "    'title': 'BITS: Bi-level Imitation for Traffic Simulation'},\n",
       "   {'paperId': '748c9aa5a31f279fa07b84238aa5ba748e9df40d',\n",
       "    'title': 'Efficient Planning in a Compact Latent Action Space'},\n",
       "   {'paperId': 'ae40437fb4733c9c4364475fb7b86e9c35d147dc',\n",
       "    'title': 'Multi-goal Reinforcement Learning via Exploring Successor Matching'},\n",
       "   {'paperId': '42929aa6ebf8cdc0e7d7662751dc228de07800bb',\n",
       "    'title': 'Spectral Decomposition Representation for Reinforcement Learning'},\n",
       "   {'paperId': 'a86cdd50fc76d6d6641a66376697c61fb1228b59',\n",
       "    'title': 'Using Google Classroom to Increase Students Motivation to Learn English at SMP Negeri 1 Limboto Gorontalo'},\n",
       "   {'paperId': '1c33f76c6f182023a2eefb3bb46cba13fb2345a4',\n",
       "    'title': 'Learning Dynamic Manipulation Skills from Haptic-Play'},\n",
       "   {'paperId': '90ef1e2c1d0447f3be97cbbd4543d358584e3ab0',\n",
       "    'title': 'A Motion Capture and Imitation Learning Based Approach to Robot Control'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': 'a94aaf192fc1d46d697e4d7eb3e999021ec88b46',\n",
       "    'title': 'Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8',\n",
       "    'title': 'Behavior Transformers: Cloning k modes with one stone'},\n",
       "   {'paperId': '66abbce7cded7521f05943269a32ec5f874e5c56',\n",
       "    'title': 'ToolTango: Common sense Generalization in Predicting Sequential Tool Interactions for Robot Plan Synthesis'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '4247b93593b6ecd70262a1b1c7021dcecc26c8e0',\n",
       "    'title': 'Contrastive Learning as Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '59f91478c1f63b0fb5628bbd1af8267792708443',\n",
       "    'title': 'Intra-agent speech permits zero-shot task acquisition'},\n",
       "   {'paperId': 'cb3631f12b4465f4396380b61a651f0c74763480',\n",
       "    'title': \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via f-Advantage Regression\"},\n",
       "   {'paperId': '577181417b173a90cab43bee6b3951c346385180',\n",
       "    'title': 'Evaluating Multimodal Interactive Agents'},\n",
       "   {'paperId': '6c8ddb095dc34c3e292c689cd77ebbd348b54aaf',\n",
       "    'title': 'A Proprioceptive Haptic Device Design for Teaching Bimanual Manipulation'},\n",
       "   {'paperId': '216eecaee7d331cf3f4a40dc81ec6a21459e2898',\n",
       "    'title': 'SARI: Shared Autonomy across Repeated Interaction'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '81ebda8a734ff917681c6d73c179f6614b281818',\n",
       "    'title': 'Context-Aware Language Modeling for Goal-Oriented Dialogue Systems'},\n",
       "   {'paperId': '15ac70d077bb735eed4a8502ce49aa7782c803fd',\n",
       "    'title': 'What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data'},\n",
       "   {'paperId': 'f1a36b4283e45082183c36ae8f29a77a07f91abd',\n",
       "    'title': 'Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'c35831b0adef60e4497815fae077ff31137c73e1',\n",
       "    'title': 'Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation'},\n",
       "   {'paperId': 'd870142a600b0e9676854c7e2314ed09b624dbcb',\n",
       "    'title': 'Robot peels banana with goal-conditioned dual-action deep imitation learning'},\n",
       "   {'paperId': '32e6c81eeecea06d102f2dc8edcf8b5018ba1a80',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline RL'},\n",
       "   {'paperId': '4ca19525f996c9e60cca4f7e04ea76a4a1160a11',\n",
       "    'title': 'PLATO: Predicting Latent Affordances Through Object-Centric Play'},\n",
       "   {'paperId': '8d3fddfe59a21b8245b375d33fe91e215237ddb1',\n",
       "    'title': 'Affordance Learning from Play for Sample-Efficient Policy Learning'},\n",
       "   {'paperId': '1952fc35b919f149a4cc2647c114b7038593391c',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions from Language'},\n",
       "   {'paperId': '5ee839d965a1596298895ace7d003b98e165962c',\n",
       "    'title': 'Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates'},\n",
       "   {'paperId': '9fc6b6d3fead719cdc95263e62209548223576d1',\n",
       "    'title': 'It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum Generation'},\n",
       "   {'paperId': '7f712d58084e32ddc1b0cd60932f8bc0a0916330',\n",
       "    'title': 'Rethinking Goal-conditioned Supervised Learning and Its Connection to Offline RL'},\n",
       "   {'paperId': 'a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb',\n",
       "    'title': 'ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '63afc8d1a187d2f2faf603a51d3987db89574308',\n",
       "    'title': 'RvS: What is Essential for Offline RL via Supervised Learning?'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '4be02694125b71876552900a53c85c47a2a83614',\n",
       "    'title': 'CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks'},\n",
       "   {'paperId': '387a17823d7c47c0bd3390a124708933032989e0',\n",
       "    'title': 'Generalized Decision Transformer for Offline Hindsight Information Matching'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': 'fd399c7068512858b27535f75c8c31d2442dbaac',\n",
       "    'title': 'Towards More Generalizable One-shot Visual Imitation Learning'},\n",
       "   {'paperId': '52aeb38922f1f60ef4032012c70f9d5363547e03',\n",
       "    'title': 'C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': '59924d04b20a0c38dfb7f2bda3b7d62e41dcbb53',\n",
       "    'title': 'Learn Proportional Derivative Controllable Latent Space from Pixels'},\n",
       "   {'paperId': '0d2b7a2f9ac7823f34d45fcd06d738d6a5aa5b0d',\n",
       "    'title': 'Learning Periodic Tasks from Human Demonstrations'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '2c5056c6d7509ddbe567f11fdde70fd264aea89b',\n",
       "    'title': 'Geometric Task Networks: Learning Efficient and Explainable Skill Coordination for Object Manipulation'},\n",
       "   {'paperId': '4efda0ac574575fd935f318544c6e8cc4f81e617',\n",
       "    'title': 'UMPNet: Universal Manipulation Policy Network for Articulated Objects'},\n",
       "   {'paperId': '391c892f63bc072325064329c62ebd265a0d4b60',\n",
       "    'title': 'Universal Manipulation Policy Network for Articulated Objects'},\n",
       "   {'paperId': '04db65e53c8aa97af53e9944115d68c2bb381b5d',\n",
       "    'title': 'Playful Interactions for Representation Learning'},\n",
       "   {'paperId': 'a164aed0523d4f8e9375002fb2738e74d638b9eb',\n",
       "    'title': 'Learning latent actions to control assistive robots'},\n",
       "   {'paperId': '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "    'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans'},\n",
       "   {'paperId': 'd1da8fa082b16714780f63fa8275529d425bca6a',\n",
       "    'title': 'Imitation Learning: Progress, Taxonomies and Challenges.'},\n",
       "   {'paperId': 'a912cad9f9c7418240da2636c92c54df5d82006b',\n",
       "    'title': 'Training a Resilient Q-network against Observational Interference'},\n",
       "   {'paperId': '73e240cb95309142b61db3e9afd9282bf0b5464c',\n",
       "    'title': 'Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey'},\n",
       "   {'paperId': '6df5d79885bf1af65323a055684b2b92439a65c3',\n",
       "    'title': 'GRIMGEP: Learning Progress for Robust Goal Sampling in Visual Deep Reinforcement Learning'},\n",
       "   {'paperId': '4d4ea4b8cfcfcaee478a543298a090117daf5a60',\n",
       "    'title': 'G ENERALIZED D ECISION T RANSFORMER FOR O FFLINE H INDSIGHT I NFORMATION M ATCHING'},\n",
       "   {'paperId': '5ae0c18b522d4bb4ae08a4e18c89a645a63035eb',\n",
       "    'title': 'R ETHINKING G OAL -C ONDITIONED S UPERVISED L EARNING AND I TS C ONNECTION TO O FFLINE RL'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '66f747c41e6ef70806f9a859b5c1c01ec44ccd7b',\n",
       "    'title': 'Detecting Incorrect Visual Demonstrations for Improved Policy Learning'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': 'b6db3ffef26d59bb27b487b0dc7e43d6aaf75faf',\n",
       "    'title': 'Assisted Teleoperation for Scalable Robot Data Collection'},\n",
       "   {'paperId': '5840bf765be8c3bcedab63f43f5982ddba26eaf9',\n",
       "    'title': 'SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING'},\n",
       "   {'paperId': 'f367ce68505e01d0452fe4be113e27f7a98c9d6d',\n",
       "    'title': 'LAD: Language Augmented Diffusion for Reinforcement Learning'},\n",
       "   {'paperId': '99dff0f1591050f09fd471692b9b62a9b744508c',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning'},\n",
       "   {'paperId': '40c099d10be5a7ff429ee13f4c3f8b49142b1587',\n",
       "    'title': 'Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces'},\n",
       "   {'paperId': 'd93f1c42e232c94cdbc393eb6ef6f3429fce6ffc',\n",
       "    'title': 'Learning to Share Autonomy from Repeated Human-Robot Interaction'},\n",
       "   {'paperId': 'a2335853a24f48f3569344062b830f2d8d0dc6ad',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions'},\n",
       "   {'paperId': '76a80a2ad76f07b7cc1bcd9197f611a7201b5dcb',\n",
       "    'title': 'Representation learning of scene images for task and motion planning'},\n",
       "   {'paperId': 'b5e124ace60793321b82ff4243f505e5cc2ca1b1',\n",
       "    'title': 'An Adaptive Imitation Learning Framework for Robotic Complex Contact-Rich Insertion Tasks'},\n",
       "   {'paperId': '9d2038e233042790929123ff62354eb18ee52e47',\n",
       "    'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning'},\n",
       "   {'paperId': 'e1ed9c7976628a03b7e1d8ea0386c97c45c8ec2f',\n",
       "    'title': 'Learning to Execute: Efficient Learning of Universal Plan-Conditioned Policies in Robotics'},\n",
       "   {'paperId': '7b599fdcbe89b694f8314e8dd77a097000dedabb',\n",
       "    'title': 'Learning Multi-Stage Tasks with One Demonstration via Self-Replay'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': 'd825092cebfba0321e821d669e837fc03d5fb1f9',\n",
       "    'title': 'Contrastive Active Inference'},\n",
       "   {'paperId': '98d0821e7165c1f5e08123e46efea804dddfb577',\n",
       "    'title': 'Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization'},\n",
       "   {'paperId': 'dbabe6ce982b0d8b3f3a842ec85ddc088733385e',\n",
       "    'title': 'Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration'},\n",
       "   {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "    'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': '0770f93973a7e5f175a85387e7cd43c4ac95fb41',\n",
       "    'title': 'The Embodied Crossmodal Self Forms Language and Interaction: A Computational Cognitive Review'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': 'c66ce9e730aabac10b2bfb03bd4a4b487e4b4f43',\n",
       "    'title': 'Learning to Share Autonomy Across Repeated Interaction'},\n",
       "   {'paperId': '3e7044375e70afe3c314c59a5439b242aec08712',\n",
       "    'title': 'Imitation Learning: Progress, Taxonomies and Opportunities'},\n",
       "   {'paperId': '26d601215e16b7b69e3ee2f88282312ba4577519',\n",
       "    'title': 'Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning'},\n",
       "   {'paperId': 'adcae35901c36325478a03b647e14222a53ea9fc',\n",
       "    'title': 'What Can I Do Here? Learning New Skills by Imagining Visual Affordances'},\n",
       "   {'paperId': 'b0829f5c4ae98bcc00e54e1b50400f0523215204',\n",
       "    'title': 'Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input'},\n",
       "   {'paperId': '25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1',\n",
       "    'title': 'Episodic Transformer for Vision-and-Language Navigation'},\n",
       "   {'paperId': 'cfce1a0f780de7939b729ad85a04a8792fafd861',\n",
       "    'title': 'Visual Perspective Taking for Opponent Behavior Modeling'},\n",
       "   {'paperId': '4e1e3bdf43c1556279e4c043dbdf5571a159fed9',\n",
       "    'title': 'Learning Visually Guided Latent Actions for Assistive Teleoperation'},\n",
       "   {'paperId': '620e5a0a45e73f2fb3fa9e01049f2237fd50bbde',\n",
       "    'title': 'Receding-Horizon Perceptive Trajectory Optimization for Dynamic Legged Locomotion with Learned Initialization'},\n",
       "   {'paperId': '677b103eecc4d34e378502d60147456875e8741b',\n",
       "    'title': 'Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills'},\n",
       "   {'paperId': '8c5b53f67096f4af520feb9f8f43feafb2b331f6',\n",
       "    'title': 'RECON: Rapid Exploration for Open-World Navigation with Latent Goal Models'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': '5bdcd9e859a5168a601c488c2e8e27e463df3fea',\n",
       "    'title': 'Robot Program Parameter Inference via Differentiable Shadow Program Inversion'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '26cf79bce38d3981707aaa94ea8291ade77dedf9',\n",
       "    'title': 'Manipulator-Independent Representations for Visual Imitation'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': '54e1040e7f9f7d6b52abf03a1353b5a377802396',\n",
       "    'title': 'Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning'},\n",
       "   {'paperId': 'feefabd3fea5f1e35ab9552c2487dba71ade544d',\n",
       "    'title': 'Dances with Robots'},\n",
       "   {'paperId': '9956e3ea2b894f45ca9070ee1984caadb74edbf7',\n",
       "    'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation'},\n",
       "   {'paperId': '0a7f43c5d6aae0b590f1ecb6616181246a3bff89',\n",
       "    'title': 'Machine Learning for Robotic Manipulation'},\n",
       "   {'paperId': '9a689727d040a6bf122f16ea50884d5cd5258321',\n",
       "    'title': 'Model-Based Visual Planning with Self-Supervised Functional Distances'},\n",
       "   {'paperId': '05aaf561b205d394454b398124d29384ada990dc',\n",
       "    'title': 'Hierarchical Planning for Long-Horizon Manipulation with Geometric and Symbolic Scene Graphs'},\n",
       "   {'paperId': '7c1b75ab7bed79163d3d830a6a83a4023b496ebb',\n",
       "    'title': 'Self-supervised Visual Reinforcement Learning with Object-centric Representations'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'e77c0dcc5704c1c5de1ee091600ab6ca14b92784',\n",
       "    'title': 'C-Learning: Learning to Achieve Goals via Recursive Classification'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '72c034e53213cc2f4913d73dd838b64d7b641585',\n",
       "    'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning'},\n",
       "   {'paperId': '1213756da1b99702ab45b4745b6053365686f4ca',\n",
       "    'title': 'A development cycle for automated self-exploration of robot behaviors'},\n",
       "   {'paperId': 'b8b9e5a50e4ab778b886d22d84f9f891b637d404',\n",
       "    'title': 'Task-Oriented Motion Mapping on Robots of Various Configuration Using Body Role Division'},\n",
       "   {'paperId': '1301e9d11b728268ed1ff3f1a9adc155308d5250',\n",
       "    'title': 'Language Conditioned Imitation Learning Over Unstructured Data'},\n",
       "   {'paperId': '831e7cbafed2dca05db1e7f5ef16d1a7614f44ec',\n",
       "    'title': 'Learning to Reach Goals via Iterated Supervised Learning'},\n",
       "   {'paperId': 'f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6',\n",
       "    'title': 'A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms'},\n",
       "   {'paperId': 'fb9486b9398132dec86e2b3d9cdf586621c227af',\n",
       "    'title': 'Hybrid Trajectory and Force Learning of Complex Assembly Tasks: A Combined Learning Framework'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '7e4fc4634a020ac5ebfc98d179c8438c7ba3e567',\n",
       "    'title': 'ESportsU Digital Warrior Camp'},\n",
       "   {'paperId': '13c4a790dd099ded14d424df332b10195ad1ee14',\n",
       "    'title': 'Discovery and Learning of Navigation Goals from Pixels in Minecraft'},\n",
       "   {'paperId': 'f0f8f9a52b8130550fa2faeebe8d1595ef9c45fc',\n",
       "    'title': 'Intrinsically Motivated Exploration of Learned Goal Spaces'},\n",
       "   {'paperId': '643812d844e5a0e65fd6ca6e155f1e6e40c02bcb',\n",
       "    'title': 'SAFARI: Safe and Active Robot Imitation Learning with Imagination'},\n",
       "   {'paperId': '8d0eeb8aee3ce93c9c04f0662ee058e8eefee6bf',\n",
       "    'title': 'Transformers for One-Shot Visual Imitation'},\n",
       "   {'paperId': '1f3886cb0d17107345bea931b08a96192292a399',\n",
       "    'title': 'Preferential Experience Collection with Frequency based Intrinsic Reward for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'f3268665dd71fc2738f8bd86bb95dda36fc47bc9',\n",
       "    'title': 'Hindsight for Foresight: Unsupervised Structured Dynamics Models from Physical Interaction'},\n",
       "   {'paperId': 'dfca81607959eb7f101911288025a598bc5a6d18',\n",
       "    'title': 'Learning to Play by Imitating Humans'},\n",
       "   {'paperId': 'cf510c1fe613390f1e708a6725662f6aa6a85d18',\n",
       "    'title': 'ToolNet: Using Commonsense Generalization for Predicting Tool Use for Robot Plan Synthesis'},\n",
       "   {'paperId': '72db25e29524f84f29bc4598d72a3a60a2b348bd',\n",
       "    'title': 'Using Human Gaze to Improve Robustness Against Irrelevant Objects in Robot Manipulation Tasks'},\n",
       "   {'paperId': '8095bdd5861d1dbe43b77997bc0dbc2fd51acb93',\n",
       "    'title': 'Grounding Language in Play'},\n",
       "   {'paperId': 'f17adb5a8ee6cf94ea7d08808fa1af3af8e22dbd',\n",
       "    'title': 'Shared Autonomy with Learned Latent Actions'},\n",
       "   {'paperId': 'd99772c59a94242cd12a61c5b4e7224bbcfe7d90',\n",
       "    'title': 'TRASS: Time Reversal as Self-Supervision'},\n",
       "   {'paperId': 'f8174da83188b4227af2afd5f7ad84a0f31e70c3',\n",
       "    'title': 'Visual Prediction of Priors for Articulated Object Interaction'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': '2aedac799f1a822933ff1bd5b67d2f158f25a827',\n",
       "    'title': 'Datasets for Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': 'bbac680797af0f7ce4cdcc6430ff001fa0dfe670',\n",
       "    'title': 'Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations'},\n",
       "   {'paperId': '6402a8d82480f33924170c99ad759a120e5f7ac9',\n",
       "    'title': 'SQUIRL: Robust and Efficient Learning from Video Demonstration of Long-Horizon Robotic Manipulation Tasks'},\n",
       "   {'paperId': '55999400a3eed52ea9dd2f4b9f1b71ccb5c51238',\n",
       "    'title': 'Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement'},\n",
       "   {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "    'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       "   {'paperId': '5a8e9d47849596ffa1c023f6bd2a1a6f51df0603',\n",
       "    'title': 'Learning Mobile Manipulation through Deep Reinforcement Learning'},\n",
       "   {'paperId': '5e9764f45e7ea6206594deb94753a5cad4e31a1a',\n",
       "    'title': 'IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data'},\n",
       "   {'paperId': 'd413f75610ad07557f54e8145289b22cfa988e5b',\n",
       "    'title': 'DexPilot: Vision-Based Teleoperation of Dexterous Robotic Hand-Arm System'},\n",
       "   {'paperId': '4d5f904f923e5e031fb500a9e9ef7699ea9283de',\n",
       "    'title': 'The Differentiable Cross-Entropy Method'},\n",
       "   {'paperId': '3550ba30ba345e9e3011df32baf2cdf5e8578e9d',\n",
       "    'title': 'Controlling Assistive Robots with Learned Latent Actions'},\n",
       "   {'paperId': '4d40f7df809576d4db22b95c4ca9cc4c66e6928d',\n",
       "    'title': 'Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation'},\n",
       "   {'paperId': '6a5477c26b1c994646878aa06a43ddf1868fa667',\n",
       "    'title': 'Learning an Expert Skill-Space for Replanning Dynamic Quadruped Locomotion over Obstacles'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': 'f68564cbbbb969d247eba8dd0140861935695a20',\n",
       "    'title': 'SELF-SUPERVISED VISUAL REINFORCEMENT LEARN-'},\n",
       "   {'paperId': '1361f51fdc499c61f191859c9c2232590825a2c0',\n",
       "    'title': 'Offline Imitation Learning with a Misspecified Simulator'},\n",
       "   {'paperId': '7f7f381fe77d1b6b21e10a9c7136a99eb111893e',\n",
       "    'title': 'Learning Affordance Landscapes for Interaction Exploration in 3D Environments'},\n",
       "   {'paperId': '7a7a7847041e7b25febb1491d65d842a6c65927e',\n",
       "    'title': 'Training Agents using Upside-Down Reinforcement Learning'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': 'cf0c21194c897012a825ede9fda2601e0c5665c3',\n",
       "    'title': 'Reusable neural skill embeddings for vision-guided whole body movement and object manipulation'},\n",
       "   {'paperId': '553f52612a83d28434e837d83c0409a26eb9bb56',\n",
       "    'title': 'Situated GAIL: Multitask imitation using task-conditioned adversarial inverse reinforcement learning'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '3e519d85cdcefdd1d2ad89829d6ad445695d8c58',\n",
       "    'title': 'RoboNet: Large-Scale Multi-Robot Learning'},\n",
       "   {'paperId': '320b227027030fc291de2896fc3c6da49d7614be',\n",
       "    'title': \"Solving Rubik's Cube with a Robot Hand\"},\n",
       "   {'paperId': '1330e85a2c39501f46815a3bd42be62a516ef2e6',\n",
       "    'title': 'Trajectory adjustment for nonprehensile manipulation using latent space of trained sequence-to-sequence model*'},\n",
       "   {'paperId': 'fd045d0adb7149b89437112ce9b6fc31dbd72cf6',\n",
       "    'title': 'Virtual Reality Teleoperation of a Humanoid Robot Using Markerless Human Upper Body Pose Imitation'},\n",
       "   {'paperId': 'ffba26a38d4b3c25d2984266f72f72889b2413ff',\n",
       "    'title': 'Learning To Reach Goals Without Reinforcement Learning'},\n",
       "   {'paperId': '118672d5762c45b9469e74dbc43f0102b04717a6',\n",
       "    'title': 'On Multi-Agent Learning in Team Sports Games'},\n",
       "   {'paperId': 'e0889fcee1acd985af76a3907d5d0029bf260be9',\n",
       "    'title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning'},\n",
       "   {'paperId': '7b32459b8ebed74efed3d29fa6703fff855ea365',\n",
       "    'title': 'Task Focused Robotic Imitation Learning'},\n",
       "   {'paperId': '78745d33e99038bcdb9025d1af866a8f4deeb1b5',\n",
       "    'title': 'Time Reversal as Self-Supervision'}],\n",
       "  'citnuminlist': 15,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " '17704b148b5c20ddf92acbaf1addda134ecbb474': {'title': 'Learning Multi-Level Hierarchies with Hindsight',\n",
       "  'year': 2017,\n",
       "  'references': [{'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': 'bdc5a10aa5805808cfca58ac527ddc23e737bee8',\n",
       "    'title': 'Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining'},\n",
       "   {'paperId': '1c59bfa0e8654ebea94277064f82062875cae8b6',\n",
       "    'title': 'Identifying useful subgoals in reinforcement learning by local graph partitioning'},\n",
       "   {'paperId': '0da75f40c6faa755543118ecd625a39edfe292ad',\n",
       "    'title': 'Hierarchical reinforcement learning with subpolicies specializing for learned subgoals'},\n",
       "   {'paperId': 'dca9444e1c69eee36c0be04703d71114a762c84a',\n",
       "    'title': 'Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning'},\n",
       "   {'paperId': '03dbb37d2373500115735ed69871e94c7f6b2c5e',\n",
       "    'title': 'Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '985f2c1baba284e9b7b604b7169a2e2778540fe6',\n",
       "    'title': 'Temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '9aa1d909544fd9ffe061b84a90eb344ac303e6d9',\n",
       "    'title': 'The MAXQ Method for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': '481bd6ad5263dc818a32e6fbb5bd96b1159b33d5',\n",
       "    'title': 'Learning to generate sub-goals for action sequences'}],\n",
       "  'citations': [{'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "    'title': 'NetHack is Hard to Hack'},\n",
       "   {'paperId': '3e45d02b763cb8197369ac0b3ef4a16f4726de85',\n",
       "    'title': 'An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes'},\n",
       "   {'paperId': '4d0baf42944b6da5c297a39f283ed2e136331af8',\n",
       "    'title': 'Conditioning Hierarchical Reinforcement Learning on Flexible Constraints'},\n",
       "   {'paperId': 'c368238f147a547a57f6e603ac7d6aadbb8ab1b2',\n",
       "    'title': 'Learning Graph-Enhanced Commander-Executor for Multi-Agent Navigation'},\n",
       "   {'paperId': 'c1f040f51c0e247d1c520ee86295a38d94e613fd',\n",
       "    'title': 'Enviroment Representations with Bisimulation Metrics for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1e3fa20330096f54c15c21c0fe389238de08db5e',\n",
       "    'title': 'XDQN: Inherently Interpretable DQN through Mimicking'},\n",
       "   {'paperId': 'beb6cc3a60d1c511b869e4bb2c744cec99cd3ef1',\n",
       "    'title': 'Adjacency Constraint for Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '288e5a79e73742f9ead3bfa9463891717414f6fd',\n",
       "    'title': 'Addressing Hindsight Bias in Multigoal Reinforcement Learning'},\n",
       "   {'paperId': '387aed1f28d352fffa04e6d18c0ec46de6f8133e',\n",
       "    'title': 'Reinforcement Learning From Hierarchical Critics'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fb5ae414dc4c17870237332a178097823cb1e634',\n",
       "    'title': 'Efficient Hierarchical Exploration with An Active Subgoal Generation Strategy'},\n",
       "   {'paperId': '776975b39a1672dad9203d9bc469d05cbe44d35e',\n",
       "    'title': 'Multiple Subgoals-guided Hierarchical Learning in Robot Navigation'},\n",
       "   {'paperId': '8c3c18e2f5dec4475c338291b6aefa87b773b10e',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Multi Discount Factors In A Differential Game'},\n",
       "   {'paperId': '41e41f650fc1926eda019be894ad96ab56315860',\n",
       "    'title': 'CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control'},\n",
       "   {'paperId': 'ce0e769936453f827aee367e3463bb9915c6d78b',\n",
       "    'title': 'Emergency action termination for immediate reaction in hierarchical reinforcement learning'},\n",
       "   {'paperId': '6ea2aa7eb2e11b7c4512d8029d071d2e2a99bb80',\n",
       "    'title': 'Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'e48e12d51d6805d42f60a148133af6dab29b3a60',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Furniture Layout in Virtual Indoor Scenes'},\n",
       "   {'paperId': '9b5f4aab169fba588e214c010345232053f8ae76',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data'},\n",
       "   {'paperId': '22b816129ca770df3a88e76f754218b242df43f9',\n",
       "    'title': 'Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization'},\n",
       "   {'paperId': '0c0b7fb0066e8c1a5a2b4e4856135650eeef7702',\n",
       "    'title': 'Causality-driven Hierarchical Structure Discovery for Reinforcement Learning'},\n",
       "   {'paperId': '39069f6dd2054d316ef1913d29cc662979bb0ea2',\n",
       "    'title': 'DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "    'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '849dcd45264e90b6ff39e6d0f02f54c66aa49dcb',\n",
       "    'title': 'Knowledge-Grounded Reinforcement Learning'},\n",
       "   {'paperId': '45bf59416efb0389daaa0b179f28751d2965547d',\n",
       "    'title': 'Multi-Task Option Learning and Discovery for Stochastic Path Planning'},\n",
       "   {'paperId': 'c49755bb3aa858faed3e5edccae46a8e71ddc765',\n",
       "    'title': 'Consistent Experience Replay in High-Dimensional Continuous Control with Decayed Hindsights'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': 'cbb4d26ee59f5135b06fe0be1fa981e6f7ee3f32',\n",
       "    'title': 'Hierarchical End-to-end Control Policy for Multi-degree-of-freedom Manipulators'},\n",
       "   {'paperId': '2a88d01f3079e68ad9b5bcb1ebe56da25679e331',\n",
       "    'title': 'Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards'},\n",
       "   {'paperId': '45644c7f952d2a5a5b4e594998e2e6dff9088118',\n",
       "    'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '78ab35278d8cb36b88e5889cb20ba8576a93b2c1',\n",
       "    'title': 'HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble'},\n",
       "   {'paperId': 'a0d772f20fcfb5ba6faea34e77c2e77a91dea81c',\n",
       "    'title': 'Learning in Bi-level Markov Games'},\n",
       "   {'paperId': '7acb94be09e00a1b12d3dbe4021398c3cdf38a58',\n",
       "    'title': 'Searching Latent Sub-Goals in Hierarchical Reinforcement Learning as Riemannian Manifold Optimization'},\n",
       "   {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "    'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       "   {'paperId': 'dd22bdf302c25b834d87b61cb55be9d23c9c2940',\n",
       "    'title': 'Value Refinement Network (VRN)'},\n",
       "   {'paperId': '4247b93593b6ecd70262a1b1c7021dcecc26c8e0',\n",
       "    'title': 'Contrastive Learning as Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '66296def050725c18093351df0c80bc348552d9e',\n",
       "    'title': 'Efficiency-reinforced Learning with Auxiliary Depth Reconstruction for Autonomous Navigation of Mobile Devices'},\n",
       "   {'paperId': '804682f4b1e700ed1bf0d5d3a26552b2de77502e',\n",
       "    'title': 'All-aspect attack guidance law for agile missiles based on deep reinforcement learning'},\n",
       "   {'paperId': '22fb881bdcdd346cc02c8704b9f54d00a220a7e9',\n",
       "    'title': 'Hierarchies of Reward Machines'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'd7fe629900764feb2aa41d49d9655a2771754415',\n",
       "    'title': 'A Hierarchical Goal-Biased Curriculum for Training Reinforcement Learning'},\n",
       "   {'paperId': '363ce160338956fd4c1df51d4d98a501fbde66cd',\n",
       "    'title': 'Hierarchical Decompositions of Stochastic Pursuit-Evasion Games'},\n",
       "   {'paperId': '44a0427036c297534d7faae89cbad70a0188e16d',\n",
       "    'title': 'Learning Generalized Policy Automata for Relational Stochastic Shortest Path Problems'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': '03fea73b0cc7cd13f4148a4ab49d670f6ddda930',\n",
       "    'title': 'Hierarchical Reinforcement Learning under Mixed Observability'},\n",
       "   {'paperId': 'baace5e9ecb020fa3202d0405b7165c9f9002934',\n",
       "    'title': 'Hierarchical intrinsically motivated agent planning behavior with dreaming in grid environments'},\n",
       "   {'paperId': 'f1a36b4283e45082183c36ae8f29a77a07f91abd',\n",
       "    'title': 'Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': '2a0792d30820411529a2a0477e8efb9f78c3112d',\n",
       "    'title': 'Hierarchical Reinforcement Learning of Locomotion Policies in Response to Approaching Objects: A Preliminary Study'},\n",
       "   {'paperId': '47f28e1057eac8f7ec99ac8250e4bfed0eafd9e9',\n",
       "    'title': 'Unified curiosity-Driven learning with smoothed intrinsic reward estimation'},\n",
       "   {'paperId': '5ee839d965a1596298895ace7d003b98e165962c',\n",
       "    'title': 'Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates'},\n",
       "   {'paperId': 'ab2542ae894b0834494c968e14f96bfd7908aa90',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Adversarially Guided Subgoals'},\n",
       "   {'paperId': '64290435a11eee0b4ea5c23cb9937d1e0af957e6',\n",
       "    'title': 'Adversarially Guided Subgoal Generation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c397a67aad5bd5d08364135ac01b028af930c604',\n",
       "    'title': 'State-Conditioned Adversarial Subgoal Generation'},\n",
       "   {'paperId': '5b97c0c6e075d8f6e8c61bdc20e03ef048e5c762',\n",
       "    'title': 'Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': '77ff294e3c9d705f252cdb803de02a8bff6bd831',\n",
       "    'title': 'Sensor-Based Navigation Using Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c0ee9dc3f231db21f8d12401df6709e6f318c22b',\n",
       "    'title': 'End-to-End Hierarchical Reinforcement Learning With Integrated Subgoal Discovery'},\n",
       "   {'paperId': '69f3cbf5c349c91c45bb3039736125a35fcda664',\n",
       "    'title': 'Verifiable and Compositional Reinforcement Learning Systems'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '167b5d14442024f3af48ae4e79c6fb41ac87b42b',\n",
       "    'title': 'Active Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': 'ae25c0c7d3e2de1c90346079e53b9e3e836c7de4',\n",
       "    'title': 'Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'c7b01136da36d68e229f268e5489c81c94586481',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation'},\n",
       "   {'paperId': '9faecf3e18a833f2d49b030d591cc2ded0b54336',\n",
       "    'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives'},\n",
       "   {'paperId': 'a4e1cf41401674c40174fa8d6073d2b3e9be78d6',\n",
       "    'title': 'What about Inputting Policy in Value Function: Policy Representation and Policy-Extended Value Function Approximator'},\n",
       "   {'paperId': 'a026fb990836c9c36f0d24bc3d079656938f5193',\n",
       "    'title': 'Curriculum Learning with Hindsight Experience Replay for Sequential Object Manipulation Tasks'},\n",
       "   {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "    'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       "   {'paperId': '46ef377684b14e0cf9d61351f5a46c458dddae0e',\n",
       "    'title': 'A Variable Clock Underlies Internally Generated Hippocampal Sequences'},\n",
       "   {'paperId': '16b4ff5a2281d8aedc5b18d90ca7d754c878a9fe',\n",
       "    'title': 'Hierarchical Actor-Critic Exploration with Synchronized, Adversarial, & Knowledge-Based Actions'},\n",
       "   {'paperId': 'f56edacf53e7803c5fee527c10ae076ca41eb8ed',\n",
       "    'title': 'H IERARCHICAL K ICKSTARTING FOR S KILL T RANSFER IN R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '474c0bc346a7d13dc513a5038d860f843baf473d',\n",
       "    'title': 'S KILL H ACK : A B ENCHMARK FOR S KILL T RANSFER IN O PEN -E NDED R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '82bf393bde761760cf0aab6bbe081408ebcb1958',\n",
       "    'title': 'Hierarchical reinforcement learning based energy management strategy for hybrid electric vehicle'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '44f4975b026c7c9ab934ead385afae080b20d66b',\n",
       "    'title': 'SelectAugment: Hierarchical Deterministic Sample Selection for Data Augmentation'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': '9cd80a501ea586086f614f08667e7ed7533a1204',\n",
       "    'title': 'From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence'},\n",
       "   {'paperId': '27bc680bf6a115cc3f28c4da462b6d25cf04cb09',\n",
       "    'title': 'Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e643eb4ec5afd45fca31cb6d4259034042268064',\n",
       "    'title': 'Provable Hierarchy-Based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '0ed02862eaa63e028ce1a747475975d0dfde513f',\n",
       "    'title': 'Design Strategy Network: A deep hierarchical framework to represent generative design strategies in complex action spaces'},\n",
       "   {'paperId': 'c2ec144b633e2dcbf889da95c711b483803e8350',\n",
       "    'title': 'Hierarchical learning from human preferences and curiosity'},\n",
       "   {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "    'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       "   {'paperId': 'a8262348003a9d93ea7ceffc887729cf88dedf06',\n",
       "    'title': 'Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms'},\n",
       "   {'paperId': '3bfc3648c66cd923cb3a740fed945eeed9c1e313',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Sensor-Based Navigation'},\n",
       "   {'paperId': '339dcc86e2e9b40d7b9190c37d7a6ead2a02e2dd',\n",
       "    'title': 'Risk Conditioned Neural Motion Planning'},\n",
       "   {'paperId': '3fb2a845177ccdee15e5642951a81c9e5c679207',\n",
       "    'title': 'A Multi-Dimensional Goal Aircraft Guidance Approach Based on Reinforcement Learning with a Reward Shaping Algorithm'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '291a42e844b9bd38c0341c6beec9b34aba3ddde8',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model'},\n",
       "   {'paperId': 'fb95d6e6e5f78f6e5c339e2058ce9ae9e803182b',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning with Imagined Subgoals'},\n",
       "   {'paperId': '1012fd16c3ca62033a712140f8b66d4533f95c3c',\n",
       "    'title': 'Lifetime policy reuse and the importance of task capacity'},\n",
       "   {'paperId': 'e9094e2a33af929d67793d480fffa6836b1bfe07',\n",
       "    'title': 'Efficient Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': 'd93b090bd7eb49fac0c87ca1761077a86de059d4',\n",
       "    'title': 'Hierarchies of Planning and Reinforcement Learning for Robot Navigation'},\n",
       "   {'paperId': 'f8677102449c803512c87b240b3989e4e9276cbc',\n",
       "    'title': 'Toward next-generation learned robot manipulation'},\n",
       "   {'paperId': '7a5be21149843fbe210f88ea368888a381f6a301',\n",
       "    'title': 'Outcome-Driven Reinforcement Learning via Variational Inference'},\n",
       "   {'paperId': 'dac43044fef83b0fca1724675ff4baa78a4baed2',\n",
       "    'title': 'TASAC: Temporally Abstract Soft Actor-Critic for Continuous Control'},\n",
       "   {'paperId': '1508879dae81f73f56ba0cb0e25150d9c5f8f731',\n",
       "    'title': 'TAAC: Temporally Abstract Actor-Critic for Continuous Control'},\n",
       "   {'paperId': 'aa8441031608c8417260ce6d15900afa6e68089d',\n",
       "    'title': 'Hierarchical and Partially Observable Goal-driven Policy Learning with Goals Relational Graph'},\n",
       "   {'paperId': '69a26d873c1e279bb950f1d984bf9375ffc8a79c',\n",
       "    'title': 'Computational Modeling of Emotion-Motivated Decisions for Continuous Control of Mobile Robots'},\n",
       "   {'paperId': 'd9926dc56bd67f2f1b3f9caf18e53183cb3499ac',\n",
       "    'title': 'Reinforcement Learning For Data Poisoning on Graph Neural Networks'},\n",
       "   {'paperId': 'c15fd6d1f3599808f1ba4ff4803447e2898c3937',\n",
       "    'title': 'Twin Delayed Hierarchical Actor-Critic'},\n",
       "   {'paperId': '02b2b995a10524dd74678b75cecf44b4aeacaedd',\n",
       "    'title': 'Stay Alive with Many Options: A Reinforcement Learning Approach for Autonomous Navigation'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation'},\n",
       "   {'paperId': '8b97d8eca6df71e12dbc7a1b31f71e8eae14e278',\n",
       "    'title': 'An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor Robot Navigation'},\n",
       "   {'paperId': '31a95c2f8e128cef583f4dad18d65feebd47b50d',\n",
       "    'title': 'Moving Forward in Formation: A Decentralized Hierarchical Learning Approach to Multi-Agent Moving Together'},\n",
       "   {'paperId': '0d44b0b683d9d55ed7c5a17a6b94141a6fc7d147',\n",
       "    'title': 'Efficient Robotic Object Search Via HIEM: Hierarchical Policy Learning With Intrinsic-Extrinsic Modeling'},\n",
       "   {'paperId': 'c3f5dd6ce62b5fba652bf593e498f1c80b06083e',\n",
       "    'title': 'MAP Propagation Algorithm: Faster Learning with a Team of Reinforcement Learning Agents'},\n",
       "   {'paperId': '5764095b0186a3fc3832c1052aa14996a5927edc',\n",
       "    'title': 'RODE: Learning Roles to Decompose Multi-Agent Tasks'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': 'c88c99fc89a32883384b5a629a8905504e42ac72',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning'},\n",
       "   {'paperId': 'f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6',\n",
       "    'title': 'A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms'},\n",
       "   {'paperId': 'ec1022b679a402f5bb8c8377811cab5ff7cae68d',\n",
       "    'title': 'L EARNING S UBGOAL R EPRESENTATIONS WITH S LOW D YNAMICS'},\n",
       "   {'paperId': '9dcf49ac50b69e30df5e332658dd31245d44865d',\n",
       "    'title': 'MEng Individual Project State-space decomposition for Reinforcement Learning'},\n",
       "   {'paperId': 'e9810e7ae50715760eaa66f3fcfc1c8d0fc2e26a',\n",
       "    'title': 'Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '79a90e7378d4b4a9e3ee9c70973e1bf61816a533',\n",
       "    'title': 'Goal Modelling for Deep Reinforcement Learning Agents'},\n",
       "   {'paperId': '3ba5343e8945f26c161bb512bf3d7ea5d4434f32',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Universal Policies for Multi-Step Robotic Manipulation'},\n",
       "   {'paperId': 'f4fd4a5a0901675e19bb4054d8861d2697194fd5',\n",
       "    'title': 'Deep Reinforcement Learning: A State-of-the-Art Walkthrough'},\n",
       "   {'paperId': '0cc33ee14a76f7964ee528950baa0981934e899a',\n",
       "    'title': 'Are We On The Same Page? Hierarchical Explanation Generation for Planning Tasks in Human-Robot Teaming using Reinforcement Learning'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': 'b9e4bccd2e69132af961fceeb51d418ee106e3ff',\n",
       "    'title': 'Active Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'f4a917986f02a6c7581eb18dbd2e568bf8005b61',\n",
       "    'title': 'Automatic Curriculum Generation by Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1fcba5d6c94d464af11243b3b4fc8f0f0f0736eb',\n",
       "    'title': 'HILONet: Hierarchical Imitation Learning from Non-Aligned Observations'},\n",
       "   {'paperId': '2a11fcc744c89cf49e5a983330f6ac32f191a3b2',\n",
       "    'title': 'Real-Time Object Navigation With Deep Neural Networks and Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '587ef257f8c9b7a70ce3c42b71fcf5206b58c9ae',\n",
       "    'title': 'Represent Your Own Policies : Reinforcement Learning with Policy-extended Value Function Approximator'},\n",
       "   {'paperId': '55d296154f3d6518e93ccf3fdd4f691077b5ceda',\n",
       "    'title': 'What About Taking Policy as Input of Value Function: Policy-extended Value Function Approximator'},\n",
       "   {'paperId': '63dbf95306acb4f17b6cfd8fa327b98289640b6e',\n",
       "    'title': 'An Alternative to Backpropagation in Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e3450150135b97ad379cc35fe44afde021c37d44',\n",
       "    'title': 'Deep Reinforcement Learning with Temporal Logics'},\n",
       "   {'paperId': '1e5c15cd300aa79a1d65880af528f2655473508e',\n",
       "    'title': 'Hierarchial Reinforcement Learning in StarCraft II with Human Expertise in Subgoals Selection'},\n",
       "   {'paperId': '114b52291b549466a4b1027f4248a122c1c3920c',\n",
       "    'title': 'Learning Compositional Neural Programs for Continuous Control'},\n",
       "   {'paperId': '361ccae6cb40343c8824c9d64104ff8261a7c089',\n",
       "    'title': 'Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '7a9d14cf9f22e624cc9f850881a0a924de56a7a9',\n",
       "    'title': 'Value Preserving State-Action Abstractions'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': '94d02cb4a0901f4f336ffa939f6b9991f287948c',\n",
       "    'title': 'Curious Hierarchical Actor-Critic Reinforcement Learning'},\n",
       "   {'paperId': 'e1a39a6614503546bbb72a8c75aaf0ae93a3ac01',\n",
       "    'title': 'Option Discovery using Deep Skill Chaining'},\n",
       "   {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "    'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'a8482deaddfcd27b6c38aceafa1cc7063a50a445',\n",
       "    'title': 'Constructing Abstraction Hierarchies for Robust, Real-Time Control'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': '075692902b221544d582f748855ce5fae808f83c',\n",
       "    'title': 'Towards Intelligent Pick and Place Assembly of Individualized Products Using Reinforcement Learning'},\n",
       "   {'paperId': '7b0871c783e721bfbf9b5d16e575130a07a672cd',\n",
       "    'title': 'Generalized Hindsight for Reinforcement Learning'},\n",
       "   {'paperId': '1b3738ea2122de7bc6e650443c5f018b65cd5a99',\n",
       "    'title': 'Parameter Sharing in Coagent Networks'},\n",
       "   {'paperId': 'a42744f40650673b910a6fa925a217491a074ed9',\n",
       "    'title': 'On Simple Reactive Neural Networks for Behaviour-Based Reinforcement Learning'},\n",
       "   {'paperId': 'dc38805003827a16da53299969b323114dc6707a',\n",
       "    'title': 'Planning with Abstract Learned Models While Learning Transferable Subtasks'},\n",
       "   {'paperId': '33b459027bd696fc7a7aba5e5cc85c440ad1cd2f',\n",
       "    'title': 'MGHRL: Meta Goal-Generation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3169f9cdc12d53b072459e0b7886b7322a5c6cfc',\n",
       "    'title': 'EXPLAINABLE REINFORCEMENT LEARNING THROUGH GOAL-BASED EXPLANATIONS'},\n",
       "   {'paperId': '4a469b594a1525a2c7d260570eae46d78690b171',\n",
       "    'title': 'Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24–27, 2020, Proceedings'},\n",
       "   {'paperId': 'ed63612ab053b384c01d22e77da727928e652807',\n",
       "    'title': 'CURRICULUM LEARNING WITH HINDSIGHT EXPERIENCE REPLAY FOR SEQUENTIAL OBJECT MANIPULATION TASKS'},\n",
       "   {'paperId': 'd802b949ff627d51fcb148685256f9fd25f848d5',\n",
       "    'title': 'Hindsight Planner'},\n",
       "   {'paperId': '1d0e22458e1dd3021ee27555d83bcaa007c029b3',\n",
       "    'title': 'Mastering Basketball With Deep Reinforcement Learning: An Integrated Curriculum Training Approach'},\n",
       "   {'paperId': 'edd581cfab92dee74b86509ab7cd668cdbbbdcdc',\n",
       "    'title': 'Optimization of Dynamic Dispatch for Multiarea Integrated Energy System Based on Hierarchical Learning Method'},\n",
       "   {'paperId': 'd9e14b81e7acf2e17de20df113018943978509ae',\n",
       "    'title': 'Inter-Level Cooperation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9fec0fe8ea164c659412b611f420149496ccd237',\n",
       "    'title': 'Automatic Composite Action Discovery for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '7b4848bad51ebd38fb068e73abc3c6d865fd692f',\n",
       "    'title': 'Planning with Goal-Conditioned Policies'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       "   {'paperId': 'e8faa4fa0909db69cc4039af43c1ffe921ca7977',\n",
       "    'title': 'Hierarchical Actor-Critic with Hindsight for Mobile Robot with Continuous State Space'},\n",
       "   {'paperId': '4c52f87830d6c0f9d7e61defa695bd65a8aca067',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks'},\n",
       "   {'paperId': '7e522be5f714f4f0d56b808a318a66eb206c3968',\n",
       "    'title': 'Efficient meta reinforcement learning via meta goal generation'},\n",
       "   {'paperId': '45ac74350d21387c42ff92e90cd088bf310e1542',\n",
       "    'title': 'Guided goal generation for hindsight multi-goal reinforcement learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '5a22ce57b02c8aa446c793435a2235bbe6afbc65',\n",
       "    'title': 'Exploration via Hindsight Goal Generation'},\n",
       "   {'paperId': 'd8886451bd6f3dbc48f98107901220cd845a7ea6',\n",
       "    'title': 'Learning Navigation Subroutines from Egocentric Videos'},\n",
       "   {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "    'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       "   {'paperId': 'af1dbd44d60e42b52f7629099376e1eaed3685ed',\n",
       "    'title': 'From Semantics to Execution: Integrating Action Planning With Reinforcement Learning for Robotic Causal Problem-Solving'},\n",
       "   {'paperId': 'e15db5ed7bcf4dd6b6fe77978b73dacde85d5001',\n",
       "    'title': 'From semantics to execution: Integrating action planning with reinforcement learning for robotic tool use'},\n",
       "   {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "    'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a4fdfea4a0810303a484b6eeab56cbb2a3bccf7f',\n",
       "    'title': 'META REINFORCEMENT LEARNING VIA META GOAL GENERATION'},\n",
       "   {'paperId': '3fcd4cd7e7af3ac266ea95336ce87c74f8811871',\n",
       "    'title': 'Policy Continuation with Hindsight Inverse Dynamics'},\n",
       "   {'paperId': 'f52449c24b55852bff87205f50d30339166ba8df',\n",
       "    'title': 'Policy Continuation and Policy Evolution with Hindsight Inverse Dynamics'},\n",
       "   {'paperId': '4c19fcb0f8c041996c7cbb0deab15fbec5d1b5d9',\n",
       "    'title': 'Meta-Learning via Weighted Gradient Update'},\n",
       "   {'paperId': '98b41528c58e6f5b7b28be5b54029e52ca90c4ab',\n",
       "    'title': 'Learning to Learn: Hierarchical Meta-Critic Networks'},\n",
       "   {'paperId': '77550f428249ce7bbd68adc78a74b6bd73ef389a',\n",
       "    'title': 'SKILL DISCOVERY WITH WELL-DEFINED OBJECTIVES'},\n",
       "   {'paperId': '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85',\n",
       "    'title': 'Learning Abstract Options'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'}],\n",
       "  'citnuminlist': 6,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " '7aea82f3b7726b0bd3bb3931dff10c93d1907abf': {'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca',\n",
       "    'title': 'Neural probabilistic motor primitives for humanoid control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '7cccc3b6c6d50bc9b75781573cb065e7b758c931',\n",
       "    'title': 'Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control'},\n",
       "   {'paperId': '304833b01820f8e23d73b088a56208dd84209f67',\n",
       "    'title': 'Riemannian Motion Policies'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': 'c27db32efa8137cbf654902f8f728f338e55cd1c',\n",
       "    'title': 'Mastering the game of Go without human knowledge'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'e822b8ea156649133b0a9ae3670535f49bd53605',\n",
       "    'title': 'Learning to Schedule Control Fragments for Physics-Based Characters Using Deep Q-Learning'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5151d6cb3a4eaec14a56944d58338251fca344ab',\n",
       "    'title': 'Overcoming catastrophic forgetting in neural networks'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '505808c55b2d96ad72f4b7bca04572655742b87d',\n",
       "    'title': 'Sim-to-Real Robot Learning from Pixels with Progressive Nets'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "    'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'bc69708aeaae562ab1406ca7dd0e50c1ec247635',\n",
       "    'title': 'Terrain-adaptive locomotion skills using deep reinforcement learning'},\n",
       "   {'paperId': 'b23c97e59f44bc88121c65d1ac41f2cbddcefbd2',\n",
       "    'title': 'Deep Reinforcement Learning in Parameterized Action Space'},\n",
       "   {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "    'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       "   {'paperId': '223af46e6193561b406dd4956b6df3087c502349',\n",
       "    'title': 'Overcoming catastrophic forgetting in neural networks'},\n",
       "   {'paperId': '9f7d7dc88794d28865f28d7bba3858c81bdbc3db',\n",
       "    'title': 'Deep Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': None,\n",
       "    'title': ', James Kirkpatrick , Koray Kavukcuoglu , Razvan Pascanu , and Raia Hadsell . Progressive neural networks'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'b8de958fead0d8a9619b55c7299df3257c624a96',\n",
       "    'title': 'DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition'},\n",
       "   {'paperId': '3a81cfb4a7a880b7cf8979f6067732e961aceb7c',\n",
       "    'title': 'Probabilistic Movement Primitives'},\n",
       "   {'paperId': '21603ea83e7506abe9ad8ae1da0b85764c08228d',\n",
       "    'title': 'Robust task-based control policies for physics-based characters'},\n",
       "   {'paperId': '71d0ac0b5bcda045551f86fff971096b5b11525d',\n",
       "    'title': 'Learning complex motions by sequencing simpler motion templates'},\n",
       "   {'paperId': '221472c2fa00a5f7ff5400c60ffddc928e50cab8',\n",
       "    'title': 'SIMBICON: simple biped locomotion control'},\n",
       "   {'paperId': '64a9fea05ff78283f05f396f2ce68e8ee7ccbf2c',\n",
       "    'title': 'Learning omnidirectional path following using dimensionality reduction'},\n",
       "   {'paperId': 'dbb3342599c9b431a3152a0d5c813d3e56967a27',\n",
       "    'title': 'Multi-Task Feature Learning'},\n",
       "   {'paperId': '46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e',\n",
       "    'title': 'Reducing the Dimensionality of Data with Neural Networks'},\n",
       "   {'paperId': '161ffb54a3fdf0715b198bb57bd22f910242eb49',\n",
       "    'title': 'Multitask Learning'},\n",
       "   {'paperId': 'c9a54c513552c88ea815232a4f377662c8e24ad8',\n",
       "    'title': 'Composable controllers for physics-based character animation'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'b1362879e77efef96ab552f5cb1198c2a67204d6',\n",
       "    'title': 'Introduction to Reinforcement Learning'},\n",
       "   {'paperId': '371c9dc680e916f79d9c78fcf6c894a2dd299095',\n",
       "    'title': 'Is Learning The n-th Thing Any Easier Than Learning The First?'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': 'c8d90974c3f3b40fa05e322df2905fc16204aa56',\n",
       "    'title': 'Adaptive Mixtures of Local Experts'},\n",
       "   {'paperId': None,\n",
       "    'title': 'George van den Driessche, Thore Graepel, and Demis Hassabis'},\n",
       "   {'paperId': None, 'title': 'Sfu motion capture database'}],\n",
       "  'citations': [{'paperId': '67722409a74bf8a61b370a6fd5a524b279e2b6f4',\n",
       "    'title': 'Simulation and Retargeting of Complex Multi-Character Interactions'},\n",
       "   {'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': 'e151e836bfa2ccb0f4eea6895b560c19be4a3d5b',\n",
       "    'title': 'ACE: Adversarial Correspondence Embedding for Cross Morphology Motion Retargeting from Human to Nonhuman Characters'},\n",
       "   {'paperId': '7a993a0aa4929754bc80070e4e1f96d56fd2543d',\n",
       "    'title': 'Perpetual Humanoid Control for Real-time Simulated Avatars'},\n",
       "   {'paperId': 'ca6c9837ef0e3728e72fa6a5257880e2dd042a3c',\n",
       "    'title': 'Composite Motion Learning with Task Control'},\n",
       "   {'paperId': '98d98c4cbcb0c6d413bc3bdb1a1052542ac636b2',\n",
       "    'title': 'Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement Learning'},\n",
       "   {'paperId': '9c6f0ca7ff6555f308fdb6235bf0dd11ec091929',\n",
       "    'title': 'DiffMimic: Efficient Motion Mimicking with Differentiable Physics'},\n",
       "   {'paperId': '3e2cb09e9bd733e9268696e216d8d7b56ebc5853',\n",
       "    'title': 'DribbleBot: Dynamic Legged Manipulation in the Wild'},\n",
       "   {'paperId': '8bd4a524f7cbaa27344b747ace152ac88a7faf39',\n",
       "    'title': 'Multi-Task Reinforcement Learning in Continuous Control with Successor Feature-Based Concurrent Composition'},\n",
       "   {'paperId': '45a37ee40248d845e608c336edecb288a04ba044',\n",
       "    'title': 'Compositionality and Bounds for Optimal Value Functions in Reinforcement Learning'},\n",
       "   {'paperId': '244aecf3afb80c3ad936c1a6d8d7f3677bb18f33',\n",
       "    'title': 'Synthesizing Physical Character-Scene Interactions'},\n",
       "   {'paperId': 'f298492d9b51c713216bec7500997e43fb37f4c6',\n",
       "    'title': 'A Constrained-Optimization Approach to the Execution of Prioritized Stacks of Learned Multi-Robot Tasks'},\n",
       "   {'paperId': '6d7af0617b0fc390baddb93816975f1bdb1985cd',\n",
       "    'title': 'Unsupervised Learning of Temporal Abstractions With Slot-Based Transformers'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       "    'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors'},\n",
       "   {'paperId': 'db8d70d9da6b957a00ec7e8cc67493340c39aa29',\n",
       "    'title': 'Policy Transfer via Skill Adaptation and Composition'},\n",
       "   {'paperId': '3098fd3d604086202f51aee3b5e6071639908be1',\n",
       "    'title': 'Utilizing Prior Solutions for Reward Shaping and Composition in Entropy-Regularized Reinforcement Learning'},\n",
       "   {'paperId': '39f4e1d98864dc551eb16b2d8e134141a26f28fc',\n",
       "    'title': 'Synthesizing Get‐Up Motions for Physics‐based Characters'},\n",
       "   {'paperId': 'e3358af35dbe44867da4d0ca43baee172c7d2273',\n",
       "    'title': 'Theoretical Research on the Training Mode of Fashion Design Talents Under the Background of Internet +'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '3e437b8b131fca7e3fa64a305e1bb43087ccaf9a',\n",
       "    'title': 'Learning High-Risk High-Precision Motion Control'},\n",
       "   {'paperId': '65441ccf98e2055264b1e30e5ab11fbf99a49705',\n",
       "    'title': 'Efficient Multi-Task Learning via Iterated Single-Task Transfer'},\n",
       "   {'paperId': 'a5978dc8e57c27966d218a06e4ccc8aa3a507507',\n",
       "    'title': 'ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters'},\n",
       "   {'paperId': '849dcd45264e90b6ff39e6d0f02f54c66aa49dcb',\n",
       "    'title': 'Knowledge-Grounded Reinforcement Learning'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'd9c0c3eea7a2581e434b4c9d58ce405638f7dbd5',\n",
       "    'title': 'Multi-expert synthesis for versatile locomotion and manipulation skills'},\n",
       "   {'paperId': '6239a83689d30f769c09c874003cd843750343b1',\n",
       "    'title': 'QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars'},\n",
       "   {'paperId': '5d925a022c6e20654ffcf1177e46ac5454593ea9',\n",
       "    'title': 'D&D: Learning Human Dynamics from Dynamic Camera'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': 'ddb0acda05adf6ae3bc4763575dde39423e8ef6c',\n",
       "    'title': 'Neural3Points: Learning to Generate Physically Realistic Full‐body Motion for Virtual Reality Users'},\n",
       "   {'paperId': '3e38ccc5ba9a4b4290060716614e02b7ba54df79',\n",
       "    'title': 'MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control'},\n",
       "   {'paperId': '1c33f76c6f182023a2eefb3bb46cba13fb2345a4',\n",
       "    'title': 'Learning Dynamic Manipulation Skills from Haptic-Play'},\n",
       "   {'paperId': '0b063435ec5f8a5d4f4e75306240e7e0b4facc4c',\n",
       "    'title': 'Learning Soccer Juggling Skills with Layer-wise Mixture-of-Experts'},\n",
       "   {'paperId': '9337d750993d8715c872db8d406480d58464555a',\n",
       "    'title': 'How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition'},\n",
       "   {'paperId': '6a88b9241025357154479687a94d791a3204c958',\n",
       "    'title': 'Physics-based character controllers using conditional VAEs'},\n",
       "   {'paperId': '8319a62ed2168196d794e6166e40b6de7ddba346',\n",
       "    'title': 'Embodied Scene-aware Human Pose Estimation'},\n",
       "   {'paperId': '16b242611c0e5986fb695c0278de6840170ee0dc',\n",
       "    'title': 'Lifelong learning for robust AI systems'},\n",
       "   {'paperId': '680a374378f2c1a3f40a58bc3fa4da1a1b678924',\n",
       "    'title': 'Learning to Use Chopsticks in Diverse Styles'},\n",
       "   {'paperId': '0417d220540173987693e6d9707d34cf204bc025',\n",
       "    'title': 'Learning to use chopsticks in diverse gripping styles'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'ddedebc48e8c76aec6db486e79363fd4c823650e',\n",
       "    'title': 'Skill Machines: Temporal Logic Composition in Reinforcement Learning'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '46bd3c20e6f7a9b6e413ea9f3452965601fd6de9',\n",
       "    'title': 'Unsupervised Reinforcement Learning for Transferable Manipulation Skill Discovery'},\n",
       "   {'paperId': '96c9b1f1da5368f5f900e2091633b1139d92ecc8',\n",
       "    'title': 'Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning'},\n",
       "   {'paperId': '9c4241dd5b0dc3e7ac0e6e68a720d09af89c8e2a',\n",
       "    'title': 'Deep Reinforcement Learning for Humanoid Robot Behaviors'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': '32e6c81eeecea06d102f2dc8edcf8b5018ba1a80',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline RL'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '7267d12df350d88dd181199ab881590a85182bcc',\n",
       "    'title': 'Learning Free Gait Transition for Quadruped Robots Via Phase-Guided Controller'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '4424652c21574544a177d5879a9ad95d3c6d2fdc',\n",
       "    'title': 'A Survey on Deep Learning for Skeleton‐Based Human Animation'},\n",
       "   {'paperId': 'f7dbc89b857167caa1e2ff716b7cfbac48da20fe',\n",
       "    'title': 'Learning Multi-Objective Curricula for Robotic Policy Learning'},\n",
       "   {'paperId': '09c04311402819d6171cdb47b2a381f10d4f564e',\n",
       "    'title': 'Hierarchical Primitive Composition: Simultaneous Activation of Skills with Inconsistent Action Dimensions in Multiple Hierarchies'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': 'c7b01136da36d68e229f268e5489c81c94586481',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': '29be0eddfff83aab67a2edab40e04d97a226c0c5',\n",
       "    'title': 'Learning Task-Agnostic Action Spaces for Movement Optimization'},\n",
       "   {'paperId': '77b2b707bc416293053d4244d57a035b78444e80',\n",
       "    'title': 'Programmatic Reinforcement Learning without Oracles'},\n",
       "   {'paperId': '99dff0f1591050f09fd471692b9b62a9b744508c',\n",
       "    'title': 'Latent-Variable Advantage-Weighted Policy Optimization for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '1e0f91b88b60b3bb7f35824d0526263bb6e39781',\n",
       "    'title': 'OstrichRL: A Musculoskeletal Ostrich Simulation to Study Bio-mechanical Locomotion'},\n",
       "   {'paperId': 'cf6ecf11402d4e675201bf6743fca1457fba7fc1',\n",
       "    'title': 'Transition Motion Tensor: A Data-Driven Approach for Versatile and Controllable Agents in Physically Simulated Environments'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': '248e7ec04d11114c9090a00da27ba6eb5b5bdc1a',\n",
       "    'title': 'Motor Babble: Morphology-Driven Coordinated Control of Articulated Characters'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': '9e755a823378e19aece4a6838830f933c9a9c200',\n",
       "    'title': 'Learning Multi-Objective Curricula for Deep Reinforcement Learning'},\n",
       "   {'paperId': '963ed9ef7929d61a333528ffd3bfc3919f3950a0',\n",
       "    'title': 'Learning to Design and Construct Bridge without Blueprint'},\n",
       "   {'paperId': '90287fb351bbdefa85b25c19e01545effa8243f9',\n",
       "    'title': 'Learning a family of motor skills from a single motion clip'},\n",
       "   {'paperId': '477eee0c0180785b8281c3789b8a4c39c12725c3',\n",
       "    'title': 'Interactive Characters for Virtual Reality Stories'},\n",
       "   {'paperId': '5ed0e750ab088c3a1ffcfb168b6c36f08dffd031',\n",
       "    'title': 'Disentangled Attention as Intrinsic Regularization for Bimanual Multi-Object Manipulation'},\n",
       "   {'paperId': '918e8199fd89093f557baf72d92df743d085371b',\n",
       "    'title': 'Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation'},\n",
       "   {'paperId': 'd44c89071b087c318aa3bee56d9a5b71e11e5e7f',\n",
       "    'title': 'DAIR: Disentangled Attention Intrinsic Regularization for Safe and Efficient Bimanual Manipulation'},\n",
       "   {'paperId': '7a6a4aebcd3a3562ea50913bc44336dba48a84d1',\n",
       "    'title': 'Reward Machines for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': '4f9f09d1ab684b145627f5cbad5560f364e51559',\n",
       "    'title': 'Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning'},\n",
       "   {'paperId': 'b1cbe4d5400d6ae593c7de69ec14f6d0a3a85159',\n",
       "    'title': 'Discovering diverse athletic jumping strategies'},\n",
       "   {'paperId': '1f84920f3f7699e06a9524b544507c4fd29ff7f7',\n",
       "    'title': 'Policy Fusion for Adaptive and Customizable Reinforcement Learning Agents'},\n",
       "   {'paperId': 'e7c33544f157974083e9b106605f417722777352',\n",
       "    'title': 'Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'd9e8ac993cd75c1ab9364ba30a86444f2537a9c4',\n",
       "    'title': 'SimPoE: Simulated Character Control for 3D Human Pose Estimation'},\n",
       "   {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "    'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': '335f33b9fbbfd0a7da6eb36af4942829d1104ffb',\n",
       "    'title': 'Toward Robust Long Range Policy Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': '52b884481962fc8dd4e12bdcdcc72b1bb1bf3ca8',\n",
       "    'title': 'Learning When to Switch: Composing Controllers to Traverse a Sequence of Terrain Artifacts'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '3cfbd8c5f6bb22f95fbd9d08e2e350053ccec691',\n",
       "    'title': 'PFPN: Continuous Control of Physically Simulated Characters using Particle Filtering Policy Network'},\n",
       "   {'paperId': '99e2a731e92273ba3e963da75c70d6d9f5c1e66f',\n",
       "    'title': 'Contextual policy transfer in reinforcement learning domains via deep mixtures-of-experts'},\n",
       "   {'paperId': 'ed580995e4a51424d8f1a20f5b64200fe227c2cd',\n",
       "    'title': 'Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control'},\n",
       "   {'paperId': '1b305b762bd6d9c929090631234b8a0721cc0ec7',\n",
       "    'title': 'GSC: Graph-based Skill Composition for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e9810e7ae50715760eaa66f3fcfc1c8d0fc2e26a',\n",
       "    'title': 'Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition'},\n",
       "   {'paperId': '3ba5343e8945f26c161bb512bf3d7ea5d4434f32',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Universal Policies for Multi-Step Robotic Manipulation'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '59b934124c2bf4355dc2692898564428d5e5e13a',\n",
       "    'title': 'G ENERALISATION IN L IFELONG R EINFORCEMENT L EARNING THROUGH L OGICAL C OMPOSITION'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': 'e5ab04396a0b29efae5ef8b1ce258644322359ea',\n",
       "    'title': 'Overleaf Example'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '465ac254433a45ed55ef0da99aee7f81a12b31b8',\n",
       "    'title': 'Training Physics-based Controllers for Articulated Characters with Deep Reinforcement Learning'},\n",
       "   {'paperId': '2710a86f8de71105c8ce14e58784f3e05c8bbe38',\n",
       "    'title': 'Generation of multiagent animation for object transportation using deep reinforcement learning and blend‐trees'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': 'e9420bd6621bf8c09dde9baa28c6b7cd49fe65d1',\n",
       "    'title': 'Disentangled Planning and Control in Vision Based Robotics via Reward Machines'},\n",
       "   {'paperId': '872edada2165ad65c1664b813efdb92e3bec1b36',\n",
       "    'title': 'Multi-expert learning of adaptive legged locomotion'},\n",
       "   {'paperId': '8b62d928a7be4a6f408cc7a433c215a749604a95',\n",
       "    'title': 'UniCon: Universal Neural Controller For Physics-based Character Motion'},\n",
       "   {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "    'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '5f952b6c4fe9fa6d0fbe7122d9e4f40921118c87',\n",
       "    'title': 'Independent Skill Transfer for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'b2df444b1e52d9e3d73fe1b54027513d6c28128d',\n",
       "    'title': 'A scalable approach to control diverse behaviors for physically simulated characters'},\n",
       "   {'paperId': '50e83f27c10ce007a4a7267734e4fb05d0a7ca52',\n",
       "    'title': 'Jump Operator Planning: Goal-Conditioned Policy Ensembles and Zero-Shot Transfer'},\n",
       "   {'paperId': '5c52d9c34ccc85468473e0c9cc0b537837039f65',\n",
       "    'title': 'Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions'},\n",
       "   {'paperId': '8770e9a03eecb959931aad9a0f22272d9de9d0ab',\n",
       "    'title': 'Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis'},\n",
       "   {'paperId': 'a5366810c5b6196413d0655eaea3371cde19acf5',\n",
       "    'title': 'Gaussian Gated Linear Networks'},\n",
       "   {'paperId': '45d7a12b198bdd74d91b8a69bc88b1d9bec2091f',\n",
       "    'title': 'Learning Situational Driving'},\n",
       "   {'paperId': '9fcb490ae970dac6af11a9fc206b1e718eef6894',\n",
       "    'title': 'CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': '23256893b76931dec86ff7a240178fac3b5e1838',\n",
       "    'title': 'Off-Policy Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '121cca1bb7cec48fa9080801927f50d99193eae6',\n",
       "    'title': 'Learning to Coordinate Manipulation Skills via Skill Behavior Diversification'},\n",
       "   {'paperId': '0b9bfbd7a7f4595d99ab7f73862fbcfbac38c4d7',\n",
       "    'title': 'Multi-Task Reinforcement Learning with Soft Modularization'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': '796ab1a0edaff21d808a7380de7d4a6cd1be2ffd',\n",
       "    'title': 'Temporal and state abstractions for efficient learning, transfer and composition in humans'},\n",
       "   {'paperId': 'f8a97eac3c05d6f09772b24a1a1f6e66a2c17033',\n",
       "    'title': 'Product Kanerva Machines: Factorized Bayesian Memory'},\n",
       "   {'paperId': '32f39d580c7f200e2f6dd7c22ed460f6d9567426',\n",
       "    'title': 'A Boolean Task Algebra for Reinforcement Learning'},\n",
       "   {'paperId': '9bc59a45fd09a1d0f9db1a69d7e074f62193456b',\n",
       "    'title': 'Phase Portraits as Movement Primitives for Fast Humanoid Robot Control'},\n",
       "   {'paperId': '817fc6b231599a257302cd853ce4fa9572c8e849',\n",
       "    'title': 'Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '516643d89835d9ac40bee7a88ec9516f399c3b9a',\n",
       "    'title': 'Multi-task Batch Reinforcement Learning with Metric Learning'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '054146c30ca12c7d4e322d154e1b24f22e4c0c25',\n",
       "    'title': 'HAT: Hierarchical Alternative Training for Long Range Policy Transfer'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '1a82aa3f16eceed3b87cd3e042eeb6ae6f6e4766',\n",
       "    'title': 'VIA SKILL BEHAVIOR DIVERSIFICATION'},\n",
       "   {'paperId': '391140770ca8502e08e0ba2b39a6f3b3fafb4b91',\n",
       "    'title': 'Supplementary Material: A Boolean Task Algebra For Reinforcement Learning'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'a1ee8eeb4b6071b2651b8c9bd986ad2d9099d7a9',\n",
       "    'title': 'Logical Composition for Lifelong Reinforcement Learning'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': 'cf0c21194c897012a825ede9fda2601e0c5665c3',\n",
       "    'title': 'Reusable neural skill embeddings for vision-guided whole body movement and object manipulation'},\n",
       "   {'paperId': 'bb4fb36bd871b1757fa736ea4c3d56f8d5b9b1fa',\n",
       "    'title': 'Low Dimensional Motor Skill Learning Using Coactivation'},\n",
       "   {'paperId': '22005795db3a0e85c9091a855427a39b5f8bb33a',\n",
       "    'title': 'Neural Embedding for Physical Manipulations'},\n",
       "   {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "    'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '86569aabd7e3ce14a25a8c01a3873cb3f24b1c97',\n",
       "    'title': 'Case study of model-based reinforcement learning with skill library in peeling banana task'}],\n",
       "  'citnuminlist': 10,\n",
       "  'refnuminlist': 9,\n",
       "  'isKeypaper': True},\n",
       " '96ae5d3ac1a1dcc365684bc92fcfa4d40d802bca': {'title': 'Neural probabilistic motor primitives for humanoid control',\n",
       "  'year': 2018,\n",
       "  'references': [{'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '482376177d6ed10aa2975f9858a91e49ec121b00',\n",
       "    'title': 'Physics-based motion capture imitation with deep reinforcement learning'},\n",
       "   {'paperId': '18d4f415b39650006d92e42345264c33750273d0',\n",
       "    'title': 'Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning'},\n",
       "   {'paperId': '2444be7584d1f5a7e2aa9f65078de09154f14ea1',\n",
       "    'title': 'Born Again Neural Networks'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'e54a7bf9b5f610a2c19e058e005296049e180aad',\n",
       "    'title': 'Knowledge Transfer with Jacobian Matching'},\n",
       "   {'paperId': 'c335ff618991f0a4cdde09271284172a7e5f6b7f',\n",
       "    'title': 'Emergent Complexity via Multi-Agent Competition'},\n",
       "   {'paperId': None,\n",
       "    'title': 'In short, this approach for producing experts largely follows Peng et al'},\n",
       "   {'paperId': None, 'title': 'The approach we use for producing experts'},\n",
       "   {'paperId': None,\n",
       "    'title': 'reference and episodes are early-terminated when the character falls. Here we use an off-policy RL algorithm, SVG(0) (Heess et al., 2015) with Retrace (Munos et al., 2016)'},\n",
       "   {'paperId': None,\n",
       "    'title': 'tracks/imitates a motion capture reference clip (Peng et al., 2018)'},\n",
       "   {'paperId': '9917363277c783a01bff32af1c27fc9b373ad55d',\n",
       "    'title': 'DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning'},\n",
       "   {'paperId': 'cf90552b5d2e992e93ab838fd615e1c36618e31c',\n",
       "    'title': 'Distral: Robust multitask reinforcement learning'},\n",
       "   {'paperId': 'cddb1f7f9f004396a2efef285caf29d7780a8e21',\n",
       "    'title': 'Robust Imitation of Diverse Behaviors'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': '71e92abb1e504d56b3f18b4909d73eee3b6048fb',\n",
       "    'title': 'Sobolev Training for Neural Networks'},\n",
       "   {'paperId': 'a59658d7b74f63a34e7182addbba1214775f49f5',\n",
       "    'title': 'DART: Noise Injection for Robust Imitation Learning'},\n",
       "   {'paperId': '5c57bb5630835a05eb1c3d0df3e12d6180d75de2',\n",
       "    'title': 'One-Shot Imitation Learning'},\n",
       "   {'paperId': 'a181fb5a42ad8fe2cc27b5542fa40384e9a8d72c',\n",
       "    'title': 'Deep Variational Information Bottleneck'},\n",
       "   {'paperId': 'de19027b36ba0e9d669ce2f4194e5f3937aa0517',\n",
       "    'title': 'A Probabilistic Representation for Dynamic Movement Primitives'},\n",
       "   {'paperId': 'dc3e905bfb27d21675ee1720413e007b014b37d3',\n",
       "    'title': 'Safe and Efficient Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '1def5d3711ebd1d86787b1ed57c91832c5ddc90b',\n",
       "    'title': 'Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning'},\n",
       "   {'paperId': '1c4927af526d5c28f7c2cfa492ece192d80a61d4',\n",
       "    'title': 'Policy Distillation'},\n",
       "   {'paperId': 'dd5275dba51a5dc2641f7bee378a4b7aca88e952',\n",
       "    'title': 'Interactive Control of Diverse Complex Characters with Neural Networks'},\n",
       "   {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "    'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       "   {'paperId': '6563de8911541fab9fec2d91654a20d98eb164c3',\n",
       "    'title': 'Learning reduced-order feedback policies for motion skills'},\n",
       "   {'paperId': '0c908739fbff75f03469d13d4a1a07de3414ee19',\n",
       "    'title': 'Distilling the Knowledge in a Neural Network'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': '9d242175cbe2f082da78e469bc9b23144c33b320',\n",
       "    'title': 'Learning modular policies for robotics'},\n",
       "   {'paperId': '484ad17c926292fbe0d5211540832a8c8a8e958b',\n",
       "    'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': '3a81cfb4a7a880b7cf8979f6067732e961aceb7c',\n",
       "    'title': 'Probabilistic Movement Primitives'},\n",
       "   {'paperId': '71b552b2e058d5a6a760ba203f10f13be759edd3',\n",
       "    'title': 'Synthesis and stabilization of complex behaviors through online trajectory optimization'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': '79ca373e522fb3f368f864c113269baf85596611',\n",
       "    'title': 'Sampling-based contact-rich motion control'},\n",
       "   {'paperId': '6a28056e9670c4d7681045777e9e02c76f318c54',\n",
       "    'title': 'Control-Limited Differential Dynamic Programming'},\n",
       "   {'paperId': '768cf286ea76b4143e771df00b961e3b64722c16',\n",
       "    'title': 'LQR-trees: Feedback motion planning on sparse randomized trees'},\n",
       "   {'paperId': '843959ffdccf31c6694d135fad07425924f785b1',\n",
       "    'title': 'Extracting and composing robust features with denoising autoencoders'},\n",
       "   {'paperId': '4af2b9e7591ee995124f5f77a84ceb14d11555df',\n",
       "    'title': 'Combining modules for movement'},\n",
       "   {'paperId': '596e59d1a988ad9852dc29d2311689a271bb4f4e',\n",
       "    'title': 'The organization of behavioral repertoire in motor cortex.'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': '278ae6522a10aca453e56b742dbe9bec918ac681',\n",
       "    'title': 'Unsupervised learning of sensory-motor primitives'},\n",
       "   {'paperId': 'ef66de6946ee147359f087bdee1f516d1385e780',\n",
       "    'title': 'Minimax Differential Dynamic Programming: An Application to Robust Biped Walking'},\n",
       "   {'paperId': '6eeb5c7fab71cd930c9599f4fadfe206b8d61040',\n",
       "    'title': 'The role and use of the stochastic linear-quadratic-Gaussian problem in control system design'},\n",
       "   {'paperId': None, 'title': 'Jacobson and David Q . Mayne'},\n",
       "   {'paperId': 'b37213f2d1d37d22590278acd0c93f2fdc48e59c',\n",
       "    'title': 'A Second-order Gradient Method for Determining Optimal Trajectories of Non-linear Discrete-time Systems'}],\n",
       "  'citations': [{'paperId': '67722409a74bf8a61b370a6fd5a524b279e2b6f4',\n",
       "    'title': 'Simulation and Retargeting of Complex Multi-Character Interactions'},\n",
       "   {'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': '09bc7b16e793369f673368eccc7cd7e9e0467a0b',\n",
       "    'title': 'On the Value of Myopic Behavior in Policy Reuse'},\n",
       "   {'paperId': '5a85dfbbb40730516097af15d4c8b875c628b1b9',\n",
       "    'title': 'Synthesizing Diverse Human Motions in 3D Indoor Scenes'},\n",
       "   {'paperId': 'ca6c9837ef0e3728e72fa6a5257880e2dd042a3c',\n",
       "    'title': 'Composite Motion Learning with Task Control'},\n",
       "   {'paperId': 'ae75404a597b573edd05a129819da8c2385af709',\n",
       "    'title': 'RoboPianist: A Benchmark for High-Dimensional Robot Control'},\n",
       "   {'paperId': 'ce211df49cb2fc836d7d49c628c63df291f96a6b',\n",
       "    'title': 'Learning to Transfer In‐Hand Manipulations Using a Greedy Shape Curriculum'},\n",
       "   {'paperId': '8d32b150ce19200a2d2d71f68b63972080ef99ad',\n",
       "    'title': 'Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach'},\n",
       "   {'paperId': 'd70f4d07501b55e94e37d495e153d51053a0b7d3',\n",
       "    'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '53d661d536965daf4ff6dcfd3b7e42ffa9061d78',\n",
       "    'title': 'Diverse Policy Optimization for Structured Action Space'},\n",
       "   {'paperId': '89b453e03da2b2e6ad81217c8e5c6e6e3f5cf5f4',\n",
       "    'title': 'Behaviour Discriminator: A Simple Data Filtering Method to Improve Offline Policy Learning'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '39f4e1d98864dc551eb16b2d8e134141a26f28fc',\n",
       "    'title': 'Synthesizing Get‐Up Motions for Physics‐based Characters'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'a5978dc8e57c27966d218a06e4ccc8aa3a507507',\n",
       "    'title': 'ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'd9c0c3eea7a2581e434b4c9d58ce405638f7dbd5',\n",
       "    'title': 'Multi-expert synthesis for versatile locomotion and manipulation skills'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': 'ddb0acda05adf6ae3bc4763575dde39423e8ef6c',\n",
       "    'title': 'Neural3Points: Learning to Generate Physically Realistic Full‐body Motion for Virtual Reality Users'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '748c9aa5a31f279fa07b84238aa5ba748e9df40d',\n",
       "    'title': 'Efficient Planning in a Compact Latent Action Space'},\n",
       "   {'paperId': '3e38ccc5ba9a4b4290060716614e02b7ba54df79',\n",
       "    'title': 'MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control'},\n",
       "   {'paperId': '6a88b9241025357154479687a94d791a3204c958',\n",
       "    'title': 'Physics-based character controllers using conditional VAEs'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '6f837af0fb2c4e88b747f2b8517f349c7e5379b0',\n",
       "    'title': 'Human-AI Shared Control via Policy Dissection'},\n",
       "   {'paperId': '90fd4fba0de407228fc0938e5aba5fe00bed8d0b',\n",
       "    'title': 'Data augmentation for efficient learning from parametric experts'},\n",
       "   {'paperId': '50d42fc003eda0bfedcae3aa483f93b7a59931d5',\n",
       "    'title': 'Learning to Brachiate via Simplified Model Imitation'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': 'effa7077e6580a8ab22c30d3c876744b4e51cd6e',\n",
       "    'title': 'Deep learning, reinforcement learning, and world models'},\n",
       "   {'paperId': '89ee7f49698bb15f7599aa52b9101065e805720c',\n",
       "    'title': 'Imitate and Repurpose: Learning Reusable Robot Movement Skills From Human and Animal Behaviors'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'abccbc60b6e4dcc1e46baee34d01d4fb4abbff48',\n",
       "    'title': 'Neural Circuit Architectural Priors for Embodied Control'},\n",
       "   {'paperId': '9625c3afbacb8cacbfacceecac4e85b8a4d1eba4',\n",
       "    'title': 'The Wanderings of Odysseus in 3D Scenes'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '794836294c5d8f4e06e554636992ca5e49869574',\n",
       "    'title': 'Leaving flatland: Advances in 3D behavioral measurement'},\n",
       "   {'paperId': '6d6e58607cf068273a2da5c66513b4cd4f110b2e',\n",
       "    'title': 'Learning Coordinated Terrain-Adaptive Locomotion by Imitating a Centroidal Dynamics Planner'},\n",
       "   {'paperId': '4424652c21574544a177d5879a9ad95d3c6d2fdc',\n",
       "    'title': 'A Survey on Deep Learning for Skeleton‐Based Human Animation'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "    'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '23312c34ed62d93b1f9827e570f796953295afb1',\n",
       "    'title': 'Kinematic Motion Retargeting via Neural Latent Optimization for Learning Sign Language'},\n",
       "   {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "    'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       "   {'paperId': '29be0eddfff83aab67a2edab40e04d97a226c0c5',\n",
       "    'title': 'Learning Task-Agnostic Action Spaces for Movement Optimization'},\n",
       "   {'paperId': 'bafbb3c535d9ee0fbffaad266f732a3892f53b4e',\n",
       "    'title': 'Deep reinforcement learning for real-world quadrupedal locomotion: a comprehensive review'},\n",
       "   {'paperId': 'a7e3d0968b7e6cd996ca2c9fa31f6b504448c8f6',\n",
       "    'title': 'Human-AI Shared Control via Frequency-based Policy Dissection'},\n",
       "   {'paperId': '1e0f91b88b60b3bb7f35824d0526263bb6e39781',\n",
       "    'title': 'OstrichRL: A Musculoskeletal Ostrich Simulation to Study Bio-mechanical Locomotion'},\n",
       "   {'paperId': '56f9fc0b8f9766931ffb9dba985b6b3067d4c2bf',\n",
       "    'title': 'Motion recommendation for online character control'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '311e349df0d2a9f2ca4b441e18e960ccb24b512a',\n",
       "    'title': 'Model-based Motion Imitation for Agile, Diverse and Generalizable Quadupedal Locomotion'},\n",
       "   {'paperId': 'cd727f2e860a0a7f38e7b32c20f5129dd6681502',\n",
       "    'title': 'FastMimic: Model-based Motion Imitation for Agile, Diverse and Generalizable Quadrupedal Locomotion'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '0d7ef58674dda91cebc26d38ed572e81873db344',\n",
       "    'title': 'Imitation by Predicting Observations'},\n",
       "   {'paperId': '477eee0c0180785b8281c3789b8a4c39c12725c3',\n",
       "    'title': 'Interactive Characters for Virtual Reality Stories'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'd9e8ac993cd75c1ab9364ba30a86444f2537a9c4',\n",
       "    'title': 'SimPoE: Simulated Character Control for 3D Human Pose Estimation'},\n",
       "   {'paperId': '26cf79bce38d3981707aaa94ea8291ade77dedf9',\n",
       "    'title': 'Manipulator-Independent Representations for Visual Imitation'},\n",
       "   {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "    'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '335f33b9fbbfd0a7da6eb36af4942829d1104ffb',\n",
       "    'title': 'Toward Robust Long Range Policy Transfer'},\n",
       "   {'paperId': '8672295b3f49cb1955efd541a168fe43551e9e9a',\n",
       "    'title': 'Continuous Whole-Body 3D Kinematic Recordings across the Rodent Behavioral Repertoire'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '6be24dd6896353368e9d01418090dbe5211402de',\n",
       "    'title': 'Planning in Learned Latent Action Spaces for Generalizable Legged Locomotion'},\n",
       "   {'paperId': 'ed580995e4a51424d8f1a20f5b64200fe227c2cd',\n",
       "    'title': 'Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': 'aaac91a5ebef1ec06f15434fa1a3e15c4ed82d12',\n",
       "    'title': 'SuperTrack: Motion Tracking for Physically Simulated Characters using Supervised Learning'},\n",
       "   {'paperId': 'a059870910339596c5c57969bc2397a7e268435e',\n",
       "    'title': 'Learning Embodied Agents with Scalably-Supervised Reinforcement Learning'},\n",
       "   {'paperId': '8b62d928a7be4a6f408cc7a433c215a749604a95',\n",
       "    'title': 'UniCon: Universal Neural Controller For Physics-based Character Motion'},\n",
       "   {'paperId': '5cde940de9cc7cac8595e790e397ca56b5316202',\n",
       "    'title': 'REALab: An Embedded Perspective on Tampering'},\n",
       "   {'paperId': '1f8763f2d33fc65e7236b82fe96258c99256b5e8',\n",
       "    'title': 'Avoiding Tampering Incentives in Deep RL via Decoupled Approval'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'e4a46c64aafbef0406e9cfa90dd9c43e3e07598c',\n",
       "    'title': 'One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "    'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       "   {'paperId': 'e996f8d229bb86e03999864e814f4039d4afc5af',\n",
       "    'title': 'Hyperparameter Selection for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'b2df444b1e52d9e3d73fe1b54027513d6c28128d',\n",
       "    'title': 'A scalable approach to control diverse behaviors for physically simulated characters'},\n",
       "   {'paperId': '7acbdb961f67d50fef359066f2a1d7755cf16ee2',\n",
       "    'title': 'Critic Regularized Regression'},\n",
       "   {'paperId': '05ea5e3bbcc22ccf57f9b458ee7d72f9c51682b1',\n",
       "    'title': 'RL Unplugged: A Suite of Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'de46f4e4613364792bbd13f185c381ab656a27ef',\n",
       "    'title': 'RL Unplugged: Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'cf8f41618fae491ca8de10404b1e65a14379f2ab',\n",
       "    'title': 'Thalamocortical motor circuit insights for more robust hierarchical control of complex sequences'},\n",
       "   {'paperId': '48c64d04c56b14f4b113b8cb85107d8f05a9102a',\n",
       "    'title': 'dm_control: Software and Tasks for Continuous Control'},\n",
       "   {'paperId': '8770e9a03eecb959931aad9a0f22272d9de9d0ab',\n",
       "    'title': 'Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis'},\n",
       "   {'paperId': '9fcb490ae970dac6af11a9fc206b1e718eef6894',\n",
       "    'title': 'CARL: Controllable Agent with Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': 'f8a97eac3c05d6f09772b24a1a1f6e66a2c17033',\n",
       "    'title': 'Product Kanerva Machines: Factorized Bayesian Memory'},\n",
       "   {'paperId': 'b3990df5c3c1618fe3e1fe0ec62208da193d7f81',\n",
       "    'title': 'Goal-Conditioned Variational Autoencoder Trajectory Primitives with Continuous and Discrete Latent Codes'},\n",
       "   {'paperId': 'e9fd8b69faa4c909828134278c89d9aafffbcf39',\n",
       "    'title': 'Deep neuroethology of a virtual rodent'},\n",
       "   {'paperId': 'e7bb4419a88d15fa8e52c1f4f9cfd65ed58c7379',\n",
       "    'title': 'V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '0c4e53afb63ed48fba057c79415ec9f32fc1b107',\n",
       "    'title': 'A model of flexible motor sequencing through thalamic control of cortical dynamics'},\n",
       "   {'paperId': 'bfb4970d045a25972629ef9ba428f0b4626e29d7',\n",
       "    'title': 'Optimizing High-dimensional Learner with Low-Dimension Action Features'},\n",
       "   {'paperId': 'b65decc03155f2e88984e4fa16493f70e5413e4d',\n",
       "    'title': 'Hierarchical motor control in mammals and machines'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': 'cf0c21194c897012a825ede9fda2601e0c5665c3',\n",
       "    'title': 'Reusable neural skill embeddings for vision-guided whole body movement and object manipulation'},\n",
       "   {'paperId': 'f91c670b4125fd9a18bb34d7bd9eaa38542bb9bb',\n",
       "    'title': 'Learning predict-and-simulate policies from unorganized human motion data'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': '6ca98c37af9a135d323c32b44c0be18ab7c489ef',\n",
       "    'title': 'Augmenting learning using symmetry in a biologically-inspired domain'},\n",
       "   {'paperId': '22005795db3a0e85c9091a855427a39b5f8bb33a',\n",
       "    'title': 'Neural Embedding for Physical Manipulations'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'e9b3c086bdd4135453394e8c61d21e7f7841a31e',\n",
       "    'title': 'Iterative Reinforcement Learning Based Design of Dynamic Locomotion Skills for Cassie'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '719068eb8b8c9ab8552ec3e82c1b1088a9eacdce',\n",
       "    'title': 'Learning Locomotion Skills for Cassie: Iterative Design and Sim-to-Real'},\n",
       "   {'paperId': '0f9535573df5eeaa42f4b7dd08140c7f071e506c',\n",
       "    'title': 'CONTINUOUS CONTROL'}],\n",
       "  'citnuminlist': 13,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " '8c54e8575e7c17a4097838305915e6e7b00fd4af': {'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': 'd720d601d5b835937b96b68d7119b239d45776f0',\n",
       "    'title': 'SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': 'eb37e7b76d26b75463df22b2a3aa32b6a765c672',\n",
       "    'title': 'QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fb9693183bc74568c72188431c18cb2b07c87213',\n",
       "    'title': 'Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'd356a5603f14c7a6873272774782d7812871f952',\n",
       "    'title': 'Reinforcement and Imitation Learning for Diverse Visuomotor Skills'},\n",
       "   {'paperId': '852c931b5d9f9d4256befd725ee4185945c4964c',\n",
       "    'title': 'Temporal Difference Models: Model-Free Deep RL for Model-Based Control'},\n",
       "   {'paperId': '904307cb58795241b22cfaa34f560e610997f5c1',\n",
       "    'title': 'Divide-and-Conquer Reinforcement Learning'},\n",
       "   {'paperId': 'c28ec2a40a2c77e20d64cf1c85dc931106df8e83',\n",
       "    'title': 'Overcoming Exploration in Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e010ba3ff5744604cdbfe44a733e2a98649ee907',\n",
       "    'title': 'Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations'},\n",
       "   {'paperId': '90356d78db11e14645a96d8483268ce585dbb0c3',\n",
       "    'title': 'OptionGAN: Learning Joint Reward-Policy Options using Generative Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e37b999f0c96d7136db07b0185b837d5decd599a',\n",
       "    'title': 'Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': None, 'title': 'Hierarchical actor-critic. CoRR'},\n",
       "   {'paperId': '4c09757f31f66e483c61266a458f5aaaf8723895',\n",
       "    'title': 'Optimal control with learned local models: Application to dexterous manipulation'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '1c4927af526d5c28f7c2cfa492ece192d80a61d4',\n",
       "    'title': 'Policy Distillation'},\n",
       "   {'paperId': 'c7c19d99175f252ea6b9babe98673743b71ac49d',\n",
       "    'title': 'MuJoCo HAPTIX: A virtual reality system for hand manipulation'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': '79ab3c49903ec8cb339437ccf5cf998607fc313e',\n",
       "    'title': 'A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning'},\n",
       "   {'paperId': 'a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8',\n",
       "    'title': 'Recent Advances in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': '6df43f70f383007a946448122b75918e3a9d6682',\n",
       "    'title': 'Learning to Achieve Goals'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'How to learn from unstructured demonstrations rather than assuming meaningful demonstrations? 2. Learning options/skills vs Learning fixed-horizon low-level policies?'}],\n",
       "  'citations': [{'paperId': 'd11ae7f22045a2217fb2ef169037fba216153c63',\n",
       "    'title': 'Stabilizing Contrastive RL: Techniques for Offline Goal Reaching'},\n",
       "   {'paperId': 'f69f95835deec7748a688675721b6d581b60d42b',\n",
       "    'title': 'LIV: Language-Image Representations and Rewards for Robotic Control'},\n",
       "   {'paperId': 'ad84b7b1e94f2a88458deedc3b1972018a24c640',\n",
       "    'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation'},\n",
       "   {'paperId': '145161fdc0337a4abbd2f848e7b71c37205b5323',\n",
       "    'title': 'Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought'},\n",
       "   {'paperId': 'e17afede733e7205657d0522b800c712990dfdb5',\n",
       "    'title': 'Parallel Sampling of Diffusion Models'},\n",
       "   {'paperId': '00fcc983728346a5f3f8f005f1365be54456728e',\n",
       "    'title': 'EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought'},\n",
       "   {'paperId': 'f80de0e8d343166f8e9b2cf844253d7b24b443d4',\n",
       "    'title': 'FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation'},\n",
       "   {'paperId': '85bc55ba9ab93c09713b0891bbcf0541b8f27ea9',\n",
       "    'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '1bea3d94a6d2e768b2d35c285dc2baa68c479853',\n",
       "    'title': 'Revisiting Proprioceptive Sensing for Articulated Object Manipulation'},\n",
       "   {'paperId': '6eef081e4a2322ae9f96632032da6cbe147b9f66',\n",
       "    'title': 'Learning Video-Conditioned Policies for Unseen Manipulation Tasks'},\n",
       "   {'paperId': 'c7dea47e008a439e11439dfe6a8c1b08357fad65',\n",
       "    'title': 'Distance Weighted Supervised Learning for Offline Interaction Data'},\n",
       "   {'paperId': 'b1e55b32c29a9a46d5951f239989ee1b86965e3b',\n",
       "    'title': 'EC^2: Emergent Communication for Embodied Control'},\n",
       "   {'paperId': '5840808d5f8a0f3f3938897914b8a00586c32092',\n",
       "    'title': 'Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments'},\n",
       "   {'paperId': '016315a3df05cb07a6f67fa5f6a3265b55909644',\n",
       "    'title': 'Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation'},\n",
       "   {'paperId': 'd3590a7a7d65c960a8cff45ad868d52bb8cc2844',\n",
       "    'title': 'For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1334a47e8f4e4ffd04ff534329d76a5e5cc16f46',\n",
       "    'title': 'Goal-Conditioned Imitation Learning using Score-based Diffusion Policies'},\n",
       "   {'paperId': 'ae3bf9b2b2d5239d51107027b2b11e5097616272',\n",
       "    'title': 'A Simple Approach for General Task-Oriented Picking using Placing constraints'},\n",
       "   {'paperId': '26063b5f5efa71e1c3b726f6f8c792368ba43ce4',\n",
       "    'title': 'DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics'},\n",
       "   {'paperId': 'c1b247c76c71153726c50d8109e300aee09ec3fa',\n",
       "    'title': 'Information Maximizing Curriculum: A Curriculum-Based Approach for Training Mixtures of Experts'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': 'e9abbbf1e64cd972fb2e8bbc1ffe983c8cdc640e',\n",
       "    'title': 'A Survey of Demonstration Learning'},\n",
       "   {'paperId': 'c45f28fdd456ecddee950ad3fa24fb2ea1929b8a',\n",
       "    'title': 'Efficient Learning of High Level Plans from Play'},\n",
       "   {'paperId': '613fc9579505bed343e6381b6fb1c1d8ac9ffa4f',\n",
       "    'title': 'Ignorance is Bliss: Robust Control via Information Gating'},\n",
       "   {'paperId': '8f2bef6c4ddbf5c4997d5ddd89b83bd49db53cfd',\n",
       "    'title': 'Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning'},\n",
       "   {'paperId': 'bdba3bd30a49ea4c5b20b43dbd8f0eb59e9d80e2',\n",
       "    'title': 'Diffusion Policy: Visuomotor Policy Learning via Action Diffusion'},\n",
       "   {'paperId': 'd70f4d07501b55e94e37d495e153d51053a0b7d3',\n",
       "    'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments'},\n",
       "   {'paperId': 'c293b63151e9e02f7694938cabb7bbae442ae4d2',\n",
       "    'title': 'The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'e966cca871cef85f3bfb9a6c69cdcbec23357c1d',\n",
       "    'title': 'Controllability-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'd914dc7f5d9291ee2127936e3206c90ca1fcea71',\n",
       "    'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation'},\n",
       "   {'paperId': 'ceac3aaba97b40e9ace78dfad0331f28efb36e02',\n",
       "    'title': 'Designing an offline reinforcement learning objective from scratch'},\n",
       "   {'paperId': 'b43330013a5abcccd366d71f2f66c493c790abc6',\n",
       "    'title': 'Imitating Human Behaviour with Diffusion Models'},\n",
       "   {'paperId': '18ef9c5a2d728703dc8e576e4b07a0c5c82df77d',\n",
       "    'title': 'PIRLNav: Pretraining with Imitation and RL Finetuning for ObjectNav'},\n",
       "   {'paperId': 'b0092b450aaa3e5c15f0fc9949341f332759d418',\n",
       "    'title': 'Learning From Guided Play: Improving Exploration for Adversarial Imitation Learning With Simple Auxiliary Tasks'},\n",
       "   {'paperId': '6ace8342b5bb52e5db1bc7f6bc42b4c4c4d7b938',\n",
       "    'title': 'A Context-based Multi-task Hierarchical Inverse Reinforcement Learning Algorithm'},\n",
       "   {'paperId': '61e7a3d5606043594a8ce377870479f77a6b58c2',\n",
       "    'title': 'A Survey on Offline Reinforcement Learning: Taxonomy, Review, and Open Problems'},\n",
       "   {'paperId': '0e27c71bb7b9574ccd75a53665d8cf7c7483dd07',\n",
       "    'title': 'The StarCraft Multi-Agent Exploration Challenges: Learning Multi-Stage Tasks and Environmental Factors Without Precise Reward Functions'},\n",
       "   {'paperId': '7ccfd68dc071d6060962b17dd25b0c7fbfd58a9e',\n",
       "    'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures'},\n",
       "   {'paperId': 'ca137b4f6ba25ca2f26952044c6a2d06ae3e607f',\n",
       "    'title': 'Cross-Domain Transfer via Semantic Skill Imitation'},\n",
       "   {'paperId': '9ffc8f7b3fbd5e609f609b1c20206129f22b4eb7',\n",
       "    'title': 'CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning'},\n",
       "   {'paperId': 'db8d70d9da6b957a00ec7e8cc67493340c39aa29',\n",
       "    'title': 'Policy Transfer via Skill Adaptation and Composition'},\n",
       "   {'paperId': '3b873d954a0f0f8d50cdeb500c105e993cf2a424',\n",
       "    'title': 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection'},\n",
       "   {'paperId': '5cef819106a386fe876a868b52d933e98e5c32e4',\n",
       "    'title': 'Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment'},\n",
       "   {'paperId': '47c83b11225951a8273103a1262f404fc1174e22',\n",
       "    'title': 'E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance'},\n",
       "   {'paperId': '9d1445f1845a2880ff9c752845660e9c294aa7b5',\n",
       "    'title': 'Flow to Control: Offline Reinforcement Learning with Lossless Primitive Discovery'},\n",
       "   {'paperId': 'afd85d75d9ed910f185573a90f74b69379d5f05b',\n",
       "    'title': 'A Brief Review of Recent Hierarchical Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': 'b9ecf9fb3b2d5a70451fa30911142e55848fde2d',\n",
       "    'title': 'Model-based Trajectory Stitching for Improved Offline Reinforcement Learning'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '811eca8c2e6b25c99f488413dd6993ab3771292e',\n",
       "    'title': 'Adaptive Behavior Cloning Regularization for Stable Offline-to-Online Reinforcement Learning'},\n",
       "   {'paperId': 'e2cfb105c99c038d31b52e47f12ee43918f17d3d',\n",
       "    'title': 'Implicit Offline Reinforcement Learning via Supervised Learning'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': '9b5f4aab169fba588e214c010345232053f8ae76',\n",
       "    'title': 'From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data'},\n",
       "   {'paperId': 'e6548d97d82aa2710019951eb4eac034e1747aa1',\n",
       "    'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations'},\n",
       "   {'paperId': 'e199e494ca2006a350c51cecfd70659d55e14675',\n",
       "    'title': 'You Only Live Once: Single-Life Reinforcement Learning'},\n",
       "   {'paperId': 'ad4707bb87c6fc087e09d9f6609665b53835c899',\n",
       "    'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction'},\n",
       "   {'paperId': '30d0a5a5f8e776c844a37afe3b1ace0b21b24859',\n",
       "    'title': 'Retrospectives on the Embodied AI Workshop'},\n",
       "   {'paperId': '87a00037444092e8ada8d3bb4c1f8c6baededdc0',\n",
       "    'title': 'Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks'},\n",
       "   {'paperId': '0e34addae55a571d7efd3a5e2543e86dd7d41a83',\n",
       "    'title': 'Interactive Language: Talking to Robots in Real Time'},\n",
       "   {'paperId': 'd27be7f60b256d5dc8facb1337aede953b8ca9ef',\n",
       "    'title': 'Behavior policy learning: Learning multi-stage tasks via solution sketches and model-based controllers'},\n",
       "   {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "    'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '25425e299101b13ec2872417a14f961f4f8aa18e',\n",
       "    'title': 'VIMA: General Robot Manipulation with Multimodal Prompts'},\n",
       "   {'paperId': '3fbe2e8413df0207c26ff393c9aaa8488e3ca4c3',\n",
       "    'title': 'VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training'},\n",
       "   {'paperId': '289c7bd0420a88bbff6b99826c2c03f15d9ba750',\n",
       "    'title': 'B2RL: an open-source dataset for building batch reinforcement learning'},\n",
       "   {'paperId': 'c49755bb3aa858faed3e5edccae46a8e71ddc765',\n",
       "    'title': 'Consistent Experience Replay in High-Dimensional Continuous Control with Decayed Hindsights'},\n",
       "   {'paperId': 'b9a3fb062ac940012f69e9cca5f34bb717370c3a',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '951da1a5a0c2bfb6f2d16dccfc488acb71f1447f',\n",
       "    'title': 'An Open Tele-Impedance Framework to Generate Data for Contact-Rich Tasks in Robotic Manipulation'},\n",
       "   {'paperId': '7b50825e62a207ac85e3d568730436d972f31597',\n",
       "    'title': 'Hierarchical Decision Transformer'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': '6c6cfdaea4d7dd986f95f84acba6ad7d72fff655',\n",
       "    'title': 'A Memory-Related Multi-Task Method Based on Task-Agnostic Exploration'},\n",
       "   {'paperId': '57e62822d1f7ebba53aca99fd9b8d646df88e8bf',\n",
       "    'title': 'Task-Agnostic Learning to Accomplish New Tasks'},\n",
       "   {'paperId': '35c756a9100a8ab4573f6fd8f3d91d20e074e3cd',\n",
       "    'title': 'Multi-State-Space Reasoning Reinforcement Learning for Long-Horizon RFID-Based Robotic Searching and Planning Tasks'},\n",
       "   {'paperId': '8aefba738f9e2a886ef2cc18c924f8a9b710b354',\n",
       "    'title': 'Markovian policy network for efficient robot learning'},\n",
       "   {'paperId': 'f6ec139d19a90f79407e7a3c5cb2c8ec8fb2b3a1',\n",
       "    'title': 'Some Supervision Required: Incorporating Oracle Policies in Reinforcement Learning via Epistemic Uncertainty Metrics'},\n",
       "   {'paperId': 'ae40437fb4733c9c4364475fb7b86e9c35d147dc',\n",
       "    'title': 'Multi-goal Reinforcement Learning via Exploring Successor Matching'},\n",
       "   {'paperId': '1c33f76c6f182023a2eefb3bb46cba13fb2345a4',\n",
       "    'title': 'Learning Dynamic Manipulation Skills from Haptic-Play'},\n",
       "   {'paperId': '5b93eb7af42d546c9d2d7ac249fee7a2d238df32',\n",
       "    'title': 'Offline Reinforcement Learning at Multiple Frequencies'},\n",
       "   {'paperId': '78ab35278d8cb36b88e5889cb20ba8576a93b2c1',\n",
       "    'title': 'HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '0ac948bc087603176b47c4af29ef7f240d27b541',\n",
       "    'title': \"Don't Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning\"},\n",
       "   {'paperId': '01d4cc6e7c89f42ad1fc27b57439c9b6c2797fb8',\n",
       "    'title': 'Behavior Transformers: Cloning k modes with one stone'},\n",
       "   {'paperId': '57dc168fe91271c2ba7687c5cf23790d8975d5fa',\n",
       "    'title': 'Social Network Structure Shapes Innovation: Experience-sharing in RL with SAPIENS'},\n",
       "   {'paperId': 'dabaacd7bb97c16417deb4141217b27ef5983541',\n",
       "    'title': 'Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations'},\n",
       "   {'paperId': '4014bf79a220a09d1e380624adff53f5314a7e41',\n",
       "    'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks'},\n",
       "   {'paperId': 'aaf9b9581a848bcf63a10ca6b02d8ef71ff6fdfc',\n",
       "    'title': 'Know Your Boundaries: The Necessity of Explicit Behavioral Cloning in Offline RL'},\n",
       "   {'paperId': '7091030c831a81f5968285e91340b9fd5a107f6c',\n",
       "    'title': 'Training and Inference on Any-Order Autoregressive Models the Right Way'},\n",
       "   {'paperId': 'f593dc96b20ce8427182e773e3b2192d707706a8',\n",
       "    'title': 'Hierarchical Planning Through Goal-Conditioned Offline Reinforcement Learning'},\n",
       "   {'paperId': '6c8ddb095dc34c3e292c689cd77ebbd348b54aaf',\n",
       "    'title': 'A Proprioceptive Haptic Device Design for Teaching Bimanual Manipulation'},\n",
       "   {'paperId': '3b51a29424b619ec5ce29125c4b88d8e24a09328',\n",
       "    'title': 'Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '3f6d6ed110f3262fdc194184c54dd63701b3bca9',\n",
       "    'title': 'Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?'},\n",
       "   {'paperId': 'f1a36b4283e45082183c36ae8f29a77a07f91abd',\n",
       "    'title': 'Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204',\n",
       "    'title': 'R3M: A Universal Visual Representation for Robot Manipulation'},\n",
       "   {'paperId': 'bec65b17fd26e61972a160962f296121c586d43c',\n",
       "    'title': 'Explicit User Manipulation in Reinforcement Learning Based Recommender Systems'},\n",
       "   {'paperId': '54127f35a311ece62bf141f4f60c24f66839f2f3',\n",
       "    'title': 'An Independently Learnable Hierarchical Model for Bilateral Control-Based Imitation Learning Applications'},\n",
       "   {'paperId': '14bcebd262027e3b16766dc2ce128d3710c4ab3e',\n",
       "    'title': 'Blocks Assemble! Learning to Assemble with Large-Scale Structured Reinforcement Learning'},\n",
       "   {'paperId': '4ca19525f996c9e60cca4f7e04ea76a4a1160a11',\n",
       "    'title': 'PLATO: Predicting Latent Affordances Through Object-Centric Play'},\n",
       "   {'paperId': '7b3d26bd1d65ed5937c76043b5cd058260d8469f',\n",
       "    'title': 'The Unsurprising Effectiveness of Pre-Trained Vision Models for Control'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': '00438218d81c2d50fc96592e16c07ae720440bb6',\n",
       "    'title': 'Reinforcement Learning with Sparse Rewards using Guidance from Offline Demonstration'},\n",
       "   {'paperId': 'dcc28a076ab4b0af03a4b13881069b18c4aa1a00',\n",
       "    'title': 'SMODICE: Versatile Offline Imitation Learning via State Occupancy Matching'},\n",
       "   {'paperId': '447ac3d0969ee6fa755483b78567c387d92c33d4',\n",
       "    'title': 'Versatile Offline Imitation from Observations and Examples via Regularized State-Occupancy Matching'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '63afc8d1a187d2f2faf603a51d3987db89574308',\n",
       "    'title': 'RvS: What is Essential for Offline RL via Supervised Learning?'},\n",
       "   {'paperId': '5dd82eee3efefb96aeaaae8b817b6be2e204dc2f',\n",
       "    'title': 'Autonomous Reinforcement Learning: Formalism and Benchmarking'},\n",
       "   {'paperId': '52aeb38922f1f60ef4032012c70f9d5363547e03',\n",
       "    'title': 'C-Planning: An Automatic Curriculum for Learning Goal-Reaching Tasks'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': '7a9846fbb9a580f522ff93f201a6bf15f80d112b',\n",
       "    'title': 'CORA: Benchmarks, Baselines, and Metrics as a Platform for Continual Reinforcement Learning Agents'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '7e4df8928676380cf766eed6ecb20dc8ebb24bd5',\n",
       "    'title': 'RHH-LGP: Receding Horizon And Heuristics-Based Logic-Geometric Programming For Task And Motion Planning'},\n",
       "   {'paperId': '371e78d3afbfce780f46bf97ad6f16bb652cd6e4',\n",
       "    'title': 'Understanding Domain Randomization for Sim-to-real Transfer'},\n",
       "   {'paperId': 'ee21c47254d1bcf33e13bf746218021816443745',\n",
       "    'title': 'Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation'},\n",
       "   {'paperId': '259b4f5ed43fda5dd3510821b40fac13021e7605',\n",
       "    'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '165f948f15cfd676c6f285aecc81f6f9d284da71',\n",
       "    'title': 'Efficient and Interpretable Robot Manipulation with Graph Neural Networks'},\n",
       "   {'paperId': '008d9e6a2d9bd8e3c2698aded41f82c644a248db',\n",
       "    'title': 'Imitation Learning for Variable Speed Contact Motion for Operation up to Control Bandwidth'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '18d69b50ecf56a16906ce57d058a9394b36d9009',\n",
       "    'title': 'W HAT M AKES C ERTAIN P RE -T RAINED V ISUAL R EP - RESENTATIONS B ETTER FOR R OBOTIC L EARNING ?'},\n",
       "   {'paperId': '4fb3695d7a3cba3db438cda198c724225ab48a38',\n",
       "    'title': 'Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods'},\n",
       "   {'paperId': '7730caf10a353db1cbee194b12ee89546d9fbb55',\n",
       "    'title': 'S OCIAL N ETWORK S TRUCTURE S HAPES I NNOVATION : E XPERIENCE SHARING IN RL WITH SAPIENS'},\n",
       "   {'paperId': '60fe59d1d594044bd19fee87be0926feb81a3f2d',\n",
       "    'title': 'Robust Manipulation with Spatial Features'},\n",
       "   {'paperId': '4dec6c9295e24dc884991893e30dec664034b928',\n",
       "    'title': 'SPRINT: Scalable Semantic Policy Pre-Training via Language Instruction Relabeling'},\n",
       "   {'paperId': 'b6db3ffef26d59bb27b487b0dc7e43d6aaf75faf',\n",
       "    'title': 'Assisted Teleoperation for Scalable Robot Data Collection'},\n",
       "   {'paperId': '3f14defdaf32541312c1378ac4c1dcf1e685789b',\n",
       "    'title': 'Squeezing more value out of your historical data: data-augmented behavioural cloning as launchpad for reinforcement learning'},\n",
       "   {'paperId': '5840bf765be8c3bcedab63f43f5982ddba26eaf9',\n",
       "    'title': 'SPRINT: S CALABLE S EMANTIC P OLICY P RE T RAINING VIA L ANGUAGE I NSTRUCTION R ELABELING'},\n",
       "   {'paperId': '3ec1723060c31489c5ab953b9402302d360eab83',\n",
       "    'title': 'Coordinating Heterogeneous Teams for Urban Search and Rescue'},\n",
       "   {'paperId': '800a1917c57c5701cc974e5498ad27a61ae0f292',\n",
       "    'title': 'Exploring Long-Horizon Reasoning with Deep RL in Combinatorially Hard Tasks'},\n",
       "   {'paperId': '17cff2d7c1b50cef6f875997d27e79de56b5b0b1',\n",
       "    'title': 'P LANNING TO P RACTICE : E FFICIENT O NLINE F INE T UNING BY C OMPOSING G OALS IN L ATENT S PACE'},\n",
       "   {'paperId': '0e66997f28780f5883987dced13560034caa7afe',\n",
       "    'title': 'A UTONOMOUS R EINFORCEMENT L EARNING : F ORMALISM AND B ENCHMARKING'},\n",
       "   {'paperId': '9d2038e233042790929123ff62354eb18ee52e47',\n",
       "    'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning'},\n",
       "   {'paperId': '7b599fdcbe89b694f8314e8dd77a097000dedabb',\n",
       "    'title': 'Learning Multi-Stage Tasks with One Demonstration via Self-Replay'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '035968f4b88b508f8850c893f520fec977b1a548',\n",
       "    'title': 'Grasp Planning Based on Deep Reinforcement Learning: A Brief Survey'},\n",
       "   {'paperId': '4545358c54c90dc5d5eb8f11a3c610b3eda88b55',\n",
       "    'title': 'Discovering and Achieving Goals via World Models'},\n",
       "   {'paperId': '752ce134e4548e5b713bf759e8514b9bb71f10c8',\n",
       "    'title': 'You Only Evaluate Once: a Simple Baseline Algorithm for Offline RL'},\n",
       "   {'paperId': '06858604cc652722ca5092072c50a066000c565e',\n",
       "    'title': 'Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks'},\n",
       "   {'paperId': '0f929e132ccc363a2d707c24319c2894935d93c8',\n",
       "    'title': 'Video2Skill: Adapting Events in Demonstration Videos to Skills in an Environment using Cyclic MDP Homomorphisms'},\n",
       "   {'paperId': '1f89b7bca554fe9f15ed67ae00feeb2a20a2bb4d',\n",
       "    'title': 'HAC Explore: Accelerating Exploration with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': 'fb95d6e6e5f78f6e5c339e2058ce9ae9e803182b',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning with Imagined Subgoals'},\n",
       "   {'paperId': '6d0055cc647f7a021abd3df9cc2591cb2534d0cb',\n",
       "    'title': 'Towards Exploiting Geometry and Time for Fast Off-Distribution Adaptation in Multi-Task Robot Learning'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': '8511cafc50b15d8c56c29f601a846fd6dfd888f0',\n",
       "    'title': 'Believe What You See: Implicit Constraint Approach for Offline Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '0ea89ca0929b9323ba3d65bc6bce654d37cdcea4',\n",
       "    'title': 'Hierarchical Learning from Demonstrations for Long-Horizon Tasks'},\n",
       "   {'paperId': 'b0829f5c4ae98bcc00e54e1b50400f0523215204',\n",
       "    'title': 'Learning Geometric Reasoning and Control for Long-Horizon Tasks from Visual Input'},\n",
       "   {'paperId': '3b60094ae6c02794fb4c8dde8d4567c14af0410b',\n",
       "    'title': 'Learning a Skill-sequence-dependent Policy for Long-horizon Manipulation Tasks'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '54e1040e7f9f7d6b52abf03a1353b5a377802396',\n",
       "    'title': 'Robotic Imitation of Human Assembly Skills Using Hybrid Trajectory and Force Learning'},\n",
       "   {'paperId': '2abd57691e54a06eb84433238ce6522697ea5dc8',\n",
       "    'title': 'Bilateral Control-Based Imitation Learning for Velocity-Controlled Robot'},\n",
       "   {'paperId': 'c15fd6d1f3599808f1ba4ff4803447e2898c3937',\n",
       "    'title': 'Twin Delayed Hierarchical Actor-Critic'},\n",
       "   {'paperId': '943f1cad798837cd5bda3574c2f6f31a718c5294',\n",
       "    'title': 'Near Real-World Benchmarks for Offline Reinforcement Learning'},\n",
       "   {'paperId': '841f8e46c359b86ed1da7dafa3062ef9f351b5a4',\n",
       "    'title': 'NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning'},\n",
       "   {'paperId': '12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation'},\n",
       "   {'paperId': '0a7f43c5d6aae0b590f1ecb6616181246a3bff89',\n",
       "    'title': 'Machine Learning for Robotic Manipulation'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'e77c0dcc5704c1c5de1ee091600ab6ca14b92784',\n",
       "    'title': 'C-Learning: Learning to Achieve Goals via Recursive Classification'},\n",
       "   {'paperId': 'd01fac9eefa883f311e43eee94d830658d7cb5cc',\n",
       "    'title': 'Motion Generation Using Bilateral Control-Based Imitation Learning With Autoregressive Learning'},\n",
       "   {'paperId': '72c034e53213cc2f4913d73dd838b64d7b641585',\n",
       "    'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning'},\n",
       "   {'paperId': '07740f2998a5a69cbf199ad29afef5eec678a2a3',\n",
       "    'title': 'ScrewNet: Category-Independent Articulation Model Estimation From Depth Images Using Screw Theory'},\n",
       "   {'paperId': '1301e9d11b728268ed1ff3f1a9adc155308d5250',\n",
       "    'title': 'Language Conditioned Imitation Learning Over Unstructured Data'},\n",
       "   {'paperId': '831e7cbafed2dca05db1e7f5ef16d1a7614f44ec',\n",
       "    'title': 'Learning to Reach Goals via Iterated Supervised Learning'},\n",
       "   {'paperId': 'da18109d2725757842c5b5bca85025f23677f323',\n",
       "    'title': 'IKEA Furniture Assembly Environment for Long-Horizon Complex Manipulation Tasks'},\n",
       "   {'paperId': '261ada5d3d3a7653faf91194c10f61a098908171',\n",
       "    'title': 'Replay Buffer start Strategy a ) Strategy b ) EncoderEncoder Encoder Encoder EncoderGoal Goal DBTaskDemonstrationsif successful Online Goal Selection'},\n",
       "   {'paperId': '98273abd3ebf45923eeab0bd326c970a6f2b6ffe',\n",
       "    'title': 'Discovering and Achieving Goals with World Models'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'},\n",
       "   {'paperId': '8c0878331197fe7745cb7a6ef4b3a7fee6698c07',\n",
       "    'title': 'Toward robots that learn to summarize their actions in natural language: a set of tasks'},\n",
       "   {'paperId': 'ede66a0627ee774052c4234311be1966cf927634',\n",
       "    'title': 'Bottom-up Discovery of Reusable Sensorimotor Skills from Unstructured Demonstrations'},\n",
       "   {'paperId': 'd437e3ade8bcd85120d0c47bf3e97e8a93c95133',\n",
       "    'title': 'A Unified View of Memory Models'},\n",
       "   {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       "   {'paperId': 'fb9486b9398132dec86e2b3d9cdf586621c227af',\n",
       "    'title': 'Hybrid Trajectory and Force Learning of Complex Assembly Tasks: A Combined Learning Framework'},\n",
       "   {'paperId': '32c47bccaac50e30255b54c3452ba631138fb824',\n",
       "    'title': 'Imitation Learning for Variable Speed Object Manipulation'},\n",
       "   {'paperId': '643812d844e5a0e65fd6ca6e155f1e6e40c02bcb',\n",
       "    'title': 'SAFARI: Safe and Active Robot Imitation Learning with Imagination'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '3c279a4760315ca9ab29255ff7ff0a9c1717948b',\n",
       "    'title': 'Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for Physically Embedded 3D Sokoban'},\n",
       "   {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "    'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       "   {'paperId': 'a1257d485970d89125795e1c9308cd4bb54309c5',\n",
       "    'title': 'Variable Compliance Control for Robotic Peg-in-Hole Assembly: A Deep Reinforcement Learning Approach'},\n",
       "   {'paperId': '21310151ec00bbd5b25f59c1599995fb56da565e',\n",
       "    'title': 'Generalization Guarantees for Imitation Learning'},\n",
       "   {'paperId': '1fca46fa0b6e334d9a20eb8f4eb01a6c556ef261',\n",
       "    'title': 'Generalization Guarantees for Multi-Modal Imitation Learning'},\n",
       "   {'paperId': '114b52291b549466a4b1027f4248a122c1c3920c',\n",
       "    'title': 'Learning Compositional Neural Programs for Continuous Control'},\n",
       "   {'paperId': '57398b124b73081a5339c850aee9fedd4ce2bd8d',\n",
       "    'title': 'Deep PQR: Solving Inverse Reinforcement Learning using Anchor Actions'},\n",
       "   {'paperId': '38acfd88d7ee8517a1deff75f1e80cab713e76fc',\n",
       "    'title': 'Identifying Reward Functions using Anchor Actions'},\n",
       "   {'paperId': 'c328a1ea31eaa4b357e584285ffb6ff2671b5bd4',\n",
       "    'title': 'Concept2Robot: Learning manipulation concepts from instructions and human demonstrations'},\n",
       "   {'paperId': '57a121e51b4cd1cd2a81506ce32196217b439a46',\n",
       "    'title': 'Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving'},\n",
       "   {'paperId': '0272b14dd471fe7b81df703af1b71d7600b77215',\n",
       "    'title': 'Accelerating Online Reinforcement Learning with Offline Datasets'},\n",
       "   {'paperId': 'dfca81607959eb7f101911288025a598bc5a6d18',\n",
       "    'title': 'Learning to Play by Imitating Humans'},\n",
       "   {'paperId': '28db20a81eec74a50204686c3cf796c42a020d2e',\n",
       "    'title': 'Conservative Q-Learning for Offline Reinforcement Learning'},\n",
       "   {'paperId': '8095bdd5861d1dbe43b77997bc0dbc2fd51acb93',\n",
       "    'title': 'Grounding Language in Play'},\n",
       "   {'paperId': 'a326d9f2d2d351001fece788165dbcbb524da2e4',\n",
       "    'title': 'D4RL: Datasets for Deep Data-Driven Reinforcement Learning'},\n",
       "   {'paperId': '4d70340983c43447e78096d30cfa2745a7be26e0',\n",
       "    'title': 'Learning Hybrid Object Kinematics for Efficient Hierarchical Planning Under Uncertainty'},\n",
       "   {'paperId': '1361f51fdc499c61f191859c9c2232590825a2c0',\n",
       "    'title': 'Offline Imitation Learning with a Misspecified Simulator'},\n",
       "   {'paperId': 'ffba26a38d4b3c25d2984266f72f72889b2413ff',\n",
       "    'title': 'Learning To Reach Goals Without Reinforcement Learning'},\n",
       "   {'paperId': '7724b771fbb8006f4acf3a3376fbb1988beb1405',\n",
       "    'title': 'An AI Integrator for Efficient Customization of Collaborative Robots'}],\n",
       "  'citnuminlist': 8,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " '2fed116dea9c36914b52b55e0f9688ccf641ee07': {'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning',\n",
       "  'year': 2019,\n",
       "  'references': [{'paperId': 'd37a34c204a8beefcaef4dddddb7a90c16e973d4',\n",
       "    'title': 'Learning dexterous in-hand manipulation'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'b08256a6a3faba052bdd3445d917dd656d248477',\n",
       "    'title': 'Credit Assignment Techniques in Stochastic Computation Graphs'},\n",
       "   {'paperId': '7d5dd17e1a10c0388d2e134dedcfe4e8b224352b',\n",
       "    'title': 'Self-supervised Learning of Image Embedding for Continuous Control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '4b847e03b04f02395f8cb864b323a7eb98dcba7e',\n",
       "    'title': 'Multitask Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c852a954c3a74df410231d601857b7005076de9',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Hindsight'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fb9693183bc74568c72188431c18cb2b07c87213',\n",
       "    'title': 'Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '809f951c77b5a39e2a9d556e9cf9938de87f2393',\n",
       "    'title': 'An Inference-Based Policy Gradient Method for Learning Options'},\n",
       "   {'paperId': '8a05d72b6f54479c5ab3ceaccd221b2119f0ea7d',\n",
       "    'title': 'The Mirage of Action-Dependent Baselines in Reinforcement Learning'},\n",
       "   {'paperId': '0b670a55f83a08d0f049b30f48450fe608f90613',\n",
       "    'title': 'Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '96e81cabed55630f2ad3e1346300bd7a7a17f060',\n",
       "    'title': 'When Waiting is not an Option : Learning Options with a Deliberation Cost'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Their method can be included in our framework, and hence benefits from our new theoretical insights'},\n",
       "   {'paperId': 'c27db32efa8137cbf654902f8f728f338e55cd1c',\n",
       "    'title': 'Mastering the game of Go without human knowledge'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': '9862caed8ee93321c78b0196e0b7eef516b545ba',\n",
       "    'title': 'Reverse Curriculum Generation for Reinforcement Learning'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '1464776f20e2bccb6182f183b5ff2e15b0ae5e56',\n",
       "    'title': 'Benchmarking Deep Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "    'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       "   {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "    'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': None, 'title': 'ISSN 21530866'},\n",
       "   {'paperId': '512ea8d0c5b5de896129e76d4276f7b996fe88d8',\n",
       "    'title': 'Learning Stochastic Feedforward Neural Networks'},\n",
       "   {'paperId': 'b354ee518bfc1ac0d8ac447eece9edb69e92eae1',\n",
       "    'title': 'MuJoCo: A physics engine for model-based control'},\n",
       "   {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "    'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': '9aff035de60fa73a167189743e0a179741b44aa9',\n",
       "    'title': 'Hierarchical Policy Gradient Algorithms'},\n",
       "   {'paperId': 'b18833db0de9393d614d511e60821a1504fc6cd1',\n",
       "    'title': 'A Natural Policy Gradient'},\n",
       "   {'paperId': '985f2c1baba284e9b7b604b7169a2e2778540fe6',\n",
       "    'title': 'Temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'George Van Den Driessche, Thore Graepel, and Demis Hassabis'}],\n",
       "  'citations': [{'paperId': 'd95b56b949610ab87866652b1944207c10b68631',\n",
       "    'title': 'A Hierarchical Approach to Population Training for Human-AI Collaboration'},\n",
       "   {'paperId': 'd5781022f211bc2bc8eaeeb574e0fef86a58ec45',\n",
       "    'title': 'PushWorld: A benchmark for manipulation planning with tools and movable obstacles'},\n",
       "   {'paperId': '3ecbf75ca51133eb59a66ddb28d057a14dd538c1',\n",
       "    'title': 'Reusable Options through Gradient-based Meta Learning'},\n",
       "   {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "    'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '18d02042724a7fe29ae6aa9460353d7e2425ca86',\n",
       "    'title': 'Matching options to tasks using Option-Indexed Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '47713fb4a46d47a2bb56a958c5b0403de1858085',\n",
       "    'title': 'Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9274ebb09a17afd4d5e80a714d8afbc7e21bec50',\n",
       "    'title': 'Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents'},\n",
       "   {'paperId': '41e41f650fc1926eda019be894ad96ab56315860',\n",
       "    'title': 'CIM: Constrained Intrinsic Motivation for Sparse-Reward Continuous Control'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '57a6d470eacba2233caf811dfe036bba145fa292',\n",
       "    'title': 'DMAP: a Distributed Morphological Attention Policy for Learning to Locomote with a Changing Body'},\n",
       "   {'paperId': 'f2250d71e0159e8ed2b2ad0e20f3b7da52c2ef3e',\n",
       "    'title': 'UAV path planning based on the improved PPO algorithm'},\n",
       "   {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "    'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       "   {'paperId': '16b242611c0e5986fb695c0278de6840170ee0dc',\n",
       "    'title': 'Lifelong learning for robust AI systems'},\n",
       "   {'paperId': '03fea73b0cc7cd13f4148a4ab49d670f6ddda930',\n",
       "    'title': 'Hierarchical Reinforcement Learning under Mixed Observability'},\n",
       "   {'paperId': '7726f053a24ae08b1514d3df2bbcd7885a44f53f',\n",
       "    'title': 'SAGE: Generating Symbolic Goals for Myopic Models in Deep Reinforcement Learning'},\n",
       "   {'paperId': '1952fc35b919f149a4cc2647c114b7038593391c',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions from Language'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': 'a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb',\n",
       "    'title': 'ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': 'c7b01136da36d68e229f268e5489c81c94586481',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation'},\n",
       "   {'paperId': '40c099d10be5a7ff429ee13f4c3f8b49142b1587',\n",
       "    'title': 'Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces'},\n",
       "   {'paperId': 'a2335853a24f48f3569344062b830f2d8d0dc6ad',\n",
       "    'title': 'LISA: Learning Interpretable Skill Abstractions'},\n",
       "   {'paperId': '2c89e7d31c31a63ac72f86067b1876c18f51ec32',\n",
       "    'title': 'Guided Imitation of Task and Motion Planning'},\n",
       "   {'paperId': '0b43ccd62334cd38e3752e92ce0274cf48e3a35e',\n",
       "    'title': 'Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '9291fab6800d152955c813331ef9465efad3d07f',\n",
       "    'title': 'Multitask Adaptation by Retrospective Exploration with Learned World Models'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '93a44d0cc9cb92b0f8a4b948de50298309229057',\n",
       "    'title': 'Layered Relative Entropy Policy Search'},\n",
       "   {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "    'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       "   {'paperId': '1508879dae81f73f56ba0cb0e25150d9c5f8f731',\n",
       "    'title': 'TAAC: Temporally Abstract Actor-Critic for Continuous Control'},\n",
       "   {'paperId': 'abd77509ef1cc739c0757ae657025fcee13c98dc',\n",
       "    'title': 'Program Synthesis Guided Reinforcement Learning for Partially Observed Environments'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '630b70d8fb6dd3c01983f5a1cefc8b5fbb6af1d3',\n",
       "    'title': 'Anchor: The achieved goal to replace the subgoal for hierarchical reinforcement learning'},\n",
       "   {'paperId': '5d83fb8e2382a31a5c926cd1ef3dcfdad0971c3f',\n",
       "    'title': 'Accelerate Then Imitate: Learning from Task and Motion Planing'},\n",
       "   {'paperId': '3ba5343e8945f26c161bb512bf3d7ea5d4434f32',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Universal Policies for Multi-Step Robotic Manipulation'},\n",
       "   {'paperId': '82336295513221f4ceeca5246d664f28cfda4321',\n",
       "    'title': 'Metis: Learning to Schedule Long-Running Applications in Shared Container Clusters at Scale'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': '7b0871c783e721bfbf9b5d16e575130a07a672cd',\n",
       "    'title': 'Generalized Hindsight for Reinforcement Learning'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'd9e14b81e7acf2e17de20df113018943978509ae',\n",
       "    'title': 'Inter-Level Cooperation in Hierarchical Reinforcement Learning'}],\n",
       "  'citnuminlist': 3,\n",
       "  'refnuminlist': 9,\n",
       "  'isKeypaper': True},\n",
       " 'a13a62d16ebeaff7c5125a2b60cfc30a35cd55af': {'title': 'Joint Optimization of Quota Policy Design and Electric Market Behavior Based on Renewable Portfolio Standard in China',\n",
       "  'year': 2021,\n",
       "  'references': [{'paperId': '11b2340425252d010165af740f13f8340ba842b1',\n",
       "    'title': 'Optimal Price-maker Trading Strategy of Wind Power Producer Using Virtual Bidding'},\n",
       "   {'paperId': '721539714cbcba10882d7cf8a22e6e57359291c1',\n",
       "    'title': 'A Price-Based Iterative Double Auction for Charger Sharing Markets'},\n",
       "   {'paperId': '1f669c4a2475d5e418f92e6c3cd7293dc1551301',\n",
       "    'title': \"Can the Renewable Portfolio Standards improve social welfare in China's electricity market？\"},\n",
       "   {'paperId': '4e93cc94601827d6ebd27efa811b35fbb73dc986',\n",
       "    'title': 'Provincial CO2 Emission Measurement and Analysis of the Construction Industry under China’s Carbon Neutrality Target'},\n",
       "   {'paperId': 'ceda5db43ab51b729e88dc61df5cd03b02991c78',\n",
       "    'title': 'Impact of market design on cost-effectiveness of renewable portfolio standards'},\n",
       "   {'paperId': 'eb79d1b9f0208531c09ca310f4a022ff5a788076',\n",
       "    'title': 'Integration of tradable green certificates trading and carbon emissions trading: How will Chinese power industry do?'},\n",
       "   {'paperId': '3cde135235676a64d55ce6ba5c10f5467d2ed0d8',\n",
       "    'title': 'Renewable energy investment and carbon emissions under cap-and-trade mechanisms'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Multi-agent deep deterministic policy gradient algorithm via prioritized experience selected method'},\n",
       "   {'paperId': '6a626a9f41a1fbb095642cf15ef01a1d3c5946f7',\n",
       "    'title': 'Green innovation transformation, economic sustainability and energy consumption during China’s new normal stage'},\n",
       "   {'paperId': '2b901ccb410e33d112d0b7bf199c027fc41abc3a',\n",
       "    'title': 'Will China achieve its renewable portfolio standard targets? An analysis from the perspective of supply and demand'},\n",
       "   {'paperId': '16e83f3f0f78ceb203746eeb88f1f5aae9ba3092',\n",
       "    'title': 'Deep reinforcement learning: a survey'},\n",
       "   {'paperId': '326d54de0ecdb364bc7493d21aa1947a747723eb',\n",
       "    'title': 'A Stochastic Bilevel Model for an Electricity Retailer in a Liberalized Distributed Renewable Energy Market'},\n",
       "   {'paperId': '26acac67e0488b7388d0b987fc9b28ef77999297',\n",
       "    'title': 'Efficiency of tradable green certificate markets in China'},\n",
       "   {'paperId': '554653772268e36e63064d1ee77024bedf251a98',\n",
       "    'title': \"Second-Order Stochastic Dominance Constraints for Risk Management of a Wind Power Producer's Optimal Bidding Strategy\"},\n",
       "   {'paperId': 'aa47cef6dd8de1acd8ba380f47627f5a9b1fc8cc',\n",
       "    'title': 'Research on Evolution and Development of Power Generation Scale and Cost under Tradable Green Certificates Market in China'},\n",
       "   {'paperId': '4740cd5366a57df1abdc413dd48892b417ccd901',\n",
       "    'title': 'Can One Reinforce Investments in Renewable Energy Stock Indices with the ESG Index?'},\n",
       "   {'paperId': 'e0b53bc295dfe17dfd71bfe2d24b2584e5df058f',\n",
       "    'title': 'A new emission trading with bounded rational countries: a network game approach'},\n",
       "   {'paperId': '3f94f6989aa20e861c4cea385512404453b002d4',\n",
       "    'title': 'Joint Demand Response and Energy Trading for Electric Vehicles in Off-Grid System'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Multi-population asymmetric evolutionary game dynamics and its applications in power demand-side response in smart grid'},\n",
       "   {'paperId': 'eb86ae80e96a2fbaa2964ce10f0d82a9dc466ec5',\n",
       "    'title': 'Oligopoly Games and the Cournot–Bertrand Model: A Survey'},\n",
       "   {'paperId': '41ef1697c01f0be949c3a308fab8a6a902af6279',\n",
       "    'title': 'The relationship between renewable energy consumption and economic growth'},\n",
       "   {'paperId': '730cc2e805e61ed72248b937b0379a0e2c2e1152',\n",
       "    'title': 'Latent semantic analysis of game models using LSTM'},\n",
       "   {'paperId': '69d27f7404e8eda1b63a478626b672a93921739b',\n",
       "    'title': 'From feed-in tariff to renewable portfolio standards: An evolutionary game theory perspective'},\n",
       "   {'paperId': '5c8102edc02bbc1d61aa96c83af6138b569183eb',\n",
       "    'title': 'A One-Dimensional Stack Model for Redox Flow Battery Analysis and Operation'},\n",
       "   {'paperId': '86c5149d161524b575907a9195ea5411e795b151',\n",
       "    'title': 'The effects of electric power lines on the breeding ecology of greater sage-grouse'},\n",
       "   {'paperId': '09223c91658a19c9bf82b933c36817fcd8afa652',\n",
       "    'title': 'Possible design with equity and responsibility in China’s renewable portfolio standards'},\n",
       "   {'paperId': '4d99c47ed14eb1514cabe0c155a5110234cf9f4a',\n",
       "    'title': 'The costs and value of renewable portfolio standards in meeting decarbonization goals'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4debb99c0c63bfaa97dd433bc2828e4dac81c48b',\n",
       "    'title': 'Addressing Function Approximation Error in Actor-Critic Methods'},\n",
       "   {'paperId': 'd355e339298fc2ab920688c1709d4ba6476a2bc6',\n",
       "    'title': 'Distributed Distributional Deterministic Policy Gradients'},\n",
       "   {'paperId': '8d07c745d02b3e5ed7c2a049643afed000bf78c7',\n",
       "    'title': 'Optimal regulation of renewable energy: A comparison of Feed-in Tariffs and Tradable Green Certificates in the Spanish electricity system'},\n",
       "   {'paperId': 'eb70c8d28d24d8a4311a645dd9e99c378bf59439',\n",
       "    'title': 'Evaluation of Achievable Vehicle-to-Grid Capacity Using Aggregate PEV Model'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': '220b6b5a81541e80d937cb5a81ba8aa19691af9c',\n",
       "    'title': 'On social utility payoffs in games: a methodological comparison between Behavioural and Rational Game Theory'},\n",
       "   {'paperId': '07b6c8b461124c5ffd6f124658cab3e21d68258f',\n",
       "    'title': 'Rational expectations in games'},\n",
       "   {'paperId': '0734799f716af4a925616b97294553f5c3bcdcb6',\n",
       "    'title': 'Nash-Cournot Equilibria in Electric Power Markets with Piecewise Linear Demand Functions and Joint Constraints'},\n",
       "   {'paperId': '97efafdb4a3942ab3efba53ded7413199f79c054',\n",
       "    'title': 'Reinforcement Learning: An Introduction'},\n",
       "   {'paperId': None,\n",
       "    'title': 'degrees in electrical engineering from Tsinghua University, China. He is currently an Associate Professor with the School of Economics and Management'}],\n",
       "  'citations': [{'paperId': '3d0b4d34f92a3d7a64ea801fc1cb97022caf6488',\n",
       "    'title': 'Application-Driven Data Management Framework for Wind Farms by Cloud-Edge-End Collaboration'},\n",
       "   {'paperId': '9b84f0ceb157df5436872f6e5abc9c2cf1fbbd84',\n",
       "    'title': 'Visual Analysis of the Green Power Trading Research Based on the CiteSpace and VOSviewer'},\n",
       "   {'paperId': '42987d2ccece20750fc3c5d64c710dae40586a6d',\n",
       "    'title': 'Self-powered Triboelectric Vibration Sensor Network Construction for Overhead Transmission Line Vibration Mapping'},\n",
       "   {'paperId': '14d3499f49a113ed550f46a9765c38eaed4e31cb',\n",
       "    'title': 'How will tradable green certificates affect electricity trading markets under renewable portfolio standards? A China perspective'},\n",
       "   {'paperId': '24c8ae4aba61d77393f6f1dbeef24582b7cb9138',\n",
       "    'title': 'Event Triggered-MPC for Energy Management of Battery-Supercapacitor-PV Hybrid Power Source'}],\n",
       "  'citnuminlist': 0,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'fbf03bf621ffee283911e765d525a75fc0d11bae': {'title': 'CompILE: Compositional Imitation Learning and Execution',\n",
       "  'year': 2018,\n",
       "  'references': [{'paperId': '36cddcdafdcbfe830a644777befa4a707839053c',\n",
       "    'title': 'KeyIn: Discovering Subgoal Structure with Keyframe-based Video Prediction'},\n",
       "   {'paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "    'title': 'Composing Complex Skills by Learning Transition Policies'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '973e116b2b2949fdb8c533a8d84ddd811c0920cf',\n",
       "    'title': 'Time-Agnostic Prediction: Predicting Predictable Video Frames'},\n",
       "   {'paperId': '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85',\n",
       "    'title': 'Learning Abstract Options'},\n",
       "   {'paperId': '963cc19101ac6409ba6db0bc07d8ce1baecaf84a',\n",
       "    'title': 'Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models'},\n",
       "   {'paperId': 'c35f8f48f748c040c33bbe1f21c794e209341987',\n",
       "    'title': 'Neural Program Synthesis from Diverse Demonstration Videos'},\n",
       "   {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "    'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       "   {'paperId': '6aae1bc6c8e38c9a1d24a2b48bfe066f3591e1bc',\n",
       "    'title': 'Subgoal Discovery for Hierarchical Dialogue Policy Learning'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': '80196cdfcd0c6ce2953bf65a7f019971e2026386',\n",
       "    'title': 'IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures'},\n",
       "   {'paperId': 'a9a3ed69c94a3e1c08ef1f833d9199f57736238b',\n",
       "    'title': 'DeepMind Control Suite'},\n",
       "   {'paperId': '9bdf53f298d6829ca457ff256560637945a9a501',\n",
       "    'title': 'Constructing Experience: Event Models from Perception to Action'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': '2b292d7660b488315adbdac45e3dbf9cb3947a35',\n",
       "    'title': 'Event boundaries in memory and cognition'},\n",
       "   {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "    'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       "   {'paperId': '204e3073870fae3d05bcbc2f6a8e263d9b72e776',\n",
       "    'title': 'Attention is All you Need'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '96dd1fc39a368d23291816d57763bc6eb4f7b8d6',\n",
       "    'title': 'Dense-Captioning Events in Videos'},\n",
       "   {'paperId': 'a59658d7b74f63a34e7182addbba1214775f49f5',\n",
       "    'title': 'DART: Noise Injection for Robust Imitation Learning'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6db294e9309349d82df47a912ccc47eab1dc1358',\n",
       "    'title': 'Sequence Modeling via Segmentations'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a90226c41b79f8b06007609f39f82757073641e2',\n",
       "    'title': 'beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework'},\n",
       "   {'paperId': 'e20d94e43a20315f76067a00d67e4f7523ca7311',\n",
       "    'title': 'Recurrent Hidden Semi-Markov Model'},\n",
       "   {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "    'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       "   {'paperId': '515a21e90117941150923e559729c59f5fdade1c',\n",
       "    'title': 'The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables'},\n",
       "   {'paperId': '9e4134d0fba86de14c31c8f9e9a237e2faaffc68',\n",
       "    'title': 'Discovering Event Structure in Continuous Narrative Perception and Memory'},\n",
       "   {'paperId': '105788dd22393d5a4333c167814ec3d38c7d6612',\n",
       "    'title': 'Latent Sequence Decompositions'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'c24bbbc5139eb2f8c5d0579174dbeae5cbaedbfc',\n",
       "    'title': 'DAPs: Deep Action Proposals for Action Understanding'},\n",
       "   {'paperId': '97fb4e3d45bb098e27e0071448b6152217bd35a5',\n",
       "    'title': 'Layer Normalization'},\n",
       "   {'paperId': '2cd8e8f510c89c7c18268e8ad51c061e459ad321',\n",
       "    'title': 'A Decomposable Attention Model for Natural Language Inference'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '9ca3af4440eb4aa4fd0a65dfa559685b2c39cd42',\n",
       "    'title': 'Composing graphical models with neural networks for structured representations and fast inference'},\n",
       "   {'paperId': '3f25e17eb717e5894e0404ea634451332f85d287',\n",
       "    'title': 'Learning Structured Output Representation using Deep Conditional Generative Models'},\n",
       "   {'paperId': 'dce1da37953f6f54abaf5e34d1096dbbb822f416',\n",
       "    'title': 'Towards learning hierarchical skills for multi-phase manipulation tasks'},\n",
       "   {'paperId': 'a6cb366736791bcccc5c8639de5a8f9636bf87e8',\n",
       "    'title': 'Adam: A Method for Stochastic Optimization'},\n",
       "   {'paperId': 'd4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0',\n",
       "    'title': 'Show and tell: A neural image caption generator'},\n",
       "   {'paperId': '54e325aee6b2d476bbbb88615ac15e251c6e8214',\n",
       "    'title': 'Generative Adversarial Nets'},\n",
       "   {'paperId': 'cea967b59209c6be22829699f05b8b1ac4dc092d',\n",
       "    'title': 'Sequence to Sequence Learning with Neural Networks'},\n",
       "   {'paperId': '484ad17c926292fbe0d5211540832a8c8a8e958b',\n",
       "    'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': '682553316f25d737adea45b1223dc7bdba99d3af',\n",
       "    'title': 'Incremental Semantically Grounded Learning from Demonstration'},\n",
       "   {'paperId': 'f4b4a1f89046325c5f6526b3df3eb09284cb73e9',\n",
       "    'title': 'Supervised Sequence Labelling'},\n",
       "   {'paperId': 'f6640a0d16fc3d86dc24ec3b61d6b54d2ebbf0e2',\n",
       "    'title': 'What Constitutes an Episode in Episodic Memory?'},\n",
       "   {'paperId': '46f31f9069bb934498a288126053bcab01ff34aa',\n",
       "    'title': 'A Bayesian framework for word segmentation: Exploring the effects of context'},\n",
       "   {'paperId': '845c20b482765922c67d3ee0ae650c069d7013b0',\n",
       "    'title': 'Topic segmentation with an aspect hidden Markov model'},\n",
       "   {'paperId': '2613e934022204b91d7b60ff407eb4d72c4cb341',\n",
       "    'title': 'Perceiving, remembering, and communicating structure in events.'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '44d2abe2175df8153f465f6c39b68b76a0d40ab9',\n",
       "    'title': 'Long Short-Term Memory'},\n",
       "   {'paperId': 'b891bc93b0354acc68bdb71f91e03478d3fb95b6',\n",
       "    'title': 'Inquiries Into Truth and Interpretation'}],\n",
       "  'citations': [{'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': '2ddd59c64c0407f2386241b3d5f369bbea4baa8e',\n",
       "    'title': 'Learning Rational Subgoals from Demonstrations and Instructions'},\n",
       "   {'paperId': 'd914dc7f5d9291ee2127936e3206c90ca1fcea71',\n",
       "    'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation'},\n",
       "   {'paperId': '329d809a69eada0521da1b536bb9db9348d51868',\n",
       "    'title': 'Hierarchical Imitation Learning with Vector Quantized Models'},\n",
       "   {'paperId': '6d7af0617b0fc390baddb93816975f1bdb1985cd',\n",
       "    'title': 'Unsupervised Learning of Temporal Abstractions With Slot-Based Transformers'},\n",
       "   {'paperId': '6bbf2b090d3741068728d6cc30bc4cab15271360',\n",
       "    'title': 'COACH: Cooperative Robot Teaching'},\n",
       "   {'paperId': '8cbff5eeb1d503e79423a148aa494458c308f772',\n",
       "    'title': 'Long-horizon video prediction using a dynamic latent hierarchy'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "    'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'c90a33f1f0049d524e9b5b3174d35611fd9a8096',\n",
       "    'title': 'Pretraining in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': '568764405056b12e6b957349be3f42d094762864',\n",
       "    'title': 'Sub-Task Imputation via Self-Labelling to Train Image Moderation Models on Sparse Noisy Data'},\n",
       "   {'paperId': '90c7e3f6d1b10fa31d1c2b7c3413805eee0607d8',\n",
       "    'title': 'Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': '381af7aa06e245b72d20d5cbcbed7438dbf62de9',\n",
       "    'title': 'Option-Aware Adversarial Inverse Reinforcement Learning for Robotic Control'},\n",
       "   {'paperId': '8993e42af1aa905f322324833455c6ce207b09e2',\n",
       "    'title': 'Learning generalizable behaviors from demonstration'},\n",
       "   {'paperId': '83f1343500c9f0df62da0d61736738d8d7a9bba0',\n",
       "    'title': 'Zero-Shot Policy Transfer with Disentangled Task Representation of Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '49d710921b04885470e17d28f8c46fb309bdf9ac',\n",
       "    'title': 'DIDER: Discovering Interpretable Dynamically Evolving Relations'},\n",
       "   {'paperId': 'f3bf39ec3ff3464d234bd7ffe89199feeb4795c4',\n",
       "    'title': 'Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning'},\n",
       "   {'paperId': '8e9d84a7b2db57adda8d639c6d54c8977ef10761',\n",
       "    'title': 'Skill-based Model-based Reinforcement Learning'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '4b516216d7d150a081fd74993bddf36b6b22c118',\n",
       "    'title': 'Chain of Thought Imitation with Procedure Cloning'},\n",
       "   {'paperId': '90b9a4f529e28148c4eaf281d3042cda849e91e3',\n",
       "    'title': 'Visual language navigation: a survey and open challenges'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': '1f9a4883a768157f5a98921c6c0f4126e0280759',\n",
       "    'title': 'Hierarchical Imitation Learning via Subgoal Representation Learning for Dynamic Treatment Recommendation'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'ea3fa9fb842e055c3c5e231838b548c7aaa90fa2',\n",
       "    'title': 'NeuroLISP: High-level symbolic programming with attractor neural networks'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '5ba6bda6979b155f194f68d933656253cc90c9f8',\n",
       "    'title': 'Variational Predictive Routing with Nested Subjective Timescales'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': 'b9f0d5f62bfb9f1da1fd773609328bcbd529ef2d',\n",
       "    'title': 'WFA-IRL: Inverse Reinforcement Learning of Autonomous Behaviors Encoded as Weighted Finite Automata'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': '9bd851ebdc5a2e45299ad71fbcd0380a92fc2153',\n",
       "    'title': 'Left to the Reader: Abstracting Solutions in Mathematical Reasoning'},\n",
       "   {'paperId': '6174a1c497b5d3a683f39bba6904a37c99b488df',\n",
       "    'title': 'Multi-task Policy Learning with Minimal Human Supervision'},\n",
       "   {'paperId': '0f9efe5d3b4dc962aca9b850f14ee03d7afe65dd',\n",
       "    'title': 'L ONG - HORIZON V IDEO P REDICTION USING A D YNAMIC L ATENT H IERARCHY'},\n",
       "   {'paperId': '193dadd9de36ef8e6883088fbd35d38fa7ce590e',\n",
       "    'title': 'Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots'},\n",
       "   {'paperId': 'e42f0ad8ddf657fb709087d488477a37a7629c7a',\n",
       "    'title': 'V ARIATIONAL P REDICTIVE R OUTING WITH N ESTED S UBJECTIVE T IMESCALES'},\n",
       "   {'paperId': '9d2038e233042790929123ff62354eb18ee52e47',\n",
       "    'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': 'acff9c4e2ad66dd785f75cf91dd0aa442c6cae14',\n",
       "    'title': 'Adversarial Option-Aware Hierarchical Imitation Learning'},\n",
       "   {'paperId': 'a917315029b227c20f6fe720ff7d92e90be0fc0e',\n",
       "    'title': 'Deep Switching State Space Model (DS3M) for Nonlinear Time Series Forecasting with Regime Switching'},\n",
       "   {'paperId': '5f34ba6e738837f1163b08241f579ea60673d050',\n",
       "    'title': 'Segmenting Hybrid Trajectories using Latent ODEs'},\n",
       "   {'paperId': '39cc9292cb602970453c7677ada5d575d03d1d77',\n",
       "    'title': 'SKID RAW: Skill Discovery From Raw Trajectories'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '622799c27af43fc91738537d96ae2ceb78b10eb0',\n",
       "    'title': 'AutoPreview: A Framework for Autopilot Behavior Understanding'},\n",
       "   {'paperId': '9956e3ea2b894f45ca9070ee1984caadb74edbf7',\n",
       "    'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation'},\n",
       "   {'paperId': '24a5ebb09502fcd29b7af851f40f858137d41818',\n",
       "    'title': 'Compositional memory in attractor neural networks with one-step learning'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '2e0bc6cc4153025cd37d957cb896abe237502cd5',\n",
       "    'title': 'NewtonianVAE: Proportional Control and Goal Identification from Pixels via Physical Latent Spaces'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': None, 'title': 'TRANSLATING ROBOT SKILLS: LEARNING UNSUPER-'},\n",
       "   {'paperId': 'c9af30358358b15d05ce72a86ec5f0ce883afdc6',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Single Demonstration'},\n",
       "   {'paperId': 'b80bc7458f043306fb0fe2b61e2ddfc63e2ddb41',\n",
       "    'title': 'Graph Deep Learning: State of the Art and Challenges'},\n",
       "   {'paperId': 'ec8eee61f42e07228bc78faac9817cff3e000ebf',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Demonstration'},\n",
       "   {'paperId': '15afee24a087ef8cdce92a15bf46a403a0439d85',\n",
       "    'title': 'Deep Imitation Learning for Bimanual Robotic Manipulation'},\n",
       "   {'paperId': 'a75dace80c238148d8ca23279a6453a32aa83535',\n",
       "    'title': 'Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': 'be3594b3dc07e181d82bd24a6a30caeb5a06bbb4',\n",
       "    'title': 'PICO: Primitive Imitation for COntrol'},\n",
       "   {'paperId': '20f3973efecab0ee66adcb07dc33b7cdcfbfecf7',\n",
       "    'title': 'Modeling Long-horizon Tasks as Sequential Interaction Landscapes'},\n",
       "   {'paperId': '45d7a12b198bdd74d91b8a69bc88b1d9bec2091f',\n",
       "    'title': 'Learning Situational Driving'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'bbac680797af0f7ce4cdcc6430ff001fa0dfe670',\n",
       "    'title': 'Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations'},\n",
       "   {'paperId': 'b6456512bbb44079972cd7e3a5d7652c76fd26c9',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Adaptive Policy Transfer'},\n",
       "   {'paperId': '038a9aa35575cbce413696be4e7c5bb6247b0ee4',\n",
       "    'title': 'Efficient Deep Reinforcement Learning through Policy Transfer'},\n",
       "   {'paperId': 'f8a97eac3c05d6f09772b24a1a1f6e66a2c17033',\n",
       "    'title': 'Product Kanerva Machines: Factorized Bayesian Memory'},\n",
       "   {'paperId': '17f23a5fdc202bd208bbc87de2a4824e6c125d5e',\n",
       "    'title': 'Active Task-Inference-Guided Deep Inverse Reinforcement Learning'},\n",
       "   {'paperId': '19b924dd9121f01165276a7afb764cf394acb80b',\n",
       "    'title': 'Contrastive Learning of Structured World Models'},\n",
       "   {'paperId': '9d999574ad0eab7a70f96e7bdfc61648cf3ffc16',\n",
       "    'title': 'PODNet: A Neural Network for Discovery of Plannable Options'},\n",
       "   {'paperId': 'caf492da0665cfff029d8a073d4f1d4dde1e53ef',\n",
       "    'title': 'Collapsed Amortized Variational Inference for Switching Nonlinear Dynamical Systems'},\n",
       "   {'paperId': '4f0c4189d9a82f94dcd84fe879cfbe124aaf270b',\n",
       "    'title': 'Keyframing the Future: Keyframe Discovery for Visual Prediction and Planning'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '6225f9fe891b233db76cef6d48ef786a7ecaffc5',\n",
       "    'title': 'Hybrid system identification using switching density networks'},\n",
       "   {'paperId': '77fba2047620db853c1152fe5f4175c161ef7045',\n",
       "    'title': 'C OLLAPSED AMORTIZED VARIATIONAL INFERENCE FOR SWITCHING NONLINEAR DYNAMICAL SYSTEMS'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'}],\n",
       "  'citnuminlist': 9,\n",
       "  'refnuminlist': 5,\n",
       "  'isKeypaper': True},\n",
       " 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef': {'title': 'Latent Space Policies for Hierarchical Reinforcement Learning',\n",
       "  'year': 2018,\n",
       "  'references': [{'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '565af8f2ef461b1d7368f3e9899e0f576e4f0a24',\n",
       "    'title': 'Learning an Embedding Space for Transferable Robot Skills'},\n",
       "   {'paperId': '811df72e210e20de99719539505da54762a11c6d',\n",
       "    'title': 'Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor'},\n",
       "   {'paperId': 'a86723b15c2b585900575a9198124484144af9ed',\n",
       "    'title': 'Overlapping layered learning'},\n",
       "   {'paperId': 'dce6f9d4017b1785979e7520fd0834ef8cf02f4b',\n",
       "    'title': 'Proximal Policy Optimization Algorithms'},\n",
       "   {'paperId': 'ce9a9ee7a531455fd5de053790ff0bea2839def5',\n",
       "    'title': 'Hierarchy Through Composition with Multitask LMDPs'},\n",
       "   {'paperId': 'd0352057e2b99f65f8b5244a0b912026c86d7b21',\n",
       "    'title': 'Equivalence Between Policy Gradients and Soft Q-Learning'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '96a067e188f1c89db9faea1fea2314a15ae51bbc',\n",
       "    'title': 'Bridging the Gap Between Value and Policy Based Reinforcement Learning'},\n",
       "   {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "    'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '09879f7956dddc2a9328f5c1472feeb8402bcbcf',\n",
       "    'title': 'Density estimation using Real NVP'},\n",
       "   {'paperId': '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "    'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '4a026fd65af4ba3575e64174de56fee093fa3330',\n",
       "    'title': 'Taming the Noise in Reinforcement Learning via Soft Updates'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': 'a4cec122a08216fe8a3bc19b22e78fbaea096256',\n",
       "    'title': 'Deep Learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'abe27770c742ae359d7c111c4be21d7e46993a8d',\n",
       "    'title': 'Motor skill learning with local trajectory methods'},\n",
       "   {'paperId': '2319a491378867c7049b3da055c5df60e1671158',\n",
       "    'title': 'Playing Atari with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'b70dd1fccbba3466f22b592ca090836513ebf494',\n",
       "    'title': 'Data-Efficient Generalization of Robot Skills with Contextual Policy Search'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': 'c06fa15eceb21136e07b04324c3ba4e085b00f10',\n",
       "    'title': 'Reinforcement Learning With Sequences of Motion Primitives for Robust Manipulation'},\n",
       "   {'paperId': '4b8f52da1aa977de0ad3ede54b36730cfbf700fd',\n",
       "    'title': 'On Stochastic Optimal Control and Reinforcement Learning by Approximate Inference'},\n",
       "   {'paperId': '6606561e23d536d0b84e64978e3937508f564738',\n",
       "    'title': 'Variational Inference for Policy Search in changing situations'},\n",
       "   {'paperId': '5cbfbbca3a1ea8ee39254dd4ef07b3d67761c39a',\n",
       "    'title': 'Relative Entropy Policy Search'},\n",
       "   {'paperId': '2a65434d43ffa6554eaf14b728780919ad4f33eb',\n",
       "    'title': 'Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy'},\n",
       "   {'paperId': '7a7a23f2c39f9b1526bc8853c6c71a5b7f89e68c',\n",
       "    'title': 'Robot trajectory optimization using approximate inference'},\n",
       "   {'paperId': '11b6bdfe36c48b11367b27187da11d95892f0361',\n",
       "    'title': 'Maximum Entropy Inverse Reinforcement Learning'},\n",
       "   {'paperId': '8570302f7b63e8fcf87030f556b065fd8c260021',\n",
       "    'title': 'Linearly-solvable Markov decision problems'},\n",
       "   {'paperId': '46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e',\n",
       "    'title': 'Reducing the Dimensionality of Data with Neural Networks'},\n",
       "   {'paperId': 'b107acedcf1953ff498ff459f915845962c47674',\n",
       "    'title': 'Path integrals and symmetry breaking for optimal control theory'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'}],\n",
       "  'citations': [{'paperId': '211041e7a7086c96583f5312ade0b7866e3f1dc9',\n",
       "    'title': 'Learning disentangled skills for hierarchical reinforcement learning through trajectory autoencoder with weak labels'},\n",
       "   {'paperId': '0cceb527d62abaf587268319575595eb6c93bc50',\n",
       "    'title': 'An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '43737655c34a6f2a1446d1574e7830560c34bd2f',\n",
       "    'title': 'CFlowNets: Continuous Control with Generative Flow Networks'},\n",
       "   {'paperId': '6544da8ac5bf60c38c92a3ce47d9d4edab28ba14',\n",
       "    'title': 'Dynamic Obstacle Avoidance for Cable-Driven Parallel Robots With Mobile Bases via Sim-to-Real Reinforcement Learning'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '09093b425cd2315a874cfd57053897b1a1065a6b',\n",
       "    'title': 'DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization'},\n",
       "   {'paperId': '337d69e8b1261709ba90f115a221fdad2a357a85',\n",
       "    'title': 'Implicit Ensemble Training for Efficient and Robust Multiagent Reinforcement Learning'},\n",
       "   {'paperId': 'c7808662ee54690f2ee0648bd95f8e1747ba0e6f',\n",
       "    'title': 'Learning Agile, Vision-based Drone Flight: from Simulation to Reality'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '08640a3d174ef8a1c5fca2de3bd4969e592daca2',\n",
       "    'title': 'Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '68694b651be457f1ae68d1b8023081453331f549',\n",
       "    'title': 'GEAR: a graph edge attention routing algorithm solving combinatorial optimization problem with graph edge cost'},\n",
       "   {'paperId': '3e5b3258df48dd41f53ffbd76ea391775b1d9dd3',\n",
       "    'title': 'Policy Gradient With Serial Markov Chain Reasoning'},\n",
       "   {'paperId': '227f04217a212a5e1f7a8e38831431e4b1ab325f',\n",
       "    'title': 'Latent State Marginalization as a Low-cost Approach for Improving Exploration'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'd9c0c3eea7a2581e434b4c9d58ce405638f7dbd5',\n",
       "    'title': 'Multi-expert synthesis for versatile locomotion and manipulation skills'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '798d1cdbbe8c855cc856e9050cc0129dfb3b8255',\n",
       "    'title': 'VMAPD: Generate Diverse Solutions for Multi-Agent Games with Recurrent Trajectory Discriminators'},\n",
       "   {'paperId': '948f51f46b3313d8de258915b96378d694a29f4d',\n",
       "    'title': 'MaxEnt Dreamer: Maximum Entropy Reinforcement Learning with World Model'},\n",
       "   {'paperId': '577be8d620ec7bcd080d4001f2511245c58e9639',\n",
       "    'title': 'Learning fast and agile quadrupedal locomotion over complex terrain'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': '79c6f464efc2f4242a470395a7e4c9d13ed12114',\n",
       "    'title': 'Augment-Connect-Explore: a Paradigm for Visual Action Planning with Data Scarcity'},\n",
       "   {'paperId': 'a891551ec2d0fc3c2cd2334c9cb5810b0efff3b0',\n",
       "    'title': 'Spatial Concept-based Topometric Semantic Mapping for Hierarchical Path-planning from Speech Instructions'},\n",
       "   {'paperId': '18a3abb08ad204d331e09f99d08227d7fae084a5',\n",
       "    'title': 'Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping'},\n",
       "   {'paperId': '178caa0af7d4c76231d5ef675d73ea73355643c9',\n",
       "    'title': 'Implicit Kinematic Policies: Unifying Joint and Cartesian Action Spaces in End-to-End Robot Learning'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': 'a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef',\n",
       "    'title': 'A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization'},\n",
       "   {'paperId': 'b13f11ef64df0213bdd310f8cb8854a55f51458d',\n",
       "    'title': 'Exploration with Multi-Sample Target Values for Distributional Reinforcement Learning'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': 'c541bf912c26be28f0586fd2cce48525c2869964',\n",
       "    'title': 'Improving Actor-Critic Reinforcement Learning Via Hamiltonian Monte Carlo Method'},\n",
       "   {'paperId': 'e901fea06813b3ea8e91e5b641c117af56fcb4dd',\n",
       "    'title': 'Predictive Coding, Variational Autoencoders, and Biological Connections'},\n",
       "   {'paperId': 'f6367ece08e0e33e1803196f928a71c2dc7eac8d',\n",
       "    'title': 'Improving the Sample Eﬃciency and Safety of Reinforcement Learning via Monte Carlo Method'},\n",
       "   {'paperId': 'dde41671ebd2a16408d0e61c37efca60b7720895',\n",
       "    'title': 'Toward Human Cognition-inspired High-Level Decision Making For Hierarchical Reinforcement Learning Agents'},\n",
       "   {'paperId': '4270f2493dfd9ae26b9f7c707cf1398ddbbdc0a1',\n",
       "    'title': 'Maximum Entropy Population Based Training for Zero-Shot Human-AI Coordination'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '0b43ccd62334cd38e3752e92ce0274cf48e3a35e',\n",
       "    'title': 'Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving'},\n",
       "   {'paperId': 'ccb01a90b16b119db0201ed012f989ed48c68d9f',\n",
       "    'title': 'Temporal Abstraction in Reinforcement Learning with the Successor Representation'},\n",
       "   {'paperId': '291a42e844b9bd38c0341c6beec9b34aba3ddde8',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model'},\n",
       "   {'paperId': '590f2577b7706d26ec8c3751b5a18ace445af023',\n",
       "    'title': 'Riemannian Convex Potential Maps'},\n",
       "   {'paperId': '8786bb16bea2e1a4d9952d0abae5f0591959b3b7',\n",
       "    'title': 'Automatic Curricula via Expert Demonstrations'},\n",
       "   {'paperId': 'cd34bfe07509945dcc9a8a3aafd6820f276ac176',\n",
       "    'title': 'Decoupled Greedy Learning of CNNs for Synchronous and Asynchronous Distributed Learning'},\n",
       "   {'paperId': 'bfef39ab7421dd3bcbb75662a9be5cdcf8bd475d',\n",
       "    'title': 'A General Offline Reinforcement Learning Framework for Interactive Recommendation'},\n",
       "   {'paperId': '052072277bd342d67f7431239ad941cb11a9b725',\n",
       "    'title': 'Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'd08607be67efc7aa9394363a7db16164ffe7c613',\n",
       "    'title': 'A Bayesian Approach to Identifying Representational Errors'},\n",
       "   {'paperId': '69d19b74946f1d7613734ca6d9154a937d575b9c',\n",
       "    'title': 'Improving Actor-Critic Reinforcement Learning via Hamiltonian Policy'},\n",
       "   {'paperId': '2e08702b428d7948052a05ccf20ed0ecb261aacc',\n",
       "    'title': 'Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction'},\n",
       "   {'paperId': '335f33b9fbbfd0a7da6eb36af4942829d1104ffb',\n",
       "    'title': 'Toward Robust Long Range Policy Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': 'b164a7dd0d6124e3a86df8a8e7374f9be8364033',\n",
       "    'title': 'Iterative Amortized Policy Optimization'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '1213756da1b99702ab45b4745b6053365686f4ca',\n",
       "    'title': 'A development cycle for automated self-exploration of robot behaviors'},\n",
       "   {'paperId': '23c990e22c9db6502d5a5a8c8749cbad1c291742',\n",
       "    'title': 'Hindsight Expectation Maximization for Goal-conditioned Reinforcement Learning'},\n",
       "   {'paperId': '5646b7e555fc7768db1e3e9a792b59a6553b1d7e',\n",
       "    'title': 'Reinforcement Learning for Combinatorial Optimization: A Survey'},\n",
       "   {'paperId': '501c02c7caa7fc2c7077405299b4cbe7d294b170',\n",
       "    'title': 'Normalizing Flows for Probabilistic Modeling and Inference'},\n",
       "   {'paperId': '3cca78b650a90ec83eb8f2d4287b86408676fd4d',\n",
       "    'title': 'Generative Actor-Critic: An Off-policy Algorithm Using the Push-forward Model'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': 'a2e0f316b9bfe24f66464edb55a8615b01f38904',\n",
       "    'title': 'Trajectory Diversity for Zero-Shot Coordination'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': '872edada2165ad65c1664b813efdb92e3bec1b36',\n",
       "    'title': 'Multi-expert learning of adaptive legged locomotion'},\n",
       "   {'paperId': '34b6871b40d3389f1d5c2a89fc75664d8619490c',\n",
       "    'title': 'Embracing Change: Continual Learning in Deep Neural Networks'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '53681a2cc0b2f0ef8df73df76e79017cfa399a74',\n",
       "    'title': 'Energy-based Surprise Minimization for Multi-Agent Value Factorization'},\n",
       "   {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "    'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       "   {'paperId': 'a60b2bbd511beb7123ff10a5928ad38d9be5952e',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning a latent space of robot skills'},\n",
       "   {'paperId': 'cbd2935be2cfc61ead29d3af9529519e8a5f4172',\n",
       "    'title': 'OCEAN: Online Task Inference for Compositional Tasks with Context Adaptation'},\n",
       "   {'paperId': '9e6b5a0d062fbe8d9a9a58115c93471eb8cf6005',\n",
       "    'title': 'Reinforced Wasserstein Training for Severity-Aware Semantic Segmentation in Autonomous Driving'},\n",
       "   {'paperId': '98ab6919065643ebb30178062e89f9741f95ff05',\n",
       "    'title': 'Learning Control for Robotic Manipulator with Free Energy'},\n",
       "   {'paperId': '952b100591b056912a43065cbe0010a5bd6907fb',\n",
       "    'title': 'Developing cooperative policies for multi-stage tasks'},\n",
       "   {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "    'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       "   {'paperId': 'f2cbb43dbfbeca86ee6541035099128f21a78f09',\n",
       "    'title': 'Skill Discovery of Coordination in Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '446ef7a53114ce1f7aff58dc4f232a4b58629f7f',\n",
       "    'title': 'Severity-Aware Semantic Segmentation With Reinforced Wasserstein Training'},\n",
       "   {'paperId': '1f28d7a9137b6ca53be6093a872b67ea2578b9c2',\n",
       "    'title': 'Feudal Steering: Hierarchical Learning for Steering Angle Prediction'},\n",
       "   {'paperId': '89139a394ccdc8784ddbdf7d76d56e3d242cf557',\n",
       "    'title': 'Learning Combinatorial Optimization on Graphs: A Survey With Applications to Networking'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'a3d1024c3f57f6d0cf7335b0711089c725de1ccc',\n",
       "    'title': 'SUMO: Unbiased Estimation of Log Marginal Probability for Latent Variable Models'},\n",
       "   {'paperId': 'ba17ba2578c94e5483b44f383f2c7673c0cc937c',\n",
       "    'title': 'Sample Efficient Ensemble Learning with Catalyst.RL'},\n",
       "   {'paperId': 'd57e3aa00a438a4015f43ef999f67c227c287925',\n",
       "    'title': 'Adaptive Discretization for Continuous Control using Particle Filtering Policy Network.'},\n",
       "   {'paperId': '83e42f1e617201893718a90cf83230a509e93b4d',\n",
       "    'title': 'Human-Behavior and QoE-Aware Dynamic Channel Allocation for 5G Networks: A Latent Contextual Bandit Learning Approach'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '91d31764b05cd6b406e7fef3de5b797aaefed041',\n",
       "    'title': 'Discretizing Continuous Action Space for On-Policy Optimization'},\n",
       "   {'paperId': '8f04ff612d2afd7100091760d9e94da190a8a38b',\n",
       "    'title': 'Decoupled Greedy Learning of CNNs'},\n",
       "   {'paperId': '5a3b64e38054bb9372a70cf2abd0234d72e1035b',\n",
       "    'title': 'Promoting Stochasticity for Expressive Policies via a Simple and Efficient Regularization Method'},\n",
       "   {'paperId': '50a9ab1de0c26819e4036a998d572c3612da70e1',\n",
       "    'title': 'Maximum Entropy Reinforcement Learning'},\n",
       "   {'paperId': 'e2d948105dcc0d650c5d74cbec1730886850386c',\n",
       "    'title': 'Sequential Autoregressive Flow-Based Policies'},\n",
       "   {'paperId': 'b9b4edac76e7e9e949fafaec4472715c5f3af8b6',\n",
       "    'title': 'Improved Exploration with Stochastic Policies in Deep Reinforcement Learning'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': '61e57667cea382b72fbc9b1131520cef74bf8624',\n",
       "    'title': 'Discrete and Continuous Action Representation for Practical RL in Video Games'},\n",
       "   {'paperId': '28ba35600900671c7a3ec0ba2b61801624e01ae3',\n",
       "    'title': 'Distributed Soft Actor-Critic with Multivariate Reward Representation and Knowledge Distillation'},\n",
       "   {'paperId': '901bad1d73e36d0dbd7f03438013d4acbbddf7b1',\n",
       "    'title': 'Combinatorial Optimization by Graph Pointer Networks and Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3ae36bc1c8647ea6aa9039f5382a3353d4566ae0',\n",
       "    'title': 'Quinoa: a Q-function You Infer Normalized Over Actions'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': '68d4d8a1b2cf589de9f116cf748c9e8f11ab852f',\n",
       "    'title': 'MAVEN: Multi-Agent Variational Exploration'},\n",
       "   {'paperId': '63508a5af094298a05de70eabeeb116ded649cb6',\n",
       "    'title': 'Adapting Behaviour for Learning Progress'},\n",
       "   {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "    'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       "   {'paperId': '22005795db3a0e85c9091a855427a39b5f8bb33a',\n",
       "    'title': 'Neural Embedding for Physical Manipulations'},\n",
       "   {'paperId': '66605b6ceae9847156526e46ca9fe467804fca54',\n",
       "    'title': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '165d76bf04c1cfb24d53944930ae19ab102b383c',\n",
       "    'title': 'Quantile Regression Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ec3b00e03e679f189c418461684062325f277565',\n",
       "    'title': 'Learning Policies through Quantile Regression'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '390c7c230c223498c281a204006c5fc141759460',\n",
       "    'title': 'Transfer Learning by Modeling a Distribution over Policies'},\n",
       "   {'paperId': '4167fbb4c9d3e4d0ab4982d4d102edc5c935ae1e',\n",
       "    'title': 'Towards concept based software engineering for intelligent agents'},\n",
       "   {'paperId': '6b36776e5c0473d82cbdd2c92cd97cca7925ae08',\n",
       "    'title': 'TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '0d6c52ac0424321e08cee82dd2ccb1fe0e826c01',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': 'd569567c994b443ead90c709f0f1718ed6f0ec50',\n",
       "    'title': 'Maximum Entropy-Regularized Multi-Goal Reinforcement Learning'},\n",
       "   {'paperId': 'c3bd5dd10f555bc1622cd37598bdbdf5635c94fe',\n",
       "    'title': 'Leveraging exploration in off-policy algorithms via normalizing flows'},\n",
       "   {'paperId': '8513f667ab7438c11f58755cb87db2d9785952a4',\n",
       "    'title': 'Towards Concept Based Software Engineering for Intelligent Agents'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '3ea8bbe660d14d4f25d7713fad8d80b5bd52a30c',\n",
       "    'title': 'Catalyst.RL: A Distributed Framework for Reproducible RL Research'},\n",
       "   {'paperId': '9b7c248b0219c17158810ed7dce06a50a562bbd0',\n",
       "    'title': 'Latent Space Reinforcement Learning for Steering Angle Prediction'},\n",
       "   {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "    'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       "   {'paperId': '2ed619fbc7902155d54f6f21da16ad6c120eac63',\n",
       "    'title': 'Learning to Walk via Deep Reinforcement Learning'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1de1e749668a65cf6b88b8138389581108bb129a',\n",
       "    'title': 'Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning'},\n",
       "   {'paperId': 'e7069f324d16847e9939bded648032e54c8c9331',\n",
       "    'title': '10-708 Final Report:Investigating Max-Entropy Latent-Space Policiesfor Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '39995a05908b23249025de5a6c3c439d0dd33d9d',\n",
       "    'title': 'MUTUAL-INFORMATION REGULARIZATION'},\n",
       "   {'paperId': '58751ad1dd047cd96285c90a99de8c3697c1bcd9',\n",
       "    'title': 'Soft Option Transfer'},\n",
       "   {'paperId': '87d84a825924444efd8f9d063dcdf3c2fb9904df',\n",
       "    'title': 'Multi-Task Reinforcement Learning without Interference'},\n",
       "   {'paperId': 'd5fc515cdee0cfc3370fb199091896e289dacaa9',\n",
       "    'title': 'Learning Transferable Knowledge Through Embedding Spaces'},\n",
       "   {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "    'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '5c90f7c36065bcf1c2e75252a1c87001cab8d985',\n",
       "    'title': 'ICLR 2019 Expert Replay Buffer ActorEnvironment Discriminator Policy Replay Buffer Absorbing State Wrapper s a Critic'},\n",
       "   {'paperId': '12c0751b4f51ed833172a713b7e32390032ead93',\n",
       "    'title': 'Soft Actor-Critic Algorithms and Applications'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '5c91208414f02eaec2281aabd30b50f0ce5b9da6',\n",
       "    'title': 'Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fb363b670b6a48020e3504714c444291785b3fbc',\n",
       "    'title': 'Boosting Trust Region Policy Optimization by Normalizing Flows Policy'},\n",
       "   {'paperId': '3996f47c119223fb547905784124a0a2aa09c9a4',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning composable robot skills'},\n",
       "   {'paperId': '44e197f8dbbd5b92f900337d6d18168c74ce2c3c',\n",
       "    'title': 'Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement Learning'},\n",
       "   {'paperId': '2e7b6e73398af01bc975e9bf9374ee5f255252b3',\n",
       "    'title': 'VFunc: a Deep Generative Model for Functions'},\n",
       "   {'paperId': '8343cff9950d74429c66a4a3a13269b1275e852f',\n",
       "    'title': 'Implicit Policy for Reinforcement Learning'},\n",
       "   {'paperId': '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba',\n",
       "    'title': 'Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review'},\n",
       "   {'paperId': '4155ecb89086261704bae0040abcf326c41c21f8',\n",
       "    'title': 'Extending the Hierarchical Deep Reinforcement Learning framework'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '876053977063ab843dd24c78425cbad1779a62ed',\n",
       "    'title': 'Expert Replay Buffer ActorEnvironment Discriminator Policy Replay Buffer Absorbing State Wrapper s a'},\n",
       "   {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'}],\n",
       "  'citnuminlist': 13,\n",
       "  'refnuminlist': 6,\n",
       "  'isKeypaper': True},\n",
       " '565af8f2ef461b1d7368f3e9899e0f576e4f0a24': {'title': 'Learning an Embedding Space for Transferable Robot Skills',\n",
       "  'year': 2018,\n",
       "  'references': [],\n",
       "  'citations': [{'paperId': '7e81c841a5784e98f800c200445f4e1063e50d0b',\n",
       "    'title': 'Learning Embeddings for Sequential Tasks Using Population of Agents'},\n",
       "   {'paperId': '86a575987a11045541c967843945bdcf8dca2d03',\n",
       "    'title': 'Representation-Driven Reinforcement Learning'},\n",
       "   {'paperId': 'c26642dd7c92c842621b7424ff39596907df0c91',\n",
       "    'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data'},\n",
       "   {'paperId': '319a867ef0dbc9db0e19d530ebfc4292ae8548b4',\n",
       "    'title': 'Learning Generalizable Pivoting Skills'},\n",
       "   {'paperId': '9ac8cd5990afa60d710ded46cc16473d7c918885',\n",
       "    'title': 'Seek for commonalities: Shared features extraction for multi-task reinforcement learning via adversarial training'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '26115e5d6f20c9e7560e1709f3c36847deffa148',\n",
       "    'title': 'A general Markov decision process formalism for action-state entropy-regularized reward maximization'},\n",
       "   {'paperId': '09093b425cd2315a874cfd57053897b1a1065a6b',\n",
       "    'title': 'DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization'},\n",
       "   {'paperId': '31eba23839649c21c3e462a7568b6b72041d4b5c',\n",
       "    'title': 'Meta-Reinforcement Learning in Non-Stationary and Dynamic Environments'},\n",
       "   {'paperId': '18d02042724a7fe29ae6aa9460353d7e2425ca86',\n",
       "    'title': 'Matching options to tasks using Option-Indexed Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '372715a73955b7fbc1daf816bd52c0641b3ff5f2',\n",
       "    'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'ec1e08dc5cbec362ea0a331e32bfa3d30e47c24f',\n",
       "    'title': 'Learning Landmark-Oriented Subgoals for Visual Navigation Using Trajectory Memory'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '0fd6c747b48526ba4abc05b4ae9260f93718ce8f',\n",
       "    'title': 'Efficient Meta Reinforcement Learning for Preference-based Fast Adaptation'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '23d9e8319f58451a20fac7fc1316c4e664d66eb1',\n",
       "    'title': 'Knowing the Past to Predict the Future: Reinforcement Virtual Learning'},\n",
       "   {'paperId': '65441ccf98e2055264b1e30e5ab11fbf99a49705',\n",
       "    'title': 'Efficient Multi-Task Learning via Iterated Single-Task Transfer'},\n",
       "   {'paperId': '2ba0eb8aa87e5f3653a7735ae01aacefdb60ba11',\n",
       "    'title': 'Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables'},\n",
       "   {'paperId': 'ad4707bb87c6fc087e09d9f6609665b53835c899',\n",
       "    'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction'},\n",
       "   {'paperId': '1cc13cff6f12d450457f51eb8d5d8e20bce47b56',\n",
       "    'title': 'Skill-Based Reinforcement Learning with Intrinsic Reward Matching'},\n",
       "   {'paperId': '5e5419b46a9449d23c1e223d067310e56192b5dc',\n",
       "    'title': 'Neuroevolution is a Competitive Alternative to Reinforcement Learning for Skill Discovery'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "    'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '798d1cdbbe8c855cc856e9050cc0129dfb3b8255',\n",
       "    'title': 'VMAPD: Generate Diverse Solutions for Multi-Agent Games with Recurrent Trajectory Discriminators'},\n",
       "   {'paperId': '88f48480ffb3c36683a63b0f6d5932b40bfbef64',\n",
       "    'title': 'Celebrating Robustness in Efficient Off-Policy Meta-Reinforcement Learning'},\n",
       "   {'paperId': '0ac948bc087603176b47c4af29ef7f240d27b541',\n",
       "    'title': \"Don't Start From Scratch: Leveraging Prior Data to Automate Robotic Reinforcement Learning\"},\n",
       "   {'paperId': '3682d32d7761c2f81cdb07ca9c75363d83681e3e',\n",
       "    'title': 'Fairness and Bias in Robot Learning'},\n",
       "   {'paperId': '6eac59511159a027489e4cc507c05d55ab2d105c',\n",
       "    'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '78839ec995beab7f5fa8ce8d549fb4cf04b33d45',\n",
       "    'title': 'Meta-Learning Parameterized Skills'},\n",
       "   {'paperId': '44df7090a9b18b19196018b5705dfc9153e068ed',\n",
       "    'title': 'BulletArm: An Open-Source Robotic Manipulation Benchmark and Learning Framework'},\n",
       "   {'paperId': 'a66e581eca1e6531298563f39d8b4ccdcf9489e2',\n",
       "    'title': 'Fast Inference and Transfer of Compositional Task Structures for Few-shot Task Generalization'},\n",
       "   {'paperId': 'da970e0aec6248bddde99747fbc39a417fd2e6c8',\n",
       "    'title': 'Seeking entropy: complex behavior from intrinsic motivation to occupy action-state path space'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '1ee71262c4525bca44c6da5b8e1321a67a484953',\n",
       "    'title': 'Dexterous Manipulation for Multi-Fingered Robotic Hands With Reinforcement Learning: A Review'},\n",
       "   {'paperId': '23bac2542b145bf2fcd17d7fa0a02ae03d0a45f7',\n",
       "    'title': 'Skill-based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '15ac70d077bb735eed4a8502ce49aa7782c803fd',\n",
       "    'title': 'What Matters in Language Conditioned Robotic Imitation Learning Over Unstructured Data'},\n",
       "   {'paperId': '77d3d69f1c4c160e3765c416bc13aed863176197',\n",
       "    'title': 'One After Another: Learning Incremental Skills for a Changing World'},\n",
       "   {'paperId': '549bfdfd9fa718331076810f0d5817adcd79fe69',\n",
       "    'title': 'AutoDIME: Automatic Design of Interesting Multi-Agent Environments'},\n",
       "   {'paperId': '178caa0af7d4c76231d5ef675d73ea73355643c9',\n",
       "    'title': 'Implicit Kinematic Policies: Unifying Joint and Cartesian Action Spaces in End-to-End Robot Learning'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': '9fc6b6d3fead719cdc95263e62209548223576d1',\n",
       "    'title': 'It Takes Four to Tango: Multiagent Selfplay for Automatic Curriculum Generation'},\n",
       "   {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '5903fa23a67f15d5e56c5939b83e256ad9469411',\n",
       "    'title': 'Robust Policy Learning over Multiple Uncertainty Sets'},\n",
       "   {'paperId': 'a1189ba5d86d32bc5fecd32ee905f8ff4767cbdb',\n",
       "    'title': 'ASHA: Assistive Teleoperation via Human-in-the-Loop Reinforcement Learning'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '64c141dbeecdca2c9e494aa2aa518b19e7f71d97',\n",
       "    'title': 'Rethinking Learning Dynamics in RL using Adversarial Networks'},\n",
       "   {'paperId': 'eac4172613328fa02dd8fa320112c4de3a5f280d',\n",
       "    'title': 'Boosting Exploration in Multi-Task Reinforcement Learning using Adversarial Networks'},\n",
       "   {'paperId': '546bff6c12ea395690292f204a7e019a8b3b87a0',\n",
       "    'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '05bf6cddb70c7e0e9c894907e7ff4ff2a9d0fb5c',\n",
       "    'title': 'Physical Derivatives: Computing policy gradients by physical forward-propagation'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '65e36b8fc38819944528d232368d8669feb3b01a',\n",
       "    'title': 'Wasserstein Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': '3dc6141adfdaa3c11ca672029002fa6eb11aba0d',\n",
       "    'title': 'Learning a subspace of policies for online adaptation in Reinforcement Learning'},\n",
       "   {'paperId': 'f7dbc89b857167caa1e2ff716b7cfbac48da20fe',\n",
       "    'title': 'Learning Multi-Objective Curricula for Robotic Policy Learning'},\n",
       "   {'paperId': '40888b859c5b40868943162e3c4769dae1aed716',\n",
       "    'title': 'The Information Geometry of Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': 'edfbc2c80e92f3af8c7c88dd767d6dcf6285196e',\n",
       "    'title': 'Discovering Synergies for Robot Manipulation with Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'ee21c47254d1bcf33e13bf746218021816443745',\n",
       "    'title': 'Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation'},\n",
       "   {'paperId': '5c85edd4b333e78b2c42bdde6f3eec5f911bdbdc',\n",
       "    'title': 'Lifelong Robotic Reinforcement Learning by Retaining Experiences'},\n",
       "   {'paperId': '7981ed44d7c6c63990dca2ea3e29299dfd98ce87',\n",
       "    'title': 'Learning Latent Actions without Human Demonstrations'},\n",
       "   {'paperId': '61f371768cdc093828f432660e22f7a17f22e2af',\n",
       "    'title': 'Offline Meta-Reinforcement Learning with Online Self-Supervision'},\n",
       "   {'paperId': '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "    'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': 'ae25c0c7d3e2de1c90346079e53b9e3e836c7de4',\n",
       "    'title': 'Learn Goal-Conditioned Policy with Intrinsic Motivation for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'aedd968f09752786785d2de91151d22a7dc36117',\n",
       "    'title': 'Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models'},\n",
       "   {'paperId': '13e9ed9c6c282b3b347c8eec8d88df9e559c72ec',\n",
       "    'title': 'Planning and Learning using Adaptive Entropy Tree Search'},\n",
       "   {'paperId': '9faecf3e18a833f2d49b030d591cc2ded0b54336',\n",
       "    'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives'},\n",
       "   {'paperId': '73e240cb95309142b61db3e9afd9282bf0b5464c',\n",
       "    'title': 'Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey'},\n",
       "   {'paperId': 'a4e1cf41401674c40174fa8d6073d2b3e9be78d6',\n",
       "    'title': 'What about Inputting Policy in Value Function: Policy Representation and Policy-Extended Value Function Approximator'},\n",
       "   {'paperId': '00e6834700e9805cb1618433f05915c261a6ba08',\n",
       "    'title': 'Multi-Task Learning for Dense Prediction Tasks: A Survey'},\n",
       "   {'paperId': '15dab38b0841db198d6e2cac98deaee9e4f5b4d4',\n",
       "    'title': 'Library of behaviors and tools for robot manipulation'},\n",
       "   {'paperId': 'e30727b7375efbe5db4d0ccaaa56920fb14547b3',\n",
       "    'title': 'S IMULATION -A CQUIRED L ATENT A CTION S PACES FOR D YNAMICS G ENERALIZATION'},\n",
       "   {'paperId': 'cce29e5a9fa8882e3520c5cde12246b7aca50dbd',\n",
       "    'title': 'S KILL - BASED M ETA -R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '137bf940a8f9f0425987c1a5608c66ccafe00158',\n",
       "    'title': 'Greedy when Sure and Conservative when Uncertain about the Opponents'},\n",
       "   {'paperId': '40c099d10be5a7ff429ee13f4c3f8b49142b1587',\n",
       "    'title': 'Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces'},\n",
       "   {'paperId': '704263cf8142d3e9b1e8ce6d5c8faf07f8118e48',\n",
       "    'title': 'M ULTI - TASK R EINFORCEMENT L EARNING WITH T ASK R EPRESENTATION M ETHOD'},\n",
       "   {'paperId': 'e4dddc411739f85097c20aa71ebf27357b17bfb3',\n",
       "    'title': 'Neural Stochastic Dual Dynamic Programming'},\n",
       "   {'paperId': '61a9a1abb5e285914407b237a14472b69d30a41f',\n",
       "    'title': 'An Adaptive Framework for Reliable Trajectory Following in Changing-Contact Robot Manipulation Tasks'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': '0c41b086c2ad9958efe5adcd4975704ccf82b224',\n",
       "    'title': 'Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task Learning'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '81541b0f1b818da50f48a3e1932774508ffa399e',\n",
       "    'title': 'A Simple Approach to Continual Learning by Transferring Skill Parameters'},\n",
       "   {'paperId': 'e5d128eee302958cca0d744046ceed841c5b3d8e',\n",
       "    'title': 'Pick Your Battles: Interaction Graphs as Population-Level Objectives for Strategic Diversity'},\n",
       "   {'paperId': '9e755a823378e19aece4a6838830f933c9a9c200',\n",
       "    'title': 'Learning Multi-Objective Curricula for Deep Reinforcement Learning'},\n",
       "   {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "    'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       "   {'paperId': 'b708a26a9172fe842a2911ca66f9345ee12c8aad',\n",
       "    'title': 'Aggregation Transfer Learning for Multi-Agent Reinforcement learning'},\n",
       "   {'paperId': 'a176b0de62840f7118006277d94bbc1547162a4d',\n",
       "    'title': 'Learning to Synthesize Programs as Interpretable and Generalizable Policies'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': 'c2730b528439f409a0ab10d4d0149a6cf0617a82',\n",
       "    'title': 'Learning to Transfer: A Foliated Theory'},\n",
       "   {'paperId': '105f44c9d445de2b93d1297c2d5ac10cc776d654',\n",
       "    'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills'},\n",
       "   {'paperId': '13266056be66c3663683d450c02b249bbfbfa652',\n",
       "    'title': 'Unbiased Methods for Multi-Goal Reinforcement Learning'},\n",
       "   {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "    'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '26d601215e16b7b69e3ee2f88282312ba4577519',\n",
       "    'title': 'Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning'},\n",
       "   {'paperId': '5c37023c35fc1c95565d56b4fc4821fcf768651a',\n",
       "    'title': 'Reward is enough for convex MDPs'},\n",
       "   {'paperId': 'f8677102449c803512c87b240b3989e4e9276cbc',\n",
       "    'title': 'Toward next-generation learned robot manipulation'},\n",
       "   {'paperId': 'ca51a42cd413e8a552ab217f507e355c0e0c59b1',\n",
       "    'title': 'Context-Based Soft Actor Critic for Environments with Non-stationary Dynamics'},\n",
       "   {'paperId': 'c1d7caee5cbd8bb4b5e85141400941dc48db71ac',\n",
       "    'title': 'Policy manifold search: exploring the manifold hypothesis for diversity-based neuroevolution'},\n",
       "   {'paperId': '3e85d208b1b927fdb69ecf8336c70995818aaebd',\n",
       "    'title': 'MT-Opt: Continuous Multi-Task Robotic Reinforcement Learning at Scale'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': 'f3a6ebc77e2e8af4a82b6ab5f93e9896333aa1ff',\n",
       "    'title': 'Modelling Behavioural Diversity for Learning in Open-Ended Games'},\n",
       "   {'paperId': '44f5f9dbe7ba6d3132d0f9482c7783d72548a307',\n",
       "    'title': 'Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic Platforms'},\n",
       "   {'paperId': '335f33b9fbbfd0a7da6eb36af4942829d1104ffb',\n",
       "    'title': 'Toward Robust Long Range Policy Transfer'},\n",
       "   {'paperId': 'f389c09ad85263bdd1bb2518d61c90a8a558441d',\n",
       "    'title': 'MulCode: A Multi-task Learning Approach for Source Code Understanding'},\n",
       "   {'paperId': '35036e009fa42c38741a1155853e1fdb47ce583b',\n",
       "    'title': 'GELATO: Geometrically Enriched Latent Model for Offline Reinforcement Learning'},\n",
       "   {'paperId': '6dbff5c7a1992b039c50ac3e0170436808bbb0a0',\n",
       "    'title': 'Uncertainty Estimation Using Riemannian Model Dynamics for Offline Reinforcement Learning'},\n",
       "   {'paperId': 'e40cdb1f398dd288bc6da351364213d46e9fb855',\n",
       "    'title': 'Program Synthesis Guided Reinforcement Learning'},\n",
       "   {'paperId': 'abd77509ef1cc739c0757ae657025fcee13c98dc',\n",
       "    'title': 'Program Synthesis Guided Reinforcement Learning for Partially Observed Environments'},\n",
       "   {'paperId': '38fa4bd7e944b73d128b93f2d279416f93074222',\n",
       "    'title': 'Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation'},\n",
       "   {'paperId': '7c1b75ab7bed79163d3d830a6a83a4023b496ebb',\n",
       "    'title': 'Self-supervised Visual Reinforcement Learning with Object-centric Representations'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': '0d6a4e45acde6f47d704ed0752f17f7ab52223af',\n",
       "    'title': 'Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning'},\n",
       "   {'paperId': '72a2c7140b0d362413a726d8c280205db553d3c4',\n",
       "    'title': 'Abstract Value Iteration for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '0a7f12e84401582f72b9e0726bcd139eb171acb7',\n",
       "    'title': 'Local Information Agent Modelling in Partially-Observable Environments'},\n",
       "   {'paperId': '47cbaef8aabdef629183d69faf7bee9a4ad670e5',\n",
       "    'title': 'Agent Modelling under Partial Observability for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6',\n",
       "    'title': 'A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms'},\n",
       "   {'paperId': '339e2610de8487ccb54af05cb59b63854d25f02d',\n",
       "    'title': 'Meta Learning via Learned Loss'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': 'c37190f3e03017e5577f68cf4a43f72b3dbb1245',\n",
       "    'title': 'Latent Geodesics of Model Dynamics for Ofﬂine Reinforcement Learning'},\n",
       "   {'paperId': 'bedefdad46d7faf04443564b650b8b3b9111900e',\n",
       "    'title': 'Visually-Grounded Library of Behaviors for Manipulating Diverse Objects across Diverse Conﬁgurations and Views'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': 'c7629a4d7e1c87fd1ea73850bcb800538fd0aa4b',\n",
       "    'title': 'VariBAD: Variational Bayes-Adaptive Deep RL via Meta-Learning'},\n",
       "   {'paperId': 'a059870910339596c5c57969bc2397a7e268435e',\n",
       "    'title': 'Learning Embodied Agents with Scalably-Supervised Reinforcement Learning'},\n",
       "   {'paperId': 'c01572623718ac9a8f66c08e2176eb68927eaac1',\n",
       "    'title': 'Hybrid Models for Control of Changing-Contact Manipulation Tasks'},\n",
       "   {'paperId': '105aefdc812c469971f1355475cf913e98a27f02',\n",
       "    'title': 'FEW-SHOT LEARNING WITH WEAK SUPERVISION'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': 'b7da8f985f84e1a788e5e9563cae2fd183fb83e0',\n",
       "    'title': 'Robust and Efficient Planning using Adaptive Entropy Tree Search'},\n",
       "   {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "    'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       "   {'paperId': 'f4fd4a5a0901675e19bb4054d8861d2697194fd5',\n",
       "    'title': 'Deep Reinforcement Learning: A State-of-the-Art Walkthrough'},\n",
       "   {'paperId': '1760e16f7d6603c1f4981a3d7a9a188699ede2db',\n",
       "    'title': 'Policy Manifold Search for Improving Diversity-based Neuroevolution'},\n",
       "   {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "    'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       "   {'paperId': '3da314388876286aacb6d9b355439ee68700576d',\n",
       "    'title': 'Automatische Programmierung von Produktionsmaschinen'},\n",
       "   {'paperId': '7fad5ce5ef04b84d3cee1ab79f16532b93c8aad5',\n",
       "    'title': 'BSE-MAML: Model Agnostic Meta-Reinforcement Learning via Bayesian Structured Exploration'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '55d296154f3d6518e93ccf3fdd4f691077b5ceda',\n",
       "    'title': 'What About Taking Policy as Input of Value Function: Policy-extended Value Function Approximator'},\n",
       "   {'paperId': '587ef257f8c9b7a70ce3c42b71fcf5206b58c9ae',\n",
       "    'title': 'Represent Your Own Policies : Reinforcement Learning with Policy-extended Value Function Approximator'},\n",
       "   {'paperId': 'd052f58cfa47ff972280063682a78516aa500353',\n",
       "    'title': 'Learning Dexterous Manipulation from Suboptimal Experts'},\n",
       "   {'paperId': '6cf28553e91f14422941b9289b3281a4d09a072f',\n",
       "    'title': 'Local Search for Policy Iteration in Continuous Control'},\n",
       "   {'paperId': '79f054f309a5e104aac046522346e53ad4fc7fd5',\n",
       "    'title': 'Temporal Difference Uncertainties as a Signal for Exploration'},\n",
       "   {'paperId': '0c87073736489f9ea702934542e80763a8b31129',\n",
       "    'title': 'Preparing to adapt is key for Olympic curling robots'},\n",
       "   {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "    'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       "   {'paperId': '0922fc9d7aa9610896890f4d417ca7e72e417cdf',\n",
       "    'title': 'Importance Weighted Policy Learning and Adaption'},\n",
       "   {'paperId': '6be61525ee8b21c3bef6564df17b435fc4f84282',\n",
       "    'title': 'Action and Perception as Divergence Minimization'},\n",
       "   {'paperId': 'a60b2bbd511beb7123ff10a5928ad38d9be5952e',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning a latent space of robot skills'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': '49a4ce05b418a1a3852670f841c4e30cc67ea241',\n",
       "    'title': 'A Foliated View of Transfer Learning'},\n",
       "   {'paperId': 'ee3efe8c9577a7b7cf02a1d9bc3a78d51252d1f7',\n",
       "    'title': 'Deep Q-learning for the Control of PLC-based Automated Production Systems'},\n",
       "   {'paperId': 'fca48f5295103766d90e63760867e8761bfb7070',\n",
       "    'title': 'Fast Adaptation to New Environments via Policy-Dynamics Value Functions'},\n",
       "   {'paperId': '2983104e8aafe3e30c84a51545b0da127e053225',\n",
       "    'title': 'Fast Adaptation via Policy-Dynamics Value Functions'},\n",
       "   {'paperId': '48843f326ac4821afa04db53561a1568c76f9558',\n",
       "    'title': 'Latent Context Based Soft Actor-Critic'},\n",
       "   {'paperId': '2a830450f65dc41bc196777356701bed52d91e34',\n",
       "    'title': 'Local Information Opponent Modelling Using Variational Autoencoders.'},\n",
       "   {'paperId': '4c81e2999cb6a9642fba026613327753235ac0d1',\n",
       "    'title': 'Opponent Modelling with Local Information Variational Autoencoders'},\n",
       "   {'paperId': 'a19d6b825029cc909d0a2ddfcbb4d159f8fccbd5',\n",
       "    'title': 'Reinforcement Learning as Iterative and Amortised Inference'},\n",
       "   {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "    'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       "   {'paperId': 'f2cbb43dbfbeca86ee6541035099128f21a78f09',\n",
       "    'title': 'Skill Discovery of Coordination in Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '596579e8e72efdc3cdcc6f7faa84545c2b9b7423',\n",
       "    'title': 'Learning Adaptive Exploration Strategies in Dynamic Environments Through Informed Policy Regularization'},\n",
       "   {'paperId': 'f8174da83188b4227af2afd5f7ad84a0f31e70c3',\n",
       "    'title': 'Visual Prediction of Priors for Articulated Object Interaction'},\n",
       "   {'paperId': '4b11359f5afc79a6291f91a2c5ae4ad55fc7ed13',\n",
       "    'title': 'Plan-Space State Embeddings for Improved Reinforcement Learning'},\n",
       "   {'paperId': '121cca1bb7cec48fa9080801927f50d99193eae6',\n",
       "    'title': 'Learning to Coordinate Manipulation Skills via Skill Behavior Diversification'},\n",
       "   {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "    'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': '6a248e075035cc6f17a64ed4336a507faad1f72e',\n",
       "    'title': 'Revisiting Multi-Task Learning in the Deep Learning Era'},\n",
       "   {'paperId': '0b4a7f0e8a73c093447d882064360f4560fbcbec',\n",
       "    'title': 'Weakly-Supervised Reinforcement Learning for Controllable Behavior'},\n",
       "   {'paperId': 'a53b324a5f1970f2babec35a95e061c8f9c75933',\n",
       "    'title': 'When Models Meet Data'},\n",
       "   {'paperId': 'ff8fb97ef3bb21611b918bb49d60df778096e9d2',\n",
       "    'title': 'Dimensionality Reduction with Principal Component Analysis'},\n",
       "   {'paperId': '6f5d4554ea3dd6f2c1d0bc5da774765726686071',\n",
       "    'title': 'Classification with Support Vector Machines'},\n",
       "   {'paperId': '0b9bfbd7a7f4595d99ab7f73862fbcfbac38c4d7',\n",
       "    'title': 'Multi-Task Reinforcement Learning with Soft Modularization'},\n",
       "   {'paperId': '845aeb9dcf12efba0760c6eb2e2ac56ea27f3247',\n",
       "    'title': 'Option Discovery in the Absence of Rewards with Manifold Analysis'},\n",
       "   {'paperId': '8ea8df252347b608b49f92baac7a925b63af24f2',\n",
       "    'title': 'Scalable Multi-Task Imitation Learning with Autonomous Improvement'},\n",
       "   {'paperId': 'e33e1e8db2b3f6511c0a40ee668d2a8f2f56c0e3',\n",
       "    'title': 'Modelling Latent Skills for Multitask Language Generation'},\n",
       "   {'paperId': '4f97e87512eb8bf48ce695443e958725c54908b6',\n",
       "    'title': 'Mathematics for Machine Learning'},\n",
       "   {'paperId': '67a65737d17713ae8bee1b69b853ee658bc6626f',\n",
       "    'title': 'Generalized Hidden Parameter MDPs Transferable Model-based RL in a Handful of Trials'},\n",
       "   {'paperId': '449c5660d637741f7aa7ff42549c32b43c9968bf',\n",
       "    'title': 'Gradient Surgery for Multi-Task Learning'},\n",
       "   {'paperId': '2369a2c4cc462b6e1476b2a58d4c88cf2fce060e',\n",
       "    'title': 'Adversarial Skill Networks: Unsupervised Robot Skill Learning from Video'},\n",
       "   {'paperId': '361e953f792a585496834ee14216b94d0ce9ae74',\n",
       "    'title': 'VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning'},\n",
       "   {'paperId': 'a2fdfda785b3a2a0178d174daa515377c531f222',\n",
       "    'title': 'RLBench: The Robot Learning Benchmark & Learning Environment'},\n",
       "   {'paperId': '886f9e4e78b13db6b965388f7fe044ea8c5c0f61',\n",
       "    'title': 'Variational Autoencoders for Opponent Modeling in Multi-Agent Systems'},\n",
       "   {'paperId': '3550ba30ba345e9e3011df32baf2cdf5e8578e9d',\n",
       "    'title': 'Controlling Assistive Robots with Learned Latent Actions'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '7388826b5ee00efe17cb7f19a623d9b5e955ae70',\n",
       "    'title': 'Skew-Fit: State-Covering Self-Supervised Reinforcement Learning'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': 'a3e4c634d35eaee493f37a2129e47756b4f9f9aa',\n",
       "    'title': 'Demystifying Reproducibility in Meta- and Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'ec684b9cf2433680f6bd70779186f34bcd5b4f06',\n",
       "    'title': 'MaxEnt Reward Expected Reward Latent Representations Missing Data Controllable Future Factorized Target Perception Action Both Low Entropy Preferences Empowerment Skill Discovery Amortized Inference Maximum Likelihood Variational Inference Input Density Exploration Information GainFiltering Latent S'},\n",
       "   {'paperId': 'f68564cbbbb969d247eba8dd0140861935695a20',\n",
       "    'title': 'SELF-SUPERVISED VISUAL REINFORCEMENT LEARN-'},\n",
       "   {'paperId': '5247c7ff3ae79c96ae53c312686920b7eafbf40d',\n",
       "    'title': 'Learning Hybrid Models for Variable Impedance Control of Changing-Contact Manipulation Tasks'},\n",
       "   {'paperId': 'f238f455a2c506a70b3e07ad5bfea0d9fc15b57d',\n",
       "    'title': 'Abstract Value Iteration for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '8a6dc3e46d28db731d170bed8db055d00e4695bc',\n",
       "    'title': 'Learning Agent Representations for Ice Hockey'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '8d4de07338edaf140d4b035a90e9eb178f260612',\n",
       "    'title': 'Probing Dynamic Environments with Informed Policy Regularization'},\n",
       "   {'paperId': '867cc74781225da4e08a77fc35037ba77911e455',\n",
       "    'title': 'Hindsight Credit Assignment'},\n",
       "   {'paperId': '7e6471a725924938cbe0b5f9c124a46ecbcb345c',\n",
       "    'title': '[Re] Learning to Learn By Self-Critique'},\n",
       "   {'paperId': '3dd63dab767343aec85d88bbc1e281a47ea07410',\n",
       "    'title': 'Maximum Entropy Diverse Exploration: Disentangling Maximum Entropy Reinforcement Learning'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': 'bb4fb36bd871b1757fa736ea4c3d56f8d5b9b1fa',\n",
       "    'title': 'Low Dimensional Motor Skill Learning Using Coactivation'},\n",
       "   {'paperId': '0bc855f84668b35cb65618d996d09f6e434d28c9',\n",
       "    'title': 'Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning'},\n",
       "   {'paperId': '7e12836510909097f592cbec8c2f646f1b3790e5',\n",
       "    'title': 'OffWorld Gym: open-access physical robotics environment for real-world reinforcement learning benchmark and research'},\n",
       "   {'paperId': '701e5d53031ebcd6955dc6da1bd9525e392fe770',\n",
       "    'title': 'Imagined Value Gradients: Model-Based Policy Optimization with Transferable Latent Dynamics Models'},\n",
       "   {'paperId': '691bf656a923ab05fcdaa74bbf8c4096968e6895',\n",
       "    'title': 'Composable Instructions and Prospection Guided Visuomotor Control for Robotic Manipulation'},\n",
       "   {'paperId': '13b00c6c8e6fd35a540b08904824aff0d6b66897',\n",
       "    'title': 'Meta-Inverse Reinforcement Learning with Probabilistic Context Variables'},\n",
       "   {'paperId': 'bc70d4c7d375986e79875cae2d714ea0521fbd5a',\n",
       "    'title': 'Predictive Auxiliary Variational Autoencoder for Representation Learning of Global Speech Characteristics'},\n",
       "   {'paperId': 'c8b39c98ea96d962c7f30780565bd2a03bbb1a7a',\n",
       "    'title': 'Learning Action-Transferable Policy with Action Embedding'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': '7c25b27c1496401058f84159e050fd366906699d',\n",
       "    'title': 'Skill Transfer in Deep Reinforcement Learning under Morphological Heterogeneity'},\n",
       "   {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "    'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       "   {'paperId': '19d9134effebb79799b1ed6189109b5c7bc56e24',\n",
       "    'title': 'Sharing Experience in Multitask Reinforcement Learning'},\n",
       "   {'paperId': '633266814150ab66f0474d7b9a6807b729c7e0af',\n",
       "    'title': 'Environment Probing Interaction Policies'},\n",
       "   {'paperId': '22005795db3a0e85c9091a855427a39b5f8bb33a',\n",
       "    'title': 'Neural Embedding for Physical Manipulations'},\n",
       "   {'paperId': '66605b6ceae9847156526e46ca9fe467804fca54',\n",
       "    'title': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '4127183a8c37efa2c07fa13f1b4179a52c04709c',\n",
       "    'title': 'Disentangled Skill Embeddings for Reinforcement Learning'},\n",
       "   {'paperId': '390c7c230c223498c281a204006c5fc141759460',\n",
       "    'title': 'Transfer Learning by Modeling a Distribution over Policies'},\n",
       "   {'paperId': 'f85ed65604976df89f9d991177c9a428d2168020',\n",
       "    'title': 'Options as responses: Grounding behavioural hierarchies in multi-agent RL'},\n",
       "   {'paperId': '8c7bb0db448bf54cf0af6ef38db5e63402ce72bd',\n",
       "    'title': 'AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence'},\n",
       "   {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "    'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '6b36776e5c0473d82cbdd2c92cd97cca7925ae08',\n",
       "    'title': 'TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'ec9cf6922aae97b7c728693e78bd5cb812cc4e1e',\n",
       "    'title': 'Learning Novel Policies For Tasks'},\n",
       "   {'paperId': '60207969bdffbffb2b69c37d7d25231867a8f356',\n",
       "    'title': 'The Body is Not a Given: Joint Agent Policy Learning and Morphology Evolution'},\n",
       "   {'paperId': '549c9dfb32e85d9ef5a48566767be42ad132a3c4',\n",
       "    'title': 'Information asymmetry in KL-regularized RL'},\n",
       "   {'paperId': 'be928f91385999fa90d1e2fe06058f9dbcfd7186',\n",
       "    'title': 'Routing Networks and the Challenges of Modular and Compositional Computation'},\n",
       "   {'paperId': '4625628163a2ee0e6cd320cd7a14b4ccded2a631',\n",
       "    'title': 'Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '99a7df93a2e16bd7ac3349d52cc34417cda7909d',\n",
       "    'title': 'Learning Latent Plans from Play'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': '8a5c62f9c49a943b66fb1ae379442497609c8596',\n",
       "    'title': 'Emergent Coordination Through Competition'},\n",
       "   {'paperId': '766d48ad414065e5e2fa4fef13c8951f28b58bf4',\n",
       "    'title': 'The Natural Language of Actions'},\n",
       "   {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "    'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       "   {'paperId': '7d5dd17e1a10c0388d2e134dedcfe4e8b224352b',\n",
       "    'title': 'Self-supervised Learning of Image Embedding for Continuous Control'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': '0aecfe5ffbba42d26b5483078c25688da90f8fca',\n",
       "    'title': 'VPE: Variational Policy Embedding for Transfer Reinforcement Learning'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '87d84a825924444efd8f9d063dcdf3c2fb9904df',\n",
       "    'title': 'Multi-Task Reinforcement Learning without Interference'},\n",
       "   {'paperId': '2c5531652b11b39205d901542d7fcefc819088e2',\n",
       "    'title': 'Latent Representation of Tasks for Faster Learning in Reinforcement Learning'},\n",
       "   {'paperId': '9fc047b631ee023e3d91cfff3ad43a46b10dcf8c',\n",
       "    'title': 'PATA: Probabilistic Active Task Acquisition'},\n",
       "   {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "    'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '5654028d5193dbf8eaa5ab9ef6af21f6da265878',\n",
       "    'title': 'SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-'},\n",
       "   {'paperId': '50823b647d3c32fa11aba43234feb6f7fbc8fd2a',\n",
       "    'title': 'Environment Generalization in Deep Reinforcement Learning'},\n",
       "   {'paperId': 'aa6c6b9a58e0c4df5c6c11543bbe61d58b12afbb',\n",
       "    'title': 'Presented at the Task-Agnostic Reinforcement Learning Workshop at ICLR 2019 U NSUPERVISED D ISCOVERY OF D ECISION S TATES THROUGH I NTRINSIC C ONTROL'},\n",
       "   {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': 'd2cb30c20a6578e16a5465b1e9e2ac13091a193b',\n",
       "    'title': 'Efficient transfer learning and online adaptation with latent variable models for continuous control'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '5c91208414f02eaec2281aabd30b50f0ce5b9da6',\n",
       "    'title': 'Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '512a04b7c2c07770963aea441b7337c44947e61d',\n",
       "    'title': 'A Distributed Reinforcement Learning Solution With Knowledge Transfer Capability for A Bike Rebalancing Problem'},\n",
       "   {'paperId': '7d85e83ae00f2d19b3cdb01f5feeb92a2102104f',\n",
       "    'title': 'Task-Embedded Control Networks for Few-Shot Imitation Learning'},\n",
       "   {'paperId': '160d6fe8921c6e66dd5fa925f88d40d0f6682d27',\n",
       "    'title': 'Zero-Shot Skill Composition and Simulation-to-Real Transfer by Learning Task Representations'},\n",
       "   {'paperId': '3996f47c119223fb547905784124a0a2aa09c9a4',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning composable robot skills'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': '2e7b6e73398af01bc975e9bf9374ee5f255252b3',\n",
       "    'title': 'VFunc: a Deep Generative Model for Functions'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '6ecc4b1ab05f3ec12484a0ea36abfd6271c5c5ba',\n",
       "    'title': 'Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '68c108795deef06fa929d1f6e96b75dbf7ce8531',\n",
       "    'title': 'Meta-Reinforcement Learning of Structured Exploration Strategies'},\n",
       "   {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "    'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd472fd0d0c7822253fbbd0e28d3b9d685dc23a58',\n",
       "    'title': 'Simulator Predictive Control: Using Learned Task Representations and MPC for Zero-Shot Generalization and Sequencing'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'},\n",
       "   {'paperId': '7758f39652d3e85a4725d0c74872c19c9666e827',\n",
       "    'title': 'Integrating Structure with Deep Reinforcement and Imitation Learning'},\n",
       "   {'paperId': 'c89469410b2ecce60ea870a5569570b2a8902a68',\n",
       "    'title': 'Direct Policy Transfer via Hidden Parameter Markov Decision Processes'},\n",
       "   {'paperId': 'bc2aa558cbcaca3b32d36c1dafc85955fade7b15',\n",
       "    'title': 'Science Journals — AAAS'}],\n",
       "  'citnuminlist': 20,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " '3c3861c607fb79f3fbf79552018724617fc8ba1b': {'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft',\n",
       "  'year': 2016,\n",
       "  'references': [{'paperId': 'd7bd6e3addd8bc8e2e154048300eea15f030ed33',\n",
       "    'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '53c9443e4e667170acc60ca1b31a0ec7151fe753',\n",
       "    'title': 'Progressive Neural Networks'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '5129a9cbb6de3c6579f6a7d974394d392ac29829',\n",
       "    'title': 'Control of Memory, Active Perception, and Action in Minecraft'},\n",
       "   {'paperId': 'a473f545318325ba23b7a6b477485d29777ba873',\n",
       "    'title': 'ViZDoom: A Doom-based AI research platform for visual reinforcement learning'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': 'edff4138184b4f37590649b9aadc9a627942a5d0',\n",
       "    'title': 'Adaptive Skills Adaptive Partitions (ASAP)'},\n",
       "   {'paperId': 'fa7682d88e0d3d3401f2b84e1177bb1c1df285be',\n",
       "    'title': 'Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)'},\n",
       "   {'paperId': '7200969d70cf6f3fd343f48e97b8ebf7d563a584',\n",
       "    'title': 'Graying the black box: Understanding DQNs'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f88a6f6fd6611543220482e6b3a5f379b7bf5049',\n",
       "    'title': 'Increasing the Action Gap: New Operators for Reinforcement Learning'},\n",
       "   {'paperId': '4c05d7caa357148f0bbd61720bdd35f0bc05eb81',\n",
       "    'title': 'Dueling Network Architectures for Deep Reinforcement Learning'},\n",
       "   {'paperId': '1c4927af526d5c28f7c2cfa492ece192d80a61d4',\n",
       "    'title': 'Policy Distillation'},\n",
       "   {'paperId': '1def5d3711ebd1d86787b1ed57c91832c5ddc90b',\n",
       "    'title': 'Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning'},\n",
       "   {'paperId': 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca',\n",
       "    'title': 'Prioritized Experience Replay'},\n",
       "   {'paperId': '3b9732bb07dc99bde5e1f9f75251c6ea5039373e',\n",
       "    'title': 'Deep Reinforcement Learning with Double Q-Learning'},\n",
       "   {'paperId': '0d78f9bcacc9caeb1282ebfb1c8d4243fba8df74',\n",
       "    'title': 'Selecting Subgoals using Deep Learning in Minecraft : A Preliminary Report'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Adaptive Skills, Adaptive Partitions (ASAP). Neural Information Processing Systems (NIPS)'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Vizdoom : A doombased AI research platform for visual reinforcement learn'},\n",
       "   {'paperId': None, 'title': 'Adaptive Partitions (ASAP)'},\n",
       "   {'paperId': '49ed46f7acd2e421ac24e63fa6ab3a0f265aaff8',\n",
       "    'title': 'Online Planning for Large Markov Decision Processes with Hierarchical Decomposition'},\n",
       "   {'paperId': 'e6c82a3e91ef732b96012fd08a634a9b0e23d765',\n",
       "    'title': 'Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret'},\n",
       "   {'paperId': '0c908739fbff75f03469d13d4a1a07de3414ee19',\n",
       "    'title': 'Distilling the Knowledge in a Neural Network'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Humanlevel control through deep reinforcement learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'ACM Transactions on Intelligent Systems and Technology (TIST)'},\n",
       "   {'paperId': None, 'title': 'Raia. Policy Distillation. arXiv'},\n",
       "   {'paperId': None, 'title': 'The optioncritic architecture'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Mnih. Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': None, 'title': 'and Precup'},\n",
       "   {'paperId': '5ce4016826d84e1d06f177177b54ad02ecd53e7a',\n",
       "    'title': 'Time-regularized interrupting options'},\n",
       "   {'paperId': '3cdcfe28827bf5bd6f0717ba24af991746e6050e',\n",
       "    'title': 'Online Multi-Task Learning for Policy Gradient Methods'},\n",
       "   {'paperId': 'ea2b8c200d4b045803e65620242603edb9a9bf01',\n",
       "    'title': 'Scaling Up Approximate Value Iteration with Options: Better Policies with Fewer Iterations'},\n",
       "   {'paperId': 'd163ae2ae7ee2b7991ab017113f13f54fc5c8c5f',\n",
       "    'title': 'PAC-inspired Option Discovery in Lifelong Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': 'and Li'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Time regularized interrupting options. Internation Conference on Machine Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Time regularized interrupting options. Internation Conference on Machine Learn- ing'},\n",
       "   {'paperId': '8d49d34fff05285cb9a148261caff57775eb4453',\n",
       "    'title': 'ELLA: An Efficient Lifelong Learning Algorithm'},\n",
       "   {'paperId': '5776d0fea69d826519ee3649f620e8755a490efe',\n",
       "    'title': 'Lifelong Machine Learning Systems: Beyond Learning Algorithms'},\n",
       "   {'paperId': None, 'title': 'and Mannor'},\n",
       "   {'paperId': None, 'title': 'An efficient lifelong learning algorithm'},\n",
       "   {'paperId': 'abd1c342495432171beb7ca8fd9551ef13cbd0ff',\n",
       "    'title': 'ImageNet classification with deep convolutional neural networks'},\n",
       "   {'paperId': 'e2230c5954d6806661bf81e4a3999a2802a85d11',\n",
       "    'title': 'Learning Parameterized Skills'},\n",
       "   {'paperId': 'bc6dff14a130c57a91d5a21339c23471faf1d46f', 'title': 'Et al'},\n",
       "   {'paperId': '5fd6996013ee545db00a55bdfcdc1ed27436b7e1',\n",
       "    'title': 'Proceedings of the Twenty-Ninth International Conference on Machine Learning'},\n",
       "   {'paperId': '8de174ab5419b9d3127695405efd079808e956e8',\n",
       "    'title': 'Curriculum learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'In Proceedings of the 26th annual international conference on machine learning'},\n",
       "   {'paperId': '39a2cd04d81eeefe8ea8293a29532cf7ad55e34c',\n",
       "    'title': 'Reinforcement Learning for RoboCup Soccer Keepaway'},\n",
       "   {'paperId': '3dd614ec69ca6ea73d7d096752d6daf4b4efc896',\n",
       "    'title': 'Adaptive Behavior'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': 'and Precup'},\n",
       "   {'paperId': 'd6a250c3e01003240b309ed78f20551e41521896',\n",
       "    'title': 'Layered Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Layered Learning. EMCL 2000 Proceedings of the 17th International Conference on Machine Learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'EMCL 2000 Proceedings of the 17th International Conference on Machine Learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '036373f17e5e47bcadc289e6c57d61cf5e08fe3d',\n",
       "    'title': 'Hierarchical Solution of Markov Decision Processes using Macro-actions'},\n",
       "   {'paperId': '765a4131440dadaaede522135f975556451ae99a',\n",
       "    'title': 'Multi-time Models for Temporally Abstract Planning'},\n",
       "   {'paperId': '6c8f0f28bcbc358726035cfd243239fd32eb8cba',\n",
       "    'title': 'Lifelong robot learning'},\n",
       "   {'paperId': '54c4cf3a8168c1b70f91cf78a3dc98b671935492',\n",
       "    'title': 'Reinforcement learning for robots using neural networks'},\n",
       "   {'paperId': 'ffb199e36de4f34ea233a30d392fdcf0c3b25a14',\n",
       "    'title': 'Rule-Injection Hints as a Means of Improving Network Performance and Learning Time'},\n",
       "   {'paperId': '69d7086300e7f5322c06f2f242a565b3a182efb5',\n",
       "    'title': 'In Advances in Neural Information Processing Systems'},\n",
       "   {'paperId': None, 'title': 'pages 120–129'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Emma Brunskill and Lihong Li . Pacinspired option discovery in lifelong reinforcement learning'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Doina Precup , and Satinder Singh . Between MDPs and semiMDPs : A framework for temporal abstraction in reinforcement learning'}],\n",
       "  'citations': [{'paperId': 'd4f87aec1c491c4f68a460ffc734d756f660aa46',\n",
       "    'title': 'Efficient Few-Shot Classification via Contrastive Pretraining on Web Data'},\n",
       "   {'paperId': 'c695c4e68561347564ea0daa50dc339dff73d8c5',\n",
       "    'title': 'Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory'},\n",
       "   {'paperId': '427f95de4b8a1fa457d2406ebb1ff3a1b61c86fe',\n",
       "    'title': 'ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry'},\n",
       "   {'paperId': '68a6780da08bf678a2c48fb8c187626f1ea1ded1',\n",
       "    'title': '3D reconstruction based on hierarchical reinforcement learning with transferability'},\n",
       "   {'paperId': 'e51f71c171e69c3631ef520851e8f63a65d55b19',\n",
       "    'title': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters'},\n",
       "   {'paperId': '14a143d05635f79011c79d931d24efcfed26da2b',\n",
       "    'title': 'MARL Sim2real Transfer: Merging Physical Reality With Digital Virtuality in Metaverse'},\n",
       "   {'paperId': '2d1ad38d83a5b8a6bb47630972ada82b62ea4aac',\n",
       "    'title': 'Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks'},\n",
       "   {'paperId': '81943619df503fca2d3f4976f60fd7d22146403a',\n",
       "    'title': 'CLIP4MC: An RL-Friendly Vision-Language Model for Minecraft'},\n",
       "   {'paperId': '9dc7538aa8b303d919bc954f67b5ce48b3e5f333',\n",
       "    'title': 'Research on target detection method based on attention mechanism and reinforcement learning'},\n",
       "   {'paperId': 'bc3532f158d0fcf494e7774f45082065119a65e6',\n",
       "    'title': 'Hierarchical Driving Strategy for Connected and Autonomous Vehicles Making a Protected Left Turn at Signalized Intersections'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '20f30cb2cc3782c63d8adeed180467611c4b26c0',\n",
       "    'title': 'A Comprehensive Survey of Continual Learning: Theory, Method and Application'},\n",
       "   {'paperId': '5291241f4bcc077844694181ce8722e91cb41e3b',\n",
       "    'title': 'Advanced Reinforcement Learning and Its Connections with Brain Neuroscience'},\n",
       "   {'paperId': '9064845595d2fe7dd860c612050e4818a191ff62',\n",
       "    'title': 'Reinforcement Learning on Graphs: A Survey'},\n",
       "   {'paperId': '38aa58f7fdc6ff15493bcf247c1ecc9233c5edd3',\n",
       "    'title': 'HSELL-Net: A Heterogeneous Sample Enhancement Network With Lifelong Learning Under Industrial Small Samples'},\n",
       "   {'paperId': '12075ea34f5fbe32ec5582786761ab34d401209b',\n",
       "    'title': 'Exploration in Deep Reinforcement Learning: From Single-Agent to Multiagent Domain.'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e7bd1be9f78242c5ca02a0ac37b7cec0afd0fe00',\n",
       "    'title': 'Extensible Hierarchical Multi-Agent Reinforcement-Learning Algorithm in Traffic Signal Control'},\n",
       "   {'paperId': '33874300376d8f0b53968e6cfc3e7d406a6b135a',\n",
       "    'title': 'Metaverse and education: the pioneering case of Minecraft in immersive digital learning'},\n",
       "   {'paperId': 'bdcd8dd1c2051f063b651e3e94b47596c9827a3b',\n",
       "    'title': 'Building a Subspace of Policies for Scalable Continual Learning'},\n",
       "   {'paperId': '074f9767bf81f04448ce6d997b7e29dc92b10fe7',\n",
       "    'title': 'General Intelligence Requires Rethinking Exploration'},\n",
       "   {'paperId': 'e0eb9870a6c105cadd92cae8f5218b2a84955849',\n",
       "    'title': 'Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks'},\n",
       "   {'paperId': '05c79aa071d4e552e7837ce6308cda8782c10a09',\n",
       "    'title': 'Self-Transfer Learning Network for Multicolor Fabric Defect Detection'},\n",
       "   {'paperId': '0c0b7fb0066e8c1a5a2b4e4856135650eeef7702',\n",
       "    'title': 'Causality-driven Hierarchical Structure Discovery for Reinforcement Learning'},\n",
       "   {'paperId': '849dcd45264e90b6ff39e6d0f02f54c66aa49dcb',\n",
       "    'title': 'Knowledge-Grounded Reinforcement Learning'},\n",
       "   {'paperId': '4ebb88207c01936c3af2964c106e1a7b6baf35a6',\n",
       "    'title': 'Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a0d772f20fcfb5ba6faea34e77c2e77a91dea81c',\n",
       "    'title': 'Learning in Bi-level Markov Games'},\n",
       "   {'paperId': '9337d750993d8715c872db8d406480d58464555a',\n",
       "    'title': 'How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition'},\n",
       "   {'paperId': 'd9e4e362be66e3c803b171ee257bcd3032780a5e',\n",
       "    'title': 'Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs'},\n",
       "   {'paperId': '65fc1f1c567801fee3788974e753cdbf934f07e9',\n",
       "    'title': 'Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '66296def050725c18093351df0c80bc348552d9e',\n",
       "    'title': 'Efficiency-reinforced Learning with Auxiliary Depth Reconstruction for Autonomous Navigation of Mobile Devices'},\n",
       "   {'paperId': 'da150230a48011330c35f1f77bdc3a1cff637509',\n",
       "    'title': 'HR-Planner: A Hierarchical Highway Tactical Planner based on Residual Reinforcement Learning'},\n",
       "   {'paperId': '900ce02e280fca4ac53ff13a18434532cf9bb87f',\n",
       "    'title': 'Prospects for multi-agent collaboration and gaming: challenge, technology, and application'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '7a4788979c15aefe7d3a4b2223997f49c5212d26',\n",
       "    'title': 'Memory-enhanced deep reinforcement learning for UAV navigation in 3D environment'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '8ada79148436cb18ed4fe84d1b2165047447462a',\n",
       "    'title': 'ASC me to Do Anything: Multi-task Training for Embodied AI'},\n",
       "   {'paperId': '5a3fe4482c8d754b290cf90c0fc03362d02ee1a4',\n",
       "    'title': 'MuZero with Self-competition for Rate Control in VP9 Video Compression'},\n",
       "   {'paperId': '63ad803bbbf7c3b8500948d6d31f6c22341293a4',\n",
       "    'title': 'Continuous Autonomous Ship Learning Framework for Human Policies on Simulation'},\n",
       "   {'paperId': 'bca42b535c4c4ea82f011fb1f6659c94e7687b04',\n",
       "    'title': 'Hierarchical deep reinforcement learning reveals a modular mechanism of cell movement'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': 'b64b3880198289fca95e54a001da3dd336502d7a',\n",
       "    'title': 'CoMPS: Continual Meta Policy Search'},\n",
       "   {'paperId': '51517d5d900a7c0cd33e210c48cb27ef3b96e5a9',\n",
       "    'title': 'JueWu-MC: Playing Minecraft with Sample-efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'af6a932c427b04dfbca5ee3c9fd655548beb68b5',\n",
       "    'title': 'Non-IID data and Continual Learning processes in Federated Learning: A long road ahead'},\n",
       "   {'paperId': 'b029d90f33041101c93773be9040e7c5e9bfa4ca',\n",
       "    'title': 'Privacy Preserving Defense For Black Box Classifiers Against On-Line Adversarial Attacks'},\n",
       "   {'paperId': '0213fa01c7b8aa7668b11fd9edf283fe10d5719e',\n",
       "    'title': 'Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '84bc19908766c450bfb9a77e03fecf1015beb7ff',\n",
       "    'title': 'Self-Activating Neural Ensembles for Continual Reinforcement Learning'},\n",
       "   {'paperId': '9faecf3e18a833f2d49b030d591cc2ded0b54336',\n",
       "    'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives'},\n",
       "   {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "    'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       "   {'paperId': '71e3f847321560fd3cb18dacbafc1b47095d99a0',\n",
       "    'title': 'Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships'},\n",
       "   {'paperId': 'd71a3a8c63a549e7bb2eb147094ea527ee78ac7b',\n",
       "    'title': 'Representative Task Self-Selection for Flexible Clustered Lifelong Learning'},\n",
       "   {'paperId': '0ba721d29e93f51235f4306e6f06c0762a10c90d',\n",
       "    'title': 'Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning'},\n",
       "   {'paperId': 'd7988ae63e038b09c1c20ccc48f5462b76bad96b',\n",
       "    'title': 'Autonomous Swarm Shepherding Using Curriculum-Based Reinforcement Learning'},\n",
       "   {'paperId': 'b45ea295f8bab57f48680df4610909131695fe64',\n",
       "    'title': 'Reinforcement learning on graph: A survey'},\n",
       "   {'paperId': '9f96d98eb399607696668988008d195f74bbf02b',\n",
       "    'title': 'Deep Reinforcement Learning Using a Low-Dimensional Observation Filter for Visual Complex Video Game Playing'},\n",
       "   {'paperId': '66b2693f457d0b89befb525de262bdbd4ac6370c',\n",
       "    'title': 'Reinforcement Learning-Based Coverage Path Planning with Implicit Cellular Decomposition'},\n",
       "   {'paperId': '81cbc4a42e2d1296a33311bca0f578d685a2fe2a',\n",
       "    'title': 'Unlocking the potential of deep learning for marine ecology: overview, applications, and outlook'},\n",
       "   {'paperId': 'e2f83f7406eab25f0c6b82f8b04b3403da05d6b5',\n",
       "    'title': 'Promoting Coordination Through Electing First-moveAgent in Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '9201986fdabbffa88f397c43134da8d707fb7c83',\n",
       "    'title': 'Continuous Homeostatic Reinforcement Learning for Self-Regulated Autonomous Agents'},\n",
       "   {'paperId': 'efa0e86ce5ec5b05e5f8a76a23fd14628d1d6f7a',\n",
       "    'title': 'Transfer learning applied to DRL-Based heat pump control to leverage microgrid energy efficiency'},\n",
       "   {'paperId': '0a50454605da864cc4e1ac949e7a43055be11717',\n",
       "    'title': 'Recent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1 - Fundamentals and Applications in Games, Robotics and Natural Language Processing'},\n",
       "   {'paperId': 'c9a079b09f4f27bbcbf5175a4d79c96f38b9be77',\n",
       "    'title': 'Disentangling Transfer and Interference in Multi-Domain Learning'},\n",
       "   {'paperId': '9591957bb641996aab63cd2f483b54308cdb22af',\n",
       "    'title': 'A Survey of Deep Learning Techniques for Cybersecurity in Mobile Networks'},\n",
       "   {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "    'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '1012fd16c3ca62033a712140f8b66d4533f95c3c',\n",
       "    'title': 'Lifetime policy reuse and the importance of task capacity'},\n",
       "   {'paperId': 'ba1b974fc3a144deeabfc08f3edc3a6849ffc1fd',\n",
       "    'title': 'Lifelong Classification in Open World With Limited Storage Requirements'},\n",
       "   {'paperId': 'bf4a288e5bbd5756aaa00a63f2312587235d5301',\n",
       "    'title': 'AppBuddy: Learning to Accomplish Tasks in Mobile Apps via Reinforcement Learning'},\n",
       "   {'paperId': 'a8c753bbd960bbde37234217f387ca246fc2146c',\n",
       "    'title': 'Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments'},\n",
       "   {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "    'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       "   {'paperId': '982f5f3e9c21f252018bb6fe27930e4e7bda74f8',\n",
       "    'title': 'AndroidEnv: A Reinforcement Learning Platform for Android'},\n",
       "   {'paperId': '090273ad6b3720027e34f9183576dd2812bb4454',\n",
       "    'title': 'Continual World: A Robotic Benchmark For Continual Reinforcement Learning'},\n",
       "   {'paperId': '5f1adc14a77fb61aa463fac728397bd32e00b617',\n",
       "    'title': 'Challenges of real-world reinforcement learning: definitions, benchmarks and analysis'},\n",
       "   {'paperId': '14027bb5471b290ce78d2aa7154f47334f15110a',\n",
       "    'title': 'DRL: Deep Reinforcement Learning for Intelligent Robot Control - Concept, Literature, and Future'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': '45c7d3979a7e220bee12fef3aae2de5d4cce822f',\n",
       "    'title': 'Visual Explanation using Attention Mechanism in Actor-Critic-based Deep Reinforcement Learning'},\n",
       "   {'paperId': '542b8dd71309cc001082b152598f40674ec32c01',\n",
       "    'title': 'Enabling the Network to Surf the Internet'},\n",
       "   {'paperId': 'abd77509ef1cc739c0757ae657025fcee13c98dc',\n",
       "    'title': 'Program Synthesis Guided Reinforcement Learning for Partially Observed Environments'},\n",
       "   {'paperId': '8343b6f3c8424ac1a8069d31b7a0de1e8f3c40b8',\n",
       "    'title': 'Discovery of Options via Meta-Learned Subgoals'},\n",
       "   {'paperId': 'b5a72093fb370174859f61025d9fc9c05e605e8c',\n",
       "    'title': 'Hierarchical multiagent reinforcement learning schemes for air traffic management'},\n",
       "   {'paperId': 'cbad0923db89f23febcbd6192ff4149289ff2ad9',\n",
       "    'title': 'A survey on data‐efficient algorithms in big data era'},\n",
       "   {'paperId': 'eecc04e4751ef623ecd9f9e69e9601c9431152d2',\n",
       "    'title': 'Balancing Constraints and Rewards with Meta-Gradient D4PG'},\n",
       "   {'paperId': '3047992223549cc394e201a326eaa3337dc4b4b2',\n",
       "    'title': 'Provable Hierarchical Imitation Learning via EM'},\n",
       "   {'paperId': '884b0fe671f2227d10bcb04ac61767a7371bd64e',\n",
       "    'title': 'Taskology: Utilizing Task Relations at Scale'},\n",
       "   {'paperId': '4f67b5e7c1b50e2863bd01ea372f9b1db3838450',\n",
       "    'title': 'Knowledge Transfer in Vision Recognition'},\n",
       "   {'paperId': '923651bf821c799aa07f1dbf47989f6663864e79',\n",
       "    'title': 'HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning Problem'},\n",
       "   {'paperId': 'ed580995e4a51424d8f1a20f5b64200fe227c2cd',\n",
       "    'title': 'Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control'},\n",
       "   {'paperId': '1cbdb66babbc875ac7d4f02695d06f69d184d026',\n",
       "    'title': 'A Player-like Agent Reinforcement Learning Method For Automatic Evaluation of Game Map'},\n",
       "   {'paperId': '567c75aa0c146f92db19134806b5b41266b13f59',\n",
       "    'title': 'V ALUE F UNCTION S PACES : S KILL -C ENTRIC S TATE A BSTRACTIONS FOR L ONG -H ORIZON R EASONING'},\n",
       "   {'paperId': 'c79a18b7e9c845e05cbcb84a8461331c87d7479e',\n",
       "    'title': 'Supplementary Material: Discovery of Options via Meta-Learned Subgoals'},\n",
       "   {'paperId': 'ba2d3d56033df8055d1a85182109d46535c80f4f',\n",
       "    'title': 'Unity: A General Platform for Intelligent Agents Unity: A General Platform for Intelligent Agents'},\n",
       "   {'paperId': '2e5dc4b028e670d1b9ac0b4e15580cdb7197cb68',\n",
       "    'title': 'Exploration in Deep Reinforcement Learning: A Comprehensive Survey'},\n",
       "   {'paperId': '727d2d5fe17a29f7b32117645ba4c2d2d6309f54',\n",
       "    'title': 'Learning to Compose Behavior Primitives for Near-Decomposable Manipulation Tasks'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '630b70d8fb6dd3c01983f5a1cefc8b5fbb6af1d3',\n",
       "    'title': 'Anchor: The achieved goal to replace the subgoal for hierarchical reinforcement learning'},\n",
       "   {'paperId': '484ee269ce0536ae15754602cb5143191b8e7853',\n",
       "    'title': 'Towards robust and domain agnostic reinforcement learning competitions'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': 'c28dd422d4ba331fa2519df446a036b7d302aef3',\n",
       "    'title': 'Variable-Shot Adaptation for Online Meta-Learning'},\n",
       "   {'paperId': '4fcda6d3c69fbdbb6568912dfbf5c183e352a297',\n",
       "    'title': 'Incremental Learning for Autonomous Navigation of Mobile Robots based on Deep Reinforcement Learning'},\n",
       "   {'paperId': 'dbe163c870e9cc83b56b4a9e1d6791d71f67adf6',\n",
       "    'title': 'Lifelong learning of interpretable image representations'},\n",
       "   {'paperId': '156a84996911a2116bdddd89aade6921e3e68d22',\n",
       "    'title': 'Model-Free Reinforcement Learning for Real-World Robots'},\n",
       "   {'paperId': '5869591fffb4bdb1196cc71d1c4a14bb30e484b4',\n",
       "    'title': 'Robust Constrained Reinforcement Learning for Continuous Control with Model Misspecification'},\n",
       "   {'paperId': '7144fc4dd1688056468c30add521fc3a5fbe2bbb',\n",
       "    'title': 'Learning to Generalize for Sequential Decision Making'},\n",
       "   {'paperId': '6a2b43c0e5b70beaada6fb20a5920fa5b43557da',\n",
       "    'title': 'Modelling Stock Markets by Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': 'd8dab5770d347c3e7deaef23a5259d06ba5399fe',\n",
       "    'title': 'Modelling Stock Markets by Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '673d5946ddc308bf4942b290c40d54c50cdb4be0',\n",
       "    'title': 'Testing the Safety of Self-driving Vehicles by Simulating Perception and Prediction'},\n",
       "   {'paperId': '2b4c27bf37c42a0584fdc21bf91167ab62eaee56',\n",
       "    'title': 'SLER: Self-generated long-term experience replay for continual reinforcement learning'},\n",
       "   {'paperId': 'ed877fbcb2ad6f2fc5c7a8ddc6923e389dcd1c8d',\n",
       "    'title': 'SLER: Self-generated long-term experience replay for continual reinforcement learning'},\n",
       "   {'paperId': '50e0d675bc64e4648b5ceda1268f00cc9c3269c6',\n",
       "    'title': 'The formation and use of hierarchical cognitive maps in the brain: A neural network model'},\n",
       "   {'paperId': 'ea1ba2aa1066ba0393b9b388c985b41d199edac1',\n",
       "    'title': 'Learning latent representations across multiple data domains using Lifelong VAEGAN'},\n",
       "   {'paperId': 'c328a1ea31eaa4b357e584285ffb6ff2671b5bd4',\n",
       "    'title': 'Concept2Robot: Learning manipulation concepts from instructions and human demonstrations'},\n",
       "   {'paperId': '908ccb603d61291f09e6826c54f179f468c31f00',\n",
       "    'title': 'CraftAssist Instruction Parsing: Semantic Parsing for a Voxel-World Assistant'},\n",
       "   {'paperId': '77665db0b0c625761b40f0bd2dd25d76631ca26c',\n",
       "    'title': 'Deep Neural Mobile Networking'},\n",
       "   {'paperId': '019820cbb73d0651a913bb74cbfb713c8ad772df',\n",
       "    'title': 'ELSIM: End-to-end learning of reusable skills through intrinsic motivation'},\n",
       "   {'paperId': '4a4ef182464b5ed53561d6eed0c173935c054f88',\n",
       "    'title': 'Task-agnostic Exploration in Reinforcement Learning'},\n",
       "   {'paperId': 'f458f97a3cf29883b71f7d0aa5a93a185e6cb887',\n",
       "    'title': 'Lifelong Learning using Eigentasks: Task Separation, Skill Acquisition, and Selective Transfer'},\n",
       "   {'paperId': '1f28d7a9137b6ca53be6093a872b67ea2578b9c2',\n",
       "    'title': 'Feudal Steering: Hierarchical Learning for Steering Angle Prediction'},\n",
       "   {'paperId': 'cdd6aae0236ca6d10237b347c7089076c0d2562e',\n",
       "    'title': 'Selecting Auxiliary Data Using Knowledge Graphs for Image Classification with Limited Labels'},\n",
       "   {'paperId': 'a9c46dfd9a24c754a67386e02424ad68b1f4ab3b',\n",
       "    'title': 'AI Research Considerations for Human Existential Safety (ARCHES)'},\n",
       "   {'paperId': 'eb8147cca470f805412a4209c1a3ace93ee2b2b0',\n",
       "    'title': 'Concept Learning in Deep Reinforcement Learning'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': '02ca0c31f5cabb53f92937a12c1f368234a467dd',\n",
       "    'title': 'Transfer Learning Applied to Reinforcement Learning-Based HVAC Control'},\n",
       "   {'paperId': '273c84b64e16da4842d3c4b438713a81254caf90',\n",
       "    'title': 'An empirical investigation of the challenges of real-world reinforcement learning'},\n",
       "   {'paperId': 'f574ca3c79bb59e5adc9c0f45846419299677fde',\n",
       "    'title': 'Learned Weight Sharing for Deep Multi-Task Learning by Natural Evolution Strategy and Stochastic Gradient Descent'},\n",
       "   {'paperId': 'e102cd42c402026a3862d2e60a75eed7c78860a2',\n",
       "    'title': 'Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey'},\n",
       "   {'paperId': 'd569777654c81b66eedc31987e62ad2de5d9564b',\n",
       "    'title': 'Hyper-Meta Reinforcement Learning with Sparse Reward'},\n",
       "   {'paperId': '117f2e43004fa9af2790f91e04be00b57356c712',\n",
       "    'title': 'Reward Tweaking: Maximizing the Total Reward While Planning for Short Horizons.'},\n",
       "   {'paperId': '17d7f52ad92252615fa2dbe59d0642980014b23f',\n",
       "    'title': 'Maximizing the Total Reward via Reward Tweaking'},\n",
       "   {'paperId': '5bd4abd0a1336f7d880b985acc18082affa56197',\n",
       "    'title': 'Learning State Abstractions for Transfer in Continuous Control'},\n",
       "   {'paperId': 'eaeacc4d9936e7b6d016c42de1f3c536d85b7466',\n",
       "    'title': 'Blind Spot Detection for Safe Sim-to-Real Transfer'},\n",
       "   {'paperId': 'a32adb70fa969549a70891eac5f3712defa8c8f4',\n",
       "    'title': 'Safety-Oriented Stability Biases for Continual Learning'},\n",
       "   {'paperId': '5a30917733db939b004d6a637a5d316373914af4',\n",
       "    'title': 'Combining primitive DQNs for improved reinforcement learning in Minecraft'},\n",
       "   {'paperId': '20e127a1d27617b2b8545a54c016ccf2b5170b78',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Open-Domain Dialog'},\n",
       "   {'paperId': '6946ae0c23257586c12d01675e05167d74cb89fa',\n",
       "    'title': 'Robust Reinforcement Learning for Continuous Control with Model Misspecification'},\n",
       "   {'paperId': '356941da708c6d5b06bce17463aca309fd33151a',\n",
       "    'title': 'Which Tasks Should Be Learned Together in Multi-task Learning?'},\n",
       "   {'paperId': 'a7859b059cfe01d01f1bd795e86eb3f0771fb53b',\n",
       "    'title': 'Model primitives for hierarchical lifelong reinforcement learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '339cb2ec1bb54b6d918c36b9924992b85b668899',\n",
       "    'title': 'Planning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies'},\n",
       "   {'paperId': '5330b2732c12c7027811869a921f544a0bf581ca',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks'},\n",
       "   {'paperId': '1f2bc5d57ccbf5a04e7fea87f1f4db464f533ca8',\n",
       "    'title': 'Deep Learning for Video Game Playing'},\n",
       "   {'paperId': 'fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258',\n",
       "    'title': 'Teacher–Student Curriculum Learning'},\n",
       "   {'paperId': '6f1465042e7f48e63467e1edffb2469f04429da9',\n",
       "    'title': 'Learning to Generalise in Sparse Reward Navigation Environments'},\n",
       "   {'paperId': '60c56b2fa014393c7764652b4880db96a1bf3fc0',\n",
       "    'title': 'Understanding Forgetting in Artificial Neural Networks'},\n",
       "   {'paperId': '886144bdf52254bca44d3bd38bc971a5220ce288',\n",
       "    'title': 'A Gentle Introduction to Reinforcement Learning and its Application in Different Fields'},\n",
       "   {'paperId': '812ccc6db1ee2bea5eb00ad60cdf787f35484c07',\n",
       "    'title': 'Knowledge Transfer in Vision Recognition: A Survey'},\n",
       "   {'paperId': '71636f2df9fa00200d0f991a7d4beb5266384dde',\n",
       "    'title': 'Deep Reinforcement Learning-Based Access Control for Buffer-Aided Relaying Systems With Energy Harvesting'},\n",
       "   {'paperId': 'd17362c79ff42760c70235fb377923f0caad1f3a',\n",
       "    'title': 'A Survey on Visual Navigation for Artificial Agents With Deep Reinforcement Learning'},\n",
       "   {'paperId': '8fb76a141d8e72eef7d6e220ea6556a6c5755756',\n",
       "    'title': 'Learning Intrinsic Rewards as a Bi-Level Optimization Problem'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1d0e22458e1dd3021ee27555d83bcaa007c029b3',\n",
       "    'title': 'Mastering Basketball With Deep Reinforcement Learning: An Integrated Curriculum Training Approach'},\n",
       "   {'paperId': 'c5e1cbf8e76fb074bb666c695763cefb16381000',\n",
       "    'title': 'Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning'},\n",
       "   {'paperId': 'cc81e73f0e0a4da51b958a1faf511a5f58044f89',\n",
       "    'title': 'Chapter Five - Intelligent agents in games: Review with an open-source tool'},\n",
       "   {'paperId': '59ced8ec0e62852585e790b83081523e8776c431',\n",
       "    'title': 'Retrospective Analysis of the 2019 MineRL Competition on Sample Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'a2e43270a9b1421e452c2975e5163e2a216abeac',\n",
       "    'title': 'A Survey of Deep Reinforcement Learning in Video Games'},\n",
       "   {'paperId': '6de1218957fa6750893b5236a455062c6e134aca',\n",
       "    'title': 'A Regionalization Navigation Method Based on Deep Reinforcement Learning'},\n",
       "   {'paperId': '0e54edd0c55c0cb63e719b501e41790c75a2c73a',\n",
       "    'title': 'Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. (Impact des connaissances a priori sur le compromis exploration-exploitation en apprentissage par renforcement)'},\n",
       "   {'paperId': '5a74806c03699776b11e40806787a150c9b4d221', 'title': 'CoRide'},\n",
       "   {'paperId': 'd861cab02cbb314fa7f1e14103a238d66e5d8809',\n",
       "    'title': 'Research on Learning Method Based on Hierarchical Decomposition'},\n",
       "   {'paperId': 'dd5dcfe2cb6e32548eaf4ab3dcbc52c785ce9827',\n",
       "    'title': 'Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems'},\n",
       "   {'paperId': '7a141e2ba1360f50ad5ba88842f7da88c93a38f7',\n",
       "    'title': 'Faster and Safer Training by Embedding High-Level Knowledge into Deep Reinforcement Learning'},\n",
       "   {'paperId': '89f11c2e2bfc1af9ec3e703bb62c4547c4de33bd',\n",
       "    'title': 'Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes'},\n",
       "   {'paperId': '56a53e8d9afe0653af74b6a8780cadde96ee1abd',\n",
       "    'title': 'On Hierarchical Goal Based Reinforcement Learning'},\n",
       "   {'paperId': '2c5a9f09e0fab4a1ec8deac3e63f7edb3b021c45',\n",
       "    'title': 'Walking with MIND: Mental Imagery eNhanceD Embodied QA'},\n",
       "   {'paperId': 'af3857825949fc9e349f76b721c482f1aa1dbe5a',\n",
       "    'title': 'A Hierarchical Approach for MARLÖ Challenge'},\n",
       "   {'paperId': 'a1752a90c5a345b711d237bba05334dd07639725',\n",
       "    'title': 'Intelligent motivation framework based on Q-network for multiple agents in Internet of Things simulations'},\n",
       "   {'paperId': '2c87b6b48dba6315a6581098861f8bf2994799f6',\n",
       "    'title': 'MineRL: A Large-Scale Dataset of Minecraft Demonstrations'},\n",
       "   {'paperId': 'a72220915a635e29ea7a9d3d8f5da5a2a6b2ab3f',\n",
       "    'title': 'Why Build an Assistant in Minecraft?'},\n",
       "   {'paperId': '5d8d6278ad32acee4d649f32766de90cf4d787eb',\n",
       "    'title': 'CraftAssist: A Framework for Dialogue-enabled Interactive Agents'},\n",
       "   {'paperId': '67bb04ca3bb249d6e3daa01b050eb656863c2422',\n",
       "    'title': 'Combo-Action: Training Agent For FPS Game with Auxiliary Tasks'},\n",
       "   {'paperId': 'abefbcbfbcb070743e97326082514ac8a81c4e78',\n",
       "    'title': 'Stochastic Fast Gradient for Tracking'},\n",
       "   {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "    'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "    'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       "   {'paperId': '171bffb23613e89d60664e2078c9e807d0e523df',\n",
       "    'title': 'Hierarchical Decision Making by Generating and Following Natural Language Instructions'},\n",
       "   {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "    'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       "   {'paperId': '094662f00ed897720313dad92fcfc98cf2dd0e04',\n",
       "    'title': 'Embedded Meta-Learning: Toward more flexible deep-learning models'},\n",
       "   {'paperId': '7b4a65cd20b080e314632c772695094b77815a4f',\n",
       "    'title': 'Zero-shot task adaptation by homoiconic meta-mapping'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'fb960d0ea21ff4f1c51444dd6e644efeb16b5dd7',\n",
       "    'title': 'Continual and Multi-task Reinforcement Learning With Shared Episodic Memory'},\n",
       "   {'paperId': '0a92f4934924b34e434bb80bbda842d33f44c6ed',\n",
       "    'title': 'Framing Lifelong Learning as Autonomous Deployment: Tune Once Live Forever'},\n",
       "   {'paperId': '8e08789b299e252674873fa898da5e7da67bb703',\n",
       "    'title': 'The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors'},\n",
       "   {'paperId': 'f552b3783576866c768cd3da638ba494ad4534d0',\n",
       "    'title': 'CraftAssist Instruction Parsing: Semantic Parsing for a Minecraft Assistant'},\n",
       "   {'paperId': '4019625941a75ba71eefab09e8963f9e1d8905a9',\n",
       "    'title': 'Disentangling Options with Hellinger Distance Regularizer'},\n",
       "   {'paperId': 'aba5c8abc8a0d2818cd61c22f683f435628db423',\n",
       "    'title': 'Targeted Knowledge Transfer for Learning Traffic Signal Plans'},\n",
       "   {'paperId': '3dcdd63d942dc3167e34551c8bf5d66f3caee24f',\n",
       "    'title': 'Evolving indoor navigational strategies using gated recurrent units in NEAT'},\n",
       "   {'paperId': '250fabf66752c8729cbf286eadab271bd04da710',\n",
       "    'title': 'Generative Models for Novelty Detection: Applications in abnormal event and situational change detection from data series'},\n",
       "   {'paperId': 'd2ffb1c4b815f1b245f248d436baf9a3c28cc148',\n",
       "    'title': 'Construction-Planning Models in Minecraft'},\n",
       "   {'paperId': '0059bbca358280bf1991ad7a017ffbbbf18325e4',\n",
       "    'title': 'Is Artificial Intelligence New to Multimedia?'},\n",
       "   {'paperId': '14bb59a7fcc8ce883d6d8606e0e1d909b701e4e1',\n",
       "    'title': 'Enhanced Transfer Learning with ImageNet Trained Classification Layer'},\n",
       "   {'paperId': '5fc8a489204a9cdb07ec180bfa82b2a8d64eb01f',\n",
       "    'title': 'Depth Augmented Networks with Optimal Fine-tuning'},\n",
       "   {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "    'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       "   {'paperId': '875355942eb864f7a315ae01051bd8aa4f1f5183',\n",
       "    'title': 'Two-stage population based training method for deep reinforcement learning'},\n",
       "   {'paperId': '4839f473193fccb5566ae6388ba6f40defdf10dc',\n",
       "    'title': 'Model Primitive Hierarchical Lifelong Reinforcement Learning'},\n",
       "   {'paperId': 'e39520b9c4283bdab0cd06962581316819f9b014',\n",
       "    'title': 'Aggregating E-commerce Search Results from Heterogeneous Sources via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2b46e8f2f2339e4dfbc8c3db33b7a6fb65ee62ad',\n",
       "    'title': 'Deep Reinforcement Learning from Policy-Dependent Human Feedback'},\n",
       "   {'paperId': '644d20e1d1d7e55f27e5a3de42e417695d46337f',\n",
       "    'title': 'Decoding multitask DQN in the world of Minecraft'},\n",
       "   {'paperId': '1e658c39cf8695c3363457903f5bc898231611dd',\n",
       "    'title': 'An Optimization Framework for Task Sequencing in Curriculum Learning'},\n",
       "   {'paperId': '70a3f24292bdcf6e630d5b32eacf93aa3f913c59',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent networks.'},\n",
       "   {'paperId': '765dcaf34e182df21c2f4361aa073691e5902df0',\n",
       "    'title': 'Lifelong Federated Reinforcement Learning: A Learning Architecture for Navigation in Cloud Robotic Systems'},\n",
       "   {'paperId': 'b0f5e2149d126c59fcddf3050973bc8da56c989c',\n",
       "    'title': 'What Should I Do Now? Marrying Reinforcement Learning and Symbolic Planning'},\n",
       "   {'paperId': '2ed619fbc7902155d54f6f21da16ad6c120eac63',\n",
       "    'title': 'Learning to Walk via Deep Reinforcement Learning'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751',\n",
       "    'title': 'Deep Reinforcement Learning'},\n",
       "   {'paperId': '8761fe4a7dbdc9fb214b7e25096053507aeb9b69',\n",
       "    'title': 'Interactive Semantic Parsing for If-Then Recipes via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd5d61e7997228a8b96e1e4f1af5e6e1d2e0cb949',\n",
       "    'title': 'Context-Aware Policy Reuse'},\n",
       "   {'paperId': '8a5d0579590465494c9aba58a857af43b190b6a6',\n",
       "    'title': 'Deep Learning in Mobile and Wireless Networking: A Survey'},\n",
       "   {'paperId': '9ea50b3408f993853f1c5e374690e5fbe73c2a3c',\n",
       "    'title': 'Continual Lifelong Learning with Neural Networks: A Review'},\n",
       "   {'paperId': 'bec60089182e274308fe36d28297fb32ca0edd5b',\n",
       "    'title': 'Image and Video Technology: 9th Pacific-Rim Symposium, PSIVT 2019, Sydney, NSW, Australia, November 18–22, 2019, Proceedings'},\n",
       "   {'paperId': '2b7eec22946e331814f4e61a5c2dee0eadf8ef02',\n",
       "    'title': 'Learning Skill Hierarchies from Predicate Descriptions and Self-Supervision'},\n",
       "   {'paperId': '46b1214eb0b93ac370791d1eae26f3b64eb5820d',\n",
       "    'title': 'La cultura Maker en las dinámicas de construcción colaborativa de los videojugadores online. Caso de estudio «Gumiparty»'},\n",
       "   {'paperId': '02feb390612858d745ce324303bfc8f9d0148c42',\n",
       "    'title': 'Building structured hierarchical agents'},\n",
       "   {'paperId': '73e004da785cb4f354a4f23e43a368d6d5d9469d',\n",
       "    'title': 'ICLR 2020 Encoder Shared Representation Decoder Decoder Semantic Segmentation Depth Estimation Encoder Shared Representation DecoderDecoder Edge Detection Keypoint Detection Half Sized Encoder Decoder Surface Normal Prediction A B C Input Image Discarded Decoder Discarded Surface Normal Prediction D'},\n",
       "   {'paperId': 'bdd6acab0da8e9866bad4fcf43e8e7264c9e4faa',\n",
       "    'title': 'Automatically Optimized Gradient Boosting Trees for Classifying Large Volume High Cardinality Data Streams Under Concept Drift'},\n",
       "   {'paperId': '9856bc0f896e07a22e98c27136df726aecb629a3',\n",
       "    'title': 'Difference Based Metrics for Deep Reinforcement Learning Algorithms'},\n",
       "   {'paperId': 'bd26c5677f2b2a9fc7ff7e0dd0d640fc6db5d322',\n",
       "    'title': 'On the Emergence of Biologically-Plausible Representation in Recurrent Neural Networks for Navigation'},\n",
       "   {'paperId': '60a1096593531ea5615bbedbd1c9a48bd931b860',\n",
       "    'title': 'Self-developed Action Hierarchy Enhances Meta Reinforcement Learning.'},\n",
       "   {'paperId': '6446be07aac1ce8ecee77adf17e5d30bd44246c9',\n",
       "    'title': 'Safer reinforcement learning for robotics'},\n",
       "   {'paperId': '4b61c25a86083c20730c9b12737ac6ac4178c364',\n",
       "    'title': 'An Introduction to Deep Reinforcement Learning'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '8e3ec570b2d73bad954bf3af3f6e4dc0a982b596',\n",
       "    'title': 'Environments for Lifelong Reinforcement Learning'},\n",
       "   {'paperId': '999f136b4dd46c12d77a1f7fb9dfaaca403edfcf',\n",
       "    'title': 'Clustered Lifelong Learning Via Representative Task Selection'},\n",
       "   {'paperId': '1a9ce166e86b4c661ba5ca513e8a791df4f54217',\n",
       "    'title': 'Big Data Processing Architecture for Radio Signals Empowered by Deep Learning: Concept, Experiment, Applications and Challenges'},\n",
       "   {'paperId': '825c0574ebe9356ebc8f82c40454083287462d70',\n",
       "    'title': 'Modular Architecture for StarCraft II with Deep Reinforcement Learning'},\n",
       "   {'paperId': '6f69e19348870552a7ab92d038c8b8d753fe6b60',\n",
       "    'title': 'Neural Arithmetic Expression Calculator'},\n",
       "   {'paperId': '1a1a22a3281e00d4ffd69b6490abc266d4eed0b3',\n",
       "    'title': 'Learning with Training Wheels: Speeding up Training with a Simple Controller for Deep Reinforcement Learning'},\n",
       "   {'paperId': '252482d733d67230843fe99bf60427285f0ad8e8',\n",
       "    'title': 'Unity: A General Platform for Intelligent Agents'},\n",
       "   {'paperId': '5556d8b9a9f1b9a1c30343519aedda1525613a5d',\n",
       "    'title': 'Optimizing the Capacity of a Convolutional Neural Network for Image Segmentation and Pattern Recognition'},\n",
       "   {'paperId': 'f7ab4f0923173c01406224ab05bf99805cfa29fe',\n",
       "    'title': 'Contexto contemporáneo de la dimensión del espacio con la innovación tecnológica: el aprendizaje ubicuo'},\n",
       "   {'paperId': 'eb9a84875676e5633ed153143b9f9761bcdc1d96',\n",
       "    'title': 'Accelerating Drugs Discovery with Deep Reinforcement Learning: An Early Approach'},\n",
       "   {'paperId': '92250f2237378ea9af5fd4e3aaa681ed9818188b',\n",
       "    'title': 'Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration'},\n",
       "   {'paperId': '47a7b12776de302f5cf34df21eced1bff593dbcf',\n",
       "    'title': 'Combining deep reinforcement learning with prior knowledge and reasoning'},\n",
       "   {'paperId': 'd1e7f3f432801eaad8b9a0c0edf12f8d7f0c34dd',\n",
       "    'title': 'Visual Navigation with Actor-Critic Deep Reinforcement Learning'},\n",
       "   {'paperId': '576fada6a8d4581a81f9e2304dd6ec20761663ae',\n",
       "    'title': 'Learning to Navigate Through Complex Dynamic Environment With Modular Deep Reinforcement Learning'},\n",
       "   {'paperId': '6868c233c2d0fe01ecf0eda01099f6c7a0f98fb9',\n",
       "    'title': 'Embodied Question Answering'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15365821d5e2b9ebec1ed9ac314975732b688da3',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Deep Nested Agents'},\n",
       "   {'paperId': 'ea07c3c182a5d06a5096d46eb127e2c663100451',\n",
       "    'title': 'Deep Reinforcement Learning for Playing 2.5D Fighting Games'},\n",
       "   {'paperId': '2fe2cfd98e232f1396f01881853ed6b3d5e37d65',\n",
       "    'title': 'Taskonomy: Disentangling Task Transfer Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '86430b50921c7aecb212b4cd0a43a73b091cf95f',\n",
       "    'title': 'Learning to Navigate in Cities Without a Map'},\n",
       "   {'paperId': '0445c132ac43885a60c95bf579442783a92d2081',\n",
       "    'title': 'Learning to Play General Video-Games via an Object Embedding Network'},\n",
       "   {'paperId': '0cd97d54c610c5fd3d20fbe230a3423b912a63fb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: Approximating Optimal Discounted TSP Using Local Policies'},\n",
       "   {'paperId': 'e0155830d8982da4631cb71546fca782b2e00c20',\n",
       "    'title': 'Composable Planning with Attributes'},\n",
       "   {'paperId': '8c1650cb7c313ca9134edff68952c3defd793d04',\n",
       "    'title': 'Selective Experience Replay for Lifelong Learning'},\n",
       "   {'paperId': 'd72e69eacd4afeac33f71d07c484686084e55b9a',\n",
       "    'title': 'Unicorn: Continual Learning with a Universal, Off-policy Agent'},\n",
       "   {'paperId': 'b80991d12b41a5a68dc14dd87b692c0f903ceb9c',\n",
       "    'title': 'Some Considerations on Learning to Explore via Meta-Reinforcement Learning'},\n",
       "   {'paperId': '7cccc3b6c6d50bc9b75781573cb065e7b758c931',\n",
       "    'title': 'Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control'},\n",
       "   {'paperId': '4e43d0365e4e922123de54c5e9a430bbced4a817',\n",
       "    'title': 'Learning Robust Options'},\n",
       "   {'paperId': '4adfa52793b0f6f70915383bf12114f4824897e1',\n",
       "    'title': 'Building Generalizable Agents with a Realistic and Rich 3D Environment'},\n",
       "   {'paperId': '0b670a55f83a08d0f049b30f48450fe608f90613',\n",
       "    'title': 'Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '3640bf810f4cc3160c278763663de03bc5ba6bb9',\n",
       "    'title': 'Deep reinforcement learning boosted by external knowledge'},\n",
       "   {'paperId': 'b0cd469a06fb2eae3a5cc0c860aa592f71b13f6d',\n",
       "    'title': 'IQA: Visual Question Answering in Interactive Environments'},\n",
       "   {'paperId': 'e5790afc079c6f36d6fe9235d6d253f3da631f51',\n",
       "    'title': 'Embodied Question Answering'},\n",
       "   {'paperId': '6bd9642470ff8c2089427f7a6392cd17d213a334',\n",
       "    'title': 'Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments'},\n",
       "   {'paperId': '9975ca72a2507d5de379501a72a430ef31d6218b',\n",
       "    'title': 'Learning with Options that Terminate Off-Policy'},\n",
       "   {'paperId': '15b1661cbc70236140cfe221fe09c51eaf9fadd0',\n",
       "    'title': 'The Effects of Memory Replay in Reinforcement Learning'},\n",
       "   {'paperId': 'bf48f1d556fdb85d5dbe8cfd93ef13c212635bcf',\n",
       "    'title': 'Neural Task Programming: Learning to Generalize Across Hierarchical Tasks'},\n",
       "   {'paperId': 'a4a0287a3f88fe992f0f23f3f1bc7eec0c11d32c',\n",
       "    'title': 'Reinforcement Learning in POMDPs with Memoryless Options and Option-Observation Initiation Sets'},\n",
       "   {'paperId': '16f7cdfac779741033ff044883413dc5618ef0ef',\n",
       "    'title': 'Curriculum Design for Machine Learners in Sequential Decision Tasks'},\n",
       "   {'paperId': '4fac0f53c9bbf6bcaa0a152b7a2e32a910687a1f',\n",
       "    'title': 'Navigating Occluded Intersections with Autonomous Vehicles Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ea3631df5e992b601cc82d55b9efaebb62717d6',\n",
       "    'title': 'Deep Reinforcement Learning in Complex Structured Environments'},\n",
       "   {'paperId': 'ce145306d30293fc8e91cc718f9746c2f4f3eb11',\n",
       "    'title': 'A I ] 6 D ec 2 01 8 Environments for Lifelong Reinforcement Learning'},\n",
       "   {'paperId': 'c2ce1912727a3e8c88b4af65c9ca088b3c8eb1a0',\n",
       "    'title': 'Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents'},\n",
       "   {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       "   {'paperId': '282e2b7dd59bf8742d1786abbe08fd9105974686',\n",
       "    'title': 'Treball de Final de Grau Planning with Arithmetic and Geometric Attributes'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '17eea200f354da54cb261fbb60418d9363072329',\n",
       "    'title': 'Discovering multi-purpose modules through deep multitask learning'},\n",
       "   {'paperId': '7cba1fb6edb762c3db8d7b8ab169dbe9c12bb28b',\n",
       "    'title': 'The Importance of Sampling in Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'ee6d434b2935bfa407fc372e917a6b812a94feee',\n",
       "    'title': 'DEEP REINFORCEMENT LEARNING FOR PLAYING 2 . 5 D FIGHTING GAMES'},\n",
       "   {'paperId': '714bb2c21605cde034334aa9e345a9b19bc44cdf',\n",
       "    'title': 'MULTI-SKILLED MOTION CONTROL'},\n",
       "   {'paperId': '7f77020cda7f77ee0d6e642ff94969c6d48a2721',\n",
       "    'title': 'Regret Minimization in MDPs with Options without Prior Knowledge'},\n",
       "   {'paperId': '035d51494e900dda677916a43ac9d2b070d41882',\n",
       "    'title': 'Transferring Autonomous Driving Knowledge on Simulated and Real Intersections'},\n",
       "   {'paperId': '05893041d24dd404963960e73220aca83d19add4',\n",
       "    'title': 'Deep Reinforcement Learning: A Brief Survey'},\n",
       "   {'paperId': '28866d7c5e07d2ba97bbef7dcc862fac625972ec',\n",
       "    'title': 'Deep Reinforcement Learning for Dexterous Manipulation with Concept Networks'},\n",
       "   {'paperId': 'd91e0d62626dd1e33521b110e9702710ed1e5088',\n",
       "    'title': 'Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f03b9b0895f5fb3351bbf3db4b1139af85650543',\n",
       "    'title': 'Where is Misty? Interpreting Spatial Descriptors by Modeling Regions in Space'},\n",
       "   {'paperId': '498238a3bd5fd322fc3ce1572e33bbe3853a356f',\n",
       "    'title': 'A Brief Survey of Deep Reinforcement Learning'},\n",
       "   {'paperId': '69a037af886a6235d9af3cdeef4c7233d34c86ce',\n",
       "    'title': 'Growing a Brain: Fine-Tuning by Increasing Model Capacity'},\n",
       "   {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "    'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       "   {'paperId': 'c96c543df5ec531881b9ae55dfd9cb2f86b2da5d',\n",
       "    'title': 'To Go or Not to Go: A Case for Q-Learning at Unsignalized Intersections'},\n",
       "   {'paperId': '6621e4e423bfb9afc173e9d856ce0b3423df3871',\n",
       "    'title': 'Shallow Updates for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bdc26ceed5f94ffef4a86e00a2595827cd9dd97b',\n",
       "    'title': 'Adaptive Agents in Minecraft: A Hybrid Paradigm for Combining Domain Knowledge with Reinforcement Learning'},\n",
       "   {'paperId': 'b9918043fc690b97b685fd68f8f871db7250803a',\n",
       "    'title': 'Navigating Intersections with Autonomous Vehicles using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e88cf1ce84ab94e7840e3864f5c768d92aa6d9c0',\n",
       "    'title': 'Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously Handling Multiple Intersections'},\n",
       "   {'paperId': '0c43553508adc9278622e750cca1910dceb7b033',\n",
       "    'title': 'Probabilistically safe policy transfer'},\n",
       "   {'paperId': '3693414d385401997c13a4faa39a8b6c6cd4a4dd',\n",
       "    'title': 'Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization'},\n",
       "   {'paperId': 'f0a074177409db16cf3919434b44ce1b6590871b',\n",
       "    'title': 'Exploration-Exploitation in MDPs with Options'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9f1e9e56d80146766bc2316efbc54d8b770a23df',\n",
       "    'title': 'Deep Reinforcement Learning: An Overview'},\n",
       "   {'paperId': 'd35b05f440b5ba00d9429139edef7182bf9f7ce7',\n",
       "    'title': 'Learning to Navigate in Complex Environments'},\n",
       "   {'paperId': 'd7bd6e3addd8bc8e2e154048300eea15f030ed33',\n",
       "    'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'},\n",
       "   {'paperId': 'ebc3bd9cd67193b3a8f84d43d5b4377107c680dc',\n",
       "    'title': 'SITION IN MULTI-TASK REINFORCEMENT LEARNING'},\n",
       "   {'paperId': 'f4b7c69809c30413f97b01050a2e81a1a2cccabc',\n",
       "    'title': 'Attitudes, beliefs and behaviour of lecturers: do they foster a climate of lifelong learning? A case of Papua New Guinea universities'},\n",
       "   {'paperId': '5dbf9c29db4f66166a854e1a8a9d2af99358101d',\n",
       "    'title': 'Effective Master-Slave Communication On A Multi-Agent Deep Reinforcement Learning System'},\n",
       "   {'paperId': '4fd9c795f950868ce58a506c38e195f83ccb74ac',\n",
       "    'title': 'Hierarchical Task Generalization with Neural Programs'},\n",
       "   {'paperId': '01bbe6fa766c328a876784070bac01ab90b41e0d',\n",
       "    'title': 'Learning to Play Computer Games with Deep Learning and Reinforcement Learning'},\n",
       "   {'paperId': '836a917a2a7e962d0881b7222dd7f998d324a336',\n",
       "    'title': 'Improving Sequential Decision Making with Cognitive Priming'},\n",
       "   {'paperId': '114260e0755e487a883d03aa761579791a2bc0f3',\n",
       "    'title': 'IntEx 2017 Working Notes of the Workshop on Integrated Execution of Planning and Acting'},\n",
       "   {'paperId': 'a5c68d0d01ace4a7a5f4d3e0a3fccc42a7d1f354',\n",
       "    'title': 'DeepMind Lab'},\n",
       "   {'paperId': '75a760c6bd5ae15e0fc489a074bc42bc1fc4e697',\n",
       "    'title': 'Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving'},\n",
       "   {'paperId': 'b7b094ccc7aff40db7b5abeb8a7f87595ca7e528',\n",
       "    'title': 'Deep Reinforcement Learning From Raw Pixels in Doom'},\n",
       "   {'paperId': 'ac2949667ba5bffd349292b408a9c903c3803621',\n",
       "    'title': 'Visualizing Dynamics: from t-SNE to SEMI-MDPs'},\n",
       "   {'paperId': '44bd5386c819de85591d2db98a9e269398856b16',\n",
       "    'title': 'Deep Reinforcement Learning Discovers Internal Models'},\n",
       "   {'paperId': '53c9443e4e667170acc60ca1b31a0ec7151fe753',\n",
       "    'title': 'Progressive Neural Networks'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '9213538b4f6f067e70bedc0709901b39481254c1',\n",
       "    'title': 'Active Long Term Memory Networks'},\n",
       "   {'paperId': 'fa7682d88e0d3d3401f2b84e1177bb1c1df285be',\n",
       "    'title': 'Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)'},\n",
       "   {'paperId': '7200969d70cf6f3fd343f48e97b8ebf7d563a584',\n",
       "    'title': 'Graying the black box: Understanding DQNs'},\n",
       "   {'paperId': '4e63a80edee748bc31f12a02920a694c7fc68b04',\n",
       "    'title': 'Deep Reinforcement Learning in a 3-D Blockworld Environment'},\n",
       "   {'paperId': '6afd39f9c74879e7a1c825592421e8826af3d751',\n",
       "    'title': 'Iterative Hierarchical Optimization for Misspecified Problems'},\n",
       "   {'paperId': '93b4a6d068806c454851ed0270957161f8c9649f',\n",
       "    'title': 'Workshop on Machine Learning for Autonomous Vehicles 2017'},\n",
       "   {'paperId': '47c29c305d0ec04a8c8222b09141244ad759cc49',\n",
       "    'title': 'Lifelong Learning : A Reinforcement Learning Approach'}],\n",
       "  'citnuminlist': 4,\n",
       "  'refnuminlist': 1,\n",
       "  'isKeypaper': True},\n",
       " 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69': {'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations',\n",
       "  'year': 2017,\n",
       "  'references': [{'paperId': 'd720d601d5b835937b96b68d7119b239d45776f0',\n",
       "    'title': 'SWIRL: A sequential windowed inverse reinforcement learning algorithm for robot tasks with delayed rewards'},\n",
       "   {'paperId': 'b7473703c1e56b1e48b9d69eb055d95dd8ba9e83',\n",
       "    'title': 'Mutual Alignment Transfer Learning'},\n",
       "   {'paperId': 'cddb1f7f9f004396a2efef285caf29d7780a8e21',\n",
       "    'title': 'Robust Imitation of Diverse Behaviors'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '627e1688024c1fa3f876440750accea7c3010650',\n",
       "    'title': 'Principled Option Learning in Markov Decision Processes'},\n",
       "   {'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': 'e4cebbfbec5d3a74a804d9f825878b2735e3c3e7',\n",
       "    'title': 'Hierarchical Linearly-Solvable Markov Decision Problems'},\n",
       "   {'paperId': '694d284d7c63c7f29c48b540bc4ece9f63b79160',\n",
       "    'title': 'Transition state clustering: Unsupervised surgical trajectory segmentation for robot learning'},\n",
       "   {'paperId': '81f677efa34fc34183d7bee3be2d5653d2fe2acc',\n",
       "    'title': 'Learning movement primitive attractor goals and sequential skills from kinesthetic demonstrations'},\n",
       "   {'paperId': '43fc6ab25ba5236688a87a591ed58c778bdcd417',\n",
       "    'title': 'Probabilistic segmentation applied to an assembly task'},\n",
       "   {'paperId': '788c707f04c81e54cccc68fbd855fd7b99818a1d',\n",
       "    'title': 'Skills learning in robots by interaction with users and environment'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': '89b9928df443e4e686a4f82b9bd8d67dc23cfa05',\n",
       "    'title': 'Learning and generalization of complex tasks from unstructured demonstrations'},\n",
       "   {'paperId': '6b92156c9e6ea53dec9e4941b62c11a1fbda485e',\n",
       "    'title': 'Robot learning from demonstration by constructing skill trees'},\n",
       "   {'paperId': '220ad0189a4c9ee5b5c299f269a0e4bef290e8fd',\n",
       "    'title': 'Learning and generalization of motor skills by learning from demonstration'},\n",
       "   {'paperId': '98eb84463c61b22b16d2f3300b106c0e5d99662b',\n",
       "    'title': 'Hierarchical Apprenticeship Learning with Application to Quadruped Locomotion'},\n",
       "   {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "    'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': 'a6d815f8b3fd31fbf6db122a0e367d3e2a58ee9b',\n",
       "    'title': 'Optimization with EM and Expectation-Conjugate-Gradient'},\n",
       "   {'paperId': 'a28c4a55514bf6a8ddd536e2aeaa4fc6a31018c8',\n",
       "    'title': 'Recent Advances in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0a8149fb5aa8a5684e7d530c264451a5cb9250f5',\n",
       "    'title': 'Recent Advances in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '19059f39effd43aa1501c9b84fcbddfa1c925de4',\n",
       "    'title': 'Discovering Hierarchy in Reinforcement Learning with HEXQ'},\n",
       "   {'paperId': 'd1e1adfc58fb42b7d1baed538fffc5b221c0aefd',\n",
       "    'title': 'Policy Recognition in the Abstract Hidden Markov Model'},\n",
       "   {'paperId': '38688edefc7591ea2fc7d4294070e8bfe9d9ac3d',\n",
       "    'title': 'Learning Attractor Landscapes for Learning Motor Primitives'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '2966ae949d1bc255bad11045fd0ff8eb5848cf5a',\n",
       "    'title': 'Hierarchical control and learning for markov decision processes'},\n",
       "   {'paperId': '7d50991b693fc23edda316fb1487f114f6cc6706',\n",
       "    'title': 'The EM algorithm and extensions'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': '4af77aafa93f810e403461e5ee911287aa16d76e',\n",
       "    'title': 'A robust layered control system for a mobile robot'},\n",
       "   {'paperId': '351bdc21bd5e67e8d41549f9d89e4fcd84438f0f',\n",
       "    'title': 'Learning and Executing Generalized Robot Plans'},\n",
       "   {'paperId': '539036ab9e8f038c8a948596e77cc0dfcfa91fb3',\n",
       "    'title': 'An inequality and associated maximization technique in statistical estimation of probabilistic functions of a Markov process'}],\n",
       "  'citations': [{'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '18bc10da4a1162da1baf2e0a09e97f486a342423',\n",
       "    'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey'},\n",
       "   {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "    'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '834c8c95ff1129eb197bfdfa18f6bdf3c11c205c',\n",
       "    'title': 'Dichotomy of Control: Separating What You Can Control from What You Cannot'},\n",
       "   {'paperId': '85de2d2724d0d4aa0991ef9012200d5fb9246581',\n",
       "    'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning'},\n",
       "   {'paperId': 'a95cfea5cc9ce2731beb4cd264efe8155b13cd3e',\n",
       "    'title': 'Abstract Demonstrations and Adaptive Exploration for Efficient and Stable Multi-step Sparse Reward Reinforcement Learning'},\n",
       "   {'paperId': '178caa0af7d4c76231d5ef675d73ea73355643c9',\n",
       "    'title': 'Implicit Kinematic Policies: Unifying Joint and Cartesian Action Spaces in End-to-End Robot Learning'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': '176aca6a4a616398d87132b5370140da7ab80340',\n",
       "    'title': 'A Versatile Agent for Fast Learning from Human Instructors'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': '0382639a58733e95d4f093943455d58455676db0',\n",
       "    'title': 'Continuous Control with Action Quantization from Demonstrations'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '193dadd9de36ef8e6883088fbd35d38fa7ce590e',\n",
       "    'title': 'Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots'},\n",
       "   {'paperId': '06858604cc652722ca5092072c50a066000c565e',\n",
       "    'title': 'Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks'},\n",
       "   {'paperId': '18ea1e64784bd6d8b14172a8c23277531c2b5539',\n",
       "    'title': 'Sequential robot imitation learning from observations'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '9956e3ea2b894f45ca9070ee1984caadb74edbf7',\n",
       "    'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '3047992223549cc394e201a326eaa3337dc4b4b2',\n",
       "    'title': 'Provable Hierarchical Imitation Learning via EM'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': None, 'title': 'TRANSLATING ROBOT SKILLS: LEARNING UNSUPER-'},\n",
       "   {'paperId': 'c9af30358358b15d05ce72a86ec5f0ce883afdc6',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Single Demonstration'},\n",
       "   {'paperId': 'ec8eee61f42e07228bc78faac9817cff3e000ebf',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Demonstration'},\n",
       "   {'paperId': 'd9ceb68a016be1dcd8d246497d7d964ed4b22751',\n",
       "    'title': 'Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '15afee24a087ef8cdce92a15bf46a403a0439d85',\n",
       "    'title': 'Deep Imitation Learning for Bimanual Robotic Manipulation'},\n",
       "   {'paperId': 'a75dace80c238148d8ca23279a6453a32aa83535',\n",
       "    'title': 'Learning Compound Tasks without Task-specific Knowledge via Imitation and Self-supervised Learning'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '20f3973efecab0ee66adcb07dc33b7cdcfbfecf7',\n",
       "    'title': 'Modeling Long-horizon Tasks as Sequential Interaction Landscapes'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': 'ffecefc4658c8b33005654a4260cfd7d8c9714f1',\n",
       "    'title': 'Applying Depth-Sensing to Automated Surgical Manipulation with a da Vinci Robot'},\n",
       "   {'paperId': '4d40f7df809576d4db22b95c4ca9cc4c66e6928d',\n",
       "    'title': 'Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation'},\n",
       "   {'paperId': '374adc06806697f5fc6cede97fa4dfdbc5494a0d',\n",
       "    'title': 'Few-Shot Bayesian Imitation Learning with Logical Program Policies'},\n",
       "   {'paperId': 'c60fe2b929775c37e3fbed8f51fc329ac44f4902',\n",
       "    'title': 'Conditional Driving from Natural Language Instructions'},\n",
       "   {'paperId': '7723086a4a09c39a1cd91c62b2234cdf72dc1f63',\n",
       "    'title': 'Multi-Task Hierarchical Imitation Learning for Home Automation'},\n",
       "   {'paperId': '0fe8b0e61d9c8b23f0874681ee7a932add190e41',\n",
       "    'title': 'Teaching Robots to Perform Construction Tasks via Learning from Demonstration'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '3af4bf4c1bc05b029b05d333a4b963594727de4f',\n",
       "    'title': 'PLOTS: Procedure Learning from Observations using Subtask Structure'},\n",
       "   {'paperId': '542ee17f8498f2213d679aef6be94802c7bc9677',\n",
       "    'title': 'Few-Shot Bayesian Imitation Learning with Logic over Programs'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '048459fcf6befce76d277a31ead3f58e1b2de32d',\n",
       "    'title': 'Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': 'f0df16721c94e57d60d1b4ca53c7755caf22626b',\n",
       "    'title': 'Robot-Assisted Training in Laparoscopy Using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b43d8c8b25bc65cbf3097480e9000649c79b7a51',\n",
       "    'title': 'Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': 'd66851d037a6acf7c31476195e5e828c2fdbdd6d',\n",
       "    'title': 'Detaching the strings : Practical algorithms for Learning from Demonstration'},\n",
       "   {'paperId': '64f9895fc5fd914e84c9cfcb7e6067388299c103',\n",
       "    'title': 'Sample Complexity Bounds for the Linear Quadratic Regulator'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': '3af21f872a74548c2e0157141ea2c02ba6fd6238',\n",
       "    'title': 'Discovering hierarchies using Imitation Learning from hierarchy aware policies'},\n",
       "   {'paperId': '2d25a9c4538adacbc43cb65d4901cd6cd85799f8',\n",
       "    'title': 'Generalizing Robot Imitation Learning with Invariant Hidden Semi-Markov Models'},\n",
       "   {'paperId': '8123dd4348647c1bddc143004c7da21a77da21fd',\n",
       "    'title': 'Learning Traffic Behaviors by Extracting Vehicle Trajectories from Online Video Streams'},\n",
       "   {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "    'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       "   {'paperId': '672f9171a5c3af6aafd5760cb5b23e7bb7f1923d',\n",
       "    'title': 'TACO: Learning Task Decomposition via Temporal Alignment for Control'},\n",
       "   {'paperId': '73d34f743dc5076992258bae505e2b6016dd7e5b',\n",
       "    'title': 'Least-Squares Temporal Difference Learning for the Linear Quadratic Regulator'},\n",
       "   {'paperId': '608298f059b806a3c2e75b09ef619bd029321d5e',\n",
       "    'title': 'Using intermittent synchronization to compensate for rhythmic body motion during autonomous surgical cutting and debridement'},\n",
       "   {'paperId': 'ce97028f90ad3379c49a6a6b0f7beccca18caef7',\n",
       "    'title': 'Fast and Reliable Autonomous Surgical Debridement with Cable-Driven Robots Using a Two-Phase Calibration Procedure'},\n",
       "   {'paperId': 'c92cbdfa1e39d7d4264d9d05de759bbc2879e99b',\n",
       "    'title': 'Generative Models for Learning Robot Manipulation Skills from Humans'},\n",
       "   {'paperId': 'da12b03ad7cee8030b5e4c87b25edaae706d7106',\n",
       "    'title': 'A Berkeley View of Systems Challenges for AI'},\n",
       "   {'paperId': '7e2e1e9b3af2e7b60b17dfb0f0a34955fad19d9d',\n",
       "    'title': 'EasyChair Preprint No 746 Generalizing Robot Imitation Learning with Invariant Hidden Semi-Markov Models'}],\n",
       "  'citnuminlist': 9,\n",
       "  'refnuminlist': 3,\n",
       "  'isKeypaper': True},\n",
       " '049c6e5736313374c6e594c34b9be89a3a09dced': {'title': 'FeUdal Networks for Hierarchical Reinforcement Learning',\n",
       "  'year': 2017,\n",
       "  'references': [{'paperId': 'd7bd6e3addd8bc8e2e154048300eea15f030ed33',\n",
       "    'title': 'Reinforcement Learning with Unsupervised Auxiliary Tasks'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "    'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '6e90fd78e8a3b98af3954aae5209703aa966603e',\n",
       "    'title': 'Unifying Count-Based Exploration and Intrinsic Motivation'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': '7260c0692f8d265e11c4e9c4c8ef4c185bd587ad',\n",
       "    'title': 'Building machines that learn and think like people'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f88a6f6fd6611543220482e6b3a5f379b7bf5049',\n",
       "    'title': 'Increasing the Action Gap: New Operators for Reinforcement Learning'},\n",
       "   {'paperId': '7f5fc84819c0cf94b771fe15141f65b123f7b8ec',\n",
       "    'title': 'Multi-Scale Context Aggregation by Dilated Convolutions'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "    'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       "   {'paperId': 'fc487de8e8dc2e076187a31ae14a6829b4bacb24',\n",
       "    'title': 'End-to-end training of deep visuomotor policies'},\n",
       "   {'paperId': '5dc2a215bd7cd5bdd3a0baa8c967575632696fac',\n",
       "    'title': 'Universal Value Function Approximators'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': '5522764282c85aea422f1c4dc92ff7e0ca6987bc',\n",
       "    'title': 'A Clockwork RNN'},\n",
       "   {'paperId': 'f82e4ff4f003581330338aaae71f60316e58dd26',\n",
       "    'title': 'The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)'},\n",
       "   {'paperId': 'bc5752644753b117b897b9369e3826fe7831d93a',\n",
       "    'title': 'Planning with Closed Loop Macro Actions'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '985f2c1baba284e9b7b604b7169a2e2778540fe6',\n",
       "    'title': 'Temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '07b6e294c47ef0d72b3229ca6b891dd772adb47d',\n",
       "    'title': 'Theoretical Results on Reinforcement Learning with Temporally Abstract Options'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': '44d2abe2175df8153f465f6c39b68b76a0d40ab9',\n",
       "    'title': 'Long Short-Term Memory'},\n",
       "   {'paperId': '68da5e8c6678048469da5e9308fd340840e5f34e',\n",
       "    'title': 'HQ-Learning'},\n",
       "   {'paperId': '2024107fd768c22e2fd396179d17e5164c4bf7cd',\n",
       "    'title': 'Prioritized Goal Decomposition of Markov Decision Processes: Toward a Synthesis of Classical and Decision Theoretic Planning'},\n",
       "   {'paperId': '3b5db92ce2f86b2136fe7cf6a415fe1c0632a881',\n",
       "    'title': 'TD Models: Modeling the World at a Mixture of Time Scales'},\n",
       "   {'paperId': 'c2e8806f0bd1d504bcb395ef1f6fe509a023a048',\n",
       "    'title': 'Improving Generalization for Temporal Difference Learning: The Successor Representation'},\n",
       "   {'paperId': 'f4c6240b68e97d6f3b9bc67a701f10e49a1b1dab',\n",
       "    'title': 'Hierarchical Learning in Stochastic Domains: Preliminary Results'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': 'c69201d091dd92699fd90a17b9e3407319726791',\n",
       "    'title': 'Neural Sequence Chunkers'},\n",
       "   {'paperId': '7fb4d10f6d2ee3133135958aefd50bf22dcced9d',\n",
       "    'title': 'A Focused Backpropagation Algorithm for Temporal Pattern Recognition'},\n",
       "   {'paperId': 'a2b5b7691e67fd488ec008e8e850d4403abccf8d',\n",
       "    'title': 'Spatial Localization Does Not Require the Presence of Local Cues'}],\n",
       "  'citations': [{'paperId': 'a369626db68f41e18af1e3ce5c78dfe0eeb95955',\n",
       "    'title': 'Hierarchical Multi-Agent Reinforcement Learning with Intrinsic Reward Rectification'},\n",
       "   {'paperId': 'a05a0f80578e946dcc8d1773f2622748e1968833',\n",
       "    'title': 'Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge'},\n",
       "   {'paperId': 'd95b56b949610ab87866652b1944207c10b68631',\n",
       "    'title': 'A Hierarchical Approach to Population Training for Human-AI Collaboration'},\n",
       "   {'paperId': '3e45d02b763cb8197369ac0b3ef4a16f4726de85',\n",
       "    'title': 'An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes'},\n",
       "   {'paperId': 'f7f4974ceab53c6f51c8ab9a97dfd6fcd070eab0',\n",
       "    'title': 'Double Graph Attention Actor-Critic Framework for Urban Bus-Pooling System'},\n",
       "   {'paperId': '3a7f43676ed47a4324d7beb3efe946725584f066',\n",
       "    'title': 'HRL4EC: Hierarchical reinforcement learning for multi-mode epidemic control'},\n",
       "   {'paperId': '2f1fee5087d47e7a8c71763c22ac784c9565278c',\n",
       "    'title': 'Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward'},\n",
       "   {'paperId': 'd9854df0a7474414958dd7d45fbfca4cb0f4a4eb',\n",
       "    'title': 'Feudal Graph Reinforcement Learning'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8b494e93653215e0f21d658b773c7b4368c54d9d',\n",
       "    'title': 'Hierarchical Monte Carlo Tree Search for Latent Skill Planning'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'cd38e5a7e0a2a935bc354d44387df01e15b1e258',\n",
       "    'title': 'Exposure-Based Multi-Agent Inspection of a Tumbling Target Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '4c3dc49abcdb1122472d15f1f8fde4bbabc3859f',\n",
       "    'title': 'Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation'},\n",
       "   {'paperId': 'aeaddc24fe0c4179105e483e83cbe1387515914b',\n",
       "    'title': 'Predictable MDP Abstraction for Unsupervised Model-Based RL'},\n",
       "   {'paperId': '40bfcb5901a3c9cd9581433fd2eb1906abcfae6d',\n",
       "    'title': 'Hierarchical Learning with Unsupervised Skill Discovery for Highway Merging Applications'},\n",
       "   {'paperId': '27d2df5d338ac0c68b44d2bbe1694a71ce12eb95',\n",
       "    'title': 'Diversity Through Exclusion (DTE): Niche Identification for Reinforcement Learning through Value-Decomposition'},\n",
       "   {'paperId': '94d84d1403a23a8a8486a151f52a126beb16875c',\n",
       "    'title': 'Partitioning Distributed Compute Jobs with Reinforcement Learning and Graph Neural Networks'},\n",
       "   {'paperId': 'eccf9e5cdda2012172958f5c49204f11933fb250',\n",
       "    'title': 'Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs'},\n",
       "   {'paperId': '4875b7cdb20e23c4ada3ef58d48389b0c76052e3',\n",
       "    'title': 'Neural Episodic Control with State Abstraction'},\n",
       "   {'paperId': 'f296d829deabd8e5342da34bb777ffd02e7a3bc1',\n",
       "    'title': 'Deep Laplacian-based Options for Temporally-Extended Exploration'},\n",
       "   {'paperId': 'b53c510769760d2fbcb674ed14bacfe251ae0ebc',\n",
       "    'title': 'Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c109f1c22a531087ce2415711658e2949432a056',\n",
       "    'title': 'Story Shaping: Teaching Agents Human-like Behavior with Stories'},\n",
       "   {'paperId': '969f1016582c54bfde45653aa2a927d44fc5299a',\n",
       "    'title': 'A Survey of research in Deep Learning for Robotics for Undergraduate research interns'},\n",
       "   {'paperId': 'f403082b101821f5377ae82fe4ed7d02f6abd3ad',\n",
       "    'title': 'A Survey of Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'c1f040f51c0e247d1c520ee86295a38d94e613fd',\n",
       "    'title': 'Enviroment Representations with Bisimulation Metrics for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3ecbf75ca51133eb59a66ddb28d057a14dd538c1',\n",
       "    'title': 'Reusable Options through Gradient-based Meta Learning'},\n",
       "   {'paperId': '2be3222b6b9888746d5239c35ef867b10d7228a6',\n",
       "    'title': 'Planning Irregular Object Packing via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'caa03f47176505fc27e56708c2ce990c5e7abed2',\n",
       "    'title': 'Leveraging Demonstrations with Latent Space Priors'},\n",
       "   {'paperId': '9064845595d2fe7dd860c612050e4818a191ff62',\n",
       "    'title': 'Reinforcement Learning on Graphs: A Survey'},\n",
       "   {'paperId': 'beb6cc3a60d1c511b869e4bb2c744cec99cd3ef1',\n",
       "    'title': 'Adjacency Constraint for Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5685abf9e7bb2c16449ae1eb181051e503602a55',\n",
       "    'title': 'Reinforcement Learning based Recommender Systems: A Survey'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '387aed1f28d352fffa04e6d18c0ec46de6f8133e',\n",
       "    'title': 'Reinforcement Learning From Hierarchical Critics'},\n",
       "   {'paperId': '0118491c8ee739a11688e8b74c06b9cd8751823b',\n",
       "    'title': 'Learning Potential in Subgoal-Based Reward Shaping'},\n",
       "   {'paperId': '25af6b8af9c7031e485115f1b13dd5c605fa164b',\n",
       "    'title': 'Reinforcement Learning-Based Physical Cross-Layer Security and Privacy in 6G'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9274ebb09a17afd4d5e80a714d8afbc7e21bec50',\n",
       "    'title': 'Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents'},\n",
       "   {'paperId': 'b6a71f4ab3a6fa53bde2c606abd195c033e9eb31',\n",
       "    'title': 'CrowdHMT: Crowd Intelligence With the Deep Fusion of Human, Machine, and IoT'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '13b120fad7ebe58f4d2257a1ee7901c30219feaf',\n",
       "    'title': 'Policy Transfer via Enhanced Action Space'},\n",
       "   {'paperId': 'ec1e08dc5cbec362ea0a331e32bfa3d30e47c24f',\n",
       "    'title': 'Learning Landmark-Oriented Subgoals for Visual Navigation Using Trajectory Memory'},\n",
       "   {'paperId': 'db1bf436c730a44a53f6aed4c7737a511f00fec9',\n",
       "    'title': 'A Multi-Strategy Multi-Objective Hierarchical Approach for Energy Management in 5G Networks'},\n",
       "   {'paperId': 'a33c89dba5a5ed6c1b70aa56d0b0dbd6167632f7',\n",
       "    'title': 'Online Reactive Power Optimization of Distribution Network with Soft Open Point Based on Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '578f13bbe5f13e32a1e97cad0008364cfd3e894f',\n",
       "    'title': 'Melting Pot 2.0'},\n",
       "   {'paperId': '31d1c41424214067c649e75ade984cbe27918a68',\n",
       "    'title': 'An In-Depth Analysis of Cooperative Multi-Robot Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fbc6a615c97ecc596d18388f137957852ed5c9fe',\n",
       "    'title': 'Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling'},\n",
       "   {'paperId': 'ce0e769936453f827aee367e3463bb9915c6d78b',\n",
       "    'title': 'Emergency action termination for immediate reaction in hierarchical reinforcement learning'},\n",
       "   {'paperId': 'd331c92e4111fb92ace003d0389cd0df36b4058f',\n",
       "    'title': 'Beyond spiking networks: the computational advantages of dendritic amplification and input segregation'},\n",
       "   {'paperId': '56211d8b1c94032312b3ce4fd1f06bf81db8d7a7',\n",
       "    'title': 'HMDRL: Hierarchical Mixed Deep Reinforcement Learning to Balance Vehicle Supply and Demand'},\n",
       "   {'paperId': '24f5384da6cd78cb2f32a2b75d3365e0c0b431d8',\n",
       "    'title': 'Local Connection Reinforcement Learning Method for Efficient Control of Robotic Peg-in-Hole Assembly'},\n",
       "   {'paperId': '5572521b26ed3efcaafe050975b404fc5e7c89a7',\n",
       "    'title': 'Guided Skill Learning and Abstraction for Long-Horizon Manipulation'},\n",
       "   {'paperId': '25d574c88fb3ea5c78358f0a353d6b868c4bc2d2',\n",
       "    'title': 'A Multiagent Cooperative Decision-Making Method for Adaptive Intersection Complexity Based on Hierarchical RL'},\n",
       "   {'paperId': '203a1c0e5025489a52c030adbbc102a787685ee6',\n",
       "    'title': 'Unpacking Reward Shaping: Understanding the Benefits of Reward Engineering on Sample Complexity'},\n",
       "   {'paperId': 'eb904f6080e771abb808ecf4ae60d31668bdf0ed',\n",
       "    'title': 'Hierarchical Reinforcement Learning using Gaussian Random Trajectory Generation in Autonomous Furniture Assembly'},\n",
       "   {'paperId': '22b816129ca770df3a88e76f754218b242df43f9',\n",
       "    'title': 'Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization'},\n",
       "   {'paperId': '1cc13cff6f12d450457f51eb8d5d8e20bce47b56',\n",
       "    'title': 'Skill-Based Reinforcement Learning with Intrinsic Reward Matching'},\n",
       "   {'paperId': '0c0b7fb0066e8c1a5a2b4e4856135650eeef7702',\n",
       "    'title': 'Causality-driven Hierarchical Structure Discovery for Reinforcement Learning'},\n",
       "   {'paperId': 'a76df394efe042875b8650196a378387a7e576f2',\n",
       "    'title': 'Bringing Together Ergonomic Concepts and Cognitive Mechanisms for Human—AI Agents Cooperation'},\n",
       "   {'paperId': '0c4bac7f78d47b7ba2d21f86d0d9139cad9d4217',\n",
       "    'title': 'Contrastive introspection to identify critical steps in reinforcement learning'},\n",
       "   {'paperId': '39069f6dd2054d316ef1913d29cc662979bb0ea2',\n",
       "    'title': 'DHRL: A Graph-Based Approach for Long-Horizon and Sparse Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "    'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '458b84efab56e97809f3f5ed1cd8ffdcbc5a403a',\n",
       "    'title': 'HALight: Hierarchical Deep Reinforcement Learning for Cooperative Arterial Traffic Signal Control with Cycle Strategy'},\n",
       "   {'paperId': '849dcd45264e90b6ff39e6d0f02f54c66aa49dcb',\n",
       "    'title': 'Knowledge-Grounded Reinforcement Learning'},\n",
       "   {'paperId': '59efbf91c81b5837c24c84023e1a9339e0e41fce',\n",
       "    'title': 'QDN: An Efficient Value Decomposition Method for Cooperative Multi-agent Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f3ccaabbbf221582c83d4429b6da40126193dac2',\n",
       "    'title': 'Skills to Drive: Successor Features for Autonomous Highway Pilot'},\n",
       "   {'paperId': '3e425dd366e0696d5b835d8fff92add9873077da',\n",
       "    'title': 'On Efficient Reinforcement Learning for Full-length Game of StarCraft II'},\n",
       "   {'paperId': '4ebb88207c01936c3af2964c106e1a7b6baf35a6',\n",
       "    'title': 'Optimizing Industrial HVAC Systems with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6c6cfdaea4d7dd986f95f84acba6ad7d72fff655',\n",
       "    'title': 'A Memory-Related Multi-Task Method Based on Task-Agnostic Exploration'},\n",
       "   {'paperId': '57e62822d1f7ebba53aca99fd9b8d646df88e8bf',\n",
       "    'title': 'Task-Agnostic Learning to Accomplish New Tasks'},\n",
       "   {'paperId': '94ee89f048c4e9d6472ac1b1ee08b8ccd1d72dd4',\n",
       "    'title': 'Goal-Aware Generative Adversarial Imitation Learning from Imperfect Demonstration for Robotic Cloth Manipulation'},\n",
       "   {'paperId': '843b9212a2e8a59d7b3de3e28a8454c00c63b2be',\n",
       "    'title': 'Online robot guidance and navigation in non-stationary environment with hybrid Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'cbb4d26ee59f5135b06fe0be1fa981e6f7ee3f32',\n",
       "    'title': 'Hierarchical End-to-end Control Policy for Multi-degree-of-freedom Manipulators'},\n",
       "   {'paperId': 'b83a17f7bd638e6b8b696624718f4371e0981025',\n",
       "    'title': 'BITS: Bi-level Imitation for Traffic Simulation'},\n",
       "   {'paperId': 'ff272b20ad380a2a545f2e7030f5b8b9550c4385',\n",
       "    'title': 'MARTI-4: new model of human brain, considering neocortex and basal ganglia - learns to play Atari game by reinforcement learning on a single CPU'},\n",
       "   {'paperId': '54fa6e76311d54c6ddc92644f681c37c862c3f0f',\n",
       "    'title': 'Precise Mobility Intervention for Epidemic Control Using Unobservable Information via Deep Reinforcement Learning'},\n",
       "   {'paperId': '0c1f87d6093d945183cdbc55f2b9dbf77a6362f5',\n",
       "    'title': 'NENYA: Cascade Reinforcement Learning for Cost-Aware Failure Mitigation at Microsoft 365'},\n",
       "   {'paperId': '571cf0f286b3838af89f7bbc7695c6f9f1c5bc23',\n",
       "    'title': 'Hierarchical DDPG for Manipulator Motion Planning in Dynamic Environments'},\n",
       "   {'paperId': '99dd2ad7deb12286f54fd42bce92b2a18a4bee33',\n",
       "    'title': 'Interval-based melody generation via reinforcement learning'},\n",
       "   {'paperId': 'cfc5d3032c0c370892eb42687162afbb75455778',\n",
       "    'title': 'Explain My Surprise: Learning Efficient Long-Term Memory by Predicting Uncertain Outcomes'},\n",
       "   {'paperId': '45644c7f952d2a5a5b4e594998e2e6dff9088118',\n",
       "    'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '8f3166a321dfd629379bb7359898783e800211c2',\n",
       "    'title': 'Graph-Structured Policy Learning for Multi-Goal Manipulation Tasks'},\n",
       "   {'paperId': '75175ec0a794875a1b089f6de6ed57e04f352168',\n",
       "    'title': 'Towards Run-time Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '78ab35278d8cb36b88e5889cb20ba8576a93b2c1',\n",
       "    'title': 'HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble'},\n",
       "   {'paperId': '7acb94be09e00a1b12d3dbe4021398c3cdf38a58',\n",
       "    'title': 'Searching Latent Sub-Goals in Hierarchical Reinforcement Learning as Riemannian Manifold Optimization'},\n",
       "   {'paperId': '9337d750993d8715c872db8d406480d58464555a',\n",
       "    'title': 'How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition'},\n",
       "   {'paperId': '5118c3cd2be04e60b5ce35e8fe18bd7683f5237f',\n",
       "    'title': 'Scalable evolutionary hierarchical reinforcement learning'},\n",
       "   {'paperId': '8e21576387f46f1b9090bdbff1ceadf187feeada',\n",
       "    'title': 'CompoSuite: A Compositional Reinforcement Learning Benchmark'},\n",
       "   {'paperId': '4ff5e87805ff4f9c53b9c723ab99e4dc45e2e78f',\n",
       "    'title': 'Multi-Level Credit Assignment for Cooperative Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '0b3ca2a700085a877c560e20558566536f18d5a2',\n",
       "    'title': 'Modular Lifelong Reinforcement Learning via Neural Composition'},\n",
       "   {'paperId': '57fca7dd11ac5add0c2e05bd64feef7fb3bfca45',\n",
       "    'title': 'Negative Result for Learning from Demonstration: Challenges for End-Users Teaching Robots with Task And Motion Planning Abstractions'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': 'e032a053dc934cb938c79c8adab8fde38a9eb157',\n",
       "    'title': 'Variational Diversity Maximization for Hierarchical Skill Discovery'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': 'c60489b5727980e6f5fa3441f252af2bb6b40ef9',\n",
       "    'title': 'Discrete State-Action Abstraction via the Successor Representation'},\n",
       "   {'paperId': '4014bf79a220a09d1e380624adff53f5314a7e41',\n",
       "    'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks'},\n",
       "   {'paperId': 'c30e341d119f9cbab3007b1a7b25b425e555f6fa',\n",
       "    'title': 'Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '27a5d5c6d7804737b8a034ca2aee99f8d319754e',\n",
       "    'title': 'Toward Discovering Options that Achieve Faster Planning'},\n",
       "   {'paperId': '1fe3400b5da7da6e994ad93ecb179c9aa917b507',\n",
       "    'title': 'Recursive Compositional Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '4bbfad7d01d52810a18f82aa650cb8cfd7490c92',\n",
       "    'title': 'Graph neural network based agent in Google Research Football'},\n",
       "   {'paperId': '58750c7e5856c2f95f1f61b853c3d31768e7bc7e',\n",
       "    'title': 'Screening goals and selecting policies in hierarchical reinforcement learning'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': 'e8645dfcc1b4bd0918d7adcdc95750490f647284',\n",
       "    'title': 'Joint Charging and Relocation Recommendation for E-Taxi Drivers via Multi-Agent Mean Field Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '41c8e24758e53c78743b2d1baed92d463a7d6fd5',\n",
       "    'title': 'Feudal Multiagent Reinforcement Learning for Interdomain Collaborative Routing Optimization'},\n",
       "   {'paperId': '6343543986bd0b2800f0ef468604b659f0a24ec5',\n",
       "    'title': 'Possibility Before Utility: Learning And Using Hierarchical Affordances'},\n",
       "   {'paperId': 'fb8038a47940820527af13460b0e6f1dafd1b511',\n",
       "    'title': 'Temporal Abstractions-Augmented Temporally Contrastive Learning: An Alternative to the Laplacian in RL'},\n",
       "   {'paperId': '5762e9c654ae9230db5936f21780ae794a838533',\n",
       "    'title': 'Perceiving the World: Question-guided Reinforcement Learning for Text-based Games'},\n",
       "   {'paperId': 'be4833b64fc88c444d057ac12a87506d51f7eb26',\n",
       "    'title': 'Graph Neural Networks for Relational Inductive Bias in Vision-based Deep Reinforcement Learning of Robot Control'},\n",
       "   {'paperId': '7726f053a24ae08b1514d3df2bbcd7885a44f53f',\n",
       "    'title': 'SAGE: Generating Symbolic Goals for Myopic Models in Deep Reinforcement Learning'},\n",
       "   {'paperId': '620aaaa7efd2bf12c4dfafb7efcfb01cb6f5b324',\n",
       "    'title': 'On Credit Assignment in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1a6c31839b5bfaecea832920dcad3c77bd68e320',\n",
       "    'title': 'Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization'},\n",
       "   {'paperId': '63c6f3114520349312c276f60bb7b7922d4b744e',\n",
       "    'title': 'AI Planning Annotation for Sample Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'da1e404ae194dcbcce198f5335f49f090403bbd7',\n",
       "    'title': 'Exploration in Deep Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '98833f3240a77fa1cf7f67ab066297227148e2be',\n",
       "    'title': 'Deep latent-space sequential skill chaining from incomplete demonstrations'},\n",
       "   {'paperId': 'a63d6ee5364a795084d0bafc5917f09f963121bc',\n",
       "    'title': 'FIRL: Fast Imitation and Policy Reuse Learning'},\n",
       "   {'paperId': 'bd6f9099cc29cb0af2a8339e0bb68829965d74c2',\n",
       "    'title': 'Hierarchical Reinforcement Learning with AI Planning Models'},\n",
       "   {'paperId': '47f28e1057eac8f7ec99ac8250e4bfed0eafd9e9',\n",
       "    'title': 'Unified curiosity-Driven learning with smoothed intrinsic reward estimation'},\n",
       "   {'paperId': '205483fc72b6abc240b135fd092761924a9df61f',\n",
       "    'title': 'Cooperative Artificial Intelligence'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': 'a11d62fbf84829d01aebbe6bfd5a3f1eed11e3ef',\n",
       "    'title': 'A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization'},\n",
       "   {'paperId': 'c76bed3c3b26b3ac074647cc2855620521078838',\n",
       "    'title': 'Interpretable Reinforcement Learning with Multilevel Subgoal Discovery'},\n",
       "   {'paperId': '1f9a4883a768157f5a98921c6c0f4126e0280759',\n",
       "    'title': 'Hierarchical Imitation Learning via Subgoal Representation Learning for Dynamic Treatment Recommendation'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': '74e3c42e7005a0b371b0ff0cf601cbc8bb3c38bf',\n",
       "    'title': 'Soft Actor-Critic with Inhibitory Networks for Faster Retraining'},\n",
       "   {'paperId': 'b6ebd857e57a2871be22b6aeefc788aedd636e3d',\n",
       "    'title': 'Burst-Dependent Plasticity and Dendritic Amplification Support Target-Based Learning and Hierarchical Imitation Learning'},\n",
       "   {'paperId': '91a25facc0829c320f274e0196b8abc58b72f035',\n",
       "    'title': 'Learning Invariable Semantical Representation from Language for Extensible Policy Generalization'},\n",
       "   {'paperId': 'ab2542ae894b0834494c968e14f96bfd7908aa90',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Adversarially Guided Subgoals'},\n",
       "   {'paperId': '64290435a11eee0b4ea5c23cb9937d1e0af957e6',\n",
       "    'title': 'Adversarially Guided Subgoal Generation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c397a67aad5bd5d08364135ac01b028af930c604',\n",
       "    'title': 'State-Conditioned Adversarial Subgoal Generation'},\n",
       "   {'paperId': '7a7d5abd0da62c55ba3fb9d6b7f17827d7169a8f',\n",
       "    'title': 'The Paradox of Choice: Using Attention in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5b97c0c6e075d8f6e8c61bdc20e03ef048e5c762',\n",
       "    'title': 'Learning Two-Step Hybrid Policy for Graph-Based Interpretable Reinforcement Learning'},\n",
       "   {'paperId': 'fe92e5a20e265f890465c238368cdc0048f86959',\n",
       "    'title': 'Learning Multi-agent Options for Tabular Reinforcement Learning using Factor Graphs'},\n",
       "   {'paperId': '66cc87463c0f27256a18eb02915460ef0b510c0b',\n",
       "    'title': 'Prospective Learning: Back to the Future'},\n",
       "   {'paperId': 'af13b72a34a11862340cd7dfd50facc7849d88eb',\n",
       "    'title': 'Probabilistic design of optimal sequential decision-making algorithms in learning and control'},\n",
       "   {'paperId': 'bca42b535c4c4ea82f011fb1f6659c94e7687b04',\n",
       "    'title': 'Hierarchical deep reinforcement learning reveals a modular mechanism of cell movement'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': '670bf13436a5b0e9ca43d7356b90b5005f11a29e',\n",
       "    'title': 'Formal Language Generation for Fault Diagnosis With Spectral Logic via Adversarial Training'},\n",
       "   {'paperId': '5df950eadbdd3c9955d1212f91165a2914f89887',\n",
       "    'title': 'Learning to Guide and to Be Guided in the Architect-Builder Problem'},\n",
       "   {'paperId': 'fa5943a641a35d5a7afd545f5bdfd9e71cc56e1a',\n",
       "    'title': 'Combining Learning From Human Feedback and Knowledge Engineering to Solve Hierarchical Tasks in Minecraft'},\n",
       "   {'paperId': '87951cea6573eed827986371a35025e478d3c184',\n",
       "    'title': 'Stochastic Extragradient: General Analysis and Improved Rates'},\n",
       "   {'paperId': '1693ad5cdcff6ca01ca18711afc0fca79442b49a',\n",
       "    'title': 'Comparative Explanations of Recommendations'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': 'ee21c47254d1bcf33e13bf746218021816443745',\n",
       "    'title': 'Bottom-Up Skill Discovery From Unsegmented Demonstrations for Long-Horizon Robot Manipulation'},\n",
       "   {'paperId': '48e841a2cf2a13152f507afd9cb4e83090d72039',\n",
       "    'title': 'Context-Specific Representation Abstraction for Deep Option Learning'},\n",
       "   {'paperId': '77ff294e3c9d705f252cdb803de02a8bff6bd831',\n",
       "    'title': 'Sensor-Based Navigation Using Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "    'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       "   {'paperId': '62a72176c1d34b645b5f57f069a3c081131a0f79',\n",
       "    'title': 'Computational Benefits of Intermediate Rewards for Goal-Reaching Policy Learning'},\n",
       "   {'paperId': 'c0ee9dc3f231db21f8d12401df6709e6f318c22b',\n",
       "    'title': 'End-to-End Hierarchical Reinforcement Learning With Integrated Subgoal Discovery'},\n",
       "   {'paperId': '69f3cbf5c349c91c45bb3039736125a35fcda664',\n",
       "    'title': 'Verifiable and Compositional Reinforcement Learning Systems'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '167b5d14442024f3af48ae4e79c6fb41ac87b42b',\n",
       "    'title': 'Active Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '21f5d94cb6674604df1927dcdd8f0f5186a09f92',\n",
       "    'title': 'Hierarchical Multiagent Reinforcement Learning for Allocating Guaranteed Display Ads'},\n",
       "   {'paperId': '03f4f05139f2e7a543671db7c09bd2d56d1122ce',\n",
       "    'title': 'State-Temporal Compression in Reinforcement Learning With the Reward-Restricted Geodesic Metric'},\n",
       "   {'paperId': 'f5249109f30033b0f67097ee5922cdb173fa0a06',\n",
       "    'title': 'SocialInteractionGAN: Multi-person Interaction Sequence Generation'},\n",
       "   {'paperId': 'c7b01136da36d68e229f268e5489c81c94586481',\n",
       "    'title': 'Hierarchical Reinforcement Learning With Universal Policies for Multistep Robotic Manipulation'},\n",
       "   {'paperId': '262363cc8ea2ae44467531d78b5ac05cc577c542',\n",
       "    'title': 'Learning Skills to Navigate without a Master: A Sequential Multi-Policy Reinforcement Learning Algorithm'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': '73e240cb95309142b61db3e9afd9282bf0b5464c',\n",
       "    'title': 'Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey'},\n",
       "   {'paperId': '1a778a5e339d8c022e4b9915d4bba9caaa6d58ca',\n",
       "    'title': 'Guided Policy Search Based Control of a High Dimensional Advanced Manufacturing Process'},\n",
       "   {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "    'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       "   {'paperId': '29c3ed6c15fdfab4109f8c8a4347f458c5dac69d',\n",
       "    'title': 'The survey: Text generation models in deep learning'},\n",
       "   {'paperId': 'f7c15e9ac6653330b7dd18a89301a3b333927db3',\n",
       "    'title': 'A review of cooperative multi-agent deep reinforcement learning'},\n",
       "   {'paperId': '474c0bc346a7d13dc513a5038d860f843baf473d',\n",
       "    'title': 'S KILL H ACK : A B ENCHMARK FOR S KILL T RANSFER IN O PEN -E NDED R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'f56edacf53e7803c5fee527c10ae076ca41eb8ed',\n",
       "    'title': 'H IERARCHICAL K ICKSTARTING FOR S KILL T RANSFER IN R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'f385f866bd45a9e2ab53f2d356b6f9945f52fb75',\n",
       "    'title': 'The Paradox of Choice: On the Role of Attention in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0752f3c3d47886af5251e2ceb0f9199c57c443b0',\n",
       "    'title': 'Learning Long-Term Crop Management Strategies with CyclesGym'},\n",
       "   {'paperId': '6174a1c497b5d3a683f39bba6904a37c99b488df',\n",
       "    'title': 'Multi-task Policy Learning with Minimal Human Supervision'},\n",
       "   {'paperId': 'c9e602b5f1df92388e0b6755e7c851f2ec8d7495',\n",
       "    'title': 'Cooperative Multi-Robot Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '310a77ea80bff7b70a4a31743315c6442aef2119',\n",
       "    'title': 'Unmanned Aerial Vehicle Swarm Cooperative Decision-Making for SEAD Mission: A Hierarchical Multiagent Reinforcement Learning Approach'},\n",
       "   {'paperId': '4aedc1f9adab93d95ebd7ca1d476e3302e312bc8',\n",
       "    'title': 'Digitalization: A Threat or an Opportunity in Replacement of Labor and Job Task A generic perspective at developing economy, a case in Ghana’s digitalization policy .'},\n",
       "   {'paperId': '800a1917c57c5701cc974e5498ad27a61ae0f292',\n",
       "    'title': 'Exploring Long-Horizon Reasoning with Deep RL in Combinatorially Hard Tasks'},\n",
       "   {'paperId': '0596a04cd9511060de4bf19f1c362cbb64662bca',\n",
       "    'title': 'Reinforcement Learning in the Presence of Sensing Costs A THESIS SUBMITTED IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF'},\n",
       "   {'paperId': '16b4ff5a2281d8aedc5b18d90ca7d754c878a9fe',\n",
       "    'title': 'Hierarchical Actor-Critic Exploration with Synchronized, Adversarial, & Knowledge-Based Actions'},\n",
       "   {'paperId': 'b45ea295f8bab57f48680df4610909131695fe64',\n",
       "    'title': 'Reinforcement learning on graph: A survey'},\n",
       "   {'paperId': '7319249a853288dce45ca331a87dce052a2abfba',\n",
       "    'title': 'Disentangling Controlled Effects for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4ed32676549a7123482f80fb24e98f3db16caa89',\n",
       "    'title': 'Adversarial Machine Learning in Text Processing: A Literature Survey'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '07f9a3f969e53c8d161c8161243bf74295ac7650',\n",
       "    'title': 'Reinforcement Learning for Logistics and Supply Chain Management: Methodologies, State of the Art, and Future Opportunities'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '7372b0a5f456eba1788e9bd5776c1214f9933bc3',\n",
       "    'title': 'Flexible Option Learning'},\n",
       "   {'paperId': 'fd2d7659505d4e41ec34639ab42041a987971d3b',\n",
       "    'title': 'Training Generative Adversarial Networks with Adaptive Composite Gradient'},\n",
       "   {'paperId': '0b43ccd62334cd38e3752e92ce0274cf48e3a35e',\n",
       "    'title': 'Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving'},\n",
       "   {'paperId': '5d4b708d50e40422d0cc1459b8ce7de3fdb2515a',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning for Robotic Assembly Control Applications'},\n",
       "   {'paperId': 'ec206abd93c1f2fb53cf8feacaf80fac4c6df3eb',\n",
       "    'title': 'Extending the Capabilities of Reinforcement Learning Through Curriculum: A Review of Methods and Applications'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '27bc680bf6a115cc3f28c4da462b6d25cf04cb09',\n",
       "    'title': 'Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '72052073bbcf9a7ae1782790a8b07b4309b36d96',\n",
       "    'title': 'Toward an Adaptive Threshold on Cooperative Bandwidth Management Based on Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': 'e643eb4ec5afd45fca31cb6d4259034042268064',\n",
       "    'title': 'Provable Hierarchy-Based Meta-Reinforcement Learning'},\n",
       "   {'paperId': '8a17c4a4e6ff86f444c7c6811e307d52f443d149',\n",
       "    'title': 'HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism'},\n",
       "   {'paperId': 'cb94ad7fe273cb3df2335aadeecf60061ff8e0f7',\n",
       "    'title': 'Feudal Reinforcement Learning by Reading Manuals'},\n",
       "   {'paperId': 'c2ec144b633e2dcbf889da95c711b483803e8350',\n",
       "    'title': 'Hierarchical learning from human preferences and curiosity'},\n",
       "   {'paperId': 'f8717dd893b150977971cadaf893615770e2a251',\n",
       "    'title': 'Generalization in Text-based Games via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "    'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       "   {'paperId': 'a6961a96ada0f00aae3f48f446f708c035c3a8c2',\n",
       "    'title': 'WDIBS: Wasserstein deterministic information bottleneck for state abstraction to balance state-compression and performance'},\n",
       "   {'paperId': '466d2a533aa3a451de7259b58bd3ea071bfbd7fa',\n",
       "    'title': 'Deep reinforcement learning in transportation research: A review'},\n",
       "   {'paperId': '873f50a378866e0e1172aa30d43a20caf663f98c',\n",
       "    'title': 'Efficient hierarchical policy network with fuzzy rules'},\n",
       "   {'paperId': '3bfc3648c66cd923cb3a740fed945eeed9c1e313',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Sensor-Based Navigation'},\n",
       "   {'paperId': '595430cd0f39d6055dd5c36bb2f8e099efbc43c1',\n",
       "    'title': 'Subgoal Search For Complex Reasoning Tasks'},\n",
       "   {'paperId': '6cd97caff11acc3a02c6b09b5c59baa949747700',\n",
       "    'title': 'Dialogue Based Disease Screening Through Domain Customized Reinforcement Learning'},\n",
       "   {'paperId': 'c0c9f77cb097f2ce53feb91802bcfbae57fcc42f',\n",
       "    'title': 'BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments'},\n",
       "   {'paperId': 'a30904d356f61dea1a1966571dbec8d2375e862e',\n",
       "    'title': 'The Multi-Dimensional Actions Control Approach for Obstacle Avoidance Based on Reinforcement Learning'},\n",
       "   {'paperId': 'be793883b04967307b4c59764c0199d65bec5972',\n",
       "    'title': 'Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning'},\n",
       "   {'paperId': '291a42e844b9bd38c0341c6beec9b34aba3ddde8',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model'},\n",
       "   {'paperId': '8ab9cd19f5e5311427aef5ddadae4bb41d23917f',\n",
       "    'title': 'Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot'},\n",
       "   {'paperId': 'fa1c24730eeb24089d498e670a6b67259385c09f',\n",
       "    'title': 'Shortest-Path Constrained Reinforcement Learning for Sparse Reward Tasks'},\n",
       "   {'paperId': 'c305673d100e2eeefab5839542156658450fe053',\n",
       "    'title': 'Hierarchical Neural Dynamic Policies'},\n",
       "   {'paperId': 'cfcbeec1ae2f7d13ec1576aa30cb98f5326ffa71',\n",
       "    'title': 'Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '75f425319694047f763f02f2a07912cd5621cfa4',\n",
       "    'title': 'Evaluating the progress of Deep Reinforcement Learning in the real world: aligning domain-agnostic and domain-specific research'},\n",
       "   {'paperId': '8804d2afbed99f80b900b667391d1e9c7136e6b4',\n",
       "    'title': 'Low-Dimensional State and Action Representation Learning with MDP Homomorphism Metrics'},\n",
       "   {'paperId': 'bea5fb2d64a88cc3b0d40f77c615f7b73ee7328d',\n",
       "    'title': 'How the Mind Creates Structure: Hierarchical Learning of Action Sequences.'},\n",
       "   {'paperId': 'fb95d6e6e5f78f6e5c339e2058ce9ae9e803182b',\n",
       "    'title': 'Goal-Conditioned Reinforcement Learning with Imagined Subgoals'},\n",
       "   {'paperId': '107e4ea37d2e5364893107a8ce072972c4a10dfb',\n",
       "    'title': 'Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "    'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'e9094e2a33af929d67793d480fffa6836b1bfe07',\n",
       "    'title': 'Efficient Hierarchical Exploration with Stable Subgoal Representation Learning'},\n",
       "   {'paperId': '0ea89ca0929b9323ba3d65bc6bce654d37cdcea4',\n",
       "    'title': 'Hierarchical Learning from Demonstrations for Long-Horizon Tasks'},\n",
       "   {'paperId': 'a8c753bbd960bbde37234217f387ca246fc2146c',\n",
       "    'title': 'Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments'},\n",
       "   {'paperId': 'd93b090bd7eb49fac0c87ca1761077a86de059d4',\n",
       "    'title': 'Hierarchies of Planning and Reinforcement Learning for Robot Navigation'},\n",
       "   {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "    'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       "   {'paperId': '31ccf47456a4d0a3b7d3ba8afbb56aceaf5ebacd',\n",
       "    'title': 'Room Clearance with Feudal Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '7284ae678ab87196d0e0ed393e0796e6772dc7d4',\n",
       "    'title': 'Hierarchically and Cooperatively Learning Traffic Signal Control'},\n",
       "   {'paperId': '6360ca733fbbb9ef6d46c0fd94a8124abd89ed91',\n",
       "    'title': 'Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition'},\n",
       "   {'paperId': '6ebeef3f32f0e1e67bded9362dacc01d12bee5c3',\n",
       "    'title': 'DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning'},\n",
       "   {'paperId': '32a921c37bc5d4d9b1b906f812e7a41fe5689085',\n",
       "    'title': 'Scalable, Decentralized Multi-Agent Reinforcement Learning Methods Inspired by Stigmergy and Ant Colonies'},\n",
       "   {'paperId': 'd51fe299b77deb1907216fc1fec3924848f83c31',\n",
       "    'title': 'Regioned Episodic Reinforcement Learning'},\n",
       "   {'paperId': 'e76a973d2bd97b6f4bc59c8759348efa5b126304',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Air-to-Air Combat'},\n",
       "   {'paperId': '5ee0e8402b38f132750580b507c07d9481915ee1',\n",
       "    'title': 'Reinforcement Learning in Sparse-Reward Environments With Hindsight Policy Gradients'},\n",
       "   {'paperId': '94545df9b0c571c95b538cba275541b07c200396',\n",
       "    'title': 'Exploring Clustering-Based Reinforcement Learning for Personalized Book Recommendation in Digital Library'},\n",
       "   {'paperId': '31525dafad7221b819a28c0675ba51e52e4b7ccf',\n",
       "    'title': 'Reward Redistribution for Reinforcement Learning of Dynamic Nonprehensile Manipulation'},\n",
       "   {'paperId': '6f812978ce061b6c650ffd70ad3cf7dc3eef6003',\n",
       "    'title': 'DisCo RL: Distribution-Conditioned Reinforcement Learning for General-Purpose Policies'},\n",
       "   {'paperId': '8351f660e67b6e792a4791b7a9ce27e6b1720236',\n",
       "    'title': 'Hierarchical Cross-Modal Agent for Robotics Vision-and-Language Navigation'},\n",
       "   {'paperId': 'e7c33544f157974083e9b106605f417722777352',\n",
       "    'title': 'Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning'},\n",
       "   {'paperId': '98cb29c03a0882fde368db28ade214c57c3239d6',\n",
       "    'title': 'Multi-agent deep reinforcement learning: a survey'},\n",
       "   {'paperId': 'dac43044fef83b0fca1724675ff4baa78a4baed2',\n",
       "    'title': 'TASAC: Temporally Abstract Soft Actor-Critic for Continuous Control'},\n",
       "   {'paperId': '1508879dae81f73f56ba0cb0e25150d9c5f8f731',\n",
       "    'title': 'TAAC: Temporally Abstract Actor-Critic for Continuous Control'},\n",
       "   {'paperId': '8ed8f0d3fcf9c774f08ebdfe6aaf31e5922846d4',\n",
       "    'title': 'Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning'},\n",
       "   {'paperId': 'f3dc8caf1223f3e42ae7c80727e63cef28c8e31d',\n",
       "    'title': 'Reward Shaping with Dynamic Trajectory Aggregation'},\n",
       "   {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "    'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': 'ff798aa7847c7d8e6320578a31589373c8485ecf',\n",
       "    'title': 'Development of Explainable, Knowledge-Guided AI Models to Enhance the E3SM Land Model Development and Uncertainty Quantification'},\n",
       "   {'paperId': '51bf405f82ac16f8f4d4ceaef27924f25a95c84c',\n",
       "    'title': 'Exploration with Intrinsic Motivation using Object-Action-Outcome Latent Space'},\n",
       "   {'paperId': '69a26d873c1e279bb950f1d984bf9375ffc8a79c',\n",
       "    'title': 'Computational Modeling of Emotion-Motivated Decisions for Continuous Control of Mobile Robots'},\n",
       "   {'paperId': '6fef3e81535fc15f7249d5e45055016e29fe3d6f',\n",
       "    'title': 'Action Redundancy in Reinforcement Learning'},\n",
       "   {'paperId': 'f2211f596497501d621eab00e2deec12d32d8aa9',\n",
       "    'title': 'Delayed Rewards Calibration via Reward Empirical Sufficiency'},\n",
       "   {'paperId': '40b6d7d961772f7fe2e23683da960ce7c7e5d059',\n",
       "    'title': 'Learning Memory-Dependent Continuous Control from Demonstrations'},\n",
       "   {'paperId': 'ccc61cbfdda84865977a5d3fd2632df1ac08b546',\n",
       "    'title': 'TradeR: Practical Deep Hierarchical Reinforcement Learning for Trade Execution'},\n",
       "   {'paperId': 'c15fd6d1f3599808f1ba4ff4803447e2898c3937',\n",
       "    'title': 'Twin Delayed Hierarchical Actor-Critic'},\n",
       "   {'paperId': '02b2b995a10524dd74678b75cecf44b4aeacaedd',\n",
       "    'title': 'Stay Alive with Many Options: A Reinforcement Learning Approach for Autonomous Navigation'},\n",
       "   {'paperId': 'b3c29068f2fe844c8c6116a220ff24733d8620dd',\n",
       "    'title': 'Scalable Voltage Control using Structure-Driven Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '9de637abc24e15a438c7f0bbc1b26c829cb5609a',\n",
       "    'title': 'HAMMER: Multi-Level Coordination of Reinforcement Learning Agents via Learned Messaging'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '59e1f4e89f1a0fde5ecc9edee63a03159089c372',\n",
       "    'title': 'Deciding What to Learn: A Rate-Distortion Approach'},\n",
       "   {'paperId': 'cc1a5a76c96a3b440b13f946e5c9f84cc7290ce0',\n",
       "    'title': 'Adversarial Machine Learning in Text Analysis and Generation'},\n",
       "   {'paperId': '12ce3a14da5a7e22bcb3b14452dd9d3bb8f5cf36',\n",
       "    'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation'},\n",
       "   {'paperId': '5cf549e26b4dce19d5bc783de83047479ce6218a',\n",
       "    'title': 'World Model as a Graph: Learning Latent Landmarks for Planning'},\n",
       "   {'paperId': '31a95c2f8e128cef583f4dad18d65feebd47b50d',\n",
       "    'title': 'Moving Forward in Formation: A Decentralized Hierarchical Learning Approach to Multi-Agent Moving Together'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '72c034e53213cc2f4913d73dd838b64d7b641585',\n",
       "    'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning'},\n",
       "   {'paperId': '3047992223549cc394e201a326eaa3337dc4b4b2',\n",
       "    'title': 'Provable Hierarchical Imitation Learning via EM'},\n",
       "   {'paperId': '5764095b0186a3fc3832c1052aa14996a5927edc',\n",
       "    'title': 'RODE: Learning Roles to Decompose Multi-Agent Tasks'},\n",
       "   {'paperId': 'e6283a93c69cd1583bba0d54ff0a86b643f09900',\n",
       "    'title': 'Correcting Experience Replay for Multi-Agent Communication'},\n",
       "   {'paperId': 'a2b00e570440619bbe6a483df6b0da46b1aae665',\n",
       "    'title': 'Fast and slow curiosity for high-level exploration in reinforcement learning'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '71065d3de6d962596915d3b91d981d7871a3fc68',\n",
       "    'title': 'On the Impossibility of Global Convergence in Multi-Loss Optimization'},\n",
       "   {'paperId': 'c88c99fc89a32883384b5a629a8905504e42ac72',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning'},\n",
       "   {'paperId': '4ee2b13dbd7706ac070c35c885bd2a36efc2728e',\n",
       "    'title': 'Large-Scale Traffic Signal Control Using a Novel Multiagent Reinforcement Learning'},\n",
       "   {'paperId': 'f1b13e39bfb8fe631e3b148bbec7e960901307cd',\n",
       "    'title': 'Hindsight Trust Region Policy Optimization'},\n",
       "   {'paperId': 'f33ae3a6f47ff3897a7ff12c6a0bacec2223d6d6',\n",
       "    'title': 'A Review of Robot Learning for Manipulation: Challenges, Representations, and Algorithms'},\n",
       "   {'paperId': '2e73d8598a3974cec59fd57ad9ab147c93c94649',\n",
       "    'title': 'Danger-Aware Adaptive Composition of DRL Agents for Self-Navigation'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '20b4870c708ece6089fef7f664259f5e83c64913',\n",
       "    'title': 'Teachability And Interpretability In Reinforcement Learning'},\n",
       "   {'paperId': '3ee545c4019b65bb5734367094039b1229913a9b',\n",
       "    'title': 'Digital-Based Learning Application Model in Schools'},\n",
       "   {'paperId': 'ec1022b679a402f5bb8c8377811cab5ff7cae68d',\n",
       "    'title': 'L EARNING S UBGOAL R EPRESENTATIONS WITH S LOW D YNAMICS'},\n",
       "   {'paperId': '6fe9857b729b59c653b6ff6a249a8b47dafe6ef6',\n",
       "    'title': 'B RAIN INSIGHTS IMPROVE RNN S ’ ACCURACY AND ROBUSTNESS FOR HIERARCHICAL CONTROL OF CON TINUALLY LEARNED AUTONOMOUS MOTOR MOTIFS'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '9b8dc80a0d214ffee871c2960b31a3596460968c',\n",
       "    'title': 'Generating High-Quality Explanations for Navigation in Partially-Revealed Environments'},\n",
       "   {'paperId': 'e671586d09923535d2d6f3c65e1f1ddda38af95c',\n",
       "    'title': 'Subgoal Search For Complex Reasoning Tasks'},\n",
       "   {'paperId': 'e46ad66a25a0155e9d61b4cbd1b1251a375ddde8',\n",
       "    'title': 'A Survey On Model-Free Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e9810e7ae50715760eaa66f3fcfc1c8d0fc2e26a',\n",
       "    'title': 'Attaining Interpretability in Reinforcement Learning via Hierarchical Primitive Composition'},\n",
       "   {'paperId': '3752ecb9fac3d105e1f177892a891bb9a39d66d4',\n",
       "    'title': 'Reinforcement Learning: An Industrial Perspective'},\n",
       "   {'paperId': '1a037dc6c6ff13c8d0196986b8ca2227361fda34',\n",
       "    'title': 'Empirical Suﬃciency Featuring Reward Delay Calibration in Reinforcement Learning'},\n",
       "   {'paperId': 'ede66a0627ee774052c4234311be1966cf927634',\n",
       "    'title': 'Bottom-up Discovery of Reusable Sensorimotor Skills from Unstructured Demonstrations'},\n",
       "   {'paperId': '599fcb9ca476098f33334852cc360bae9f1f7ee2',\n",
       "    'title': '[Appendix] Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '20fe92e8681fb7bee3a901538444633a24064379',\n",
       "    'title': 'Off-Policy Differentiable Logic Reinforcement Learning'},\n",
       "   {'paperId': '79a90e7378d4b4a9e3ee9c70973e1bf61816a533',\n",
       "    'title': 'Goal Modelling for Deep Reinforcement Learning Agents'},\n",
       "   {'paperId': '2634d753899c065f412c9b299b432cbd5b601b32',\n",
       "    'title': 'Computational Benefits of Intermediate Rewards for Hierarchical Planning'},\n",
       "   {'paperId': '630b70d8fb6dd3c01983f5a1cefc8b5fbb6af1d3',\n",
       "    'title': 'Anchor: The achieved goal to replace the subgoal for hierarchical reinforcement learning'},\n",
       "   {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       "   {'paperId': '53c819b7def6d814abbd7e14ea2051ee8edf1718',\n",
       "    'title': 'On the Feasibility of Using GANs for Claim Verification- Experiments and Analysis'},\n",
       "   {'paperId': 'f8775c411d9c8bed4a28e9749f43e7f76cbe81f7',\n",
       "    'title': 'Low-Cost Multi-Agent Navigation via Reinforcement Learning With Multi-Fidelity Simulator'},\n",
       "   {'paperId': '3ba5343e8945f26c161bb512bf3d7ea5d4434f32',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Universal Policies for Multi-Step Robotic Manipulation'},\n",
       "   {'paperId': '902bfb886c03db9b5fce2fae06cbce71bb9d5214',\n",
       "    'title': 'LEOC: A Principled Method in Integrating Reinforcement Learning and Classical Control Theory'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '7d271c0d65e3a1dcf2900fb1b2a41b6e6d07c3a2',\n",
       "    'title': 'Stability and Synchronization of Switched Multi-Rate Recurrent Neural Networks'},\n",
       "   {'paperId': 'a7622b6ac5e380f3a7ee6906b1b80c237cab8644',\n",
       "    'title': 'DELAYED REWARDS CALIBRATION VIA REWARD EM-'},\n",
       "   {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "    'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       "   {'paperId': 'f4fd4a5a0901675e19bb4054d8861d2697194fd5',\n",
       "    'title': 'Deep Reinforcement Learning: A State-of-the-Art Walkthrough'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': '37d233904845f80babde265f3e5d896a204ec659',\n",
       "    'title': 'Large Scale Deep Reinforcement Learning in War-games'},\n",
       "   {'paperId': 'b9e4bccd2e69132af961fceeb51d418ee106e3ff',\n",
       "    'title': 'Active Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '3ce5c04591ed4f0f94556f1fd0a1d6c16cdae4e4',\n",
       "    'title': 'Transfer of Hierarchical Reinforcement Learning Structures for Robotic Manipulation Tasks'},\n",
       "   {'paperId': 'b667641dc6acd7c0233503615942ea00ea9875f5',\n",
       "    'title': 'Continual Learning of Control Primitives: Skill Discovery via Reset-Games'},\n",
       "   {'paperId': '3da314388876286aacb6d9b355439ee68700576d',\n",
       "    'title': 'Automatische Programmierung von Produktionsmaschinen'},\n",
       "   {'paperId': '1fcba5d6c94d464af11243b3b4fc8f0f0f0736eb',\n",
       "    'title': 'HILONet: Hierarchical Imitation Learning from Non-Aligned Observations'},\n",
       "   {'paperId': '6fb1c349f407bb40add9c3f743a8fe040859cd6e',\n",
       "    'title': 'Diversity-Enriched Option-Critic'},\n",
       "   {'paperId': '82336295513221f4ceeca5246d664f28cfda4321',\n",
       "    'title': 'Metis: Learning to Schedule Long-Running Applications in Shared Container Clusters at Scale'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '5e10cce7bc1a480bf9ce0f2fb19ffc8d8a800e77',\n",
       "    'title': 'Learning Multi-Agent Coordination for Enhancing Target Coverage in Directional Sensor Networks'},\n",
       "   {'paperId': '27211f1e5ba90cc936a39c9d23fafff133cd6dd7',\n",
       "    'title': 'Deep Reinforcement Learning and Transportation Research: A Comprehensive Review'},\n",
       "   {'paperId': '3c279a4760315ca9ab29255ff7ff0a9c1717948b',\n",
       "    'title': 'Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for Physically Embedded 3D Sokoban'},\n",
       "   {'paperId': '560e9a8a1b027adbefb838ca337aa73af7998ca9',\n",
       "    'title': 'Disentangling causal effects for hierarchical reinforcement learning'},\n",
       "   {'paperId': '07aad25b60339eb870257275e4a9386b7cbc8494',\n",
       "    'title': 'Hierarchical Reinforcement Learning Based on Continuous Subgoal Space'},\n",
       "   {'paperId': '331acc8cc481f90a43fc96553ca0e479cf75ff15',\n",
       "    'title': 'AI and Wargaming'},\n",
       "   {'paperId': '0e91ffedfd9f17ae3370cbde776c22ed82fc81d9',\n",
       "    'title': 'Fast and slow curiosity for high-level exploration in reinforcement learning'},\n",
       "   {'paperId': '33395582d3c4e2336b5860e60ca5c02f751b24d0',\n",
       "    'title': 'Intrinsic Motivation in Object-Action-Outcome Blending Latent Space'},\n",
       "   {'paperId': '102d3e153bb402a8533af2ee03cb0ba536d09f2f',\n",
       "    'title': 'Audio-Visual Waypoints for Navigation'},\n",
       "   {'paperId': '50e0d675bc64e4648b5ceda1268f00cc9c3269c6',\n",
       "    'title': 'The formation and use of hierarchical cognitive maps in the brain: A neural network model'},\n",
       "   {'paperId': '114b52291b549466a4b1027f4248a122c1c3920c',\n",
       "    'title': 'Learning Compositional Neural Programs for Continuous Control'},\n",
       "   {'paperId': '41166ed506a8d6d2f46a42cce4d25c74f50a8b47',\n",
       "    'title': 'MaHRL: Multi-goals Abstraction Based Deep Hierarchical Reinforcement Learning for Recommendations'},\n",
       "   {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "    'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       "   {'paperId': '66d4ad13c9010f8a2ccb4dbe6211bb8f53dbd756',\n",
       "    'title': 'OPtions as REsponses: Grounding behavioural hierarchies in multi-agent reinforcement learning'},\n",
       "   {'paperId': 'b1b0f9c33e4b3253a27e651d3ed47d29867d4df4',\n",
       "    'title': 'Learning Abstract Models for Strategic Exploration and Fast Reward Transfer'},\n",
       "   {'paperId': 'b2df444b1e52d9e3d73fe1b54027513d6c28128d',\n",
       "    'title': 'A scalable approach to control diverse behaviors for physically simulated characters'},\n",
       "   {'paperId': '952b100591b056912a43065cbe0010a5bd6907fb',\n",
       "    'title': 'Developing cooperative policies for multi-stage tasks'},\n",
       "   {'paperId': '57a121e51b4cd1cd2a81506ce32196217b439a46',\n",
       "    'title': 'Reinforcement Learning based Control of Imitative Policies for Near-Accident Driving'},\n",
       "   {'paperId': 'aecb95605083c460feb289ec40901e328805fae5',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientific Implications'},\n",
       "   {'paperId': 'b78c3c878955ea09d288dd4aad8ebdd5490f31bf',\n",
       "    'title': 'Perception-Prediction-Reaction Agents for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'cf8f41618fae491ca8de10404b1e65a14379f2ab',\n",
       "    'title': 'Thalamocortical motor circuit insights for more robust hierarchical control of complex sequences'},\n",
       "   {'paperId': '8bb64f92fc760570d6e854418d28bf69adca0aa3',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Deep Goal Reasoning: An Expressiveness Analysis'},\n",
       "   {'paperId': '361ccae6cb40343c8824c9d64104ff8261a7c089',\n",
       "    'title': 'Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6ec8797952213227eea2e63620f4d7c060d598d5',\n",
       "    'title': 'Hierarchical reinforcement learning for efficient exploration and transfer'},\n",
       "   {'paperId': '51408188c013b80faf38a69dc15f729ce2b9ac9e',\n",
       "    'title': 'Reinforcement Learning for Multi-Product Multi-Node Inventory Management in Supply Chains'},\n",
       "   {'paperId': '3228f6fc7902b23c26378e59f8c8820412de7f42',\n",
       "    'title': 'The NetHack Learning Environment'},\n",
       "   {'paperId': '1f28d7a9137b6ca53be6093a872b67ea2578b9c2',\n",
       "    'title': 'Feudal Steering: Hierarchical Learning for Steering Angle Prediction'},\n",
       "   {'paperId': '2e1ac521d53e09e90fd9b4bf949f667d756853de',\n",
       "    'title': 'Acme: A Research Framework for Distributed Reinforcement Learning'},\n",
       "   {'paperId': '7a7c7607b15c2a85f05d52a68cfe9f39258db6bf',\n",
       "    'title': \"Crawling in Rogue's Dungeons With Deep Reinforcement Techniques\"},\n",
       "   {'paperId': '3473d81d07b7301adcf75272d780746a70698716',\n",
       "    'title': 'End-to-end deep reinforcement learning in computer systems'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': '94d02cb4a0901f4f336ffa939f6b9991f287948c',\n",
       "    'title': 'Curious Hierarchical Actor-Critic Reinforcement Learning'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "    'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'caa31faab39d34ecbb8100911640dfaec76f9ee9',\n",
       "    'title': 'Multiplicative Interactions and Where to Find Them'},\n",
       "   {'paperId': '6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d',\n",
       "    'title': 'Program Guided Agent'},\n",
       "   {'paperId': 'e1a39a6614503546bbb72a8c75aaf0ae93a3ac01',\n",
       "    'title': 'Option Discovery using Deep Skill Chaining'},\n",
       "   {'paperId': '4ad257a38360f03b905102af120a2726669334e7',\n",
       "    'title': 'Divide-and-Conquer Monte Carlo Tree Search For Goal-Directed Planning'},\n",
       "   {'paperId': '92a7844833c8d15f2780132bff6aeec5feb3e7d4',\n",
       "    'title': 'Generating Ampicillin-Level Antimicrobial Peptides with Activity-Aware Generative Adversarial Networks'},\n",
       "   {'paperId': 'dc4421f05836ace15c4536f0d137794b2c442918',\n",
       "    'title': 'Reinforcement Learning via Reasoning from Demonstration'},\n",
       "   {'paperId': 'e2823e21cbdcfb6f38f012943120ea4ee0a8aedc',\n",
       "    'title': 'Copy or Rewrite: Hybrid Summarization with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd81dc7748954ced45470d46aadefc5ad3098fd31',\n",
       "    'title': 'Feudal Multi-Agent Deep Reinforcement Learning for Traffic Signal Control'},\n",
       "   {'paperId': 'f35d2c0c5610652e8495254652b5d3f2ba41cce0',\n",
       "    'title': 'Multi-Agent Reinforcement Learning for Problems with Combined Individual and Team Reward'},\n",
       "   {'paperId': '845aeb9dcf12efba0760c6eb2e2ac56ea27f3247',\n",
       "    'title': 'Option Discovery in the Absence of Rewards with Manifold Analysis'},\n",
       "   {'paperId': 'bf51c533b8e292e8671d0f9f072b3cd99eb8de92',\n",
       "    'title': 'HiCoACR: A reconfiguration decision-making model for reconfigurable security protocol based on hierarchically collaborative ant colony'},\n",
       "   {'paperId': '6426a3f15b90f4777877f3db49db54db0509c0ba',\n",
       "    'title': \"Estimating Q(s, s') with Deep Deterministic Dynamics Gradients\"},\n",
       "   {'paperId': '07969ae28689f123367b4a678519d285b9109a07',\n",
       "    'title': 'Follow the Neurally-Perturbed Leader for Adversarial Training'},\n",
       "   {'paperId': '075692902b221544d582f748855ce5fae808f83c',\n",
       "    'title': 'Towards Intelligent Pick and Place Assembly of Individualized Products Using Reinforcement Learning'},\n",
       "   {'paperId': '886b604f74d65bb420bc1311e9647c44fe8fa91e',\n",
       "    'title': 'Temporal-adaptive Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'eaeacc4d9936e7b6d016c42de1f3c536d85b7466',\n",
       "    'title': 'Blind Spot Detection for Safe Sim-to-Real Transfer'},\n",
       "   {'paperId': '441e66e7fd773444ef6534b42c662116199409e6',\n",
       "    'title': 'Crossmodal attentive skill learner: learning in Atari and beyond with audio–video inputs'},\n",
       "   {'paperId': '60c8b24913090e168bdb174f7d3e9e96dd0c5a8c',\n",
       "    'title': 'Options of Interest: Temporal Abstraction with Interest Functions'},\n",
       "   {'paperId': '2599c0ecb4cefa22ec745d2cb71c7081109920f3',\n",
       "    'title': 'Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning'},\n",
       "   {'paperId': 'bfd215e196d895aa61aa1fa3e9cde3defcab771e',\n",
       "    'title': 'Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery'},\n",
       "   {'paperId': '20e127a1d27617b2b8545a54c016ccf2b5170b78',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Open-Domain Dialog'},\n",
       "   {'paperId': '3adc129e0b4304a0b87f096e72a2450b914703bd',\n",
       "    'title': 'Option Encoder: A Framework for Discovering a Policy Basis in Reinforcement Learning'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': '58a5f31ac9da1c04f21f2453b07b64b067ed24f0',\n",
       "    'title': 'Computational evidence for hierarchically structured reinforcement learning in humans'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8db16192c8abf006b90d22aab3e7fe3694640836',\n",
       "    'title': 'Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e3e97246503889369c840c63e4fdbf9a301d7f59',\n",
       "    'title': 'Continual Reinforcement Learning in 3D Non-stationary Environments'},\n",
       "   {'paperId': 'a7859b059cfe01d01f1bd795e86eb3f0771fb53b',\n",
       "    'title': 'Model primitives for hierarchical lifelong reinforcement learning'},\n",
       "   {'paperId': '3edda9dff4106c4ac16be6f7dedce1a64ad760ea',\n",
       "    'title': 'DeepCrawl: Deep Reinforcement Learning for Turn-based Strategy Games'},\n",
       "   {'paperId': '7310c425c1fb44f43f983a959e46f45c1947fc9a',\n",
       "    'title': 'Only Relevant Information Matters: Filtering Out Noisy Samples To Boost RL'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': 'b57c24654d21b16067493e4e217f63281dcba086',\n",
       "    'title': 'TTR-Based Reward for Reinforcement Learning with Implicit Model Priors'},\n",
       "   {'paperId': '01bf4baec2aee64714c8fd29515ac9e2ed903392',\n",
       "    'title': 'Learning Hierarchical Teaching Policies for Cooperative Agents'},\n",
       "   {'paperId': '339cb2ec1bb54b6d918c36b9924992b85b668899',\n",
       "    'title': 'Planning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies'},\n",
       "   {'paperId': '5330b2732c12c7027811869a921f544a0bf581ca',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks'},\n",
       "   {'paperId': 'a9b80b3cffb758bea670220fa6762eb343865419',\n",
       "    'title': 'Language GANs Falling Short'},\n",
       "   {'paperId': 'bf2a70ecfe3a0b701a16104cd8b880a41353dd2f',\n",
       "    'title': 'Scaling All-Goals Updates in Reinforcement Learning Using Convolutional Neural Networks'},\n",
       "   {'paperId': '08c0c0ca05dfe6bc6cd3a3c6f9d6449ad373eb87',\n",
       "    'title': 'CM3: Cooperative Multi-goal Multi-stage Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '665a5673d33a90a1b71c0d5b1be127a76af43be7',\n",
       "    'title': 'Video Description'},\n",
       "   {'paperId': '3169f9cdc12d53b072459e0b7886b7322a5c6cfc',\n",
       "    'title': 'EXPLAINABLE REINFORCEMENT LEARNING THROUGH GOAL-BASED EXPLANATIONS'},\n",
       "   {'paperId': '323590ce6ffe2fa3a9272ff5af33502b0855d1fd',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientiﬁc Implications'},\n",
       "   {'paperId': '42c7b3e0149f1236c9c215e8c2d1fbf9e2e2be27',\n",
       "    'title': 'A Learning-based Approach for Stream Scheduling in Multipath-QUIC'},\n",
       "   {'paperId': '1ab39dfdcbb6ec442785c3655c8263ec0ecc52a2',\n",
       "    'title': 'DIVIDE-AND-CONQUER MONTE CARLO TREE SEARCH'},\n",
       "   {'paperId': 'e3406542068ed3de2fdc3bbbc2c618ee6258f01d',\n",
       "    'title': 'Reinforcement Learning for the Beginning of Starcraft II Game'},\n",
       "   {'paperId': 'a1a112aec9e0f9cdcfd27a85b443291af792d4f9',\n",
       "    'title': 'Detection of Scenes Features for Path Following on a Local Map of a Mobile Robot Using Neural Networks'},\n",
       "   {'paperId': '4a469b594a1525a2c7d260570eae46d78690b171',\n",
       "    'title': 'Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24–27, 2020, Proceedings'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '728cfe9697d7f7a9940dca17a4045fd10d6c0bf4',\n",
       "    'title': 'IHRL: Interactive Influence-based Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b4d7384c8a64c1887e9b718c74dfd9b669534337',\n",
       "    'title': 'Towards Knowledge Infusion for Robust and Transferable Machine Learning in IoT'},\n",
       "   {'paperId': 'd17362c79ff42760c70235fb377923f0caad1f3a',\n",
       "    'title': 'A Survey on Visual Navigation for Artificial Agents With Deep Reinforcement Learning'},\n",
       "   {'paperId': '8fb76a141d8e72eef7d6e220ea6556a6c5755756',\n",
       "    'title': 'Learning Intrinsic Rewards as a Bi-Level Optimization Problem'},\n",
       "   {'paperId': '4b402059c9a73487429d5cff9c32c6e79a6e54da',\n",
       "    'title': 'Challenges of Reinforcement Learning'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '475bee1610ef779fdad6cbbfc2e473544e925a1e',\n",
       "    'title': 'Bidirectional Dilated LSTM with Attention for Fine-grained Emotion Classification in Tweets'},\n",
       "   {'paperId': '1c5d2189100f34dd342fdd4714ffaad674f82e21',\n",
       "    'title': 'Hierarchical and Non-Hierarchical Multi-Agent Interactions Based on Unity Reinforcement Learning'},\n",
       "   {'paperId': '243f9d6274161aabc1bd176056db7009020de04d',\n",
       "    'title': 'Adaptive Agent-Based Simulation for Individualized Training'},\n",
       "   {'paperId': '4906707dd9a9f2821c8c229b576253333000d558',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Integrated Discovery of Salient Subgoals'},\n",
       "   {'paperId': 'd802b949ff627d51fcb148685256f9fd25f848d5',\n",
       "    'title': 'Hindsight Planner'},\n",
       "   {'paperId': '3204cae32de61e97ac35982dc452436d491edccd',\n",
       "    'title': 'Generate Symbolic Plans for Long-Horizon Robot Manipulation Tasks'},\n",
       "   {'paperId': 'f26a89180b9bf2f58b8647ea30580fed0ee3ff28',\n",
       "    'title': 'Proximal Policy Optimization with Explicit Intrinsic Motivation'},\n",
       "   {'paperId': 'c5e1cbf8e76fb074bb666c695763cefb16381000',\n",
       "    'title': 'Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': 'a2e43270a9b1421e452c2975e5163e2a216abeac',\n",
       "    'title': 'A Survey of Deep Reinforcement Learning in Video Games'},\n",
       "   {'paperId': '7771454dceec1cc83d56d3ea996851e293013e36',\n",
       "    'title': 'Weakly Supervised Video Summarization by Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e4d3b4de9a43d9abefbba6174897e82f77c74faf',\n",
       "    'title': 'Long-Term Planning and Situational Awareness in OpenAI Five'},\n",
       "   {'paperId': 'd9e14b81e7acf2e17de20df113018943978509ae',\n",
       "    'title': 'Inter-Level Cooperation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'bfb4970d045a25972629ef9ba428f0b4626e29d7',\n",
       "    'title': 'Optimizing High-dimensional Learner with Low-Dimension Action Features'},\n",
       "   {'paperId': 'aca61bde2fbfc0cc8d7ed1273231e463ec537b9e',\n",
       "    'title': 'Time-sequence Action-Decision and Navigation Through Stage Deep Reinforcement Learning in Complex Dynamic Environments'},\n",
       "   {'paperId': '7434479250744ac352f49d216ad06ea422b65e54',\n",
       "    'title': 'A Hierarchical Model for StarCraft II Mini-Game'},\n",
       "   {'paperId': 'b65decc03155f2e88984e4fa16493f70e5413e4d',\n",
       "    'title': 'Hierarchical motor control in mammals and machines'},\n",
       "   {'paperId': '389efb5d26fc7a913cc71eb2638b0c06f189cb3d',\n",
       "    'title': 'DeepSynth: Program Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning'},\n",
       "   {'paperId': '7b4848bad51ebd38fb068e73abc3c6d865fd692f',\n",
       "    'title': 'Planning with Goal-Conditioned Policies'},\n",
       "   {'paperId': 'ba12216ac6f4c1c75588fdef5f1319e12b186af3',\n",
       "    'title': 'Efficient Exploration through Intrinsic Motivation Learning for Unsupervised Subgoal Discovery in Model-Free Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd861cab02cbb314fa7f1e14103a238d66e5d8809',\n",
       "    'title': 'Research on Learning Method Based on Hierarchical Decomposition'},\n",
       "   {'paperId': '3f116db230b58e6908f5d5791c9761260ad2d7b2',\n",
       "    'title': 'Graph-Based Design of Hierarchical Reinforcement Learning Agents'},\n",
       "   {'paperId': '36a1d4d7f96bd3946608f2de51ecef0000105ca6',\n",
       "    'title': 'Object-oriented state editing for HRL'},\n",
       "   {'paperId': 'c00028df6ee6e5d7df4d4cbdbf421a359918b15d',\n",
       "    'title': 'Learning Fairness in Multi-Agent Systems'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': '4e799493a13c786c7d1ff4c0c80c4a929960b90f',\n",
       "    'title': 'Multi-Agent Multi-Objective Deep Reinforcement Learning for Efficient and Effective Pilot Training'},\n",
       "   {'paperId': '5311c49db20ebefa64ddadc80bb6757b6eee114a',\n",
       "    'title': 'Automatic Data Augmentation by Learning the Deterministic Policy'},\n",
       "   {'paperId': '9a31be824bf74542d8b58a9661e609cc82c6a457',\n",
       "    'title': 'Algorithms for recursive delegation'},\n",
       "   {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       "   {'paperId': 'e8faa4fa0909db69cc4039af43c1ffe921ca7977',\n",
       "    'title': 'Hierarchical Actor-Critic with Hindsight for Mobile Robot with Continuous State Space'},\n",
       "   {'paperId': '3304a5c2d013a3d786a44109b03b150194dce1f4',\n",
       "    'title': 'Dilated Recurrent Neural Network for Epidemiological Predictions'},\n",
       "   {'paperId': '64656a66a14323d984ed0fd0bc9bfe5ee85b97c5',\n",
       "    'title': 'Laplacian using Abstract State Transition Graphs: A Framework for Skill Acquisition'},\n",
       "   {'paperId': '58b88ae32f2ca92a941ccd21cf042b05f465b8b3',\n",
       "    'title': 'On the necessity of abstraction'},\n",
       "   {'paperId': 'df3ac75ec8ad937b7e1d43d6e4f40aa0cfa6bc01',\n",
       "    'title': 'Playing Atari Ball Games with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c52f87830d6c0f9d7e61defa695bd65a8aca067',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks'},\n",
       "   {'paperId': '7e522be5f714f4f0d56b808a318a66eb206c3968',\n",
       "    'title': 'Efficient meta reinforcement learning via meta goal generation'},\n",
       "   {'paperId': '45ac74350d21387c42ff92e90cd088bf310e1542',\n",
       "    'title': 'Guided goal generation for hindsight multi-goal reinforcement learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '2f4a5b8afa536428f46c708cb3b4f1bc23badafe',\n",
       "    'title': 'Learning sparse representations in reinforcement learning'},\n",
       "   {'paperId': '390364c3986d05ad71a40a967b9cc12aa30e4305',\n",
       "    'title': 'A survey and critique of multiagent deep reinforcement learning'},\n",
       "   {'paperId': 'f84e1ab127729d1a039b0a094bc2a3c0d4e5beb1',\n",
       "    'title': 'Navigation in Unknown Dynamic Environments Based on Deep Reinforcement Learning'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': 'a77df5291ee644022067f34eec790ea31380792b',\n",
       "    'title': 'Are You for Real? Detecting Identity Fraud via Dialogue Interactions'},\n",
       "   {'paperId': '7c25b27c1496401058f84159e050fd366906699d',\n",
       "    'title': 'Skill Transfer in Deep Reinforcement Learning under Morphological Heterogeneity'},\n",
       "   {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "    'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       "   {'paperId': '5678aa198b35b672d5829e48c1d1bb0dec2ac4ad',\n",
       "    'title': 'Playing FPS Games With Environment-Aware Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd7830100006d605612fc56eca50f87cc64d682f8',\n",
       "    'title': 'Smart fog based workflow for traffic control networks'},\n",
       "   {'paperId': '3944f15e0c2d39d114e5a44aedd079630607521e',\n",
       "    'title': 'Semantic RL with Action Grammars: Data-Efficient Learning of Hierarchical Task Abstractions'},\n",
       "   {'paperId': '3c86aaf1ad93e1a089f5ec89b94d75de898449ec',\n",
       "    'title': 'New Era of Deeplearning-Based Malware Intrusion Detection: The Malware Detection and Prediction Based On Deep Learning'},\n",
       "   {'paperId': '855112d5c051616353f11180b9801c0ee09f2fa5',\n",
       "    'title': 'Deriving Subgoals Autonomously to Accelerate Learning in Sparse Reward Domains'},\n",
       "   {'paperId': '54a9941279ffaaac37e1d623d3d9d30dbbd35aaa',\n",
       "    'title': 'Scalable muscle-actuated human simulation and control'},\n",
       "   {'paperId': '66605b6ceae9847156526e46ca9fe467804fca54',\n",
       "    'title': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '118672d5762c45b9469e74dbc43f0102b04717a6',\n",
       "    'title': 'On Multi-Agent Learning in Team Sports Games'},\n",
       "   {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "    'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "    'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       "   {'paperId': 'e0889fcee1acd985af76a3907d5d0029bf260be9',\n",
       "    'title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning'},\n",
       "   {'paperId': 'f85ed65604976df89f9d991177c9a428d2168020',\n",
       "    'title': 'Options as responses: Grounding behavioural hierarchies in multi-agent RL'},\n",
       "   {'paperId': '77036414bf957f4de90fd9351c50efd6fbc650c9',\n",
       "    'title': 'Dilated Convolution with Dilated GRU for Music Source Separation'},\n",
       "   {'paperId': '7134b16bd8b172606d9c154e10c6d66fe2ff3f2d',\n",
       "    'title': 'Deep lifelong reinforcement learning for resilient control and coordination'},\n",
       "   {'paperId': 'af581dd2243ba89ba476f790cde876d1e1b6774b',\n",
       "    'title': 'Efficient Exploration in Reinforcement Learning through Time-Based Representations'},\n",
       "   {'paperId': 'a17ddc6e5fbe74761f8016aebd640c040907a45c',\n",
       "    'title': 'Games in Machine Learning: Differentiable n-Player Games and Structured Planning'},\n",
       "   {'paperId': 'd8886451bd6f3dbc48f98107901220cd845a7ea6',\n",
       "    'title': 'Learning Navigation Subroutines from Egocentric Videos'},\n",
       "   {'paperId': 'b00e870876ad64788ff8e66b76ae335f36432d91',\n",
       "    'title': 'Extra-gradient with player sampling for provable fast convergence in n-player games'},\n",
       "   {'paperId': '8614a91589c4d851574f964ed7a76161591c80c7',\n",
       "    'title': 'Learning Navigation Subroutines by Watching Videos'},\n",
       "   {'paperId': 'bd94b4ee87b8a43721341f78359d98276016fad3',\n",
       "    'title': 'Competitive Gradient Descent'},\n",
       "   {'paperId': 'ff13e53d3d7eca57456e208394b31f9a5de3fdc7',\n",
       "    'title': 'Options in Multi-task Reinforcement Learning - Transfer via Reflection'},\n",
       "   {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "    'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       "   {'paperId': '8c7bb0db448bf54cf0af6ef38db5e63402ce72bd',\n",
       "    'title': 'AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence'},\n",
       "   {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "    'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e15db5ed7bcf4dd6b6fe77978b73dacde85d5001',\n",
       "    'title': 'From semantics to execution: Integrating action planning with reinforcement learning for robotic tool use'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'af1dbd44d60e42b52f7629099376e1eaed3685ed',\n",
       "    'title': 'From Semantics to Execution: Integrating Action Planning With Reinforcement Learning for Robotic Causal Problem-Solving'},\n",
       "   {'paperId': 'd0ca8e0b9e1d31d832b4b25bfe8ef825abb7c833',\n",
       "    'title': 'COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration'},\n",
       "   {'paperId': 'ff6a9d9e9f8dc97141ced4eb9debd396e7344ee0',\n",
       "    'title': 'Learning and Exploiting Multiple Subgoals for Fast Exploration in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '050a20a1244c952768235f3be4613845c2f3d2ca',\n",
       "    'title': 'The 2nd Learning from Limited Labeled Data (LLD) Workshop: Representation Learning for Weak Supervision and Beyond'},\n",
       "   {'paperId': '93e256d558f972769d10484b5b10e3402c1be79b',\n",
       "    'title': 'Deep Reinforcement Learning Meets Structured Prediction'},\n",
       "   {'paperId': '6570c7cab46d2b0f3315f5abfbbd209140529c8b',\n",
       "    'title': 'Hierarchical Policy Learning is Sensitive to Goal Space Design'},\n",
       "   {'paperId': '433fcb584902e716ee25a44175e380267616b54e',\n",
       "    'title': 'Learning Compositional Neural Programs with Recursive Tree Search and Planning'},\n",
       "   {'paperId': '4b51277eac12939867ec04a81bdbc756f61ec9ea',\n",
       "    'title': 'Successor Options: An Option Discovery Framework for Reinforcement Learning'},\n",
       "   {'paperId': 'd4f24c06a9366de30dea3965d3579c58b45e3fe4',\n",
       "    'title': 'Differentiable Game Mechanics'},\n",
       "   {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "    'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       "   {'paperId': '5e96da69634a90c133213b90830a173d66d29aac',\n",
       "    'title': 'Downhole Track Detection via Multiscale Conditional Generative Adversarial Nets'},\n",
       "   {'paperId': '4019625941a75ba71eefab09e8963f9e1d8905a9',\n",
       "    'title': 'Disentangling Options with Hellinger Distance Regularizer'},\n",
       "   {'paperId': '54fc216800fb17a9b16760663ae2441b7c688c6d',\n",
       "    'title': 'Dot-to-Dot: Achieving Structured Robotic Manipulation through Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'adc4ddeb10dd8da115d4bde9569794ff409fcd40',\n",
       "    'title': 'Dot-to-Dot: Explainable Hierarchical Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': 'f719ab564b59318f6612abd3d1406edd601e4420',\n",
       "    'title': 'Adversarial Discrete Sequence Generation without Explicit NeuralNetworks as Discriminators'},\n",
       "   {'paperId': 'e86828a6c61d9cb5ecdc83cf1793c19e0d6661d2',\n",
       "    'title': 'Samples are not all useful: Denoising policy gradient updates using variance'},\n",
       "   {'paperId': 'a5661dfbf57f909d581a05d54406f80825f9f9eb',\n",
       "    'title': 'Sub-Task Discovery with Limited Supervision: A Constrained Clustering Approach'},\n",
       "   {'paperId': '950d4a792cdf2daa4ec933de8a7a27e16df82c36',\n",
       "    'title': 'Deep Hierarchical Reinforcement Learning Based Recommendations via Multi-goals Abstraction'},\n",
       "   {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "    'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '048459fcf6befce76d277a31ead3f58e1b2de32d',\n",
       "    'title': 'Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration'},\n",
       "   {'paperId': '8cff4799ad5a99e5530c7a6029766a40c0b3b5e0',\n",
       "    'title': 'Learning Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning'},\n",
       "   {'paperId': '7f1a309f6057a0a1e9d7643c1b14e28915d90aaf',\n",
       "    'title': 'Combining Optimal Control and Learning for Visual Navigation in Novel Environments'},\n",
       "   {'paperId': 'c372396db5db3ea5d1acc1255e79791c51dfc959',\n",
       "    'title': 'Learning Dynamics Model in Reinforcement Learning by Incorporating the Long Term Future'},\n",
       "   {'paperId': '4839f473193fccb5566ae6388ba6f40defdf10dc',\n",
       "    'title': 'Model Primitive Hierarchical Lifelong Reinforcement Learning'},\n",
       "   {'paperId': '7004ca8debac9c260bbff532d71a75c2048c6878',\n",
       "    'title': 'Reinforcement learning in artificial and biological systems'},\n",
       "   {'paperId': '951af7222535d934ca2b401ca0cd2181b28284f9',\n",
       "    'title': 'Reinforcement learning in artificial and biological systems'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': '8d613d8dc6ef962111a4100498d31958c0fcff55',\n",
       "    'title': 'Graph-Based Skill Acquisition For Reinforcement Learning'},\n",
       "   {'paperId': '0d076cb63c773734de2f3f5ba6130da9495a0cb1',\n",
       "    'title': 'Unsupervised State-space Decomposition in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0a11fa9e5f521a52977ba0d5cc9bbafefa37d823',\n",
       "    'title': 'Hierarchical Critics Assignment for Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': 'ec2b345267d69e8b4dca550efcd0948e0352acd9',\n",
       "    'title': 'Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning'},\n",
       "   {'paperId': 'af25760248b1f1552e58fd2e592f81985ef6403f',\n",
       "    'title': 'TF-Replicator: Distributed Machine Learning for Researchers'},\n",
       "   {'paperId': 'bf7f1ada5feecc0992f71b39c1ebeccb19ae631b',\n",
       "    'title': 'InfoBot: Transfer and Exploration via the Information Bottleneck'},\n",
       "   {'paperId': '70a3f24292bdcf6e630d5b32eacf93aa3f913c59',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent networks.'},\n",
       "   {'paperId': '12db6b8b2b19d8519c6c1332b4aad2f9ebc7fdfc',\n",
       "    'title': 'Emergence of Hierarchy via Reinforcement Learning Using a Multiple Timescale Stochastic RNN'},\n",
       "   {'paperId': '36e228197e24bb89c5944b705f727f8833db50d5',\n",
       "    'title': 'Feudal Multi-Agent Hierarchies for Cooperative Reinforcement Learning'},\n",
       "   {'paperId': 'b3f2893ebbdfab2dd8d87c565024577655095e54',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Multi-agent MOBA Game'},\n",
       "   {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "    'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       "   {'paperId': '88c38f203bbf0f881bcc42a8d1a4235b4ba7cb86',\n",
       "    'title': 'Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '894536f2ac4728850bc18705daeeda6e88f3d6f1',\n",
       "    'title': 'Universal Successor Features Approximators'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'fcd294efdd09b1309ce483f0b84d0c4b8556a1ab',\n",
       "    'title': 'Stable Opponent Shaping in Differentiable Games'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4d7bc872be3bde2c3c4c288dbbafeff80b5e0867',\n",
       "    'title': 'Multi-Agent Common Knowledge Reinforcement Learning'},\n",
       "   {'paperId': '3aabed9c0963f5924a19e8fa2c522a730db46e16',\n",
       "    'title': 'Learning Representations in Model-Free Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751',\n",
       "    'title': 'Deep Reinforcement Learning'},\n",
       "   {'paperId': '3f56ac0e4b881d25268e83961b93ee95f2807bfb',\n",
       "    'title': 'CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "    'title': 'Composing Complex Skills by Learning Transition Policies'},\n",
       "   {'paperId': '9ea92ebeb7462f2db346cfa3281ad7497b1063d6',\n",
       "    'title': 'Deep reinforcement learning with relational inductive biases'},\n",
       "   {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': 'a6738f4f0ad70bf3c2252dd026e0e2823ee4f48c',\n",
       "    'title': 'On Reinforcement Learning for Full-length Game of StarCraft'},\n",
       "   {'paperId': '56326d969591d3c30e2f48027b926be20a3e75f7',\n",
       "    'title': 'Automatically Composing Representation Transformations as a Means for Generalization'},\n",
       "   {'paperId': 'ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a',\n",
       "    'title': 'Human-level performance in 3D multiplayer games with population-based reinforcement learning'},\n",
       "   {'paperId': 'dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4',\n",
       "    'title': 'Policy Search in Continuous Action Domains: an Overview'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': '1fa1f04b80f057e477549e6b9798fab7c7e57db5',\n",
       "    'title': 'Hindsight policy gradients'},\n",
       "   {'paperId': '9d528f7e641c922bddd83f4af687806d685490d6',\n",
       "    'title': 'Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '60a1096593531ea5615bbedbd1c9a48bd931b860',\n",
       "    'title': 'Self-developed Action Hierarchy Enhances Meta Reinforcement Learning.'},\n",
       "   {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "    'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': 'daf93749171e3575cca5b584a4146b247692c86f',\n",
       "    'title': 'Learning and planning in videogames via task decomposition'},\n",
       "   {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '58751ad1dd047cd96285c90a99de8c3697c1bcd9',\n",
       "    'title': 'Soft Option Transfer'},\n",
       "   {'paperId': '9d8f7ba00a2a2aa87c90f95d99421e8a3fcbbff8',\n",
       "    'title': 'Predictive Safety Network for Resource-constrained Multi-agent Systems'},\n",
       "   {'paperId': 'c7cf6672d6b3254ea64adc70b3aa7e3aede57142',\n",
       "    'title': 'Double DQN based Autonomous Obstacle Avoidance for Quadcopter Navigation'},\n",
       "   {'paperId': '02feb390612858d745ce324303bfc8f9d0148c42',\n",
       "    'title': 'Building structured hierarchical agents'},\n",
       "   {'paperId': '8210b626959b2d3dbf7f8098b074936f375ccbdc',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Model-Free Flight Control: A sample efficient tabular approach using Q(lambda)-learning and options in a traditional flight control structure'},\n",
       "   {'paperId': 'd1060aabd99a96f191be228b7176377c3a6e8de9',\n",
       "    'title': 'Towards Practical Robot Manipulation using Relational Reinforcement Learning'},\n",
       "   {'paperId': '822cd314662479bc4bad911a3768d921614fcaa3',\n",
       "    'title': 'WHY DOES HIERARCHY (SOMETIMES) WORK'},\n",
       "   {'paperId': '63c7a404e471c900bcd7c4782d93562b58551b98',\n",
       "    'title': 'How to tell Real From Fake? Understanding how to classify human-authored and machine-generated text'},\n",
       "   {'paperId': '09671e33dd27c6e04dad282f860ef09390c33f07',\n",
       "    'title': 'Downhole Track Detection via Multi-dimensional Conditional Generative Adversarial Nets'},\n",
       "   {'paperId': '5afdf86364d79a9f7173dab39ae5c20674f4d3de',\n",
       "    'title': 'Fast online model learning for controlling complex real-world robots'},\n",
       "   {'paperId': 'af62b70e1eb89e4f412e8cc470021444f6de08f4',\n",
       "    'title': 'GROWING UP TOGETHER: STRUCTURED EXPLO-'},\n",
       "   {'paperId': 'a4fdfea4a0810303a484b6eeab56cbb2a3bccf7f',\n",
       "    'title': 'META REINFORCEMENT LEARNING VIA META GOAL GENERATION'},\n",
       "   {'paperId': '4c19fcb0f8c041996c7cbb0deab15fbec5d1b5d9',\n",
       "    'title': 'Meta-Learning via Weighted Gradient Update'},\n",
       "   {'paperId': '7878444056e3bfa95750673321300aa59f4c3b86',\n",
       "    'title': 'Hierarchical Critic Assignment for Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '585abbef4b08a4ff601a7f8463a23b43c430e9d2',\n",
       "    'title': 'A Hierarchically Collaborative Ant Colony Based Assembly Algorithm for Security Protocol'},\n",
       "   {'paperId': '2360efedce6b7bb74ac817c5c9a75cbd3c7830f5',\n",
       "    'title': 'Learning Representations in Reinforcement Learning'},\n",
       "   {'paperId': '98b41528c58e6f5b7b28be5b54029e52ca90c4ab',\n",
       "    'title': 'Learning to Learn: Hierarchical Meta-Critic Networks'},\n",
       "   {'paperId': '4c5dca886f1d5cc4213ece9cfb7895c7f48fd1be',\n",
       "    'title': 'Scalable deep reinforcement learning for physics-based motion control'},\n",
       "   {'paperId': 'b08430f554aba971044dea2dd31d9eecca41b021',\n",
       "    'title': 'Heterogeneous Knowledge Transfer via Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning'},\n",
       "   {'paperId': '3cf906d2cc57f07244cd1f74ccb1ace8e70073cc',\n",
       "    'title': 'STABLE OPPONENT SHAPING'},\n",
       "   {'paperId': 'f7879ba3a3ef21a46f1a0f1f05cba8592187e50d',\n",
       "    'title': 'Unsupervised Methods For Subgoal Discovery During Intrinsic Motivation in Model-Free Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '90dc77fa973a8a3e94926c5698c09f19a42f6946',\n",
       "    'title': 'Discovery of hierarchical representations for efficient planning'},\n",
       "   {'paperId': '5374b529ec3cbeb193485a8b2d40a9c96823f3c6',\n",
       "    'title': 'An Adversarial Algorithm for Delegation'},\n",
       "   {'paperId': 'dd23055b151de020ffb719f8bab91a228536ca5a',\n",
       "    'title': 'Hyperbolic Embeddings for Learning Options in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '072fda4494b0d522c8e97f7f1ead38001a37c0d7',\n",
       "    'title': 'Hierarchical Policy Design for Sample-Efficient Learning of Robot Table Tennis Through Self-Play'},\n",
       "   {'paperId': 'e87532f456571b3dc88a583fe9873b68d8e28a26',\n",
       "    'title': 'Idiosyncrasies and challenges of data driven learning in electronic trading'},\n",
       "   {'paperId': '5c91208414f02eaec2281aabd30b50f0ce5b9da6',\n",
       "    'title': 'Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'c6f913e4baa7f2c85363c0625c87003ad3b3a14c',\n",
       "    'title': 'Scalable agent alignment via reward modeling: a research direction'},\n",
       "   {'paperId': '14d798a15d74813a6ee7a75661925e25de69f7d1',\n",
       "    'title': 'Learning Sequential Decision Tasks for Robot Manipulation with Abstract Markov Decision Processes and Demonstration-Guided Exploration'},\n",
       "   {'paperId': '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85',\n",
       "    'title': 'Learning Abstract Options'},\n",
       "   {'paperId': 'de1609a446456b1b0e575daf8985d3727c5e967d',\n",
       "    'title': 'CURIOUS: Intrinsically Motivated Multi-Task, Multi-Goal Reinforcement Learning'},\n",
       "   {'paperId': '3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b',\n",
       "    'title': 'Is multiagent deep reinforcement learning the answer or the question? A brief survey'},\n",
       "   {'paperId': 'fc19c388c62a944072fd29ae002cefb8cafe1afc',\n",
       "    'title': 'Distributed Deep Reinforcement Learning for Fighting Forest Fires with a Network of Aerial Robots'},\n",
       "   {'paperId': '265a32d3e5a55140389df0a0b666ac5c2dfaa0bd',\n",
       "    'title': 'Curriculum Learning Based on Reward Sparseness for Deep Reinforcement Learning of Task Completion Dialogue Management'},\n",
       "   {'paperId': '439046148cd14c5c8f3a7ca63b95a6f75bda391d',\n",
       "    'title': 'Learning and Planning with a Semantic Model'},\n",
       "   {'paperId': '607eee11d64c75d837bcf98f3ec1bcd0d5727d07',\n",
       "    'title': 'Hierarchical Deep Multiagent Reinforcement Learning with Temporal Abstraction'},\n",
       "   {'paperId': '5faca17c1f8293e5d171719a6b8a289592c3d64d',\n",
       "    'title': 'Hierarchical Deep Multiagent Reinforcement Learning'},\n",
       "   {'paperId': '0d11d48303eb25a771581f404fcf4e7ad35e2f41',\n",
       "    'title': 'TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game'},\n",
       "   {'paperId': '10a82d0465a8883db12961fc93d4baf9278cd100',\n",
       "    'title': 'Expert-augmented actor-critic for ViZDoom and Montezumas Revenge'},\n",
       "   {'paperId': '0a01766797da6701034a9b4947bb2201ef2f3380',\n",
       "    'title': 'Hierarchical reinforcement learning of multiple grasping strategies with human instructions'},\n",
       "   {'paperId': 'c79b14ffb4e1bc7eb5157c6ec9d04298551d1aa4',\n",
       "    'title': 'Focus on Scene Text Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': '4c03497f2e17900cbf4066fbf68a7cbaad8376be',\n",
       "    'title': 'Representational efficiency outweighs action efficiency in human program induction'},\n",
       "   {'paperId': 'f650f1fd44ab0778d30577f8c2077b2ff58830da',\n",
       "    'title': 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement'},\n",
       "   {'paperId': '1cb6edbedc4a1ac5c32f61a435a23264e42a9071',\n",
       "    'title': 'Towards Sample Efficient Reinforcement Learning'},\n",
       "   {'paperId': '99be567c4c0e3b48c7e38161a94239939abc9b1a',\n",
       "    'title': 'Scaling Genetic Programming to Challenging Reinforcement Tasks through Emergent Modularity'},\n",
       "   {'paperId': '3254692e2794ef8c8f96374aadb27c3f3926492e',\n",
       "    'title': 'Massively Parallel Video Networks'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '5e49c80f8b12a100c5f4518897c4cbf72710c252',\n",
       "    'title': 'Relational Deep Reinforcement Learning'},\n",
       "   {'paperId': '2dae355d690827c26da2025a3d02edc0a98b21b4',\n",
       "    'title': 'Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors'},\n",
       "   {'paperId': '99967d34b2fc185090ab05494bf1b2a9b2609f8d',\n",
       "    'title': 'The streaming rollout of deep networks - towards fully model-parallel execution'},\n",
       "   {'paperId': 'ce2b26bf018206f775b1749f1624c137d60d2a1b',\n",
       "    'title': 'Learning to Transform Service Instructions into Actions with Reinforcement Learning and Knowledge Base'},\n",
       "   {'paperId': '88360b92a354593fdb227999b8cccd0e5f4d2c90',\n",
       "    'title': 'Learning to Transform Service Instructions into Actions with Reinforcement Learning and Knowledge Base'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c852a954c3a74df410231d601857b7005076de9',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Hindsight'},\n",
       "   {'paperId': '15365821d5e2b9ebec1ed9ac314975732b688da3',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Deep Nested Agents'},\n",
       "   {'paperId': 'd4575be3cf5128bb9b86fd9d37222b3202c0c922',\n",
       "    'title': 'A Deep Hierarchical Reinforcement Learning Algorithm in Partially Observable Markov Decision Processes'},\n",
       "   {'paperId': '495b453c80b2dc2b7d9705afa1abd1740875f674',\n",
       "    'title': \"Crawling in Rogue's dungeons with (partitioned) A3C\"},\n",
       "   {'paperId': '6aae1bc6c8e38c9a1d24a2b48bfe066f3591e1bc',\n",
       "    'title': 'Subgoal Discovery for Hierarchical Dialogue Policy Learning'},\n",
       "   {'paperId': 'd72de6a4d3349847c6037ff775975b62a78e45b7',\n",
       "    'title': 'Delegating via Quitting Games'},\n",
       "   {'paperId': '35271d36cb20bf8d716e79c9dd15d738d955a931',\n",
       "    'title': 'On Learning Intrinsic Rewards for Policy Gradient Methods'},\n",
       "   {'paperId': '094a437e4caf4e2329bbc3410f64208d949c18a4',\n",
       "    'title': 'An Adaptive Clipping Approach for Proximal Policy Optimization'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0cd97d54c610c5fd3d20fbe230a3423b912a63fb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: Approximating Optimal Discounted TSP Using Local Policies'},\n",
       "   {'paperId': 'bf8d58faf972ad0a1026c0a7c5577c07996ef3a7',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning for Continuous Action Control'},\n",
       "   {'paperId': 'fb9693183bc74568c72188431c18cb2b07c87213',\n",
       "    'title': 'Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': 'e0155830d8982da4631cb71546fca782b2e00c20',\n",
       "    'title': 'Composable Planning with Attributes'},\n",
       "   {'paperId': 'd72e69eacd4afeac33f71d07c484686084e55b9a',\n",
       "    'title': 'Unicorn: Continual Learning with a Universal, Off-policy Agent'},\n",
       "   {'paperId': 'a4d513cfc9d4902ef1a80198582f29b8ba46ac28',\n",
       "    'title': 'The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation'},\n",
       "   {'paperId': '674255f825742477f71e3b6728c602214f46c69d',\n",
       "    'title': 'Learning High-level Representations from Demonstrations'},\n",
       "   {'paperId': '249408527106d7595d45dd761dd53c83e5a02613',\n",
       "    'title': 'NerveNet: Learning Structured Policy with Graph Neural Networks'},\n",
       "   {'paperId': '809f951c77b5a39e2a9d556e9cf9938de87f2393',\n",
       "    'title': 'An Inference-Based Policy Gradient Method for Learning Options'},\n",
       "   {'paperId': 'b80991d12b41a5a68dc14dd87b692c0f903ceb9c',\n",
       "    'title': 'Some Considerations on Learning to Explore via Meta-Reinforcement Learning'},\n",
       "   {'paperId': '52f7ae53e58c5098133d041794b4465d36c2fdb6',\n",
       "    'title': 'The Mechanics of n-Player Differentiable Games'},\n",
       "   {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "    'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '927d904d2aad002eb71e8c6ee45218f31a103100',\n",
       "    'title': 'Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents'},\n",
       "   {'paperId': '74b284a66e75b65f5970d05bac000fe91243ee49',\n",
       "    'title': 'Video Captioning via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a9d8c7bcbe7d6a46edacf6d8241c3a719e897b21',\n",
       "    'title': 'Crossmodal Attentive Skill Learner'},\n",
       "   {'paperId': '58fb60c5592224901a26dd84220a2f3332c1fcf5',\n",
       "    'title': 'Eigenoption Discovery through the Deep Successor Representation'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '0ab3f7ecbdc5a33565a234215604a6ca9d155a33',\n",
       "    'title': 'Rainbow: Combining Improvements in Deep Reinforcement Learning'},\n",
       "   {'paperId': '55f730385016802dd6452a34d0271b5c47ded332',\n",
       "    'title': 'Deep Abstract Q-Networks'},\n",
       "   {'paperId': '485552d2711868b54d5fcddc92c746b09afeab07',\n",
       "    'title': 'Long Text Generation via Adversarial Training with Leaked Information'},\n",
       "   {'paperId': '3b290ffa1f4f8226e326f00984acecdfbe9e28bf',\n",
       "    'title': 'Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents'},\n",
       "   {'paperId': 'e7815702c62779c47ce09bbb569f1f13dcb3b3a1',\n",
       "    'title': 'Learning to Design Games: Strategic Environments in Reinforcement Learning'},\n",
       "   {'paperId': 'ee1ac4a86cafa34e4905327f3436fea2d5994fc8',\n",
       "    'title': 'The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning'},\n",
       "   {'paperId': '3ea3631df5e992b601cc82d55b9efaebb62717d6',\n",
       "    'title': 'Deep Reinforcement Learning in Complex Structured Environments'},\n",
       "   {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'},\n",
       "   {'paperId': '63d2c9d6f6aab3a897af07d2ae5f7a132139686c',\n",
       "    'title': 'Cooperative versus Adversarial Learning: Generating Political Text'},\n",
       "   {'paperId': '2bd506e55ffe397c768b4f816e3027f5b9b632e1',\n",
       "    'title': 'Hierarchy-Driven Exploration for Reinforcement Learning'},\n",
       "   {'paperId': '962f2e63d9066deb9ef07867af70d304ae9e8c1c',\n",
       "    'title': 'CSCRS Road Safety Fellowship Report: A Human-Machine Collaborative Acceleration Controller Attained from Pixel Learning and Evolution Strategies'},\n",
       "   {'paperId': '5918dae72b6b1a758df18573de5c5cca33808d6a',\n",
       "    'title': 'Automated Content Generation with Semantic Analysis and Deep Learning'},\n",
       "   {'paperId': '924187223cf271b7d369b0629d5298e2ea3e1e79',\n",
       "    'title': 'Exploring Bipedal Walking Through Machine Learning Techniques'},\n",
       "   {'paperId': '342d2239ed949e442248462eb90145575494f894',\n",
       "    'title': 'Abstractive Text Summarization Using Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '1b93209238542728082641bac37ceeb7b9a4e8c4',\n",
       "    'title': 'Search in Continuous Action Domains : an Overview'},\n",
       "   {'paperId': '4d2683e8c2055e13e8739707428239c995c7a5e3',\n",
       "    'title': 'Learning a Semantic Prior for Guided Navigation'},\n",
       "   {'paperId': '1b0f13a1a18e8375194b6af2618212dbde23c411',\n",
       "    'title': 'Recent progress of deep reinforcement learning : from AlphaGo to AlphaGo Zero'},\n",
       "   {'paperId': 'd101d8f3a3cf2782ead5ff2e52f4443172833786',\n",
       "    'title': 'Abstractive Document Summarisation using Generative Adversarial Networks'},\n",
       "   {'paperId': '7cba1fb6edb762c3db8d7b8ab169dbe9c12bb28b',\n",
       "    'title': 'The Importance of Sampling in Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'f879f5fead23cf17ca106fb50d4d56bde0082d79',\n",
       "    'title': 'Learning and Optimization for Mixed Autonomy Systems - A Mobility Context'},\n",
       "   {'paperId': '4155ecb89086261704bae0040abcf326c41c21f8',\n",
       "    'title': 'Extending the Hierarchical Deep Reinforcement Learning framework'},\n",
       "   {'paperId': '92b26d7673fd77d6be8da55f88506c268bae408f',\n",
       "    'title': 'PROXIMITY REWARD INDUCTION'},\n",
       "   {'paperId': 'b65a6be07ce9c86797e6917258cf5ba45273ee73',\n",
       "    'title': 'NON-PARAMETRIC DISCRIMINATIVE REWARDS'},\n",
       "   {'paperId': 'ad65a10d18b162524e78f023c5fa0d10ceb56888',\n",
       "    'title': 'Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning'},\n",
       "   {'paperId': '99bcbc1f2b2a563285dc473be7ee9d50721f5f53',\n",
       "    'title': 'Human-level performance in first-person multiplayer games with population-based deep reinforcement learning'},\n",
       "   {'paperId': '1b47776ecc194616d5ae789357ac69b1298e47ae',\n",
       "    'title': 'Frames CNN Low-level Encoder ( Bi-LSTM ) High-level Encoder ( LSTM ) Worker Manager Internal Critic Environment segment signal goal state reward action HRL Agent context context'},\n",
       "   {'paperId': '77a31a4601444a3f7aeed15061b08684d0bea92b',\n",
       "    'title': 'Exploration-Exploitation Trade-off in Deep Reinforcement Learning'},\n",
       "   {'paperId': '619f889757ff23288ac3472896156af62fde748a',\n",
       "    'title': 'EARNING-M ODULES FOR A TARI B ALL G AMES'},\n",
       "   {'paperId': 'fd1ed642617c362acd072e70cf8c8ea229430b42',\n",
       "    'title': 'Revisiting the Master-Slave Architecture in Multi-Agent Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ef06f7016db8bcd5c5a53442a4aba1d1911a26dd',\n",
       "    'title': 'Hierarchical Actor-Critic'},\n",
       "   {'paperId': '37b5980cf1a202fbec2fec28b831b0f90b8d217a',\n",
       "    'title': 'Learning to Compose Skills'},\n",
       "   {'paperId': 'af10f3c1c0859aa620623f760c8a29e78f177f7f',\n",
       "    'title': 'Population Based Training of Neural Networks'},\n",
       "   {'paperId': 'bb35b5042ac824fb8a4c53b7932f6ee1c4cffc5e',\n",
       "    'title': 'Transferring Agent Behaviors from Videos via Motion GANs'},\n",
       "   {'paperId': '442319260d82ae8f7dec016849e566fd15cd520f',\n",
       "    'title': 'Saliency-based Sequential Image Attention with Multiset Prediction'},\n",
       "   {'paperId': '05893041d24dd404963960e73220aca83d19add4',\n",
       "    'title': 'Deep Reinforcement Learning: A Brief Survey'},\n",
       "   {'paperId': '778f8258bad0620b996666d883ce261216558ddd',\n",
       "    'title': 'Learning to Imagine Manipulation Goals for Robot Task Planning'},\n",
       "   {'paperId': '52b18b9b31d942b8fc83dc69db097557c881a641',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Parameters'},\n",
       "   {'paperId': '717826fd123486f7d57971880262977bcdea1d23',\n",
       "    'title': 'Map-based Multi-Policy Reinforcement Learning: Enhancing Adaptability of Robots by Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': '2dad7e558a1e2982d0d42042021f4cde4af04abf',\n",
       "    'title': 'Dilated Recurrent Neural Networks'},\n",
       "   {'paperId': '7741597530af2d9a9b02a2b3b8b0932c65c9c63e',\n",
       "    'title': 'Towards modeling the learning process of aviators using deep reinforcement learning'},\n",
       "   {'paperId': '3159165e8a454da67ca6a1c13118646225888286',\n",
       "    'title': 'Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'd91e0d62626dd1e33521b110e9702710ed1e5088',\n",
       "    'title': 'Learning to Maximize Return in a Stag Hunt Collaborative Scenario through Deep Reinforcement Learning'},\n",
       "   {'paperId': '498238a3bd5fd322fc3ce1572e33bbe3853a356f',\n",
       "    'title': 'A Brief Survey of Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ba058cfc0272bb115883fe9dcf1f92af515a91ab',\n",
       "    'title': 'General AI Challenge - Round One: Gradual Learning'},\n",
       "   {'paperId': '07d639436125ee204202cb0b34a081cde31e5214',\n",
       "    'title': 'Neural Adaptive Video Streaming with Pensieve'},\n",
       "   {'paperId': 'c410191139503bed47688c9bef0eddec68aaf59d',\n",
       "    'title': 'Learning to Design Games: Strategic Environments in Deep Reinforcement Learning'},\n",
       "   {'paperId': '429ed4c9845d0abd1f8204e1d7705919559bc2a2',\n",
       "    'title': 'Hindsight Experience Replay'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '4248b1c782d1e3e3b53a5126ea269518af92c68a',\n",
       "    'title': 'Beating Atari with Natural Language Guided Reinforcement Learning'},\n",
       "   {'paperId': '437dbe6b7dfc2485331732cb8ed4314369cd9721',\n",
       "    'title': 'Multi-Advisor Reinforcement Learning'},\n",
       "   {'paperId': '9f1e9e56d80146766bc2316efbc54d8b770a23df',\n",
       "    'title': 'Deep Reinforcement Learning: An Overview'},\n",
       "   {'paperId': '34da1ee1aea9e7e0575a7dd7a065dd7ba1dd76f2',\n",
       "    'title': 'CS 234 Project Final Report : Approaches to Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '97c634c3b85ef4d78451cca6efb8811617f4ff2f',\n",
       "    'title': 'Learning Goal-Directed Behaviour'},\n",
       "   {'paperId': '88c8bbf045e94fe1c8574f9cb80813d31f2bf85b',\n",
       "    'title': 'Sparse Negative Feedback for Cooperative Inverse Reinforcement Learning'},\n",
       "   {'paperId': '75c3c36a12689d3725466020ced3d701954a735b',\n",
       "    'title': 'MULTI-AGENT DEEP REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '52c2b4ab0aa08c006aa17cf8b7eb4eedf61d0ff6',\n",
       "    'title': 'A Dichotomy of Visual Relations'},\n",
       "   {'paperId': '5dbf9c29db4f66166a854e1a8a9d2af99358101d',\n",
       "    'title': 'Effective Master-Slave Communication On A Multi-Agent Deep Reinforcement Learning System'},\n",
       "   {'paperId': '4fd9c795f950868ce58a506c38e195f83ccb74ac',\n",
       "    'title': 'Hierarchical Task Generalization with Neural Programs'},\n",
       "   {'paperId': '364117dceeb7da768f52ea8251b0de4d625dc8f8',\n",
       "    'title': 'A Modular Deep-learning Environment for Rogue'},\n",
       "   {'paperId': 'c243e5efeee5f5499bafc27219f1359d1966583f',\n",
       "    'title': 'Deep Reinforcement Learning for Long Term Strategy Games'},\n",
       "   {'paperId': 'e4bc529ced68fae154e125c72af5381b1185f34e',\n",
       "    'title': 'PERCEPTUAL GOAL SPECIFICATIONS FOR REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '1172510fbdeed860aad0d3a976d8739297a92c5c',\n",
       "    'title': 'AN INFERENCE-BASED POLICY GRADIENT METHOD'},\n",
       "   {'paperId': '720c98ac522084cb0a51823f9fb6b50bc4b910c5',\n",
       "    'title': 'Knowledge Representation and Reasoning with Deep Neural Networks'},\n",
       "   {'paperId': '42b5a0f5072a21e6f59be48e0be7c98bed29050a',\n",
       "    'title': 'Seeing is believing : Contrastive Hebbian Clustering for unsupervised one-shot gameplay learning in a recurrent neural network'},\n",
       "   {'paperId': '882502c4c795f2b76e4c6a8343b199d216066353',\n",
       "    'title': 'The 2nd Learning from Limited Labeled Data (LLD) Workshop: Representation Learning for Weak Supervision and Beyond'}],\n",
       "  'citnuminlist': 15,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " '3deecaee4ec1a37de3cb10420eaabff067669e17': {'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning',\n",
       "  'year': 2016,\n",
       "  'references': [{'paperId': 'bcd857d75841aa3e92cd4284a8818aba9f6c0c3f',\n",
       "    'title': 'Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS'},\n",
       "   {'paperId': '29e944711a354c396fad71936f536e83025b6ce0',\n",
       "    'title': 'Categorical Reparameterization with Gumbel-Softmax'},\n",
       "   {'paperId': '515a21e90117941150923e559729c59f5fdade1c',\n",
       "    'title': 'The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables'},\n",
       "   {'paperId': '8fab7d7dfd233fd5d19bc2641b4c1ca74fc7bc6a',\n",
       "    'title': 'Learning modular neural network policies for multi-task and multi-robot transfer'},\n",
       "   {'paperId': '15b26d8cb35d7e795c8832fe08794224ee1e9f84',\n",
       "    'title': 'The Option-Critic Architecture'},\n",
       "   {'paperId': 'e2bd18c1039f27675bd64014117db648d969452e',\n",
       "    'title': 'Learning and Transfer of Modulated Locomotor Controllers'},\n",
       "   {'paperId': '136cf66392f1d6bf42da4cc070888996dc472b91',\n",
       "    'title': 'On Multiplicative Integration with Recurrent Neural Networks'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '35da0a2001eea88486a5de677ab97868c93d0824',\n",
       "    'title': 'InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets'},\n",
       "   {'paperId': 'fddc15480d086629b960be5bff96232f967f2252',\n",
       "    'title': 'Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding'},\n",
       "   {'paperId': '6e90fd78e8a3b98af3954aae5209703aa966603e',\n",
       "    'title': 'Unifying Count-Based Exploration and Intrinsic Motivation'},\n",
       "   {'paperId': '317cd4522b1f4a6f889743578143bb8823623f8b',\n",
       "    'title': 'VIME: Variational Information Maximizing Exploration'},\n",
       "   {'paperId': '1464776f20e2bccb6182f183b5ff2e15b0ae5e56',\n",
       "    'title': 'Benchmarking Deep Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': '846aedd869a00c09b40f1f1f35673cb22bc87490',\n",
       "    'title': 'Mastering the game of Go with deep neural networks and tree search'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "    'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': '1389772b8a0f9c7fc43057f9da41a7d0ebf0308b',\n",
       "    'title': 'Generalization and Exploration via Randomized Value Functions'},\n",
       "   {'paperId': 'ce2e0bd9135814f4018106bc31d87902b358e251',\n",
       "    'title': 'Variational Information Maximizing Exploration'},\n",
       "   {'paperId': 'a696aeab7b4c6bb47630663e7638fc0f60b584b8',\n",
       "    'title': 'Nonparametric Bayesian reward segmentation for skill discovery using inverse reinforcement learning'},\n",
       "   {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "    'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       "   {'paperId': 'bb1a17010254abfa5e1f2a17553582ce449f8e16',\n",
       "    'title': 'Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images'},\n",
       "   {'paperId': 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d',\n",
       "    'title': 'Human-level control through deep reinforcement learning'},\n",
       "   {'paperId': '66cdc28dc084af6507e979767755e99fe0b46b39',\n",
       "    'title': 'Trust Region Policy Optimization'},\n",
       "   {'paperId': 'a2785f66c20fbdf30ec26c0931584c6d6a0f4fca',\n",
       "    'title': 'DRAW: A Recurrent Neural Network For Image Generation'},\n",
       "   {'paperId': 'b6cc21b30912bdaecd9f178d700a4c545b1d0838',\n",
       "    'title': 'Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning'},\n",
       "   {'paperId': '512ea8d0c5b5de896129e76d4276f7b996fe88d8',\n",
       "    'title': 'Learning Stochastic Feedforward Neural Networks'},\n",
       "   {'paperId': 'b9e4fc24106bfced345c9cd6e24695a1e5c2e483',\n",
       "    'title': 'Autonomous reinforcement learning with hierarchical REPS'},\n",
       "   {'paperId': '225fbfd99465033e993460a1bc838a87fbf42346',\n",
       "    'title': 'Gaussian-Bernoulli deep Boltzmann machine'},\n",
       "   {'paperId': '8101ec9a994551edfdc7c79ebc89ed939cd07eb3',\n",
       "    'title': 'Hierarchical Relative Entropy Policy Search'},\n",
       "   {'paperId': '2ffc1cbe7488ba3d054b482bac5edf9d272cf99c',\n",
       "    'title': 'Autonomous Skill Acquisition on a Mobile Manipulator'},\n",
       "   {'paperId': '33224ad0cdf6e2dc4893194dd587309c7887f0ba',\n",
       "    'title': 'Formal Theory of Creativity, Fun, and Intrinsic Motivation (1990–2010)'},\n",
       "   {'paperId': '274f4ad2f9be649c297eba6bffa97540599569df',\n",
       "    'title': 'Bayesian Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': 'a9c7e2a415cac344e1b4b3126bd43f6f53264e3e',\n",
       "    'title': 'Intrinsically Motivated Hierarchical Skill Learning in Structured Environments'},\n",
       "   {'paperId': '467568f1777bc51a15a5100516cd4fe8de62b9ab',\n",
       "    'title': 'Transfer Learning for Reinforcement Learning Domains: A Survey'},\n",
       "   {'paperId': '2a8a076c26875208d52c66e07aa7f6db9a4f34b7',\n",
       "    'title': 'Representational Power of Restricted Boltzmann Machines and Deep Belief Networks'},\n",
       "   {'paperId': 'ab19a482195f4299f96b98e4eb15cb3ad4753f3b',\n",
       "    'title': 'Multi-task reinforcement learning: a hierarchical Bayesian approach'},\n",
       "   {'paperId': '16050a256dd6add1e9187e8c4f5c30c85f342fd8',\n",
       "    'title': 'Building Portable Options: Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '8978cf7574ceb35f4c3096be768c7547b28a35d0',\n",
       "    'title': 'A Fast Learning Algorithm for Deep Belief Nets'},\n",
       "   {'paperId': '1c59bfa0e8654ebea94277064f82062875cae8b6',\n",
       "    'title': 'Identifying useful subgoals in reinforcement learning by local graph partitioning'},\n",
       "   {'paperId': 'cc45fa649a3153a61182222f496eb38554caf2bc',\n",
       "    'title': 'A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems'},\n",
       "   {'paperId': '12d6fde053e2c7174a76fe1bbdb97dd039a3b662',\n",
       "    'title': 'Intrinsically Motivated Reinforcement Learning'},\n",
       "   {'paperId': '42af0ed020c2caecafb7dbe826064d7f9ba2022b',\n",
       "    'title': 'Dynamic abstraction in reinforcement learning via clustering'},\n",
       "   {'paperId': '82673205bf76c6fc788790308bc14a9a2d8e41ad',\n",
       "    'title': 'Learning Movement Primitives'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': '9360e5ce9c98166bb179ad479a9d2919ff13d022',\n",
       "    'title': 'Training Products of Experts by Minimizing Contrastive Divergence'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '52e2ac397f0c8d5f533959905df899bc328d9f85',\n",
       "    'title': 'Reinforcement Learning with Hierarchies of Machines'},\n",
       "   {'paperId': 'a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5',\n",
       "    'title': 'Connectionist Learning of Belief Networks'},\n",
       "   {'paperId': '94db34f4b68189bfcba22beab33ee3b54f10b876',\n",
       "    'title': 'Curious model-building control systems'},\n",
       "   {'paperId': '4f7476037408ac3d993f5088544aab427bc319c1',\n",
       "    'title': 'Information processing in dynamical systems: foundations of harmony theory'}],\n",
       "  'citations': [{'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "    'title': 'NetHack is Hard to Hack'},\n",
       "   {'paperId': 'e91bdc6d78cef19648c468acf9bf16a3905a5008',\n",
       "    'title': 'Constraint‐based multi‐agent reinforcement learning for collaborative tasks'},\n",
       "   {'paperId': '85bc55ba9ab93c09713b0891bbcf0541b8f27ea9',\n",
       "    'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7',\n",
       "    'title': 'Behavior Contrastive Learning for Unsupervised Skill Discovery'},\n",
       "   {'paperId': '71c508996120b06b2a056d9d073f6e6c60a671a6',\n",
       "    'title': 'Progressive Transfer Learning for Dexterous In-Hand Manipulation with Multi-Fingered Anthropomorphic Hand'},\n",
       "   {'paperId': 'f811132d3a737ee1c71b52706f0cd78b8904f056',\n",
       "    'title': 'Learning Diverse Policies with Soft Self-Generated Guidance'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1f346f74e8eabececa4896d734ab9b261f30830d',\n",
       "    'title': 'Modular Deep Learning'},\n",
       "   {'paperId': 'ff106b68484cc45a296390d14bb2cb88b75a1e0d',\n",
       "    'title': 'CERiL: Continuous Event-based Reinforcement Learning'},\n",
       "   {'paperId': 'd5781022f211bc2bc8eaeeb574e0fef86a58ec45',\n",
       "    'title': 'PushWorld: A benchmark for manipulation planning with tools and movable obstacles'},\n",
       "   {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "    'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       "   {'paperId': '3ecbf75ca51133eb59a66ddb28d057a14dd538c1',\n",
       "    'title': 'Reusable Options through Gradient-based Meta Learning'},\n",
       "   {'paperId': '2be3222b6b9888746d5239c35ef867b10d7228a6',\n",
       "    'title': 'Planning Irregular Object Packing via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6ace8342b5bb52e5db1bc7f6bc42b4c4c4d7b938',\n",
       "    'title': 'A Context-based Multi-task Hierarchical Inverse Reinforcement Learning Algorithm'},\n",
       "   {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "    'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '18d02042724a7fe29ae6aa9460353d7e2425ca86',\n",
       "    'title': 'Matching options to tasks using Option-Indexed Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'beb6cc3a60d1c511b869e4bb2c744cec99cd3ef1',\n",
       "    'title': 'Adjacency Constraint for Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '988dae20df8d69869aa41097a05d821446cff621',\n",
       "    'title': 'DisTop: Discovering a Topological representation to learn diverse and rewarding skills'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': 'ec1e08dc5cbec362ea0a331e32bfa3d30e47c24f',\n",
       "    'title': 'Learning Landmark-Oriented Subgoals for Visual Navigation Using Trajectory Memory'},\n",
       "   {'paperId': '9fa422789f9c3ae2a11aa6c624d8e23382fd07cd',\n",
       "    'title': 'CandyRL: A Hybrid Reinforcement Learning Model for Gameplay'},\n",
       "   {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "    'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': '23d9e8319f58451a20fac7fc1316c4e664d66eb1',\n",
       "    'title': 'Knowing the Past to Predict the Future: Reinforcement Virtual Learning'},\n",
       "   {'paperId': '594cc7ce6690ad6d3dd79ea57391ab1bd4d41119',\n",
       "    'title': 'Goal Exploration Augmentation via Pre-trained Skills for Sparse-Reward Long-Horizon Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '1cc13cff6f12d450457f51eb8d5d8e20bce47b56',\n",
       "    'title': 'Skill-Based Reinforcement Learning with Intrinsic Reward Matching'},\n",
       "   {'paperId': '0c0b7fb0066e8c1a5a2b4e4856135650eeef7702',\n",
       "    'title': 'Causality-driven Hierarchical Structure Discovery for Reinforcement Learning'},\n",
       "   {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "    'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'a67a926508e06212423c8d598f13c139dc053f1c',\n",
       "    'title': 'Versatile Skill Control via Self-supervised Adversarial Imitation of Unlabeled Mixed Motions'},\n",
       "   {'paperId': '75175ec0a794875a1b089f6de6ed57e04f352168',\n",
       "    'title': 'Towards Run-time Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '88f48480ffb3c36683a63b0f6d5932b40bfbef64',\n",
       "    'title': 'Celebrating Robustness in Efficient Off-Policy Meta-Reinforcement Learning'},\n",
       "   {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "    'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       "   {'paperId': '8e21576387f46f1b9090bdbff1ceadf187feeada',\n",
       "    'title': 'CompoSuite: A Compositional Reinforcement Learning Benchmark'},\n",
       "   {'paperId': 'e032a053dc934cb938c79c8adab8fde38a9eb157',\n",
       "    'title': 'Variational Diversity Maximization for Hierarchical Skill Discovery'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '4014bf79a220a09d1e380624adff53f5314a7e41',\n",
       "    'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '6d8f5366a04ed1955bb62759369c938f05da7f27',\n",
       "    'title': 'Open-Ended Reinforcement Learning with Neural Reward Functions'},\n",
       "   {'paperId': '8ada79148436cb18ed4fe84d1b2165047447462a',\n",
       "    'title': 'ASC me to Do Anything: Multi-task Training for Embodied AI'},\n",
       "   {'paperId': 'f28a97e857b7857291665c98ba7ba414d64da9c4',\n",
       "    'title': 'SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': 'e617b03e14bbdde9f0fb3f9fabca0dd29c91f174',\n",
       "    'title': 'GrASP: Gradient-Based Affordance Selection for Planning'},\n",
       "   {'paperId': '1e5b39d523392d234484aa43318329114541b527',\n",
       "    'title': 'Leveraging Approximate Symbolic Models for Reinforcement Learning via Skill Diversity'},\n",
       "   {'paperId': '9bf925ecb1e6c6bfeecfc15aec1d0c6d7c28e135',\n",
       "    'title': 'CIC: Contrastive Intrinsic Control for Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'a3b82fd7fd06daefb15c4c057d483250e1c139bd',\n",
       "    'title': 'Transfering Hierarchical Structure with Dual Meta Imitation Learning'},\n",
       "   {'paperId': '33e3f13087abd5241d55523140720f5e684b7bee',\n",
       "    'title': 'Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': 'fa66e527a84666355bb50323a12c4f79147ea3b1',\n",
       "    'title': 'RLOps: Development Life-Cycle of Reinforcement Learning Aided Open RAN'},\n",
       "   {'paperId': '0213fa01c7b8aa7668b11fd9edf283fe10d5719e',\n",
       "    'title': 'Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning'},\n",
       "   {'paperId': 'a01fae01cd9a3067fa6b8a777e70efe86bdc4699',\n",
       "    'title': 'Direct then Diffuse: Incremental Unsupervised Skill Discovery for State Covering and Goal Reaching'},\n",
       "   {'paperId': '90b237b41ca5541f7e7d0337fe053be4e6cee853',\n",
       "    'title': 'Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via Multi-Agent Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8eb73addbf8c2b52637af040755cf3ca13cdbf40',\n",
       "    'title': 'Training Transition Policies via Distribution Matching for Complex Tasks'},\n",
       "   {'paperId': '40888b859c5b40868943162e3c4769dae1aed716',\n",
       "    'title': 'The Information Geometry of Unsupervised Reinforcement Learning'},\n",
       "   {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "    'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "    'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       "   {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "    'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       "   {'paperId': '3f037bef6e3acc7c66c5aca8ca6426f984a719c9',\n",
       "    'title': 'Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': 'dde41671ebd2a16408d0e61c37efca60b7720895',\n",
       "    'title': 'Toward Human Cognition-inspired High-Level Decision Making For Hierarchical Reinforcement Learning Agents'},\n",
       "   {'paperId': '959a5b0021c44690c92426df0c516e0ab7a997f3',\n",
       "    'title': 'EAT-C: Environment-Adversarial sub-Task Curriculum for RL'},\n",
       "   {'paperId': '800a1917c57c5701cc974e5498ad27a61ae0f292',\n",
       "    'title': 'Exploring Long-Horizon Reasoning with Deep RL in Combinatorially Hard Tasks'},\n",
       "   {'paperId': 'ff33db486b561f36617bfee1d55fbd42d3045969',\n",
       "    'title': 'O PEN -E NDED R EINFORCEMENT L EARNING WITH N EU RAL R EWARD F UNCTIONS'},\n",
       "   {'paperId': 'eedca44bc8bd69e4b62742306ff70582984ca7c5',\n",
       "    'title': 'Optimizing Multi-Agent Coordination via Hierarchical Graph Probabilistic Recursive Reasoning'},\n",
       "   {'paperId': '7319249a853288dce45ca331a87dce052a2abfba',\n",
       "    'title': 'Disentangling Controlled Effects for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '1af3755f1f66df99184c0ab3d92590e28aaf9cba',\n",
       "    'title': 'Agile Control For Quadruped Robot In Complex Environment Based on Deep Reinforcement Learning Method'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '27bc680bf6a115cc3f28c4da462b6d25cf04cb09',\n",
       "    'title': 'Landmark-Guided Subgoal Generation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1718384deb5a0298c31b902fff4e0caba6aaf298',\n",
       "    'title': 'Adversarial Socialbot Learning via Multi-Agent Deep Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '56796e5fcf12f9bcdfc4aef96c12b97b6c63746e',\n",
       "    'title': 'Shared Trained Models Selection and Management for Transfer Reinforcement Learning in Open IoT'},\n",
       "   {'paperId': '98d0821e7165c1f5e08123e46efea804dddfb577',\n",
       "    'title': 'Braxlines: Fast and Interactive Toolkit for RL-driven Behavior Engineering beyond Reward Maximization'},\n",
       "   {'paperId': 'e5d128eee302958cca0d744046ceed841c5b3d8e',\n",
       "    'title': 'Pick Your Battles: Interaction Graphs as Population-Level Objectives for Strategic Diversity'},\n",
       "   {'paperId': '125b570984b6ee3867794d158587b9b43788d640',\n",
       "    'title': 'Self-supervised Reinforcement Learning with Independently Controllable Subgoals'},\n",
       "   {'paperId': 'a8262348003a9d93ea7ceffc887729cf88dedf06',\n",
       "    'title': 'Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms'},\n",
       "   {'paperId': '61e6674bcdf297dda6744c8fe69cbc0d8ec6006c',\n",
       "    'title': 'Learning Meta Representations for Agents in Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '6481e73b66577788be2d90619e0de55e78516e51',\n",
       "    'title': 'Collect & Infer - a fresh look at data-efficient Reinforcement Learning'},\n",
       "   {'paperId': 'a30904d356f61dea1a1966571dbec8d2375e862e',\n",
       "    'title': 'The Multi-Dimensional Actions Control Approach for Obstacle Avoidance Based on Reinforcement Learning'},\n",
       "   {'paperId': '0ff1247950819a6beafa369178c6c9489de3ceaa',\n",
       "    'title': 'Unsupervised Discovery of Transitional Skills for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e2dc201c06500b30ac337d7e6e85e1e76955dd4a',\n",
       "    'title': 'Unsupervised Skill-Discovery and Skill-Learning in Minecraft'},\n",
       "   {'paperId': '2d84f9f1483a73acff0f349826c6bc0ac4025075',\n",
       "    'title': 'Adaptable Agent Populations via a Generative Model of Policies'},\n",
       "   {'paperId': 'cfcbeec1ae2f7d13ec1576aa30cb98f5326ffa71',\n",
       "    'title': 'Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '107e4ea37d2e5364893107a8ce072972c4a10dfb',\n",
       "    'title': 'Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '4ff9a9248dceb00a7d02073815c085911d5c0a59',\n",
       "    'title': 'Discovering Generalizable Skills via Automated Generation of Diverse Tasks'},\n",
       "   {'paperId': 'a3a6bce70faccd8b33e44a3c9a838289a870f6ec',\n",
       "    'title': 'On Study of Mutual Information and its Estimation Methods'},\n",
       "   {'paperId': '45f573f302dc7e77cbc5d1a74ccbac3564bbebc8',\n",
       "    'title': 'PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training'},\n",
       "   {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "    'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '26d601215e16b7b69e3ee2f88282312ba4577519',\n",
       "    'title': 'Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning'},\n",
       "   {'paperId': '5c37023c35fc1c95565d56b4fc4821fcf768651a',\n",
       "    'title': 'Reward is enough for convex MDPs'},\n",
       "   {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "    'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       "   {'paperId': '4f9f09d1ab684b145627f5cbad5560f364e51559',\n",
       "    'title': 'Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning'},\n",
       "   {'paperId': '32a921c37bc5d4d9b1b906f812e7a41fe5689085',\n",
       "    'title': 'Scalable, Decentralized Multi-Agent Reinforcement Learning Methods Inspired by Stigmergy and Ant Colonies'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': '17051a6fb19dfa224bd4c4de5825ea15d765e723',\n",
       "    'title': 'Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning'},\n",
       "   {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "    'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "    'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '46130875c8c2d89ea23dfb29c3784a6e5e510e54',\n",
       "    'title': 'Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning'},\n",
       "   {'paperId': '38fa4bd7e944b73d128b93f2d279416f93074222',\n",
       "    'title': 'Diverse Auto-Curriculum is Critical for Successful Real-World Multiagent Learning Systems'},\n",
       "   {'paperId': '1adbeb95eae6bad58425cdb40565e627032f72aa',\n",
       "    'title': 'State-Aware Variational Thompson Sampling for Deep Q-Networks'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '37a1050c1c5dab50c30d14f34ae00c723f06eb9f',\n",
       "    'title': 'Continuous Transition: Improving Sample Efficiency for Continuous Control Problems via MixUp'},\n",
       "   {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "    'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0a4b550ec609a54a27f1b47fc3a228fbee040fb3',\n",
       "    'title': 'Distilling a Hierarchical Policy for Planning and Control via Representation and Reinforcement Learning'},\n",
       "   {'paperId': '0d6a4e45acde6f47d704ed0752f17f7ab52223af',\n",
       "    'title': 'Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning'},\n",
       "   {'paperId': '1213756da1b99702ab45b4745b6053365686f4ca',\n",
       "    'title': 'A development cycle for automated self-exploration of robot behaviors'},\n",
       "   {'paperId': 'c88c99fc89a32883384b5a629a8905504e42ac72',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning'},\n",
       "   {'paperId': 'a2e0f316b9bfe24f66464edb55a8615b01f38904',\n",
       "    'title': 'Trajectory Diversity for Zero-Shot Coordination'},\n",
       "   {'paperId': '530f1b3f399f4bdee1d009e1bc2824ec861f6fba',\n",
       "    'title': 'Generalized Reinforcement Learning for Gameplay'},\n",
       "   {'paperId': '5546d2a7ab3b7a8150b72e641af90a199537f7a8',\n",
       "    'title': 'E NVIRONMENT -A DVERSARIAL SUB -T ASK C URRICU LUM FOR E FFICIENT R EINFORCEMENT L EARNING A BSTRACT'},\n",
       "   {'paperId': '567c75aa0c146f92db19134806b5b41266b13f59',\n",
       "    'title': 'V ALUE F UNCTION S PACES : S KILL -C ENTRIC S TATE A BSTRACTIONS FOR L ONG -H ORIZON R EASONING'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '599fcb9ca476098f33334852cc360bae9f1f7ee2',\n",
       "    'title': '[Appendix] Unsupervised Skill Discovery with Bottleneck Option Learning'},\n",
       "   {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       "   {'paperId': '13c4a790dd099ded14d424df332b10195ad1ee14',\n",
       "    'title': 'Discovery and Learning of Navigation Goals from Pixels in Minecraft'},\n",
       "   {'paperId': '0cff1642794d9628b6e5388a4af97bf9a77ccdba',\n",
       "    'title': 'L ATENT S KILL P LANNING FOR E XPLORATION AND T RANSFER'},\n",
       "   {'paperId': '637cb5b797697930d7d1577031134801af99f83d',\n",
       "    'title': 'Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning'},\n",
       "   {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "    'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       "   {'paperId': '4d1537347d8f5c463188166ae96c3c0d7a3260fa',\n",
       "    'title': 'Skill Transfer via Partially Amortized Hierarchical Planning'},\n",
       "   {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "    'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       "   {'paperId': 'b667641dc6acd7c0233503615942ea00ea9875f5',\n",
       "    'title': 'Continual Learning of Control Primitives: Skill Discovery via Reset-Games'},\n",
       "   {'paperId': '3da314388876286aacb6d9b355439ee68700576d',\n",
       "    'title': 'Automatische Programmierung von Produktionsmaschinen'},\n",
       "   {'paperId': '2be22d8a3f39c7bcd1c7639b849e640a0003c831',\n",
       "    'title': 'Harnessing Distribution Ratio Estimators for Learning Agents with Quality and Diversity'},\n",
       "   {'paperId': '7fad5ce5ef04b84d3cee1ab79f16532b93c8aad5',\n",
       "    'title': 'BSE-MAML: Model Agnostic Meta-Reinforcement Learning via Bayesian Structured Exploration'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '07d251a8d721b5f3da6cc8a92e75840b563927f2',\n",
       "    'title': 'Maximum-Entropy Adversarial Data Augmentation for Improved Generalization and Robustness'},\n",
       "   {'paperId': '79f054f309a5e104aac046522346e53ad4fc7fd5',\n",
       "    'title': 'Temporal Difference Uncertainties as a Signal for Exploration'},\n",
       "   {'paperId': '560e9a8a1b027adbefb838ca337aa73af7998ca9',\n",
       "    'title': 'Disentangling causal effects for hierarchical reinforcement learning'},\n",
       "   {'paperId': 'f22d1a958c5fe235d454469a82f2e4d61d2aaeac',\n",
       "    'title': 'Robust RL-Based Map-Less Local Planning: Using 2D Point Clouds as Observations'},\n",
       "   {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "    'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       "   {'paperId': '6be61525ee8b21c3bef6564df17b435fc4f84282',\n",
       "    'title': 'Action and Perception as Divergence Minimization'},\n",
       "   {'paperId': 'cbd2935be2cfc61ead29d3af9529519e8a5f4172',\n",
       "    'title': 'OCEAN: Online Task Inference for Compositional Tasks with Context Adaptation'},\n",
       "   {'paperId': '50e0d675bc64e4648b5ceda1268f00cc9c3269c6',\n",
       "    'title': 'The formation and use of hierarchical cognitive maps in the brain: A neural network model'},\n",
       "   {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "    'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       "   {'paperId': '019820cbb73d0651a913bb74cbfb713c8ad772df',\n",
       "    'title': 'ELSIM: End-to-end learning of reusable skills through intrinsic motivation'},\n",
       "   {'paperId': '41382835ae60fb3280ea9a5b3004a236af1eb01b',\n",
       "    'title': 'CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information'},\n",
       "   {'paperId': '361ccae6cb40343c8824c9d64104ff8261a7c089',\n",
       "    'title': 'Generating Adjacency-Constrained Subgoals in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "    'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       "   {'paperId': '99bc5a19a5aa9f85e2c1ecdc9bf8add7b75dae45',\n",
       "    'title': 'Novel Policy Seeking with Constrained Optimization'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': 'b4873e3a17058c81a8d2bba838cdd0c415ee80e7',\n",
       "    'title': 'Guided Uncertainty-Aware Policy Optimization: Combining Learning and Model-Based Strategies for Sample-Efficient Policy Learning'},\n",
       "   {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "    'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'e014af8ae8d7dbd3c1c908dfba334a6d2181b8e1',\n",
       "    'title': 'Task-oriented Dialogue System for Automatic Disease Diagnosis via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "    'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'f423e28e9a42c0a6a37e5a09fe490d5f27c464fe',\n",
       "    'title': 'Particle-Based Adaptive Discretization for Continuous Control using Deep Reinforcement Learning'},\n",
       "   {'paperId': 'bbac680797af0f7ce4cdcc6430ff001fa0dfe670',\n",
       "    'title': 'Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations'},\n",
       "   {'paperId': '845aeb9dcf12efba0760c6eb2e2ac56ea27f3247',\n",
       "    'title': 'Option Discovery in the Absence of Rewards with Manifold Analysis'},\n",
       "   {'paperId': '11236ae4f31b428b6313559fb99300643c172cf9',\n",
       "    'title': 'Meta-learning curiosity algorithms'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "    'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       "   {'paperId': '886b604f74d65bb420bc1311e9647c44fe8fa91e',\n",
       "    'title': 'Temporal-adaptive Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '7b0871c783e721bfbf9b5d16e575130a07a672cd',\n",
       "    'title': 'Generalized Hindsight for Reinforcement Learning'},\n",
       "   {'paperId': '5a30917733db939b004d6a637a5d316373914af4',\n",
       "    'title': 'Combining primitive DQNs for improved reinforcement learning in Minecraft'},\n",
       "   {'paperId': '81a5864c21bf8e4018ac9004d618ccb99e261965',\n",
       "    'title': 'Efficient hindsight reinforcement learning using demonstrations for robotic tasks with sparse rewards'},\n",
       "   {'paperId': '3f787ac280c3d0dbb16eb2456e62ffbf1a56ff62',\n",
       "    'title': 'Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation'},\n",
       "   {'paperId': '46106fcd08223540d080f674b26770c1fa8a52ff',\n",
       "    'title': 'Influence-Based Multi-Agent Exploration'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': '4cd909c37ee27a97b57f36bd0cad1a2fcb23c441',\n",
       "    'title': 'Deep Reinforcement Learning with Adaptive Update Target Combination'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'edcd3a5e8e0e6fd27f95c34348404ea449b6927d',\n",
       "    'title': 'Mega-Reward: Achieving Human-Level Play without Extrinsic Rewards'},\n",
       "   {'paperId': 'a7859b059cfe01d01f1bd795e86eb3f0771fb53b',\n",
       "    'title': 'Model primitives for hierarchical lifelong reinforcement learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '7388826b5ee00efe17cb7f19a623d9b5e955ae70',\n",
       "    'title': 'Skew-Fit: State-Covering Self-Supervised Reinforcement Learning'},\n",
       "   {'paperId': '5330b2732c12c7027811869a921f544a0bf581ca',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks'},\n",
       "   {'paperId': 'ec684b9cf2433680f6bd70779186f34bcd5b4f06',\n",
       "    'title': 'MaxEnt Reward Expected Reward Latent Representations Missing Data Controllable Future Factorized Target Perception Action Both Low Entropy Preferences Empowerment Skill Discovery Amortized Inference Maximum Likelihood Variational Inference Input Density Exploration Information GainFiltering Latent S'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '728cfe9697d7f7a9940dca17a4045fd10d6c0bf4',\n",
       "    'title': 'IHRL: Interactive Influence-based Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd802b949ff627d51fcb148685256f9fd25f848d5',\n",
       "    'title': 'Hindsight Planner'},\n",
       "   {'paperId': '839ea5421bc5515f1465d49613972b64cce61302',\n",
       "    'title': 'Effective, interpretable algorithms for curiosity automatically discovered by evolutionary search'},\n",
       "   {'paperId': 'c5e1cbf8e76fb074bb666c695763cefb16381000',\n",
       "    'title': 'Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning'},\n",
       "   {'paperId': 'd9e14b81e7acf2e17de20df113018943978509ae',\n",
       "    'title': 'Inter-Level Cooperation in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0d2bda1c16a1e4907175df2a1041068ccbe701ae',\n",
       "    'title': 'Learning Stochastic Weight Masking to Resist Adversarial Attacks'},\n",
       "   {'paperId': '6fcc8c61041b495c82d339d0cb17147bee2cf0e1',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery'},\n",
       "   {'paperId': 'd861cab02cbb314fa7f1e14103a238d66e5d8809',\n",
       "    'title': 'Research on Learning Method Based on Hierarchical Decomposition'},\n",
       "   {'paperId': 'f58a77a92b795241943b7ff1740dfcc58039589c',\n",
       "    'title': 'TendencyRL: Multi-stage Discriminative Hints for Efficient Goal-Oriented Reverse Curriculum Learning'},\n",
       "   {'paperId': '3f116db230b58e6908f5d5791c9761260ad2d7b2',\n",
       "    'title': 'Graph-Based Design of Hierarchical Reinforcement Learning Agents'},\n",
       "   {'paperId': '38eb1086cebd6d92b061d8282dc22d6cf4181b6b',\n",
       "    'title': 'Multi-controller multi-objective locomotion planning for legged robots'},\n",
       "   {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       "   {'paperId': '701e5d53031ebcd6955dc6da1bd9525e392fe770',\n",
       "    'title': 'Imagined Value Gradients: Model-Based Policy Optimization with Transferable Latent Dynamics Models'},\n",
       "   {'paperId': '5225b6a414254e6d8e4a474d84011a974b5eb6ba',\n",
       "    'title': 'Markov Information Bottleneck to Improve Information Flow in Stochastic Neural Networks'},\n",
       "   {'paperId': '68d4d8a1b2cf589de9f116cf748c9e8f11ab852f',\n",
       "    'title': 'MAVEN: Multi-Agent Variational Exploration'},\n",
       "   {'paperId': 'df3ac75ec8ad937b7e1d43d6e4f40aa0cfa6bc01',\n",
       "    'title': 'Playing Atari Ball Games with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c52f87830d6c0f9d7e61defa695bd65a8aca067',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks'},\n",
       "   {'paperId': 'd85cc7cd2e424f598c447c72473078957e55b74b',\n",
       "    'title': 'Skew-Explore: Learn faster in continuous spaces with sparse rewards'},\n",
       "   {'paperId': '89f11c2e2bfc1af9ec3e703bb62c4547c4de33bd',\n",
       "    'title': 'Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes'},\n",
       "   {'paperId': '5a44e0877ae4d3b6322164a02d204429604e3daf',\n",
       "    'title': 'Construction of Macro Actions for Deep Reinforcement Learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '57c84b34c8516659d8e1aec2be736f9adce61878',\n",
       "    'title': 'Stochastic Activation Actor Critic Methods'},\n",
       "   {'paperId': '7723086a4a09c39a1cd91c62b2234cdf72dc1f63',\n",
       "    'title': 'Multi-Task Hierarchical Imitation Learning for Home Automation'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': 'a77df5291ee644022067f34eec790ea31380792b',\n",
       "    'title': 'Are You for Real? Detecting Identity Fraud via Dialogue Interactions'},\n",
       "   {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "    'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       "   {'paperId': 'f5cf9f849b67d19b6bb378fe14433d7d8f4b4ddd',\n",
       "    'title': 'Constructive Policy: Reinforcement Learning Approach for Connected Multi-Agent Systems'},\n",
       "   {'paperId': '127a8f944a8010f768aac9d01cdba5548456b217',\n",
       "    'title': 'Skill based transfer learning with domain adaptation for continuous reinforcement learning domains'},\n",
       "   {'paperId': '3944f15e0c2d39d114e5a44aedd079630607521e',\n",
       "    'title': 'Semantic RL with Action Grammars: Data-Efficient Learning of Hierarchical Task Abstractions'},\n",
       "   {'paperId': '6225f9fe891b233db76cef6d48ef786a7ecaffc5',\n",
       "    'title': 'Hybrid system identification using switching density networks'},\n",
       "   {'paperId': 'fad3d8d8ba17799f362ba1436cf0dccb00d12cee',\n",
       "    'title': 'Online Multi-modal Imitation Learning via Lifelong Intention Encoding'},\n",
       "   {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "    'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "    'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       "   {'paperId': '6d399bff0205977f51ff2334168c89320206493d',\n",
       "    'title': 'Goal-conditioned Imitation Learning'},\n",
       "   {'paperId': '390c7c230c223498c281a204006c5fc141759460',\n",
       "    'title': 'Transfer Learning by Modeling a Distribution over Policies'},\n",
       "   {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "    'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       "   {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "    'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': '6b36776e5c0473d82cbdd2c92cd97cca7925ae08',\n",
       "    'title': 'TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '7bd95a62fd6320730cbb24a0e4fafac97d840652',\n",
       "    'title': 'Meta Reinforcement Learning with Task Embedding and Shared Policy'},\n",
       "   {'paperId': '074b4702465630da81a3b40600183e58a42c404d',\n",
       "    'title': 'Apprentissage séquentiel budgétisé pour la classification extrême et la découverte de hiérarchie en apprentissage par renforcement. (Budgeted sequential learning for extreme classification and for the discovery of hierarchy in reinforcement learning)'},\n",
       "   {'paperId': '27d5c1a6b35a591739c10a04e5fc1a96b10e3177',\n",
       "    'title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning'},\n",
       "   {'paperId': 'fb960d0ea21ff4f1c51444dd6e644efeb16b5dd7',\n",
       "    'title': 'Continual and Multi-task Reinforcement Learning With Shared Episodic Memory'},\n",
       "   {'paperId': '6570c7cab46d2b0f3315f5abfbbd209140529c8b',\n",
       "    'title': 'Hierarchical Policy Learning is Sensitive to Goal Space Design'},\n",
       "   {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "    'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       "   {'paperId': 'be928f91385999fa90d1e2fe06058f9dbcfd7186',\n",
       "    'title': 'Routing Networks and the Challenges of Modular and Compositional Computation'},\n",
       "   {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "    'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': '70a3f24292bdcf6e630d5b32eacf93aa3f913c59',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent networks.'},\n",
       "   {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "    'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       "   {'paperId': '7d5dd17e1a10c0388d2e134dedcfe4e8b224352b',\n",
       "    'title': 'Self-supervised Learning of Image Embedding for Continuous Control'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751',\n",
       "    'title': 'Deep Reinforcement Learning'},\n",
       "   {'paperId': 'a88d54c168a84ed8a04d2a32be0b5939586b5792',\n",
       "    'title': 'NADPEx: An on-policy temporally consistent exploration method for deep reinforcement learning'},\n",
       "   {'paperId': '15819e90da9565c1eefc7c5e5d5a1f94767cdd04',\n",
       "    'title': 'Unsupervised Control Through Non-Parametric Discriminative Rewards'},\n",
       "   {'paperId': '1f26116ff8758190e4a5d7fa65f137b2b4befed1',\n",
       "    'title': 'InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics'},\n",
       "   {'paperId': '5bdf7fde60ad8cb413eab7201c28c796d5f23698',\n",
       "    'title': 'Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications'},\n",
       "   {'paperId': '653bdfb3c35621ee04ee5d5253dc7e3a422d69e1',\n",
       "    'title': 'Learning Self-Imitating Diverse Policies'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "    'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': 'af3ecce4aa6ba955d25fa2f7455a82c9b2b328cf',\n",
       "    'title': 'UvA-DARE (Digital Academic Repository) Stochastic Activation Actor Critic Methods'},\n",
       "   {'paperId': '5c6a0b7cc9f2455de3494baf92b07a10a711b679',\n",
       "    'title': 'UvA-DARE (Digital Academic Repository) Stochastic Activation Actor Critic Methods Stochastic Activation Actor Critic Methods ∗'},\n",
       "   {'paperId': '3eda1c2c8b6727ce5f32576a6001d7ef9b722f71',\n",
       "    'title': 'Stochastic Activation Actor Critic Methods*'},\n",
       "   {'paperId': '02feb390612858d745ce324303bfc8f9d0148c42',\n",
       "    'title': 'Building structured hierarchical agents'},\n",
       "   {'paperId': 'c558c56245d4e5b3ba1cb3ba650cb6e7f4bd0cbc',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, Sujoy; van Baar, Jeroen; Roy-Chowdhury, Amit K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       "   {'paperId': '822cd314662479bc4bad911a3768d921614fcaa3',\n",
       "    'title': 'WHY DOES HIERARCHY (SOMETIMES) WORK'},\n",
       "   {'paperId': '7fb2f8458dc50b0317e8431862345fb313cd28fa',\n",
       "    'title': 'Modern Optimization for Statistics and Learning'},\n",
       "   {'paperId': 'a3a8861363fdfbd6a0792fdffb64517e366fea01',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, S.; van Baar, J.; Roy Chowdhury, A.K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       "   {'paperId': 'e7069f324d16847e9939bded648032e54c8c9331',\n",
       "    'title': '10-708 Final Report:Investigating Max-Entropy Latent-Space Policiesfor Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b668ba0900ddcdacd0a07ff9983172f525c3c4d6',\n",
       "    'title': 'Goal-conditioned Imitation Learning'},\n",
       "   {'paperId': '5654028d5193dbf8eaa5ab9ef6af21f6da265878',\n",
       "    'title': 'SKEW-FIT: STATE-COVERING SELF-SUPERVISED RE-'},\n",
       "   {'paperId': '4c19fcb0f8c041996c7cbb0deab15fbec5d1b5d9',\n",
       "    'title': 'Meta-Learning via Weighted Gradient Update'},\n",
       "   {'paperId': '98b41528c58e6f5b7b28be5b54029e52ca90c4ab',\n",
       "    'title': 'Learning to Learn: Hierarchical Meta-Critic Networks'},\n",
       "   {'paperId': '39995a05908b23249025de5a6c3c439d0dd33d9d',\n",
       "    'title': 'MUTUAL-INFORMATION REGULARIZATION'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': '4b61c25a86083c20730c9b12737ac6ac4178c364',\n",
       "    'title': 'An Introduction to Deep Reinforcement Learning'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '415e3cf34d45f92a0515bb85611f099d86e92e1f',\n",
       "    'title': 'Learning Physically Based Humanoid Climbing Movements'},\n",
       "   {'paperId': '0a01766797da6701034a9b4947bb2201ef2f3380',\n",
       "    'title': 'Hierarchical reinforcement learning of multiple grasping strategies with human instructions'},\n",
       "   {'paperId': '97b802a9b094594f5778682ccaa56863280a00d5',\n",
       "    'title': 'An Approach to Hierarchical Deep Reinforcement Learning for a Decentralized Walking Control Architecture'},\n",
       "   {'paperId': 'a83eeb55896c963cc56244335eca2d4fee5f7a99',\n",
       "    'title': 'Interactive Reinforcement Learning from Demonstration and Human Evaluative Feedback'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': '4c03497f2e17900cbf4066fbf68a7cbaad8376be',\n",
       "    'title': 'Representational efficiency outweighs action efficiency in human program induction'},\n",
       "   {'paperId': '3aadab924520c58be81781aafd51e6807e9c4576',\n",
       "    'title': 'Visual Reinforcement Learning with Imagined Goals'},\n",
       "   {'paperId': '2e7b6e73398af01bc975e9bf9374ee5f255252b3',\n",
       "    'title': 'VFunc: a Deep Generative Model for Functions'},\n",
       "   {'paperId': 'd2eaa230a68d38e9fe508dc8f2e712712e978cdc',\n",
       "    'title': 'Budgeted Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1cb6edbedc4a1ac5c32f61a435a23264e42a9071',\n",
       "    'title': 'Towards Sample Efficient Reinforcement Learning'},\n",
       "   {'paperId': '9bc8fdfd393701dbfdd64e0a6f8bb9a1f66804c3',\n",
       "    'title': 'Marginal Policy Gradients for Complex Control'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "    'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5c55162deeea9870f765e0a9cd3e8387b05a0ea2',\n",
       "    'title': 'Disentangling the independently controllable factors of variation by interacting with the world'},\n",
       "   {'paperId': '68c108795deef06fa929d1f6e96b75dbf7ce8531',\n",
       "    'title': 'Meta-Reinforcement Learning of Structured Exploration Strategies'},\n",
       "   {'paperId': 'a8ef08940341381390d9a5672546354d0ce51328',\n",
       "    'title': 'Maximum a Posteriori Policy Optimisation'},\n",
       "   {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "    'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '90d3f103b6b03accff2799cb2bf8ca95d3d71669',\n",
       "    'title': 'Hierarchical Learning for Modular Robots'},\n",
       "   {'paperId': '6938ddf69008ac0a13afd5855c854c8a7520adc5',\n",
       "    'title': 'Hierarchical Policy Search via Return-Weighted Density Estimation'},\n",
       "   {'paperId': '58fb60c5592224901a26dd84220a2f3332c1fcf5',\n",
       "    'title': 'Eigenoption Discovery through the Deep Successor Representation'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': '64643ab9a5f70945c2b171558e121006a99d27a2',\n",
       "    'title': 'NADPE X : A N ON-POLICY TEMPORALLY CONSISTENT EXPLORATION METHOD FOR DEEP REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '5beaeff056549019926075746f8c4f78e30494b0',\n",
       "    'title': 'EARNING AN E MBEDDING S PACE FOR T RANSFERABLE R OBOT S KILLS'},\n",
       "   {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'},\n",
       "   {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '2f6bf6b57cba8be7aa4631645cc7313824bfb674',\n",
       "    'title': 'Slowness-based neural visuomotor control with an Intrinsically motivated Continuous Actor-Critic'},\n",
       "   {'paperId': '4155ecb89086261704bae0040abcf326c41c21f8',\n",
       "    'title': 'Extending the Hierarchical Deep Reinforcement Learning framework'},\n",
       "   {'paperId': 'b65a6be07ce9c86797e6917258cf5ba45273ee73',\n",
       "    'title': 'NON-PARAMETRIC DISCRIMINATIVE REWARDS'},\n",
       "   {'paperId': '4ab08b2f1193d770c241b41f5d9f1c841a3663d3',\n",
       "    'title': 'Optimizing Chemical Reactions with Deep Reinforcement Learning'},\n",
       "   {'paperId': '7f64121eaf74b8204e0445e804f93f3b53a0a64a',\n",
       "    'title': 'The Eigenoption-Critic Framework'},\n",
       "   {'paperId': 'd53d49ec21c372d4781d54346c95faa17b332e98',\n",
       "    'title': 'Layer-wise Learning of Stochastic Neural Networks with Information Bottleneck'},\n",
       "   {'paperId': '72e87d27e8b3493981daca533b3956fae8b4f316',\n",
       "    'title': 'Learning Robot Skill Embeddings'},\n",
       "   {'paperId': '7a5196d05b145ec552912dccedd16a42c88718f1',\n",
       "    'title': 'Learning Skill Embeddings for Transferable Robot Skills'},\n",
       "   {'paperId': '52b18b9b31d942b8fc83dc69db097557c881a641',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Parameters'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': 'd672baf56986a3bc5748c25362b2d2b4d65efcb8',\n",
       "    'title': 'Multi-task Learning with Gradient Guided Policy Specialization'},\n",
       "   {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "    'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       "   {'paperId': '97b16661aada70a28d2a791cf597427e2aa0ad33',\n",
       "    'title': 'Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets'},\n",
       "   {'paperId': '0c2954912936b59162881374164fe79e7b2bb66f',\n",
       "    'title': 'Discrete Sequential Prediction of Continuous Actions for Deep RL'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '850d78496304829d16d14701e4d81692f088f47d',\n",
       "    'title': 'EX2: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       "   {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "    'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       "   {'paperId': '9f1e9e56d80146766bc2316efbc54d8b770a23df',\n",
       "    'title': 'Deep Reinforcement Learning: An Overview'},\n",
       "   {'paperId': '4eb38b3460606a4042b04fc52d0044ab948b4a17',\n",
       "    'title': 'EX: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f8a257006599de5899506959de5f4a8a1b2d2fec',\n",
       "    'title': 'Options Discovery with Budgeted Reinforcement Learning'}],\n",
       "  'citnuminlist': 14,\n",
       "  'refnuminlist': 2,\n",
       "  'isKeypaper': True},\n",
       " '15b26d8cb35d7e795c8832fe08794224ee1e9f84': {'title': 'The Option-Critic Architecture',\n",
       "  'year': 2016,\n",
       "  'references': [{'paperId': '6cdc632729ddff58ff1b541f9ef3177246370fd8',\n",
       "    'title': 'Probabilistic inference for determining options in reinforcement learning'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': '0a0316936d335fa74860e1099e97e69c092ee457',\n",
       "    'title': 'Hierarchical Reinforcement Learning using Spatio-Temporal Abstractions and Deep Neural Networks'},\n",
       "   {'paperId': 'd37620e6f8fe678a43e12930743281cd8cca6a66',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation'},\n",
       "   {'paperId': 'edff4138184b4f37590649b9aadc9a627942a5d0',\n",
       "    'title': 'Adaptive Skills Adaptive Partitions (ASAP)'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': 'c4b52c13a8dca0106e56d353c9438ed60f094df6',\n",
       "    'title': 'Approximate Value Iteration with Temporally Extended Actions'},\n",
       "   {'paperId': '687d0e59d5c35f022ce4638b3e3a6142068efc94',\n",
       "    'title': 'Deterministic Policy Gradient Algorithms'},\n",
       "   {'paperId': '943dcbcb2869a3892d319d1098c61e5b140df2ab',\n",
       "    'title': 'Time-Regularized Interrupting Options (TRIO)'},\n",
       "   {'paperId': '2319a491378867c7049b3da055c5df60e1671158',\n",
       "    'title': 'Playing Atari with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'f82e4ff4f003581330338aaae71f60316e58dd26',\n",
       "    'title': 'The Arcade Learning Environment: An Evaluation Platform for General Agents (Extended Abstract)'},\n",
       "   {'paperId': 'b58ce7ceb31508c35366b8fcdc4924a4fe63a5df',\n",
       "    'title': 'Semantically Grounded Learning from Unstructured Demonstrations'},\n",
       "   {'paperId': None, 'title': 'Advantage updating'},\n",
       "   {'paperId': '6797312891a6fdbb1ec71ede57d95670254b4e90',\n",
       "    'title': 'Compositional Planning Using Optimal Option Models'},\n",
       "   {'paperId': '6228ffe7300fcb306d860327857e495c038b09f4',\n",
       "    'title': 'Linear Off-Policy Actor-Critic'},\n",
       "   {'paperId': 'b55dcba008184f55741abc3ab99eeff111d00151',\n",
       "    'title': 'Actor-Critic Reinforcement Learning with Energy-Based Policies'},\n",
       "   {'paperId': None, 'title': 'and Ciosek'},\n",
       "   {'paperId': '21d3c5df0000dd42f82ea4e22c9aa2ef869e55c8',\n",
       "    'title': 'Clustering via Dirichlet Process Mixture Models for Portable Skill Discovery'},\n",
       "   {'paperId': '28b9bb4bcdc2bff85eafd7b4187ecac210eb22ce',\n",
       "    'title': 'Unified Inter and Intra Options Learning Using Policy Gradient Methods'},\n",
       "   {'paperId': '2ffc1cbe7488ba3d054b482bac5edf9d272cf99c',\n",
       "    'title': 'Autonomous Skill Acquisition on a Mobile Manipulator'},\n",
       "   {'paperId': None, 'title': 'and Shimkin'},\n",
       "   {'paperId': 'd4dacdb982bb3ae16c607d225fe54f9c3695ad2d',\n",
       "    'title': 'Optimal policy switching algorithms for reinforcement learning'},\n",
       "   {'paperId': '14adc709a30c7bf8efcb557046bfb0c3a99182fe',\n",
       "    'title': 'Linear options'},\n",
       "   {'paperId': 'bdc5a10aa5805808cfca58ac527ddc23e737bee8',\n",
       "    'title': 'Skill Discovery in Continuous Reinforcement Learning Domains using Skill Chaining'},\n",
       "   {'paperId': None, 'title': 'and Barto'},\n",
       "   {'paperId': '0be90056a9b4f0434c05e640647f4de9ae32b1c5',\n",
       "    'title': 'Skill Characterization Based on Betweenness'},\n",
       "   {'paperId': 'f1a391bab223fc2609717316bec30ae36f8ea448',\n",
       "    'title': 'Natural Actor-Critic'},\n",
       "   {'paperId': 'a91635f8d0e7fb804efd1c38d9c24ee952ba7076',\n",
       "    'title': 'Learning to Predict by the Methods of Temporal Differences'},\n",
       "   {'paperId': 'fd4de76fadddd1cc9a91cea954200c8d656e1dbb',\n",
       "    'title': 'Using relative novelty to identify useful temporal abstractions in reinforcement learning'},\n",
       "   {'paperId': 'ef676a7ebead282f9faabf911742b7b0df5dea42',\n",
       "    'title': 'Convergence rate of linear two-time-scale stochastic approximation'},\n",
       "   {'paperId': '4c915c1eecb217c123a36dc6d3ce52d12c742614',\n",
       "    'title': 'Simple statistical gradient-following algorithms for connectionist reinforcement learning'},\n",
       "   {'paperId': 'dca9444e1c69eee36c0be04703d71114a762c84a',\n",
       "    'title': 'Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning'},\n",
       "   {'paperId': '48bf148ca96f928d762c5be9231f1cdff8090cc7',\n",
       "    'title': 'Learning Options in Reinforcement Learning'},\n",
       "   {'paperId': None, 'title': 'and Precup'},\n",
       "   {'paperId': '03dbb37d2373500115735ed69871e94c7f6b2c5e',\n",
       "    'title': 'Automatic Discovery of Subgoals in Reinforcement Learning using Diverse Density'},\n",
       "   {'paperId': '4c96ca25d889251e20e33d01f24eec175301ab94',\n",
       "    'title': 'Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition'},\n",
       "   {'paperId': '985f2c1baba284e9b7b604b7169a2e2778540fe6',\n",
       "    'title': 'Temporal abstraction in reinforcement learning'},\n",
       "   {'paperId': 'a20f0ce0616def7cc9a87446c228906cd5da093b',\n",
       "    'title': 'Policy Gradient Methods for Reinforcement Learning with Function Approximation'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': 'ac4af1df88e178386d782705acc159eaa0c3904a',\n",
       "    'title': 'Actor-Critic Algorithms'},\n",
       "   {'paperId': '59b50a775542e87f078db35b868ac10ab43d4c75',\n",
       "    'title': 'Learning from delayed rewards'},\n",
       "   {'paperId': '8090121ad488b4af27bc59bf91b62e9c6a6f49c6',\n",
       "    'title': 'Markov Decision Processes: Discrete Stochastic Dynamic Programming'},\n",
       "   {'paperId': '7a09464f26e18a25a948baaa736270bfb84b5e12',\n",
       "    'title': 'On-line Q-learning using connectionist systems'},\n",
       "   {'paperId': '831edc3d67457db83da40d260e93bfd7559347ae',\n",
       "    'title': 'Dyna, an integrated architecture for learning, planning, and reacting'},\n",
       "   {'paperId': '5c8bb027eb65b6d250a22e9b6db22853a552ac81',\n",
       "    'title': 'Learning from delayed rewards'},\n",
       "   {'paperId': '22069cd4504656d3bb85748a4d43be7a4d7d5545',\n",
       "    'title': 'Temporal credit assignment in reinforcement learning'},\n",
       "   {'paperId': '0c189864a38ccc3ebc9ba2b3dcae127e0c2eab4d',\n",
       "    'title': 'Simulation and the Monte Carlo Method (Wiley Series in Probability and Statistics)'}],\n",
       "  'citations': [{'paperId': 'a369626db68f41e18af1e3ce5c78dfe0eeb95955',\n",
       "    'title': 'Hierarchical Multi-Agent Reinforcement Learning with Intrinsic Reward Rectification'},\n",
       "   {'paperId': 'cf1334f2003d2c0467b480641464e36d0d814e22',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Modeling User Novelty-Seeking Intent in Recommender Systems'},\n",
       "   {'paperId': '615962d8969c8e0ffe43319689dce6c50cbf1f29',\n",
       "    'title': 'Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators'},\n",
       "   {'paperId': '6b92d9761db72c30544fddb47d871fecc3169992',\n",
       "    'title': 'Reinforcement Learning in Few-Shot Scenarios: A Survey'},\n",
       "   {'paperId': '211041e7a7086c96583f5312ade0b7866e3f1dc9',\n",
       "    'title': 'Learning disentangled skills for hierarchical reinforcement learning through trajectory autoencoder with weak labels'},\n",
       "   {'paperId': 'ad80306f01e62a9f6e8a222a3e7d230507228be8',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning for Self-Powered Monitoring and Communication Integrated System in High-Speed Railway Networks'},\n",
       "   {'paperId': '32ff7e5ea4ef146cc63fdee23af1cc47e89af095',\n",
       "    'title': 'NetHack is Hard to Hack'},\n",
       "   {'paperId': '427f95de4b8a1fa457d2406ebb1ff3a1b61c86fe',\n",
       "    'title': 'ChemGymRL: An Interactive Framework for Reinforcement Learning for Digital Chemistry'},\n",
       "   {'paperId': 'e9c5d17f49562d694a9aca4a2bfc5728f71413bb',\n",
       "    'title': 'Coagent Networks: Generalized and Scaled'},\n",
       "   {'paperId': '73c5d8fc9106f3f739ca786a01794ef4bd7affa2',\n",
       "    'title': 'Toward Multi-Agent Reinforcement Learning for Distributed Event-Triggered Control'},\n",
       "   {'paperId': '3e45d02b763cb8197369ac0b3ef4a16f4726de85',\n",
       "    'title': 'An Option-Dependent Analysis of Regret Minimization Algorithms in Finite-Horizon Semi-Markov Decision Processes'},\n",
       "   {'paperId': '68a6780da08bf678a2c48fb8c187626f1ea1ded1',\n",
       "    'title': '3D reconstruction based on hierarchical reinforcement learning with transferability'},\n",
       "   {'paperId': '42d6fce6c168c5495f805379235fd22c8dca7b4a',\n",
       "    'title': 'Robot Subgoal-guided Navigation in Dynamic Crowded Environments with Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': 'a8337543a621c50e98fbb3e114aea163f56df843',\n",
       "    'title': 'Learning Failure Prevention Skills for Safe Robot Manipulation'},\n",
       "   {'paperId': '0cceb527d62abaf587268319575595eb6c93bc50',\n",
       "    'title': 'An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework'},\n",
       "   {'paperId': 'f7f4974ceab53c6f51c8ab9a97dfd6fcd070eab0',\n",
       "    'title': 'Double Graph Attention Actor-Critic Framework for Urban Bus-Pooling System'},\n",
       "   {'paperId': '2f1fee5087d47e7a8c71763c22ac784c9565278c',\n",
       "    'title': 'Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward'},\n",
       "   {'paperId': 'b82d57fe520b01e590deb1b9be93fd4592e1a601',\n",
       "    'title': 'Centralized sub-critic based hierarchical-structured reinforcement learning for temporal sentence grounding'},\n",
       "   {'paperId': '5103d2af68fde5031773e3d471bef2130f5ab0ec',\n",
       "    'title': 'Reinforcement Learning with Partial Parametric Model Knowledge'},\n",
       "   {'paperId': '1eab1f32f0e77305ed6922e713a88d0840b67045',\n",
       "    'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ca5c11627c021302d3bd26a12395d15847df37db',\n",
       "    'title': 'Adaptive Skill Coordination for Robotic Mobile Manipulation'},\n",
       "   {'paperId': 'd491aeb4b26626c90e43bfad10c5a5e03de26d95',\n",
       "    'title': 'Towards Solving Fuzzy Tasks with Human Feedback: A Retrospective of the MineRL BASALT 2022 Competition'},\n",
       "   {'paperId': '5f8945ab175f68c39879964f56a9395f7aeb81f5',\n",
       "    'title': 'Learning Exploration Strategies to Solve Real-World Marble Runs'},\n",
       "   {'paperId': '69ac33d050ef6f6e098db6415e1409f0dae6bd3b',\n",
       "    'title': 'Hierarchical Reinforcement Learning Adversarial Algorithm Against Opponent with Fixed Offensive Strategy'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1f346f74e8eabececa4896d734ab9b261f30830d',\n",
       "    'title': 'Modular Deep Learning'},\n",
       "   {'paperId': 'aeaddc24fe0c4179105e483e83cbe1387515914b',\n",
       "    'title': 'Predictable MDP Abstraction for Unsupervised Model-Based RL'},\n",
       "   {'paperId': 'ef213d6543cd995ac6b1dfde7d08a7a120232391',\n",
       "    'title': 'Towards Skilled Population Curriculum for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': 'd4936c52476053d4692d24adb526aeaf93e404b8',\n",
       "    'title': 'Robust Subtask Learning for Compositional Generalization'},\n",
       "   {'paperId': '40bfcb5901a3c9cd9581433fd2eb1906abcfae6d',\n",
       "    'title': 'Hierarchical Learning with Unsupervised Skill Discovery for Highway Merging Applications'},\n",
       "   {'paperId': '9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3',\n",
       "    'title': 'Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents'},\n",
       "   {'paperId': '144bca8c2c09629e4a6289ead32842c02c391821',\n",
       "    'title': 'Multi-agent hierarchical reinforcement learning for energy management'},\n",
       "   {'paperId': '329d809a69eada0521da1b536bb9db9348d51868',\n",
       "    'title': 'Hierarchical Imitation Learning with Vector Quantized Models'},\n",
       "   {'paperId': 'eccf9e5cdda2012172958f5c49204f11933fb250',\n",
       "    'title': 'Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs'},\n",
       "   {'paperId': 'f296d829deabd8e5342da34bb777ffd02e7a3bc1',\n",
       "    'title': 'Deep Laplacian-based Options for Temporally-Extended Exploration'},\n",
       "   {'paperId': 'b53c510769760d2fbcb674ed14bacfe251ae0ebc',\n",
       "    'title': 'Select and Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '5291241f4bcc077844694181ce8722e91cb41e3b',\n",
       "    'title': 'Advanced Reinforcement Learning and Its Connections with Brain Neuroscience'},\n",
       "   {'paperId': 'f403082b101821f5377ae82fe4ed7d02f6abd3ad',\n",
       "    'title': 'A Survey of Meta-Reinforcement Learning'},\n",
       "   {'paperId': 'c1f040f51c0e247d1c520ee86295a38d94e613fd',\n",
       "    'title': 'Enviroment Representations with Bisimulation Metrics for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "    'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       "   {'paperId': '3ecbf75ca51133eb59a66ddb28d057a14dd538c1',\n",
       "    'title': 'Reusable Options through Gradient-based Meta Learning'},\n",
       "   {'paperId': '5f861f96d0234da728de00122934bce792e553fb',\n",
       "    'title': 'Design and user experience analysis of AR intelligent virtual agents on smartphones'},\n",
       "   {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "    'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '3bf488d3129140dea64fe8f0e3339632fab4ebea',\n",
       "    'title': 'Information Optimization and Transferable State Abstractions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '6960168c7720053c5fd3c4b5ca3d5a4651f8e054',\n",
       "    'title': 'Off-Beat Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '6d7af0617b0fc390baddb93816975f1bdb1985cd',\n",
       "    'title': 'Unsupervised Learning of Temporal Abstractions With Slot-Based Transformers'},\n",
       "   {'paperId': 'aea67db056c001e72c993e6607b0e7f4f115f596',\n",
       "    'title': 'A Survey on Recent Advances and Challenges in Reinforcement Learning Methods for Task-oriented Dialogue Policy Learning'},\n",
       "   {'paperId': '0b3e119248286aeedc95330ab7b67999f522d574',\n",
       "    'title': 'Explainable Reinforcement Learning for Broad-XAI: A Conceptual Framework and Survey'},\n",
       "   {'paperId': '988dae20df8d69869aa41097a05d821446cff621',\n",
       "    'title': 'DisTop: Discovering a Topological representation to learn diverse and rewarding skills'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '9539673a4711775f7b8b30830293b6d19b01edcd',\n",
       "    'title': 'Certified Reinforcement Learning with Logic Guidance'},\n",
       "   {'paperId': '0118491c8ee739a11688e8b74c06b9cd8751823b',\n",
       "    'title': 'Learning Potential in Subgoal-Based Reward Shaping'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9274ebb09a17afd4d5e80a714d8afbc7e21bec50',\n",
       "    'title': 'Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents'},\n",
       "   {'paperId': '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       "    'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors'},\n",
       "   {'paperId': 'ca137b4f6ba25ca2f26952044c6a2d06ae3e607f',\n",
       "    'title': 'Cross-Domain Transfer via Semantic Skill Imitation'},\n",
       "   {'paperId': 'e7bd1be9f78242c5ca02a0ac37b7cec0afd0fe00',\n",
       "    'title': 'Extensible Hierarchical Multi-Agent Reinforcement-Learning Algorithm in Traffic Signal Control'},\n",
       "   {'paperId': '36f0db395c45bd042622932ec607dd2a55013d40',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning for VWAP Strategy Optimization'},\n",
       "   {'paperId': '15c820a41247ab28424abdb87dafade36a3b5e64',\n",
       "    'title': 'Learning Options via Compression'},\n",
       "   {'paperId': '13b120fad7ebe58f4d2257a1ee7901c30219feaf',\n",
       "    'title': 'Policy Transfer via Enhanced Action Space'},\n",
       "   {'paperId': 'afd85d75d9ed910f185573a90f74b69379d5f05b',\n",
       "    'title': 'A Brief Review of Recent Hierarchical Reinforcement Learning for Robotic Manipulation'},\n",
       "   {'paperId': '9fa422789f9c3ae2a11aa6c624d8e23382fd07cd',\n",
       "    'title': 'CandyRL: A Hybrid Reinforcement Learning Model for Gameplay'},\n",
       "   {'paperId': 'c5ac20776ab5d8ce2cc6ec64c61907823fc42a54',\n",
       "    'title': 'Assistive Teaching of Motor Control Tasks to Humans'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'e0eb9870a6c105cadd92cae8f5218b2a84955849',\n",
       "    'title': 'Active Task Randomization: Learning Visuomotor Skills for Sequential Manipulation by Proposing Feasible and Novel Tasks'},\n",
       "   {'paperId': 'fbc6a615c97ecc596d18388f137957852ed5c9fe',\n",
       "    'title': 'Control Transformer: Robot Navigation in Unknown Environments through PRM-Guided Return-Conditioned Sequence Modeling'},\n",
       "   {'paperId': '6ea2aa7eb2e11b7c4512d8029d071d2e2a99bb80',\n",
       "    'title': 'Leveraging Sequentiality in Reinforcement Learning from a Single Demonstration'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': '56211d8b1c94032312b3ce4fd1f06bf81db8d7a7',\n",
       "    'title': 'HMDRL: Hierarchical Mixed Deep Reinforcement Learning to Balance Vehicle Supply and Demand'},\n",
       "   {'paperId': 'aa65704a16138790678e2b9b59ae679b6c9353d7',\n",
       "    'title': 'Knowledge-Guided Exploration in Deep Reinforcement Learning'},\n",
       "   {'paperId': '5b44aca33c27001589b107ac7fa11c0298fefafd',\n",
       "    'title': 'Cola-HRL: Continuous-Lattice Hierarchical Reinforcement Learning for Autonomous Driving'},\n",
       "   {'paperId': 'f61d428eeb7d81ca0c6ccec0ddcb30503db282ec',\n",
       "    'title': 'Active Predictive Coding: A Unified Neural Framework for Learning Hierarchical World Models for Perception and Planning'},\n",
       "   {'paperId': '5572521b26ed3efcaafe050975b404fc5e7c89a7',\n",
       "    'title': 'Guided Skill Learning and Abstraction for Long-Horizon Manipulation'},\n",
       "   {'paperId': 'fc5631cdd08722f51e0ce4b718de0f081ae73603',\n",
       "    'title': 'DANLI: Deliberative Agent for Following Natural Language Instructions'},\n",
       "   {'paperId': '09fc037f43fa3fbe7792ad801e71c7e0bd92a386',\n",
       "    'title': 'TAPS: Task-Agnostic Policy Sequencing'},\n",
       "   {'paperId': 'b75359b5b22024ac0aec8b942bbd86bde81f8e70',\n",
       "    'title': 'STAP: Sequencing Task-Agnostic Policies'},\n",
       "   {'paperId': '25d574c88fb3ea5c78358f0a353d6b868c4bc2d2',\n",
       "    'title': 'A Multiagent Cooperative Decision-Making Method for Adaptive Intersection Complexity Based on Hierarchical RL'},\n",
       "   {'paperId': '05c522667f9dee976764f805a2e509f7c05ca80e',\n",
       "    'title': 'RPM: Generalizable Behaviors for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '22b816129ca770df3a88e76f754218b242df43f9',\n",
       "    'title': 'Abstract-to-Executable Trajectory Translation for One-Shot Task Generalization'},\n",
       "   {'paperId': 'c0ca3ec9cf42c067a0123990137f6ee57980ab85',\n",
       "    'title': 'AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments'},\n",
       "   {'paperId': '0c4bac7f78d47b7ba2d21f86d0d9139cad9d4217',\n",
       "    'title': 'Contrastive introspection to identify critical steps in reinforcement learning'},\n",
       "   {'paperId': '550f2484459df844072731fba9b1fc084237b7f0',\n",
       "    'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '458b84efab56e97809f3f5ed1cd8ffdcbc5a403a',\n",
       "    'title': 'HALight: Hierarchical Deep Reinforcement Learning for Cooperative Arterial Traffic Signal Control with Cycle Strategy'},\n",
       "   {'paperId': '849dcd45264e90b6ff39e6d0f02f54c66aa49dcb',\n",
       "    'title': 'Knowledge-Grounded Reinforcement Learning'},\n",
       "   {'paperId': 'da621f54f5c63b8806ab7c5734e80961c8d97748',\n",
       "    'title': 'Multi-agent Deep Covering Option Discovery'},\n",
       "   {'paperId': '21691cbab7d50d9c9f92a133c32d84a4ffc670f2',\n",
       "    'title': 'Learning Dynamic Abstract Representations for Sample-Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'c991259b4dd20daec188d3097d8add1ed6456e45',\n",
       "    'title': 'Learning an Interpretable Learning Rate Schedule via the Option Framework'},\n",
       "   {'paperId': '45bf59416efb0389daaa0b179f28751d2965547d',\n",
       "    'title': 'Multi-Task Option Learning and Discovery for Stochastic Path Planning'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': 'a17a7256c04afee68f9aa0b7bfdc67fbca998b9c',\n",
       "    'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills'},\n",
       "   {'paperId': '3e425dd366e0696d5b835d8fff92add9873077da',\n",
       "    'title': 'On Efficient Reinforcement Learning for Full-length Game of StarCraft II'},\n",
       "   {'paperId': '699901877c89c9d29c88d5067559ba5d0bcf3e41',\n",
       "    'title': 'Asynchronous Actor-Critic for Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': 'bd3a0bbabae3260098e06bfb615147fb6d34e55a',\n",
       "    'title': 'Latent Plans for Task-Agnostic Offline Reinforcement Learning'},\n",
       "   {'paperId': 'a7934cf662959e452636f9f90adf3cc4fe40caa2',\n",
       "    'title': 'MO2: Model-Based Offline Options'},\n",
       "   {'paperId': '7a78a9ea1ee86ca94f03c72924b8531c7fd8a849',\n",
       "    'title': 'Strategies for Scaleable Communication and Coordination in Multi-Agent (UAV) Systems'},\n",
       "   {'paperId': 'cbb4d26ee59f5135b06fe0be1fa981e6f7ee3f32',\n",
       "    'title': 'Hierarchical End-to-end Control Policy for Multi-degree-of-freedom Manipulators'},\n",
       "   {'paperId': 'b83a17f7bd638e6b8b696624718f4371e0981025',\n",
       "    'title': 'BITS: Bi-level Imitation for Traffic Simulation'},\n",
       "   {'paperId': '748c9aa5a31f279fa07b84238aa5ba748e9df40d',\n",
       "    'title': 'Efficient Planning in a Compact Latent Action Space'},\n",
       "   {'paperId': 'ff272b20ad380a2a545f2e7030f5b8b9550c4385',\n",
       "    'title': 'MARTI-4: new model of human brain, considering neocortex and basal ganglia - learns to play Atari game by reinforcement learning on a single CPU'},\n",
       "   {'paperId': '27733a2453246ed0ba1b7e11141b78eef8862c57',\n",
       "    'title': 'DL-DRL: A double-layer deep reinforcement learning approach for large-scale task scheduling of multi-UAV'},\n",
       "   {'paperId': '571cf0f286b3838af89f7bbc7695c6f9f1c5bc23',\n",
       "    'title': 'Hierarchical DDPG for Manipulator Motion Planning in Dynamic Environments'},\n",
       "   {'paperId': '45644c7f952d2a5a5b4e594998e2e6dff9088118',\n",
       "    'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning'},\n",
       "   {'paperId': '8f3166a321dfd629379bb7359898783e800211c2',\n",
       "    'title': 'Graph-Structured Policy Learning for Multi-Goal Manipulation Tasks'},\n",
       "   {'paperId': '27abcfafee4d4af9fd0c420e20257dc303485fc7',\n",
       "    'title': 'Learning Deception Using Fuzzy Multi-Level Reinforcement Learning in a Multi-Defender One-Invader Differential Game'},\n",
       "   {'paperId': '78ab35278d8cb36b88e5889cb20ba8576a93b2c1',\n",
       "    'title': 'HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble'},\n",
       "   {'paperId': '9337d750993d8715c872db8d406480d58464555a',\n",
       "    'title': 'How to Reuse and Compose Knowledge for a Lifetime of Tasks: A Survey on Continual Learning and Functional Composition'},\n",
       "   {'paperId': '2e52648b7c89c41c8fd4c1c1a966a8ef5c874676',\n",
       "    'title': 'Learning Temporally Extended Skills in Continuous Domains as Symbolic Actions for Planning'},\n",
       "   {'paperId': '8e21576387f46f1b9090bdbff1ceadf187feeada',\n",
       "    'title': 'CompoSuite: A Compositional Reinforcement Learning Benchmark'},\n",
       "   {'paperId': 'dd22bdf302c25b834d87b61cb55be9d23c9c2940',\n",
       "    'title': 'Value Refinement Network (VRN)'},\n",
       "   {'paperId': '0b3ca2a700085a877c560e20558566536f18d5a2',\n",
       "    'title': 'Modular Lifelong Reinforcement Learning via Neural Composition'},\n",
       "   {'paperId': '57fca7dd11ac5add0c2e05bd64feef7fb3bfca45',\n",
       "    'title': 'Negative Result for Learning from Demonstration: Challenges for End-Users Teaching Robots with Task And Motion Planning Abstractions'},\n",
       "   {'paperId': 'a94aaf192fc1d46d697e4d7eb3e999021ec88b46',\n",
       "    'title': 'Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning'},\n",
       "   {'paperId': '795a6060f4cd9b61636e13c37ce0adbc6fee06fb',\n",
       "    'title': 'Soft Actor-Critic with Inhibitory Networks for Retraining UAV Controllers Faster'},\n",
       "   {'paperId': '0ab3f612db15a5a986d731283ca52e08058c9c44',\n",
       "    'title': 'Learning Neuro-Symbolic Skills for Bilevel Planning'},\n",
       "   {'paperId': '5051947cbac4223a68edc8e7e7319b5cdb2dc712',\n",
       "    'title': 'N$^2$M$^2$: Learning Navigation for Arbitrary Mobile Manipulation Motions in Unseen and Dynamic Environments'},\n",
       "   {'paperId': 'e032a053dc934cb938c79c8adab8fde38a9eb157',\n",
       "    'title': 'Variational Diversity Maximization for Hierarchical Skill Discovery'},\n",
       "   {'paperId': 'd261db40fe85710dcbd09e91b298f4fe375f293e',\n",
       "    'title': 'Metrics-only Training Neural Network for Switching among an Array of Feedback Controllers for Bicycle Model Navigation'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '4014bf79a220a09d1e380624adff53f5314a7e41',\n",
       "    'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks'},\n",
       "   {'paperId': 'f440137ff4183d3448970b518919a8c14078c04c',\n",
       "    'title': 'Simultaneous Learning and Planning in a Hierarchical Control System for a Cognitive Agent'},\n",
       "   {'paperId': '22fb881bdcdd346cc02c8704b9f54d00a220a7e9',\n",
       "    'title': 'Hierarchies of Reward Machines'},\n",
       "   {'paperId': '2decff836d5a433fa917a1f9e37466a490c84abd',\n",
       "    'title': 'SFP: State-free Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '27a5d5c6d7804737b8a034ca2aee99f8d319754e',\n",
       "    'title': 'Toward Discovering Options that Achieve Faster Planning'},\n",
       "   {'paperId': 'ddedebc48e8c76aec6db486e79363fd4c823650e',\n",
       "    'title': 'Skill Machines: Temporal Logic Composition in Reinforcement Learning'},\n",
       "   {'paperId': '5cc8cd8f13ea88b265713384906e64a7f455f61c',\n",
       "    'title': 'Coordinating Policies Among Multiple Agents via an Intelligent Communication Channel'},\n",
       "   {'paperId': '1fe3400b5da7da6e994ad93ecb179c9aa917b507',\n",
       "    'title': 'Recursive Compositional Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': '14d6d90258b9ef8b640feb80dbbfee2ebb5b0386',\n",
       "    'title': 'Developing Cooperative Policies for Multi-Stage Reinforcement Learning Tasks'},\n",
       "   {'paperId': '5642f71270f1dcadf1e572d94dade012603ac52e',\n",
       "    'title': 'Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets'},\n",
       "   {'paperId': '58750c7e5856c2f95f1f61b853c3d31768e7bc7e',\n",
       "    'title': 'Screening goals and selecting policies in hierarchical reinforcement learning'},\n",
       "   {'paperId': 'baace5e9ecb020fa3202d0405b7165c9f9002934',\n",
       "    'title': 'Hierarchical intrinsically motivated agent planning behavior with dreaming in grid environments'},\n",
       "   {'paperId': 'f1a36b4283e45082183c36ae8f29a77a07f91abd',\n",
       "    'title': 'Demonstration-Bootstrapped Autonomous Practicing via Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': '6343543986bd0b2800f0ef468604b659f0a24ec5',\n",
       "    'title': 'Possibility Before Utility: Learning And Using Hierarchical Affordances'},\n",
       "   {'paperId': '3c148b6c03c3ad5378d8370ee5d00cead7d2aee9',\n",
       "    'title': 'Lazy-MDPs: Towards Interpretable Reinforcement Learning by Learning When to Act'},\n",
       "   {'paperId': 'ca47b92d53f4554495e4452c1057e2a6674c3864',\n",
       "    'title': 'Policy Architectures for Compositional Generalization in Control'},\n",
       "   {'paperId': '69cf6df9b3ad1b99139fb2c900d4a6e03a4c3571',\n",
       "    'title': 'Learning Sensorimotor Primitives of Sequential Manipulation Tasks from Visual Demonstrations'},\n",
       "   {'paperId': '02d31b4a8ecc3f5b3afe1e9dfa680827b0b891d5',\n",
       "    'title': 'Hierarchically Structured Scheduling and Execution of Tasks in a Multi-Agent Environment'},\n",
       "   {'paperId': '1a6c31839b5bfaecea832920dcad3c77bd68e320',\n",
       "    'title': 'Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization'},\n",
       "   {'paperId': 'df60cd6d7dd0224ed36ac2bdee1feb5963764ce8',\n",
       "    'title': 'Erlang planning network: An iterative model-based reinforcement learning with multi-perspective'},\n",
       "   {'paperId': '98833f3240a77fa1cf7f67ab066297227148e2be',\n",
       "    'title': 'Deep latent-space sequential skill chaining from incomplete demonstrations'},\n",
       "   {'paperId': '63c6f3114520349312c276f60bb7b7922d4b744e',\n",
       "    'title': 'AI Planning Annotation for Sample Efficient Reinforcement Learning'},\n",
       "   {'paperId': 'bd6f9099cc29cb0af2a8339e0bb68829965d74c2',\n",
       "    'title': 'Hierarchical Reinforcement Learning with AI Planning Models'},\n",
       "   {'paperId': '76b3ba80ad7a78c887621377dba47d65cfe065a2',\n",
       "    'title': 'E2HRL: An Energy-efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '4cbfea6f45ca3b5110d5b597d7dff73bb482d572',\n",
       "    'title': 'Selective Credit Assignment'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '17925371488c18654404433f4ba12e619ae2e387',\n",
       "    'title': 'Bayesian Nonparametrics for Offline Skill Discovery'},\n",
       "   {'paperId': '41456830c59ba5921eef21dc8eb6494c9a5c7c1e',\n",
       "    'title': 'Reward-Respecting Subtasks for Model-Based Reinforcement Learning'},\n",
       "   {'paperId': '74e3c42e7005a0b371b0ff0cf601cbc8bb3c38bf',\n",
       "    'title': 'Soft Actor-Critic with Inhibitory Networks for Faster Retraining'},\n",
       "   {'paperId': '64c141dbeecdca2c9e494aa2aa518b19e7f71d97',\n",
       "    'title': 'Rethinking Learning Dynamics in RL using Adversarial Networks'},\n",
       "   {'paperId': 'ab2542ae894b0834494c968e14f96bfd7908aa90',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Adversarially Guided Subgoals'},\n",
       "   {'paperId': '64290435a11eee0b4ea5c23cb9937d1e0af957e6',\n",
       "    'title': 'Adversarially Guided Subgoal Generation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '33e3f13087abd5241d55523140720f5e684b7bee',\n",
       "    'title': 'Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning'},\n",
       "   {'paperId': '66cc87463c0f27256a18eb02915460ef0b510c0b',\n",
       "    'title': 'Prospective Learning: Back to the Future'},\n",
       "   {'paperId': '68668755ab1d7cc42392353c905d91e37e3a3481',\n",
       "    'title': 'Fragmented Spatial Maps from Surprisal: State Abstraction and Efficient Planning'},\n",
       "   {'paperId': 'c512d35fd20fbe4612f2bce2b6f5409c8b0a73e1',\n",
       "    'title': 'Automated Reinforcement Learning (AutoRL): A Survey and Open Problems'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': '86774d1910602436b1a1c8c9b25061283461ce29',\n",
       "    'title': 'MDMD options discovery for accelerating exploration in sparse-reward domains'},\n",
       "   {'paperId': 'af72500f529307d4df834f3234c6fc82be1e48c5',\n",
       "    'title': 'Optimization for Master-UAV-Powered Auxiliary-Aerial-IRS-Assisted IoT Networks: An Option-Based Multi-Agent Hierarchical Deep Reinforcement Learning Approach'},\n",
       "   {'paperId': '0998c71ab9c2c8ab22388a597bd3dedb591bb950',\n",
       "    'title': 'Continual Learning In Environments With Polynomial Mixing Times'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '13867baa329020ce45fe7f6cd887ba27e15bcfed',\n",
       "    'title': 'Towards autonomous artificial agents with an active self: Modeling sense of control in situated action'},\n",
       "   {'paperId': '0213fa01c7b8aa7668b11fd9edf283fe10d5719e',\n",
       "    'title': 'Value Function Spaces: Skill-Centric State Abstractions for Long-Horizon Reasoning'},\n",
       "   {'paperId': 'd3c6e0b80c36c14f7d1761fb881f20c35165f507',\n",
       "    'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data'},\n",
       "   {'paperId': 'e354dbb497b51df55c54664fe22af6b009c0bd20',\n",
       "    'title': 'Learning Insertion Primitives with Discrete-Continuous Hybrid Action Space for Robotic Assembly Tasks'},\n",
       "   {'paperId': '8eb73addbf8c2b52637af040755cf3ca13cdbf40',\n",
       "    'title': 'Training Transition Policies via Distribution Matching for Complex Tasks'},\n",
       "   {'paperId': '78674a58297aed34dcaed858532a9abf32a6a538',\n",
       "    'title': 'Augmenting Reinforcement Learning with Behavior Primitives for Diverse Manipulation Tasks'},\n",
       "   {'paperId': '2d4ca959cb3d544473cb661cefe76daabebcdff3',\n",
       "    'title': 'Skill Induction and Planning with Latent Language'},\n",
       "   {'paperId': 'd9212b207e49a3aa6806fb2ddadb303b7b1d47a8',\n",
       "    'title': 'Hierarchical Control of Situated Agents through Natural Language'},\n",
       "   {'paperId': 'e8c61bbc33d9c1ad5d607a4ca2950562e48650bb',\n",
       "    'title': 'Motion planning by learning the solution manifold in trajectory optimization'},\n",
       "   {'paperId': '45afe2d85f2896ce569be0d27678edcff68017e2',\n",
       "    'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans'},\n",
       "   {'paperId': 'fab111f9a2b06cd506e208a64cfa0c4f01a644ea',\n",
       "    'title': 'Orientation-Preserving Rewards’ Balancing in Reinforcement Learning'},\n",
       "   {'paperId': '94cefa04e0f834272d85cc425e0adfb27fd17e08',\n",
       "    'title': 'Same State, Different Task: Continual Reinforcement Learning without Interference'},\n",
       "   {'paperId': '541d2f57590b77e946be8dc1c128826cca461a4a',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '03f4f05139f2e7a543671db7c09bd2d56d1122ce',\n",
       "    'title': 'State-Temporal Compression in Reinforcement Learning With the Reward-Restricted Geodesic Metric'},\n",
       "   {'paperId': '91091391b1f294d7b5082ca7418aeee57b07ddfc',\n",
       "    'title': 'Whole brain Probabilistic Generative Model toward Realizing Cognitive Architecture for Developmental Robots'},\n",
       "   {'paperId': '57b63d540ce77810c18934141e6a2695ad60486c',\n",
       "    'title': 'Discovering diverse solutions in deep reinforcement learning by maximizing state-action-based mutual information'},\n",
       "   {'paperId': 'b9f0d5f62bfb9f1da1fd773609328bcbd529ef2d',\n",
       "    'title': 'WFA-IRL: Inverse Reinforcement Learning of Autonomous Behaviors Encoded as Weighted Finite Automata'},\n",
       "   {'paperId': '262363cc8ea2ae44467531d78b5ac05cc577c542',\n",
       "    'title': 'Learning Skills to Navigate without a Master: A Sequential Multi-Policy Reinforcement Learning Algorithm'},\n",
       "   {'paperId': 'bf1b1d4592e2fc9c32937c802037f4ebc94c2485',\n",
       "    'title': 'Learning Setup Policies: Reliable Transition Between Locomotion Behaviours'},\n",
       "   {'paperId': '9faecf3e18a833f2d49b030d591cc2ded0b54336',\n",
       "    'title': 'Towards Continual Reinforcement Learning: A Review and Perspectives'},\n",
       "   {'paperId': '73e240cb95309142b61db3e9afd9282bf0b5464c',\n",
       "    'title': 'Intrinsically Motivated Goal-Conditioned Reinforcement Learning: a Short Survey'},\n",
       "   {'paperId': '580d85d3ce5475a2931b2147648b1fb79ef03358',\n",
       "    'title': 'Align-RUDDER: Learning From Few Demonstrations by Reward Redistribution'},\n",
       "   {'paperId': '6f4846435e03d09662d5ecd462726f2d9c964915',\n",
       "    'title': 'Continuous Action Reinforcement Learning From a Mixture of Interpretable Experts'},\n",
       "   {'paperId': '7575b6eae80cc22b522c5ae6381cb689454fd645',\n",
       "    'title': 'Option Compatible Reward Inverse Reinforcement Learning'},\n",
       "   {'paperId': 'f56edacf53e7803c5fee527c10ae076ca41eb8ed',\n",
       "    'title': 'H IERARCHICAL K ICKSTARTING FOR S KILL T RANSFER IN R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': 'f93e60579897f557e645a4fe0033f3ba6d61cc57',\n",
       "    'title': 'A Hierarchical Reinforcement Learning Approach to Control Legged Mobile Manipulators'},\n",
       "   {'paperId': '6174a1c497b5d3a683f39bba6904a37c99b488df',\n",
       "    'title': 'Multi-task Policy Learning with Minimal Human Supervision'},\n",
       "   {'paperId': '7cad406f304658ac960b61c37b22140f5fbab435',\n",
       "    'title': 'Hierarchical Agents by Combining Language Generation and Semantic Goal Directed RL'},\n",
       "   {'paperId': 'f5296307c265f0fc8c84c9e17577964b904736f2',\n",
       "    'title': 'Robust Option Learning for Compositional Generalization'},\n",
       "   {'paperId': 'fa72639c74a53ae60e14bb2c802bd3ac681eaef5',\n",
       "    'title': 'Discovering Options by Minimizing the Number of Composed Options to Solve Multiple Tasks'},\n",
       "   {'paperId': '310a77ea80bff7b70a4a31743315c6442aef2119',\n",
       "    'title': 'Unmanned Aerial Vehicle Swarm Cooperative Decision-Making for SEAD Mission: A Hierarchical Multiagent Reinforcement Learning Approach'},\n",
       "   {'paperId': '77b2b707bc416293053d4244d57a035b78444e80',\n",
       "    'title': 'Programmatic Reinforcement Learning without Oracles'},\n",
       "   {'paperId': 'af832cdefc92214968ae91c4cdfd1f7b3cd724e1',\n",
       "    'title': 'Robust Option Learning for Adversarial Generalization'},\n",
       "   {'paperId': '800a1917c57c5701cc974e5498ad27a61ae0f292',\n",
       "    'title': 'Exploring Long-Horizon Reasoning with Deep RL in Combinatorially Hard Tasks'},\n",
       "   {'paperId': '16b4ff5a2281d8aedc5b18d90ca7d754c878a9fe',\n",
       "    'title': 'Hierarchical Actor-Critic Exploration with Synchronized, Adversarial, & Knowledge-Based Actions'},\n",
       "   {'paperId': '474c0bc346a7d13dc513a5038d860f843baf473d',\n",
       "    'title': 'S KILL H ACK : A B ENCHMARK FOR S KILL T RANSFER IN O PEN -E NDED R EINFORCEMENT L EARNING'},\n",
       "   {'paperId': '625464dd926c44eccc153799752109e9dc6f1f4f',\n",
       "    'title': 'Lazy-MDPs: Towards Interpretable RL by Learning When to Act'},\n",
       "   {'paperId': '7319249a853288dce45ca331a87dce052a2abfba',\n",
       "    'title': 'Disentangling Controlled Effects for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'ef777c461af99290142714acd87fd0530c295845',\n",
       "    'title': 'TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'eba9305af250322bc7ee1705f8ab08b9a29ffe59',\n",
       "    'title': 'Explainable artiﬁcial intelligence for autonomous driving: An overview and guide for future research directions'},\n",
       "   {'paperId': 'd513cf54ee71edfff356a1545d4a0ed31d11652e',\n",
       "    'title': 'Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions'},\n",
       "   {'paperId': 'a2e6cb2bcb7f89ded3b097005e5077e01004b33a',\n",
       "    'title': 'Specializing Versatile Skill Libraries using Local Mixture of Experts'},\n",
       "   {'paperId': '7372b0a5f456eba1788e9bd5776c1214f9933bc3',\n",
       "    'title': 'Flexible Option Learning'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '6c3e8be4b9c1e5fa784fa8a844e97dd6dc1db894',\n",
       "    'title': 'Intrinsically motivated option learning: a comparative study of recent methods'},\n",
       "   {'paperId': '8bdb077cfb33bc950b904832988f4729e64c3f0f',\n",
       "    'title': 'Towards safe, explainable, and regulated autonomous driving'},\n",
       "   {'paperId': '4e04f543f4525a7e710b374271ca600359504158',\n",
       "    'title': 'Adversarial Skill Chaining for Long-Horizon Robot Manipulation via Terminal State Regularization'},\n",
       "   {'paperId': '2a33479b41b9ef48b28f2d39207759a945818704',\n",
       "    'title': 'Model-Based Reinforcement Learning for Stochastic Hybrid Systems'},\n",
       "   {'paperId': '0b43ccd62334cd38e3752e92ce0274cf48e3a35e',\n",
       "    'title': 'Spatially and Seamlessly Hierarchical Reinforcement Learning for State Space and Policy space in Autonomous Driving'},\n",
       "   {'paperId': '5a6c6a955fb6b55e4c75593ead1262842b5301c7',\n",
       "    'title': 'Integrating Pretrained Language Model for Dialogue Policy Learning'},\n",
       "   {'paperId': 'e5fb483c877ba5e1c6cd87e8abef20de9260c65d',\n",
       "    'title': 'Learning to Explore by Reinforcement over High-Level Options'},\n",
       "   {'paperId': '5d4b708d50e40422d0cc1459b8ce7de3fdb2515a',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning for Robotic Assembly Control Applications'},\n",
       "   {'paperId': 'f144faf3964d481a1975004b41900ae7b51b573b',\n",
       "    'title': 'Fragmented Spatial Maps: State Abstraction and Efficient Planning from Surprisal'},\n",
       "   {'paperId': '4a8b0e3b9e93c52670062b15cb2a8eae25b035a6',\n",
       "    'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives'},\n",
       "   {'paperId': '78e46f9199d7ea7b0d684fe4db61d2f686c53169',\n",
       "    'title': 'Average-Reward Learning and Planning with Options'},\n",
       "   {'paperId': '13dfb80b184a6568485fbfd11e5b24d51b0f503f',\n",
       "    'title': 'Hierarchical Skills for Efficient Exploration'},\n",
       "   {'paperId': '56796e5fcf12f9bcdfc4aef96c12b97b6c63746e',\n",
       "    'title': 'Shared Trained Models Selection and Management for Transfer Reinforcement Learning in Open IoT'},\n",
       "   {'paperId': '8a17c4a4e6ff86f444c7c6811e307d52f443d149',\n",
       "    'title': 'HAVEN: Hierarchical Cooperative Multi-Agent Reinforcement Learning with Dual Coordination Mechanism'},\n",
       "   {'paperId': 'ccb01a90b16b119db0201ed012f989ed48c68d9f',\n",
       "    'title': 'Temporal Abstraction in Reinforcement Learning with the Successor Representation'},\n",
       "   {'paperId': 'c2ec144b633e2dcbf889da95c711b483803e8350',\n",
       "    'title': 'Hierarchical learning from human preferences and curiosity'},\n",
       "   {'paperId': '7ea33e54c91759fd46cd31f8fe70d727fd985d6e',\n",
       "    'title': 'Reinforcement Learning Under Algorithmic Triage'},\n",
       "   {'paperId': '06858604cc652722ca5092072c50a066000c565e',\n",
       "    'title': 'Example-Driven Model-Based Reinforcement Learning for Solving Long-Horizon Visuomotor Tasks'},\n",
       "   {'paperId': 'a6961a96ada0f00aae3f48f446f708c035c3a8c2',\n",
       "    'title': 'WDIBS: Wasserstein deterministic information bottleneck for state abstraction to balance state-compression and performance'},\n",
       "   {'paperId': '3f55c42b6c8a2a2530e1193d4bcfe998af2daf9d',\n",
       "    'title': 'Competitive physical interaction by reinforcement learning agents using intention estimation'},\n",
       "   {'paperId': '20c588e0f7f2263437e4023c6df503e91f7c2715',\n",
       "    'title': 'Temporally Abstract Partial Models'},\n",
       "   {'paperId': 'a30904d356f61dea1a1966571dbec8d2375e862e',\n",
       "    'title': 'The Multi-Dimensional Actions Control Approach for Obstacle Avoidance Based on Reinforcement Learning'},\n",
       "   {'paperId': '410520764d0d5fa9c9b9a917b5ff0d0722e36810',\n",
       "    'title': 'Hierarchical Reinforcement Learning-Based Policy Switching Towards Multi-Scenarios Autonomous Driving'},\n",
       "   {'paperId': '291a42e844b9bd38c0341c6beec9b34aba3ddde8',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Optimal Level Synchronization based on a Deep Generative Model'},\n",
       "   {'paperId': 'cfcbeec1ae2f7d13ec1576aa30cb98f5326ffa71',\n",
       "    'title': 'Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4ff9a9248dceb00a7d02073815c085911d5c0a59',\n",
       "    'title': 'Discovering Generalizable Skills via Automated Generation of Diverse Tasks'},\n",
       "   {'paperId': 'f9f340c8bd0712780148d0f431cd4914a515f4b1',\n",
       "    'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks'},\n",
       "   {'paperId': '8786bb16bea2e1a4d9952d0abae5f0591959b3b7',\n",
       "    'title': 'Automatic Curricula via Expert Demonstrations'},\n",
       "   {'paperId': '1837ccd0dbb5bf5d485941e8cf9c130edc99276b',\n",
       "    'title': 'Preferential Temporal Difference Learning'},\n",
       "   {'paperId': 'acff9c4e2ad66dd785f75cf91dd0aa442c6cae14',\n",
       "    'title': 'Adversarial Option-Aware Hierarchical Imitation Learning'},\n",
       "   {'paperId': '9776324620082ab89edad134aa259b1ae7bf6f1f',\n",
       "    'title': 'TempoRL: Learning When to Act'},\n",
       "   {'paperId': 'ea6af4f1aa3c81207db292dfbff176fb76d04752',\n",
       "    'title': 'An Energy-Efficient Hardware Accelerator for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '15551a66e463f9a6e9da7265aaef97dfc3f98a34',\n",
       "    'title': 'Learning Routines for Effective Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': '0fac8b753c0bf12ed95ca0ec82e53bba593122e2',\n",
       "    'title': 'Hierarchical Representation Learning for Markov Decision Processes'},\n",
       "   {'paperId': 'd93b090bd7eb49fac0c87ca1761077a86de059d4',\n",
       "    'title': 'Hierarchies of Planning and Reinforcement Learning for Robot Navigation'},\n",
       "   {'paperId': '0ea89ca0929b9323ba3d65bc6bce654d37cdcea4',\n",
       "    'title': 'Hierarchical Learning from Demonstrations for Long-Horizon Tasks'},\n",
       "   {'paperId': 'd8c76fd82257ebc895a954b74e156209292bf06c',\n",
       "    'title': 'Reducing the Deployment-Time Inference Control Costs of Deep Reinforcement Learning Agents via an Asymmetric Architecture'},\n",
       "   {'paperId': '3cfe1f9f3e1a6a31e2ec18c8641dbedafd538f8a',\n",
       "    'title': 'Towered Actor Critic For Handling Multiple Action Types In Reinforcement Learning For Drug Discovery'},\n",
       "   {'paperId': '6360ca733fbbb9ef6d46c0fd94a8124abd89ed91',\n",
       "    'title': 'Coach-Player Multi-Agent Reinforcement Learning for Dynamic Team Composition'},\n",
       "   {'paperId': '6ebeef3f32f0e1e67bded9362dacc01d12bee5c3',\n",
       "    'title': 'DeepSynth: Automata Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ccd751c5456d36ecc5fbd9c5b9b2b3074fd7de44',\n",
       "    'title': 'Learning Contact-aware CPG-based Locomotion in a Soft Snake Robot'},\n",
       "   {'paperId': 'd51fe299b77deb1907216fc1fec3924848f83c31',\n",
       "    'title': 'Regioned Episodic Reinforcement Learning'},\n",
       "   {'paperId': 'e76a973d2bd97b6f4bc59c8759348efa5b126304',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Air-to-Air Combat'},\n",
       "   {'paperId': '5f1adc14a77fb61aa463fac728397bd32e00b617',\n",
       "    'title': 'Challenges of real-world reinforcement learning: definitions, benchmarks and analysis'},\n",
       "   {'paperId': 'e7c33544f157974083e9b106605f417722777352',\n",
       "    'title': 'Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement Learning'},\n",
       "   {'paperId': '66d5863d31b165a493c93c88b257ea06485f7f12',\n",
       "    'title': 'Multitasking Inhibits Semantic Drift'},\n",
       "   {'paperId': 'dac43044fef83b0fca1724675ff4baa78a4baed2',\n",
       "    'title': 'TASAC: Temporally Abstract Soft Actor-Critic for Continuous Control'},\n",
       "   {'paperId': '1508879dae81f73f56ba0cb0e25150d9c5f8f731',\n",
       "    'title': 'TAAC: Temporally Abstract Actor-Critic for Continuous Control'},\n",
       "   {'paperId': '4d551dc543b7832f3e19e64597fa6bbd8486cab4',\n",
       "    'title': 'Reward Shaping with Subgoals for Social Navigation'},\n",
       "   {'paperId': '8ed8f0d3fcf9c774f08ebdfe6aaf31e5922846d4',\n",
       "    'title': 'Subgoal-Based Reward Shaping to Improve Efficiency in Reinforcement Learning'},\n",
       "   {'paperId': 'f3dc8caf1223f3e42ae7c80727e63cef28c8e31d',\n",
       "    'title': 'Reward Shaping with Dynamic Trajectory Aggregation'},\n",
       "   {'paperId': '6f12d91eefef6a28e2392f24dc228af82d59c723',\n",
       "    'title': 'Using options to improve robustness of imitation learning against adversarial attacks'},\n",
       "   {'paperId': '4e93cea327e1420078f09d3377e4ff3e51eade5a',\n",
       "    'title': 'LASER: Learning a Latent Action Space for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "    'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       "   {'paperId': '0b33c826480ab88116bd33a6c21d9665e466ccad',\n",
       "    'title': 'Learning Task Decomposition with Ordered Memory Policy Network'},\n",
       "   {'paperId': '4a5f5189336da3eb8f424b008a201e0cdb63da3b',\n",
       "    'title': 'Hierarchical Reinforcement Learning Framework for Stochastic Spaceflight Campaign Design'},\n",
       "   {'paperId': '761427520e163f79869813122f4ca6eacbe27cbe',\n",
       "    'title': 'Solving Compositional Reinforcement Learning Problems via Task Reduction'},\n",
       "   {'paperId': '318739bebb2e931b3c140d5dd592c6542f6e40a4',\n",
       "    'title': 'Discovering Diverse Solutions in Deep Reinforcement Learning'},\n",
       "   {'paperId': '36a607371e9ebb8d39e218f3711af7711db14c4f',\n",
       "    'title': 'An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning'},\n",
       "   {'paperId': 'ccc61cbfdda84865977a5d3fd2632df1ac08b546',\n",
       "    'title': 'TradeR: Practical Deep Hierarchical Reinforcement Learning for Trade Execution'},\n",
       "   {'paperId': '8343b6f3c8424ac1a8069d31b7a0de1e8f3c40b8',\n",
       "    'title': 'Discovery of Options via Meta-Learned Subgoals'},\n",
       "   {'paperId': 'b5a72093fb370174859f61025d9fc9c05e605e8c',\n",
       "    'title': 'Hierarchical multiagent reinforcement learning schemes for air traffic management'},\n",
       "   {'paperId': 'ec3927ada1ab7a33850056744eb9fc6d17172b5e',\n",
       "    'title': \"Interpretable Reinforcement Learning Inspired by Piaget's Theory of Cognitive Development\"},\n",
       "   {'paperId': '02b2b995a10524dd74678b75cecf44b4aeacaedd',\n",
       "    'title': 'Stay Alive with Many Options: A Reinforcement Learning Approach for Autonomous Navigation'},\n",
       "   {'paperId': '005acb881061eb8137e9d36a05a6a0bdf0026b61',\n",
       "    'title': 'Hierarchical Reinforcement Learning By Discovering Intrinsic Options'},\n",
       "   {'paperId': '59e1f4e89f1a0fde5ecc9edee63a03159089c372',\n",
       "    'title': 'Deciding What to Learn: A Rate-Distortion Approach'},\n",
       "   {'paperId': '2fe9fbe6c5ba35697abb67a2bb8df5e3c3fee0a7',\n",
       "    'title': 'Learning Kinematic Feasibility for Mobile Manipulation Through Deep Reinforcement Learning'},\n",
       "   {'paperId': '25c310d4f7bc581a94455a1e96373d924ec736ae',\n",
       "    'title': 'Reset-Free Lifelong Learning with Skill-Space Planning'},\n",
       "   {'paperId': '22a8ab2f4cd0777ebc93d8e414535c03d4d57615',\n",
       "    'title': 'Latent Skill Planning for Exploration and Transfer'},\n",
       "   {'paperId': '669e1ee28a3b9f9dfb5be0b2d1609b66616769a3',\n",
       "    'title': 'C-Learning: Horizon-Aware Cumulative Accessibility Estimation'},\n",
       "   {'paperId': '350904da231fa0c28e8a6b9358198366db9a2f9b',\n",
       "    'title': 'Revisiting Rainbow: Promoting more insightful and inclusive deep reinforcement learning research'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '31a95c2f8e128cef583f4dad18d65feebd47b50d',\n",
       "    'title': 'Moving Forward in Formation: A Decentralized Hierarchical Learning Approach to Multi-Agent Moving Together'},\n",
       "   {'paperId': '52b884481962fc8dd4e12bdcdcc72b1bb1bf3ca8',\n",
       "    'title': 'Learning When to Switch: Composing Controllers to Traverse a Sequence of Terrain Artifacts'},\n",
       "   {'paperId': '72a2c7140b0d362413a726d8c280205db553d3c4',\n",
       "    'title': 'Abstract Value Iteration for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0a321a38ba98499f17a2423f84972de29a5b2e7f',\n",
       "    'title': 'OPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning'},\n",
       "   {'paperId': '0d44b0b683d9d55ed7c5a17a6b94141a6fc7d147',\n",
       "    'title': 'Efficient Robotic Object Search Via HIEM: Hierarchical Policy Learning With Intrinsic-Extrinsic Modeling'},\n",
       "   {'paperId': '113fd53ae30d1b92e8d5f09b89c518032312b10d',\n",
       "    'title': 'Events and Machine Learning'},\n",
       "   {'paperId': '2244e2af88d4efc911e22cf9dd9a954eb9398607',\n",
       "    'title': 'An empowerment-based solution to robotic manipulation tasks with sparse rewards'},\n",
       "   {'paperId': '72c034e53213cc2f4913d73dd838b64d7b641585',\n",
       "    'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning'},\n",
       "   {'paperId': '3047992223549cc394e201a326eaa3337dc4b4b2',\n",
       "    'title': 'Provable Hierarchical Imitation Learning via EM'},\n",
       "   {'paperId': '8170b0ee2d609a4a7a0f3d5816fea70851b032b9',\n",
       "    'title': 'Learning to Set Waypoints for Audio-Visual Navigation'},\n",
       "   {'paperId': '79d559ce9edd57b813a2776dcca196ac6d807964',\n",
       "    'title': 'Learning Event-triggered Control from Data through Joint Optimization'},\n",
       "   {'paperId': '932c42e1e824734d67e2cb5633fabebe01d89054',\n",
       "    'title': 'Continuous decisions'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': '3c42a4e88b1ecbf72d64dfc5b8908e9a6efca71a',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning Approach for Multi-Objective Scheduling With Varying Queue Sizes'},\n",
       "   {'paperId': '4fd961ec9a6af0c0d15e0a85471d0ff596342ac5',\n",
       "    'title': 'Modelling Hierarchical Structure between Dialogue Policy and Natural Language Generator with Option Framework for Task-oriented Dialogue System'},\n",
       "   {'paperId': 'c6ba8e55f2fed2037378a94a3f8a2b1b09825636',\n",
       "    'title': 'Temporally-Extended ε-Greedy Exploration'},\n",
       "   {'paperId': '11d558cf04914a19068338705526593fe7fb6cd3',\n",
       "    'title': 'Mean-Variance Policy Iteration for Risk-Averse Reinforcement Learning'},\n",
       "   {'paperId': '5646b7e555fc7768db1e3e9a792b59a6553b1d7e',\n",
       "    'title': 'Reinforcement Learning for Combinatorial Optimization: A Survey'},\n",
       "   {'paperId': '01b60237d6e6a2534e3d05bcd40bb00bf79ccd7d',\n",
       "    'title': 'An Efficient Transfer Learning Framework for Multiagent Reinforcement Learning'},\n",
       "   {'paperId': 'c88c99fc89a32883384b5a629a8905504e42ac72',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Control Tasks With Path Planning'},\n",
       "   {'paperId': '24594fd86012b69078ea7283e1334b05a5ce3afb',\n",
       "    'title': 'Safe option-critic: learning safety in the option-critic architecture'},\n",
       "   {'paperId': '557a413341d1c259b36136bb9c2ecdba48a2254a',\n",
       "    'title': 'General Value Function Networks'},\n",
       "   {'paperId': 'c9af30358358b15d05ce72a86ec5f0ce883afdc6',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Single Demonstration'},\n",
       "   {'paperId': 'b84f75270a9c2431ddabe109ccfbd2a14a60a97e',\n",
       "    'title': 'AI Planning Annotation in Reinforcement Learning: Options and Beyond'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '6fcb288aec50e889220dc48fb5c4c8d06334f897',\n",
       "    'title': 'TRAIL: N EAR -O PTIMAL I MITATION L EARNING WITH S UBOPTIMAL D ATA'},\n",
       "   {'paperId': '1916e94c64c3dc855e1779f662c1edc60e5c15e0',\n",
       "    'title': 'Overleaf Example'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '261ada5d3d3a7653faf91194c10f61a098908171',\n",
       "    'title': 'Replay Buffer start Strategy a ) Strategy b ) EncoderEncoder Encoder Encoder EncoderGoal Goal DBTaskDemonstrationsif successful Online Goal Selection'},\n",
       "   {'paperId': '476dfdfae0cf07fdf4c7a3609763cddc1b48a6e7',\n",
       "    'title': 'ORIENTED DIALOGUE SYSTEM'},\n",
       "   {'paperId': 'a328e91fa836b01a985f51cf8c94f719f077afdf',\n",
       "    'title': 'Disagreement Options: Task Adaptation Through Temporally Extended Actions'},\n",
       "   {'paperId': '79a90e7378d4b4a9e3ee9c70973e1bf61816a533',\n",
       "    'title': 'Goal Modelling for Deep Reinforcement Learning Agents'},\n",
       "   {'paperId': 'fc842c477e38dc645b0b14958756bce713791c20',\n",
       "    'title': 'Monotonic Robust Policy Optimization with Model Discrepancy'},\n",
       "   {'paperId': '630b70d8fb6dd3c01983f5a1cefc8b5fbb6af1d3',\n",
       "    'title': 'Anchor: The achieved goal to replace the subgoal for hierarchical reinforcement learning'},\n",
       "   {'paperId': '05a855c8c86d30c5ac76b9d2a6350ff21f8d451b',\n",
       "    'title': 'ManiSkill: Learning-from-Demonstrations Benchmark for Generalizable Manipulation Skills'},\n",
       "   {'paperId': '9ab38aa8fe3f811107d08bc58290597a94f03a46',\n",
       "    'title': 'Self-Supervised Learning of Long-Horizon Manipulation Tasks with Finite-State Task Machines'},\n",
       "   {'paperId': '7e6df3eb4a9c0181f4e787843d5f83a5931eb676',\n",
       "    'title': 'Integrating Multiple Policies for Person-Following Robot Training Using Deep Reinforcement Learning'},\n",
       "   {'paperId': '69b2887a8acf345b94e58e7739eef4c38625a729',\n",
       "    'title': 'Online Learning of Shaping Reward with Subgoal Knowledge'},\n",
       "   {'paperId': '3025a6c00570d4264bf5f0db490d5f5137320de2',\n",
       "    'title': 'ERATING OFFLINE REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '0cff1642794d9628b6e5388a4af97bf9a77ccdba',\n",
       "    'title': 'L ATENT S KILL P LANNING FOR E XPLORATION AND T RANSFER'},\n",
       "   {'paperId': 'c2c57a8bc904c91654a1c63cab74104e6d057a7e',\n",
       "    'title': 'Action Selection for Composable Modular Deep Reinforcement Learning'},\n",
       "   {'paperId': '57ad4aed1f6705a89e6f3826d9de3292274f9dfe',\n",
       "    'title': 'Reinforcement Learning at the Cognitive Level in a Belief, Desire, Intention UAS Agent'},\n",
       "   {'paperId': '65a8e6321f3a20b9bd5dd7b8d05e47c75eeb7580',\n",
       "    'title': 'Skill Discovery for Exploration and Planning using Deep Skill Graphs'},\n",
       "   {'paperId': 'fde1bb6bd5f3a96cb84a90d75fc7e90f9c228572',\n",
       "    'title': 'Attention Option-Critic'},\n",
       "   {'paperId': 'f4fd4a5a0901675e19bb4054d8861d2697194fd5',\n",
       "    'title': 'Deep Reinforcement Learning: A State-of-the-Art Walkthrough'},\n",
       "   {'paperId': 'ec8eee61f42e07228bc78faac9817cff3e000ebf',\n",
       "    'title': 'Augmenting Policy Learning with Routines Discovered from a Demonstration'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': 'b9e4bccd2e69132af961fceeb51d418ee106e3ff',\n",
       "    'title': 'Active Hierarchical Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '2fcc8ec5f0946e2aa455ce24cca1b4e6f5c5bdfa',\n",
       "    'title': 'Multi-attribute Ad Hoc Network Routing Selection Based on Option-critic'},\n",
       "   {'paperId': '5fa8b76256a2125c7a72db372b6e0d6be90d3a54',\n",
       "    'title': 'Neural Dynamic Policies for End-to-End Sensorimotor Learning'},\n",
       "   {'paperId': '4d1537347d8f5c463188166ae96c3c0d7a3260fa',\n",
       "    'title': 'Skill Transfer via Partially Amortized Hierarchical Planning'},\n",
       "   {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "    'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       "   {'paperId': 'f4a917986f02a6c7581eb18dbd2e568bf8005b61',\n",
       "    'title': 'Automatic Curriculum Generation by Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b667641dc6acd7c0233503615942ea00ea9875f5',\n",
       "    'title': 'Continual Learning of Control Primitives: Skill Discovery via Reset-Games'},\n",
       "   {'paperId': 'd9ceb68a016be1dcd8d246497d7d964ed4b22751',\n",
       "    'title': 'Learning to Compose Hierarchical Object-Centric Controllers for Robotic Manipulation'},\n",
       "   {'paperId': '156a84996911a2116bdddd89aade6921e3e68d22',\n",
       "    'title': 'Model-Free Reinforcement Learning for Real-World Robots'},\n",
       "   {'paperId': '1fcba5d6c94d464af11243b3b4fc8f0f0f0736eb',\n",
       "    'title': 'HILONet: Hierarchical Imitation Learning from Non-Aligned Observations'},\n",
       "   {'paperId': '6fb1c349f407bb40add9c3f743a8fe040859cd6e',\n",
       "    'title': 'Diversity-Enriched Option-Critic'},\n",
       "   {'paperId': 'b769bbca148341f98a4d325162b1aa01ae7d71f1',\n",
       "    'title': 'Optimizing the Routing in Ad Hoc Networks With Option-Critic'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '215166434e32a8242c11d4bc1af04a8bff739aba',\n",
       "    'title': 'High-level representations through unconstrained sensorimotor learning'},\n",
       "   {'paperId': '29312aab246104b4fcb48837f4bf7aeb53d1764c',\n",
       "    'title': 'Reinforcement Learning in Latent Action Sequence Space'},\n",
       "   {'paperId': 'b68b8b980db62308864b2a7d33718182c5f8335b',\n",
       "    'title': 'Accelerating Reinforcement Learning with Learned Skill Priors'},\n",
       "   {'paperId': '64d16d0bef64c9a36ef91877c3687e260430534e',\n",
       "    'title': 'Motion Planner Augmented Reinforcement Learning for Robot Manipulation in Obstructed Environments'},\n",
       "   {'paperId': 'bf37352ed947f43e51f089de92addb377b7ad8e0',\n",
       "    'title': 'Diverse Exploration via InfoMax Options'},\n",
       "   {'paperId': '56775c1b1fdf8440913d22f8946f7fdb241bbad9',\n",
       "    'title': 'Learning Diverse Options via InfoMax Termination Critic'},\n",
       "   {'paperId': '74401fab0ed2bff5a4bf31bfae6d408788323970',\n",
       "    'title': 'Reward Propagation Using Graph Convolutional Networks'},\n",
       "   {'paperId': '3c279a4760315ca9ab29255ff7ff0a9c1717948b',\n",
       "    'title': 'Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for Physically Embedded 3D Sokoban'},\n",
       "   {'paperId': '560e9a8a1b027adbefb838ca337aa73af7998ca9',\n",
       "    'title': 'Disentangling causal effects for hierarchical reinforcement learning'},\n",
       "   {'paperId': '38b64cb991d71aae05ea58d67405084589b1fda0',\n",
       "    'title': 'Evaluating skills in hierarchical reinforcement learning'},\n",
       "   {'paperId': '07aad25b60339eb870257275e4a9386b7cbc8494',\n",
       "    'title': 'Hierarchical Reinforcement Learning Based on Continuous Subgoal Space'},\n",
       "   {'paperId': '0865dde445b38631bc24bdd7236b1e795000621f',\n",
       "    'title': 'Contextual Meta-Bandit for Recommender Systems Selection'},\n",
       "   {'paperId': '03eb3d34504b6f1a7a808d22b7c9dbd28d7ecee0',\n",
       "    'title': 'In-Silico Evaluation of Glucose Regulation Using Policy Gradient Reinforcement Learning for Patients with Type 1 Diabetes Mellitus'},\n",
       "   {'paperId': '22f178d425e6c9b4f7b8a4c8f1d6c1550cf9edcb',\n",
       "    'title': 'Physically Embedded Planning Problems: New Challenges for Reinforcement Learning'},\n",
       "   {'paperId': '114b52291b549466a4b1027f4248a122c1c3920c',\n",
       "    'title': 'Learning Compositional Neural Programs for Continuous Control'},\n",
       "   {'paperId': '9f57441051c2aecdb11b58c917c85666d86dc8c8',\n",
       "    'title': 'Learning the Solution Manifold in Optimization and Its Application in Motion Planning'},\n",
       "   {'paperId': 'b1b0f9c33e4b3253a27e651d3ed47d29867d4df4',\n",
       "    'title': 'Learning Abstract Models for Strategic Exploration and Fast Reward Transfer'},\n",
       "   {'paperId': 'dc41b7a4e6ab62e40ef93d188c2e4d4c9b4e0e49',\n",
       "    'title': 'Pre-trained Word Embeddings for Goal-conditional Transfer Learning in Reinforcement Learning'},\n",
       "   {'paperId': '1f1e38015e6f9d072c2b7bb2af1212d1c71b5b06',\n",
       "    'title': 'Learning Graph Structure With A Finite-State Automaton Layer'},\n",
       "   {'paperId': 'b2df444b1e52d9e3d73fe1b54027513d6c28128d',\n",
       "    'title': 'A scalable approach to control diverse behaviors for physically simulated characters'},\n",
       "   {'paperId': '0d875e375fe6b3f710242c0aeb2419d3843ff45f',\n",
       "    'title': 'Automatic Policy Decomposition through Abstract State Space Dynamic Specialization'},\n",
       "   {'paperId': '952b100591b056912a43065cbe0010a5bd6907fb',\n",
       "    'title': 'Developing cooperative policies for multi-stage tasks'},\n",
       "   {'paperId': 'aecb95605083c460feb289ec40901e328805fae5',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientific Implications'},\n",
       "   {'paperId': '66181aafc8572825c15268c8e676ed7de8830dff',\n",
       "    'title': 'A Survey of Planning and Learning in Games'},\n",
       "   {'paperId': 'd242950c9d4903d078055b3f5bbbad1b5e626e74',\n",
       "    'title': 'Learning Robot Skills with Temporal Variational Inference'},\n",
       "   {'paperId': '18939501fc35341ae409520f11e49ca2be1c6790',\n",
       "    'title': 'SOAC: The Soft Option Actor-Critic Architecture'},\n",
       "   {'paperId': '019820cbb73d0651a913bb74cbfb713c8ad772df',\n",
       "    'title': 'ELSIM: End-to-end learning of reusable skills through intrinsic motivation'},\n",
       "   {'paperId': '5f4e973204815f10372a47d7ff742b0b01935cca',\n",
       "    'title': 'Learning Intrinsically Motivated Options to Stimulate Policy Exploration'},\n",
       "   {'paperId': '6ec8797952213227eea2e63620f4d7c060d598d5',\n",
       "    'title': 'Hierarchical reinforcement learning for efficient exploration and transfer'},\n",
       "   {'paperId': '7a9d14cf9f22e624cc9f850881a0a924de56a7a9',\n",
       "    'title': 'Value Preserving State-Action Abstractions'},\n",
       "   {'paperId': '5de6d8b4436f6598f5bcba00bc07d864e962f1fb',\n",
       "    'title': 'Temporally-Extended {\\\\epsilon}-Greedy Exploration'},\n",
       "   {'paperId': '7a7c7607b15c2a85f05d52a68cfe9f39258db6bf',\n",
       "    'title': \"Crawling in Rogue's Dungeons With Deep Reinforcement Techniques\"},\n",
       "   {'paperId': 'a9c46dfd9a24c754a67386e02424ad68b1f4ab3b',\n",
       "    'title': 'AI Research Considerations for Human Existential Safety (ARCHES)'},\n",
       "   {'paperId': '1236684409af2735dedfb4d191afe8141baa523b',\n",
       "    'title': 'Evaluating Generalisation in General Video Game Playing'},\n",
       "   {'paperId': '7dbc413283ac8722f76ef86bd9ee82d466fc1dbf',\n",
       "    'title': 'Evaluating skills in hierarchical reinforcement learning'},\n",
       "   {'paperId': '8366088c4b8e66adf52919d72cbbd614b0798b4d',\n",
       "    'title': 'ToyArchitecture: Unsupervised learning of interpretable models of the environment'},\n",
       "   {'paperId': 'eb8147cca470f805412a4209c1a3ace93ee2b2b0',\n",
       "    'title': 'Concept Learning in Deep Reinforcement Learning'},\n",
       "   {'paperId': '08cdea48caa692e7c5317ca5fcc6f626a5413043',\n",
       "    'title': 'Learning Transferable Concepts in Deep Reinforcement Learning'},\n",
       "   {'paperId': '064c6f04352edd9dee1ba1124777b3dcc62a740a',\n",
       "    'title': 'Simple Sensor Intentions for Exploration'},\n",
       "   {'paperId': '3473d81d07b7301adcf75272d780746a70698716',\n",
       "    'title': 'End-to-end deep reinforcement learning in computer systems'},\n",
       "   {'paperId': '690c53ead57de755ab300a81ed1cd62766fb324c',\n",
       "    'title': 'DREAM Architecture: a Developmental Approach to Open-Ended Learning in Robotics'},\n",
       "   {'paperId': '94d02cb4a0901f4f336ffa939f6b9991f287948c',\n",
       "    'title': 'Curious Hierarchical Actor-Critic Reinforcement Learning'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': 'dc320373175624649ac0f3140320c6194683bfc5',\n",
       "    'title': 'Evaluating Adaptation Performance of Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '121cca1bb7cec48fa9080801927f50d99193eae6',\n",
       "    'title': 'Learning to Coordinate Manipulation Skills via Skill Behavior Diversification'},\n",
       "   {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "    'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': '1d89394ae89349847e9c8df994539ed58db1088e',\n",
       "    'title': 'Exploration in Reinforcement Learning with Deep Covering Options'},\n",
       "   {'paperId': 'e90323d515a024be8a6d0465dd90eefd681f9245',\n",
       "    'title': 'Discovering Motor Programs by Recomposing Demonstrations'},\n",
       "   {'paperId': '16dcf3c84c8ca44a5e4322de77abe0d791ca9ff2',\n",
       "    'title': 'Learning Expensive Coordination: An Event-Based Deep RL Approach'},\n",
       "   {'paperId': '6f6d6da7b4c6219c55d0da09fd2b1f9809535d6d',\n",
       "    'title': 'Program Guided Agent'},\n",
       "   {'paperId': 'e1a39a6614503546bbb72a8c75aaf0ae93a3ac01',\n",
       "    'title': 'Option Discovery using Deep Skill Chaining'},\n",
       "   {'paperId': '2ac02c3c366425882bb71bd5f8e7bc3552bed020',\n",
       "    'title': 'Molecular Design in Synthetically Accessible Chemical Space via Deep Reinforcement Learning'},\n",
       "   {'paperId': '84def8c1ae89f1f0fe197eed0c4256fbad2dc02f',\n",
       "    'title': 'Emergent Real-World Robotic Skills via Unsupervised Off-Policy Reinforcement Learning'},\n",
       "   {'paperId': 'a8482deaddfcd27b6c38aceafa1cc7063a50a445',\n",
       "    'title': 'Constructing Abstraction Hierarchies for Robust, Real-Time Control'},\n",
       "   {'paperId': '26a5ea837ccefbf12ab7289b5cc31fa77957bf8c',\n",
       "    'title': 'Per-Step Reward: A New Perspective for Risk-Averse Reinforcement Learning'},\n",
       "   {'paperId': '416e02fa848e02fe0f030c61eb0c786b0c66db5d',\n",
       "    'title': 'BabyAI++: Towards Grounded-Language Learning beyond Memorization'},\n",
       "   {'paperId': 'dc4421f05836ace15c4536f0d137794b2c442918',\n",
       "    'title': 'Reinforcement Learning via Reasoning from Demonstration'},\n",
       "   {'paperId': '273c84b64e16da4842d3c4b438713a81254caf90',\n",
       "    'title': 'An empirical investigation of the challenges of real-world reinforcement learning'},\n",
       "   {'paperId': '1c45200694b35cab0fe8dcfc3d50170347677044',\n",
       "    'title': 'Multimodal trajectory optimization for motion planning'},\n",
       "   {'paperId': 'bbac680797af0f7ce4cdcc6430ff001fa0dfe670',\n",
       "    'title': 'Learning to Generalize Across Long-Horizon Tasks from Human Demonstrations'},\n",
       "   {'paperId': '845aeb9dcf12efba0760c6eb2e2ac56ea27f3247',\n",
       "    'title': 'Option Discovery in the Absence of Rewards with Manifold Analysis'},\n",
       "   {'paperId': '1ba26d96f857883bda6b3b43c91bf6ffdc572053',\n",
       "    'title': 'Independent learning approaches: overcoming multi-agent learning pathologies in team-games'},\n",
       "   {'paperId': '18ae5f134208fb34213661d809bb6232ac2b0a30',\n",
       "    'title': 'Hierarchically Decoupled Imitation for Morphological Transfer'},\n",
       "   {'paperId': 'e2ddb6b45ce8dc3ba98a6e94294f42f403e9b545',\n",
       "    'title': 'Learning Structures: Predictive Representations, Replay, and Generalization'},\n",
       "   {'paperId': '0d4936466857c43df99b4831789f3570b8c8a46f',\n",
       "    'title': 'oIRL: Robust Adversarial Inverse Reinforcement Learning with Temporally Extended Actions'},\n",
       "   {'paperId': 'b6456512bbb44079972cd7e3a5d7652c76fd26c9',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Adaptive Policy Transfer'},\n",
       "   {'paperId': '53a91ec8342ec734e537868967988e9a13f15769',\n",
       "    'title': 'Learning When to Transfer among Agents: An Efficient Multiagent Transfer Learning Framework'},\n",
       "   {'paperId': '038a9aa35575cbce413696be4e7c5bb6247b0ee4',\n",
       "    'title': 'Efficient Deep Reinforcement Learning through Policy Transfer'},\n",
       "   {'paperId': '1da19dc8065a1881dd83f9bb9fe7dc1ba05a0e07',\n",
       "    'title': 'Control Frequency Adaptation via Action Persistence in Batch Reinforcement Learning'},\n",
       "   {'paperId': '027ebcf65f5d221c040a6586e5ed743b6d121aa6',\n",
       "    'title': 'Explore, Discover and Learn: Unsupervised Discovery of State-Covering Skills'},\n",
       "   {'paperId': '26a819ccc6ae088aed7a62f47fdbe1b840807daf',\n",
       "    'title': 'Off-policy Maximum Entropy Reinforcement Learning : Soft Actor-Critic with Advantage Weighted Mixture Policy(SAC-AWMP)'},\n",
       "   {'paperId': '886b604f74d65bb420bc1311e9647c44fe8fa91e',\n",
       "    'title': 'Temporal-adaptive Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b3738ea2122de7bc6e650443c5f018b65cd5a99',\n",
       "    'title': 'Parameter Sharing in Coagent Networks'},\n",
       "   {'paperId': 'd132063af999c329bf10b4e22d65e3e82972ded9',\n",
       "    'title': 'Coagent Networks Revisited'},\n",
       "   {'paperId': '2c4b565f9d0bce68cbb8337eab3ef3b4d6ef5824',\n",
       "    'title': 'Closing the Modelling Gap: Transfer Learning from a Low-Fidelity Simulator for Autonomous Driving'},\n",
       "   {'paperId': 'd2739419411ea8f9c65af832739ae5d96e7f5757',\n",
       "    'title': 'Learning to Locomote with Deep Neural-Network and CPG-based Control in a Soft Snake Robot'},\n",
       "   {'paperId': 'dc59362b5da2027df4608aeb0ed162483e21811b',\n",
       "    'title': 'Learning Reusable Options for Multi-Task Reinforcement Learning'},\n",
       "   {'paperId': '0f4a4f44e90ed19441b9e838246096e2500580a9',\n",
       "    'title': 'Optimal Options for Multi-Task Reinforcement Learning Under Time Constraints'},\n",
       "   {'paperId': '32f39d580c7f200e2f6dd7c22ed460f6d9567426',\n",
       "    'title': 'A Boolean Task Algebra for Reinforcement Learning'},\n",
       "   {'paperId': '60c8b24913090e168bdb174f7d3e9e96dd0c5a8c',\n",
       "    'title': 'Options of Interest: Temporal Abstraction with Interest Functions'},\n",
       "   {'paperId': '5a30917733db939b004d6a637a5d316373914af4',\n",
       "    'title': 'Combining primitive DQNs for improved reinforcement learning in Minecraft'},\n",
       "   {'paperId': '71e2d65fe500eaa13434475aeb575ab10479d042',\n",
       "    'title': 'On the Role of Weight Sharing During Deep Option Learning'},\n",
       "   {'paperId': 'e34b71be7761fa270a59d4dde4ccabee29f44266',\n",
       "    'title': 'Crowdfunding Dynamics Tracking: A Reinforcement Learning Approach'},\n",
       "   {'paperId': '2599c0ecb4cefa22ec745d2cb71c7081109920f3',\n",
       "    'title': 'Towards Practical Multi-Object Manipulation using Relational Reinforcement Learning'},\n",
       "   {'paperId': 'bfd215e196d895aa61aa1fa3e9cde3defcab771e',\n",
       "    'title': 'Hierarchical Cooperative Multi-Agent Reinforcement Learning with Skill Discovery'},\n",
       "   {'paperId': '15a34fbd5b8c63171c45a17fe9de94291ad28c24',\n",
       "    'title': 'Option-critic in cooperative multi-agent systems'},\n",
       "   {'paperId': 'ebc91356faf81b597f08b8ff2ea82407cbed0047',\n",
       "    'title': 'Meta-control of the exploration-exploitation dilemma emerges from probabilistic inference over a hierarchy of time scales'},\n",
       "   {'paperId': '2f56829302b78516cf5256f4933cd01d898b55b3',\n",
       "    'title': 'MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics'},\n",
       "   {'paperId': '05d57ef604913b8a1b12537fafd53799d65b10db',\n",
       "    'title': 'Learning Fast Adaptation With Meta Strategy Optimization'},\n",
       "   {'paperId': '817fc6b231599a257302cd853ce4fa9572c8e849',\n",
       "    'title': 'Learning Generalizable Locomotion Skills with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b49ee513a556fbc073cb9d92536a5f6948ebbc20',\n",
       "    'title': 'Off-Policy Actor-Critic with Shared Experience Replay'},\n",
       "   {'paperId': '20e127a1d27617b2b8545a54c016ccf2b5170b78',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Open-Domain Dialog'},\n",
       "   {'paperId': '4d40f7df809576d4db22b95c4ca9cc4c66e6928d',\n",
       "    'title': 'Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation'},\n",
       "   {'paperId': '3adc129e0b4304a0b87f096e72a2450b914703bd',\n",
       "    'title': 'Option Encoder: A Framework for Discovering a Policy Basis in Reinforcement Learning'},\n",
       "   {'paperId': '5ab999687734ddf8c480315bde537e76ac358a80',\n",
       "    'title': 'Dynamics-aware Embeddings'},\n",
       "   {'paperId': 'ada4fe4ad8ab37bf6380e747669399e14a5453d6',\n",
       "    'title': 'Promoting Coordination through Policy Regularization in Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': '0ee7bddcdd52388c3c01df560c35bfe7d9b9c15f',\n",
       "    'title': 'Learning to combine primitive skills: A step towards versatile robotic manipulation §'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '103cb5d78e89e63f0fbb8d9a5b2afbfc43cedd64',\n",
       "    'title': 'Reinforcement Learning with Competitive Ensembles of Information-Constrained Primitives'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8db16192c8abf006b90d22aab3e7fe3694640836',\n",
       "    'title': 'Hypothesis-Driven Skill Discovery for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '84771e205117b8bdcd0982c35b4fcd514d183afd',\n",
       "    'title': 'Composing Task-Agnostic Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'a7859b059cfe01d01f1bd795e86eb3f0771fb53b',\n",
       "    'title': 'Model primitives for hierarchical lifelong reinforcement learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': '01bf4baec2aee64714c8fd29515ac9e2ed903392',\n",
       "    'title': 'Learning Hierarchical Teaching Policies for Cooperative Agents'},\n",
       "   {'paperId': '339cb2ec1bb54b6d918c36b9924992b85b668899',\n",
       "    'title': 'Planning in Hierarchical Reinforcement Learning: Guarantees for Using Local Policies'},\n",
       "   {'paperId': '1f3ed18c1397f1c6575804fe4832ba5ef5d1e7b5',\n",
       "    'title': 'Asynchronous Coagent Networks'},\n",
       "   {'paperId': '5330b2732c12c7027811869a921f544a0bf581ca',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'd17362c79ff42760c70235fb377923f0caad1f3a',\n",
       "    'title': 'A Survey on Visual Navigation for Artificial Agents With Deep Reinforcement Learning'},\n",
       "   {'paperId': '323590ce6ffe2fa3a9272ff5af33502b0855d1fd',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientiﬁc Implications'},\n",
       "   {'paperId': '404b8118aa4e17323590cf273d990c62ba72b22c',\n",
       "    'title': 'Evolving Policy Sets for Multi-Policy Decision Making'},\n",
       "   {'paperId': 'a0f54431e5d2a9357191e0db576d79ef3209734d',\n",
       "    'title': 'ALIGN-RUDDER: LEARNING FROM FEW DEMON-'},\n",
       "   {'paperId': '55ad7e06bacdd3d7fc35c0a8b5f06d503d9c73ab',\n",
       "    'title': 'Highway Lane change under uncertainty with Deep Reinforcement Learning based motion planner'},\n",
       "   {'paperId': 'e20171b3e71fbf871748914020053a3e8855df0a',\n",
       "    'title': 'Introducing Soft Option-Critic for Blood Glucose Control in Type 1 Diabetes Exploiting Abstraction of Actions for Automated Insulin Administration'},\n",
       "   {'paperId': '87e04e50e1d425b62ab75b1c302b127e4a0fca7a',\n",
       "    'title': 'rocorl: Transferable Reinforcement Learning-Based Robust Control for Cyber-Physical Systems With Limited Data Updates'},\n",
       "   {'paperId': 'f238f455a2c506a70b3e07ad5bfea0d9fc15b57d',\n",
       "    'title': 'Abstract Value Iteration for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '5b73cd259a3fa72f95e8bac9e520250b950acf3a',\n",
       "    'title': 'Online Decision Based Visual Tracking via Reinforcement Learning'},\n",
       "   {'paperId': '4a469b594a1525a2c7d260570eae46d78690b171',\n",
       "    'title': 'Distributed Artificial Intelligence: Second International Conference, DAI 2020, Nanjing, China, October 24–27, 2020, Proceedings'},\n",
       "   {'paperId': '728cfe9697d7f7a9940dca17a4045fd10d6c0bf4',\n",
       "    'title': 'IHRL: Interactive Influence-based Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '496e0a3ce73b81bf49b7f8aeec9140e78b8f6ee2',\n",
       "    'title': 'Toward Synergism in Macro Action Ensembles'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '8fb76a141d8e72eef7d6e220ea6556a6c5755756',\n",
       "    'title': 'Learning Intrinsic Rewards as a Bi-Level Optimization Problem'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b404fae35f7ad01fa86e1a3b0a2fc50a8c665eea',\n",
       "    'title': 'Towards TempoRL: Learning When to Act'},\n",
       "   {'paperId': '243f9d6274161aabc1bd176056db7009020de04d',\n",
       "    'title': 'Adaptive Agent-Based Simulation for Individualized Training'},\n",
       "   {'paperId': '1a82aa3f16eceed3b87cd3e042eeb6ae6f6e4766',\n",
       "    'title': 'VIA SKILL BEHAVIOR DIVERSIFICATION'},\n",
       "   {'paperId': 'c5e1cbf8e76fb074bb666c695763cefb16381000',\n",
       "    'title': 'Sequential Association Rule Mining for Autonomously Extracting Hierarchical Task Structures in Reinforcement Learning'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': 'e012f897d861d81095c5fff89fab0b9e51d03c16',\n",
       "    'title': 'Epsilon-BMC: A Bayesian Ensemble Approach to Epsilon-Greedy Exploration in Model-Free Reinforcement Learning'},\n",
       "   {'paperId': 'c6c5155c38d4e4ed3db17ca3ab9bcec3a2e7be4d',\n",
       "    'title': 'Continuous-Discrete Reinforcement Learning for Hybrid Control in Robotics'},\n",
       "   {'paperId': 'a2e43270a9b1421e452c2975e5163e2a216abeac',\n",
       "    'title': 'A Survey of Deep Reinforcement Learning in Video Games'},\n",
       "   {'paperId': '9fec0fe8ea164c659412b611f420149496ccd237',\n",
       "    'title': 'Automatic Composite Action Discovery for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a6b0b3be010e3f0a30183734810cb849eba0e06c',\n",
       "    'title': 'Real-time robot path planning from simple to complex obstacle patterns via transfer learning of options'},\n",
       "   {'paperId': '4df0ed38f929eda792c0822f6229935fad4f2046',\n",
       "    'title': 'A Deep Reinforcement Learning Architecture for Multi-stage Optimal Control'},\n",
       "   {'paperId': '389efb5d26fc7a913cc71eb2638b0c06f189cb3d',\n",
       "    'title': 'DeepSynth: Program Synthesis for Automatic Task Segmentation in Deep Reinforcement Learning'},\n",
       "   {'paperId': '3562028a0decc62a89ea302dee64349bf0485286',\n",
       "    'title': 'Hierarchical Average Reward Policy Gradient Algorithms'},\n",
       "   {'paperId': '0e54edd0c55c0cb63e719b501e41790c75a2c73a',\n",
       "    'title': 'Exploration-exploitation dilemma in Reinforcement Learning under various form of prior knowledge. (Impact des connaissances a priori sur le compromis exploration-exploitation en apprentissage par renforcement)'},\n",
       "   {'paperId': '6fcc8c61041b495c82d339d0cb17147bee2cf0e1',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery'},\n",
       "   {'paperId': 'd861cab02cbb314fa7f1e14103a238d66e5d8809',\n",
       "    'title': 'Research on Learning Method Based on Hierarchical Decomposition'},\n",
       "   {'paperId': '3f116db230b58e6908f5d5791c9761260ad2d7b2',\n",
       "    'title': 'Graph-Based Design of Hierarchical Reinforcement Learning Agents'},\n",
       "   {'paperId': 'd6475fcd772287643915b72568eefba04955acf7',\n",
       "    'title': 'Sample-Efficient Deep Reinforcement Learning for Continuous Control'},\n",
       "   {'paperId': 'c00028df6ee6e5d7df4d4cbdbf421a359918b15d',\n",
       "    'title': 'Learning Fairness in Multi-Agent Systems'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': '8c54e8575e7c17a4097838305915e6e7b00fd4af',\n",
       "    'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': '02948149a953fb3a5234b81fd7226a0d340a26be',\n",
       "    'title': 'Subgoal-based Exploration via Bayesian Optimization'},\n",
       "   {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       "   {'paperId': 'e8faa4fa0909db69cc4039af43c1ffe921ca7977',\n",
       "    'title': 'Hierarchical Actor-Critic with Hindsight for Mobile Robot with Continuous State Space'},\n",
       "   {'paperId': '8ad0ad9736dc2ed7b8c59929c1f58409d487aeaf',\n",
       "    'title': 'Learning Context-Sensitive Strategies in Space Fortress'},\n",
       "   {'paperId': '64656a66a14323d984ed0fd0bc9bfe5ee85b97c5',\n",
       "    'title': 'Laplacian using Abstract State Transition Graphs: A Framework for Skill Acquisition'},\n",
       "   {'paperId': '58b88ae32f2ca92a941ccd21cf042b05f465b8b3',\n",
       "    'title': 'On the necessity of abstraction'},\n",
       "   {'paperId': 'df3ac75ec8ad937b7e1d43d6e4f40aa0cfa6bc01',\n",
       "    'title': 'Playing Atari Ball Games with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '89f11c2e2bfc1af9ec3e703bb62c4547c4de33bd',\n",
       "    'title': 'Sparse Skill Coding: Learning Behavioral Hierarchies with Sparse Codes'},\n",
       "   {'paperId': '4c52f87830d6c0f9d7e61defa695bd65a8aca067',\n",
       "    'title': 'Learning Functionally Decomposed Hierarchies for Continuous Navigation Tasks'},\n",
       "   {'paperId': '7e522be5f714f4f0d56b808a318a66eb206c3968',\n",
       "    'title': 'Efficient meta reinforcement learning via meta goal generation'},\n",
       "   {'paperId': '5a44e0877ae4d3b6322164a02d204429604e3daf',\n",
       "    'title': 'Construction of Macro Actions for Deep Reinforcement Learning'},\n",
       "   {'paperId': '45ac74350d21387c42ff92e90cd088bf310e1542',\n",
       "    'title': 'Guided goal generation for hindsight multi-goal reinforcement learning'},\n",
       "   {'paperId': '35257ba97d193f23f15e71a633a34e94dd3f5777',\n",
       "    'title': 'Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?'},\n",
       "   {'paperId': '1281fabd5d69a683756c2e804c2f1f4ed2b28511',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Unlimited Recursive Subroutine Calls'},\n",
       "   {'paperId': '237053ccc082fb44697054ee833e82f74c4bc182',\n",
       "    'title': 'An Open-Source Framework for Adaptive Traffic Signal Control'},\n",
       "   {'paperId': '7723086a4a09c39a1cd91c62b2234cdf72dc1f63',\n",
       "    'title': 'Multi-Task Hierarchical Imitation Learning for Home Automation'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': '1fdd7536d2c211564fa3b35e4ec1c12e9a5bc594',\n",
       "    'title': 'Hierarchical Reinforcement Learning Approach for the Road Intersection Task'},\n",
       "   {'paperId': '7c25b27c1496401058f84159e050fd366906699d',\n",
       "    'title': 'Skill Transfer in Deep Reinforcement Learning under Morphological Heterogeneity'},\n",
       "   {'paperId': '23f2c868e5fe90c9fb3d074987c87a5d2926b073',\n",
       "    'title': 'Multi-agent learning for security and sustainability'},\n",
       "   {'paperId': '1f1e51350458358274e0ad86ea1bfc88b92b1b6a',\n",
       "    'title': 'Combining learned skills and reinforcement learning for robotic manipulations'},\n",
       "   {'paperId': '3319c09bddf3bbfd9a116c71d2699b73b79f0463',\n",
       "    'title': 'Identifying Reusable Early-Life Options'},\n",
       "   {'paperId': '5678aa198b35b672d5829e48c1d1bb0dec2ac4ad',\n",
       "    'title': 'Playing FPS Games With Environment-Aware Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3944f15e0c2d39d114e5a44aedd079630607521e',\n",
       "    'title': 'Semantic RL with Action Grammars: Data-Efficient Learning of Hierarchical Task Abstractions'},\n",
       "   {'paperId': '54a9941279ffaaac37e1d623d3d9d30dbbd35aaa',\n",
       "    'title': 'Scalable muscle-actuated human simulation and control'},\n",
       "   {'paperId': 'c1ec9d7dbf89d28a79adf8b741a7f9a2c4106e35',\n",
       "    'title': 'On Inductive Biases in Deep Reinforcement Learning'},\n",
       "   {'paperId': '66605b6ceae9847156526e46ca9fe467804fca54',\n",
       "    'title': 'Learning World Graphs to Accelerate Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': '118672d5762c45b9469e74dbc43f0102b04717a6',\n",
       "    'title': 'On Multi-Agent Learning in Team Sports Games'},\n",
       "   {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "    'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "    'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       "   {'paperId': '1a1af8f6eebda877157393e6dd56e52ec2a99dba',\n",
       "    'title': 'The value of abstraction'},\n",
       "   {'paperId': 'e0889fcee1acd985af76a3907d5d0029bf260be9',\n",
       "    'title': 'Search on the Replay Buffer: Bridging Planning and Reinforcement Learning'},\n",
       "   {'paperId': 'f85ed65604976df89f9d991177c9a428d2168020',\n",
       "    'title': 'Options as responses: Grounding behavioural hierarchies in multi-agent RL'},\n",
       "   {'paperId': '171bffb23613e89d60664e2078c9e807d0e523df',\n",
       "    'title': 'Hierarchical Decision Making by Generating and Following Natural Language Instructions'},\n",
       "   {'paperId': 'af581dd2243ba89ba476f790cde876d1e1b6774b',\n",
       "    'title': 'Efficient Exploration in Reinforcement Learning through Time-Based Representations'},\n",
       "   {'paperId': '4167fbb4c9d3e4d0ab4982d4d102edc5c935ae1e',\n",
       "    'title': 'Towards concept based software engineering for intelligent agents'},\n",
       "   {'paperId': 'bb32c3b3e2252b13565bddcfed92f6392d07681b',\n",
       "    'title': 'Safe Policy Learning with Constrained Return Variance'},\n",
       "   {'paperId': 'e5c3432b13ef1249785c0ffab134918960c46045',\n",
       "    'title': 'CoRide: Joint Order Dispatching and Fleet Management for Multi-Scale Ride-Hailing Platforms'},\n",
       "   {'paperId': '7ee9389f3ae45620869c33c6126bb262b5c44f14',\n",
       "    'title': 'Composing Ensembles of Policies with Deep Reinforcement Learning'},\n",
       "   {'paperId': 'e15db5ed7bcf4dd6b6fe77978b73dacde85d5001',\n",
       "    'title': 'From semantics to execution: Integrating action planning with reinforcement learning for robotic tool use'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': 'af1dbd44d60e42b52f7629099376e1eaed3685ed',\n",
       "    'title': 'From Semantics to Execution: Integrating Action Planning With Reinforcement Learning for Robotic Causal Problem-Solving'},\n",
       "   {'paperId': 'ba6df472a4075feacd3ece156164e14e609fe1e3',\n",
       "    'title': 'Learning Robust Options by Conditional Value at Risk Optimization'},\n",
       "   {'paperId': 'ec9cf6922aae97b7c728693e78bd5cb812cc4e1e',\n",
       "    'title': 'Learning Novel Policies For Tasks'},\n",
       "   {'paperId': '074b4702465630da81a3b40600183e58a42c404d',\n",
       "    'title': 'Apprentissage séquentiel budgétisé pour la classification extrême et la découverte de hiérarchie en apprentissage par renforcement. (Budgeted sequential learning for extreme classification and for the discovery of hierarchy in reinforcement learning)'},\n",
       "   {'paperId': 'cab413141ee137a62ec0b6811e9e32ff3364659b',\n",
       "    'title': 'Real-time robot path planning from simple to complex obstacle patterns via transfer learning of options'},\n",
       "   {'paperId': '6a7ee7958a4fa1a55c5770664ee30ca2db4a200e',\n",
       "    'title': 'Multi-Agent Hierarchical Reinforcement Learning with Dynamic Termination'},\n",
       "   {'paperId': '27d5c1a6b35a591739c10a04e5fc1a96b10e3177',\n",
       "    'title': 'MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning'},\n",
       "   {'paperId': '6570c7cab46d2b0f3315f5abfbbd209140529c8b',\n",
       "    'title': 'Hierarchical Policy Learning is Sensitive to Goal Space Design'},\n",
       "   {'paperId': '8513f667ab7438c11f58755cb87db2d9785952a4',\n",
       "    'title': 'Towards Concept Based Software Engineering for Intelligent Agents'},\n",
       "   {'paperId': '4b51277eac12939867ec04a81bdbc756f61ec9ea',\n",
       "    'title': 'Successor Options: An Option Discovery Framework for Reinforcement Learning'},\n",
       "   {'paperId': '5c0d2e9caa303c51920c3d85e3acf4a64ca94414',\n",
       "    'title': 'DAC: The Double Actor-Critic Architecture for Learning Options'},\n",
       "   {'paperId': 'be928f91385999fa90d1e2fe06058f9dbcfd7186',\n",
       "    'title': 'Routing Networks and the Challenges of Modular and Compositional Computation'},\n",
       "   {'paperId': '4019625941a75ba71eefab09e8963f9e1d8905a9',\n",
       "    'title': 'Disentangling Options with Hellinger Distance Regularizer'},\n",
       "   {'paperId': '36d1fa96fa4e5ff6b6c3fc0034ecea8278ef2742',\n",
       "    'title': 'Human Sub-goal Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a5661dfbf57f909d581a05d54406f80825f9f9eb',\n",
       "    'title': 'Sub-Task Discovery with Limited Supervision: A Constrained Clustering Approach'},\n",
       "   {'paperId': '32a14285af705edd1844fbd5a5e901bd5be0e59c',\n",
       "    'title': 'Reinforcing Classical Planning for Adversary Driving Scenarios'},\n",
       "   {'paperId': 'fdc6ee81b1a0607264a3f6238432fd26098a600a',\n",
       "    'title': 'ToyArchitecture: Unsupervised Learning of Interpretable Models of the World'},\n",
       "   {'paperId': '11abfd523249b41ad1661712fbdd73fc4a1332f2',\n",
       "    'title': 'Single-step Options for Adversary Driving'},\n",
       "   {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "    'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '048459fcf6befce76d277a31ead3f58e1b2de32d',\n",
       "    'title': 'Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration'},\n",
       "   {'paperId': '8cff4799ad5a99e5530c7a6029766a40c0b3b5e0',\n",
       "    'title': 'Learning Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning'},\n",
       "   {'paperId': 'c372396db5db3ea5d1acc1255e79791c51dfc959',\n",
       "    'title': 'Learning Dynamics Model in Reinforcement Learning by Incorporating the Long Term Future'},\n",
       "   {'paperId': '4839f473193fccb5566ae6388ba6f40defdf10dc',\n",
       "    'title': 'Model Primitive Hierarchical Lifelong Reinforcement Learning'},\n",
       "   {'paperId': 'dcb7e2ed218198515750cb3c7712f0154b7fea75',\n",
       "    'title': 'Discovering Options for Exploration by Minimizing Cover Time'},\n",
       "   {'paperId': '7004ca8debac9c260bbff532d71a75c2048c6878',\n",
       "    'title': 'Reinforcement learning in artificial and biological systems'},\n",
       "   {'paperId': '951af7222535d934ca2b401ca0cd2181b28284f9',\n",
       "    'title': 'Reinforcement learning in artificial and biological systems'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': '3a8881aacdcf3af370a4cc375ff9f47e23fbb125',\n",
       "    'title': 'Reinforcement Learning Without Backpropagation or a Clock'},\n",
       "   {'paperId': '56ef00ba1b9c7fdba369c62b3809711ef07ef459',\n",
       "    'title': 'Asynchronous Coagent Networks: Stochastic Networks for Reinforcement Learning without Backpropagation or a Clock'},\n",
       "   {'paperId': '8d613d8dc6ef962111a4100498d31958c0fcff55',\n",
       "    'title': 'Graph-Based Skill Acquisition For Reinforcement Learning'},\n",
       "   {'paperId': '0a11fa9e5f521a52977ba0d5cc9bbafefa37d823',\n",
       "    'title': 'Hierarchical Critics Assignment for Multi-agent Reinforcement Learning'},\n",
       "   {'paperId': '70a3f24292bdcf6e630d5b32eacf93aa3f913c59',\n",
       "    'title': 'Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent networks.'},\n",
       "   {'paperId': 'e077eb243b69e8279f5173598166459d8f21100c',\n",
       "    'title': 'Constraint Satisfaction Propagation: Non-stationary Policy Synthesis for Temporal Logic Planning'},\n",
       "   {'paperId': '3844948a988be0f260e0618cfb6b47017f3adbd6',\n",
       "    'title': 'Robust temporal difference learning for critical domains'},\n",
       "   {'paperId': 'b08256a6a3faba052bdd3445d917dd656d248477',\n",
       "    'title': 'Credit Assignment Techniques in Stochastic Computation Graphs'},\n",
       "   {'paperId': '1447cb195033be291674a44a07eb18ee894c23eb',\n",
       "    'title': 'Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization'},\n",
       "   {'paperId': '88c38f203bbf0f881bcc42a8d1a4235b4ba7cb86',\n",
       "    'title': 'Escape Room: A Configurable Testbed for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'fbf03bf621ffee283911e765d525a75fc0d11bae',\n",
       "    'title': 'CompILE: Compositional Imitation Learning and Execution'},\n",
       "   {'paperId': 'd30ece33d1b0cfb09f01bd7732e7cca1b4f81a08',\n",
       "    'title': 'Natural Option Critic'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'e93c016291d9f5da54f1c1cfc2575110fe5580b9',\n",
       "    'title': 'QUOTA: The Quantile Option Architecture for Reinforcement Learning'},\n",
       "   {'paperId': '3aabed9c0963f5924a19e8fa2c522a730db46e16',\n",
       "    'title': 'Learning Representations in Model-Free Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751',\n",
       "    'title': 'Deep Reinforcement Learning'},\n",
       "   {'paperId': '1101c0781cbf36091f95a6e7f9d7352913d615ce',\n",
       "    'title': 'Compositional planning in Markov decision processes: Temporal abstraction meets generalized logic composition'},\n",
       "   {'paperId': 'e4a89a978f747d0b548f5887b2380c5f618061f0',\n",
       "    'title': 'Near-Optimal Representation Learning for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '74e12851de2d542aa2aef7b8a39ef021a5802689',\n",
       "    'title': 'Composing Complex Skills by Learning Transition Policies'},\n",
       "   {'paperId': 'c575d724615a2852f04dfce84547aa3654101150',\n",
       "    'title': 'Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference'},\n",
       "   {'paperId': 'a6738f4f0ad70bf3c2252dd026e0e2823ee4f48c',\n",
       "    'title': 'On Reinforcement Learning for Full-length Game of StarCraft'},\n",
       "   {'paperId': '1f26116ff8758190e4a5d7fa65f137b2b4befed1',\n",
       "    'title': 'InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics'},\n",
       "   {'paperId': 'a40ce2c343d0269a0c54854a95e832bd5f716b4a',\n",
       "    'title': 'Semiparametrical Gaussian Processes Learning of Forward Dynamical Models for Navigating in a Circular Maze'},\n",
       "   {'paperId': '08c3bdc572b368de4b59af446fd26b2850976630',\n",
       "    'title': 'Negative Update Intervals in Deep Multi-Agent Reinforcement Learning'},\n",
       "   {'paperId': 'd6dd369c4893de5b0a674f45dba6d41429aa0660',\n",
       "    'title': 'Combined Reinforcement Learning via Abstract Representations'},\n",
       "   {'paperId': '65769b53e71ea7c52b3a07ad32bd4fdade6a0173',\n",
       "    'title': 'Multi-task Deep Reinforcement Learning with PopArt'},\n",
       "   {'paperId': '8f64340c7fb047637152b8c954cfd5c1063ac078',\n",
       "    'title': 'Learning Adaptive Display Exposure for Real-Time Advertising'},\n",
       "   {'paperId': '56326d969591d3c30e2f48027b926be20a3e75f7',\n",
       "    'title': 'Automatically Composing Representation Transformations as a Means for Generalization'},\n",
       "   {'paperId': 'ea64dbd29ab90d8944bcc6a05680aeaacc86fc0a',\n",
       "    'title': 'Human-level performance in 3D multiplayer games with population-based reinforcement learning'},\n",
       "   {'paperId': '0ffe640ac451eee57ff84432f644e435d1e4fad0',\n",
       "    'title': 'Deictic Image Maps: An Abstraction For Learning Pose Invariant Manipulation Policies'},\n",
       "   {'paperId': 'a53874f5c63b31468ad2fe3f5dea558a6ce35820',\n",
       "    'title': 'Learning what you can do before doing anything'},\n",
       "   {'paperId': 'd5d61e7997228a8b96e1e4f1af5e6e1d2e0cb949',\n",
       "    'title': 'Context-Aware Policy Reuse'},\n",
       "   {'paperId': 'dfb2b26f15466bf3ec34fbd72a22bb9d6ecd42f4',\n",
       "    'title': 'Policy Search in Continuous Action Domains: an Overview'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '17704b148b5c20ddf92acbaf1addda134ecbb474',\n",
       "    'title': 'Learning Multi-Level Hierarchies with Hindsight'},\n",
       "   {'paperId': 'e01da604b11c764fe9838646e230cc72197d86ed',\n",
       "    'title': 'A Compression-Inspired Framework for Macro Discovery'},\n",
       "   {'paperId': '9d528f7e641c922bddd83f4af687806d685490d6',\n",
       "    'title': 'Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '02feb390612858d745ce324303bfc8f9d0148c42',\n",
       "    'title': 'Building structured hierarchical agents'},\n",
       "   {'paperId': '4aa9c831719c27b28e97aafcf0441e8eef5eaf1c',\n",
       "    'title': 'VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '77550f428249ce7bbd68adc78a74b6bd73ef389a',\n",
       "    'title': 'SKILL DISCOVERY WITH WELL-DEFINED OBJECTIVES'},\n",
       "   {'paperId': '6fdcf43f00c3c7166e235b243f517c4861a1d4b5',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION'},\n",
       "   {'paperId': '303edbd86773b68432fa2ccb7c223aa22abe08b3',\n",
       "    'title': 'Bridging the Gap: Converting Human Advice into Imagined Examples'},\n",
       "   {'paperId': '58751ad1dd047cd96285c90a99de8c3697c1bcd9',\n",
       "    'title': 'Soft Option Transfer'},\n",
       "   {'paperId': '770d79c89ca27de2efd143b16a81fe98497820a5',\n",
       "    'title': 'Swarm-inspired Reinforcement Learning via Collaborative Inter-agent Knowledge Distillation'},\n",
       "   {'paperId': 'd0d26881327fa5afaefc9afad84b1ab1ff35cf7f',\n",
       "    'title': 'MACRO ACTION ENSEMBLE SEARCHING METHODOL-'},\n",
       "   {'paperId': 'c0b1d43f03c4d959a0bf57ab03ff6ef83f849872',\n",
       "    'title': 'Intrinsically Motivated and Interactive Reinforcement Learning: a Developmental Approach. (Apprentissage par Renforcement Intrinsèquement Motivé et Interactif: une approche développementale)'},\n",
       "   {'paperId': '8210b626959b2d3dbf7f8098b074936f375ccbdc',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Model-Free Flight Control: A sample efficient tabular approach using Q(lambda)-learning and options in a traditional flight control structure'},\n",
       "   {'paperId': 'd1060aabd99a96f191be228b7176377c3a6e8de9',\n",
       "    'title': 'Towards Practical Robot Manipulation using Relational Reinforcement Learning'},\n",
       "   {'paperId': 'c558c56245d4e5b3ba1cb3ba650cb6e7f4bd0cbc',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, Sujoy; van Baar, Jeroen; Roy-Chowdhury, Amit K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       "   {'paperId': '0759ee607785c9360d6058d87e0a16036910ff2a',\n",
       "    'title': 'Reinforcement Learning for Computer Generated Forces using Open-Source Software'},\n",
       "   {'paperId': '822cd314662479bc4bad911a3768d921614fcaa3',\n",
       "    'title': 'WHY DOES HIERARCHY (SOMETIMES) WORK'},\n",
       "   {'paperId': 'a3a8861363fdfbd6a0792fdffb64517e366fea01',\n",
       "    'title': 'Learning from Trajectories via Subgoal Discovery /Author=Paul, S.; van Baar, J.; Roy Chowdhury, A.K. /CreationDate=October 31, 2019 /Subject=Applied Physics, Computer Vision, Machine Learning'},\n",
       "   {'paperId': 'e2480c8813fdeb8632a1ccbefc3dabbf86e3e83f',\n",
       "    'title': 'MULTIPOLAR: MULTI-SOURCE POLICY AGGREGA-'},\n",
       "   {'paperId': 'a4fdfea4a0810303a484b6eeab56cbb2a3bccf7f',\n",
       "    'title': 'META REINFORCEMENT LEARNING VIA META GOAL GENERATION'},\n",
       "   {'paperId': 'e7069f324d16847e9939bded648032e54c8c9331',\n",
       "    'title': '10-708 Final Report:Investigating Max-Entropy Latent-Space Policiesfor Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c19fcb0f8c041996c7cbb0deab15fbec5d1b5d9',\n",
       "    'title': 'Meta-Learning via Weighted Gradient Update'},\n",
       "   {'paperId': '2360efedce6b7bb74ac817c5c9a75cbd3c7830f5',\n",
       "    'title': 'Learning Representations in Reinforcement Learning'},\n",
       "   {'paperId': '0921fa0b7ce018fbdcf0187728172a66410e63b4',\n",
       "    'title': 'A Compression-Inspired Framework for Macro Discovery Extended'},\n",
       "   {'paperId': 'cd7c828b573c9887f728469bb08c0cc29aa76d5d',\n",
       "    'title': 'Overcoming data shortages in robotic applications'},\n",
       "   {'paperId': '98b41528c58e6f5b7b28be5b54029e52ca90c4ab',\n",
       "    'title': 'Learning to Learn: Hierarchical Meta-Critic Networks'},\n",
       "   {'paperId': '04d842f8701d30a1b128146dc75a1da218f0287e',\n",
       "    'title': 'g 3 g 2 g 1 g 0 Answer Grey'},\n",
       "   {'paperId': '60a1096593531ea5615bbedbd1c9a48bd931b860',\n",
       "    'title': 'Self-developed Action Hierarchy Enhances Meta Reinforcement Learning.'},\n",
       "   {'paperId': 'ce61de9e26235729d125266867ec3cec4feb5944',\n",
       "    'title': '“ Structure & Priors in Reinforcement Learning ” at ICLR 2019 V ALUE P RESERVING S TATE-A CTION A BSTRACTIONS ( A PPENDIX )'},\n",
       "   {'paperId': 'd687621da6c24e61d972e1fe2b4cad29dee95460',\n",
       "    'title': 'Fast and Resource-Efficient Control of Wireless Cyber-Physical Systems'},\n",
       "   {'paperId': 'b08430f554aba971044dea2dd31d9eecca41b021',\n",
       "    'title': 'Heterogeneous Knowledge Transfer via Hierarchical Teaching in Cooperative Multiagent Reinforcement Learning'},\n",
       "   {'paperId': '0cc157240449aacb372d482c532069e9895e0577',\n",
       "    'title': 'CMPSCI 687 : Reinforcement Learning Fall 2020 Class Syllabus , Notes , and Assignments'},\n",
       "   {'paperId': '4e74723cf75ab501920eccdda77968e7e97c86e2',\n",
       "    'title': 'Hierarchical Imitation Learning via Variational Inference of Control Programs'},\n",
       "   {'paperId': 'dd23055b151de020ffb719f8bab91a228536ca5a',\n",
       "    'title': 'Hyperbolic Embeddings for Learning Options in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '683599f260a877fef5e97a643852b854ae3db9a1',\n",
       "    'title': 'Compositional Imitation Learning: Explaining and executing one task at a time'},\n",
       "   {'paperId': '3af21f872a74548c2e0157141ea2c02ba6fd6238',\n",
       "    'title': 'Discovering hierarchies using Imitation Learning from hierarchy aware policies'},\n",
       "   {'paperId': '4b61c25a86083c20730c9b12737ac6ac4178c364',\n",
       "    'title': 'An Introduction to Deep Reinforcement Learning'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': 'e87532f456571b3dc88a583fe9873b68d8e28a26',\n",
       "    'title': 'Idiosyncrasies and challenges of data driven learning in electronic trading'},\n",
       "   {'paperId': '5c91208414f02eaec2281aabd30b50f0ce5b9da6',\n",
       "    'title': 'Learning Goal Embeddings via Self-Play for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd908be4e4b018d433238dbe38ebf61c154243eb4',\n",
       "    'title': 'On the Complexity of Exploration in Goal-Driven Navigation'},\n",
       "   {'paperId': '14d798a15d74813a6ee7a75661925e25de69f7d1',\n",
       "    'title': 'Learning Sequential Decision Tasks for Robot Manipulation with Abstract Markov Decision Processes and Demonstration-Guided Exploration'},\n",
       "   {'paperId': '51e7b68ca6f78e4a212af7c1d0c44382b38b9a85',\n",
       "    'title': 'Learning Abstract Options'},\n",
       "   {'paperId': '24389a037dd045be2fe517925c575b765fbbde62',\n",
       "    'title': 'Neural Modular Control for Embodied Question Answering'},\n",
       "   {'paperId': '69beea3fe3e5b60fb96c6fc3ef52ab594bebf6e5',\n",
       "    'title': 'Autonomous Sub-domain Modeling for Dialogue Policy with Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '82c0c95d58ad9c68ff53a8fab3bf71f499772ca4',\n",
       "    'title': 'A Framework to Discover and Reuse Object-Oriented Options in Reinforcement Learning'},\n",
       "   {'paperId': 'bd98c59510dad49b0e8c6d43848e570dd4e23ded',\n",
       "    'title': 'Abstract State Transition Graphs for Model-Based Reinforcement Learning'},\n",
       "   {'paperId': '49688c7eb0dbdd2ee9c459e056968594f235bf48',\n",
       "    'title': 'Regularizing Reinforcement Learning with State Abstraction'},\n",
       "   {'paperId': '439046148cd14c5c8f3a7ca63b95a6f75bda391d',\n",
       "    'title': 'Learning and Planning with a Semantic Model'},\n",
       "   {'paperId': '607eee11d64c75d837bcf98f3ec1bcd0d5727d07',\n",
       "    'title': 'Hierarchical Deep Multiagent Reinforcement Learning with Temporal Abstraction'},\n",
       "   {'paperId': '5faca17c1f8293e5d171719a6b8a289592c3d64d',\n",
       "    'title': 'Hierarchical Deep Multiagent Reinforcement Learning'},\n",
       "   {'paperId': '0d11d48303eb25a771581f404fcf4e7ad35e2f41',\n",
       "    'title': 'TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game'},\n",
       "   {'paperId': 'e4d8d9ab77e63bbc693df243ee81335e529d24c3',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Options and United Neural Network Approximation'},\n",
       "   {'paperId': 'f7f72159906791189ad5c239aca39d453c183016',\n",
       "    'title': 'Learning to Advertise with Adaptive Exposure via Constrained Two-Level Reinforcement Learning'},\n",
       "   {'paperId': '0a01766797da6701034a9b4947bb2201ef2f3380',\n",
       "    'title': 'Hierarchical reinforcement learning of multiple grasping strategies with human instructions'},\n",
       "   {'paperId': 'f17518d539e7011cdea74f0fe4706cab49d8160b',\n",
       "    'title': 'Challenges of Context and Time in Reinforcement Learning: Introducing Space Fortress as a Benchmark'},\n",
       "   {'paperId': '92250f2237378ea9af5fd4e3aaa681ed9818188b',\n",
       "    'title': 'Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration'},\n",
       "   {'paperId': '5f8645a8474017f52e4d1d4b4a0ca95d8b39f66f',\n",
       "    'title': 'Variational Option Discovery Algorithms'},\n",
       "   {'paperId': '4c03497f2e17900cbf4066fbf68a7cbaad8376be',\n",
       "    'title': 'Representational efficiency outweighs action efficiency in human program induction'},\n",
       "   {'paperId': 'b701cf7e1cda68416f9a3dd4746d5db29a94d82b',\n",
       "    'title': 'Eligibility Traces for Options'},\n",
       "   {'paperId': 'f650f1fd44ab0778d30577f8c2077b2ff58830da',\n",
       "    'title': 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement'},\n",
       "   {'paperId': 'd2eaa230a68d38e9fe508dc8f2e712712e978cdc',\n",
       "    'title': 'Budgeted Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '7a6a0f0f05c66135848618a47842233bfefa0adb',\n",
       "    'title': 'Robot Task Interruption by Learning to Switch Among Multiple Models'},\n",
       "   {'paperId': '7040e2a78bdb6ed01c237e52f0ace6c4f8608ba2',\n",
       "    'title': 'Unsupervised Learning of Sensorimotor Affordances by Stochastic Future Prediction'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '2dae355d690827c26da2025a3d02edc0a98b21b4',\n",
       "    'title': 'Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4c852a954c3a74df410231d601857b7005076de9',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Hindsight'},\n",
       "   {'paperId': '15365821d5e2b9ebec1ed9ac314975732b688da3',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Deep Nested Agents'},\n",
       "   {'paperId': 'd4575be3cf5128bb9b86fd9d37222b3202c0c922',\n",
       "    'title': 'A Deep Hierarchical Reinforcement Learning Algorithm in Partially Observable Markov Decision Processes'},\n",
       "   {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "    'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       "   {'paperId': '5a24dd4224f0ff3f41b7523c8ad92e0d1fee3a07',\n",
       "    'title': 'Constructing Temporally Extended Actions through Incremental Community Detection'},\n",
       "   {'paperId': '6aae1bc6c8e38c9a1d24a2b48bfe066f3591e1bc',\n",
       "    'title': 'Subgoal Discovery for Hierarchical Dialogue Policy Learning'},\n",
       "   {'paperId': '5304fa70d844da391cd12e45e38b57ab37195024',\n",
       "    'title': 'Market Making via Reinforcement Learning'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '73d5ca4c7f945d4d699abe8b3364776b723887a0',\n",
       "    'title': 'Modelling Human Target Reaching using A novel predictive deep reinforcement learning technique'},\n",
       "   {'paperId': '26272e1c331a0fbeb7d3d49d9d1d760ae59c01b8',\n",
       "    'title': 'Constructing Temporal Abstractions Autonomously in Reinforcement Learning'},\n",
       "   {'paperId': '3e1a140cff104984c0f0fb25ce838223dce4d892',\n",
       "    'title': 'Neuronal Circuit Policies'},\n",
       "   {'paperId': 'bf8d58faf972ad0a1026c0a7c5577c07996ef3a7',\n",
       "    'title': 'Hierarchical Deep Reinforcement Learning for Continuous Action Control'},\n",
       "   {'paperId': 'cab81775baae7ba2d056ebbc60437f2e03358ca3',\n",
       "    'title': 'Learning by Playing - Solving Sparse Reward Tasks from Scratch'},\n",
       "   {'paperId': 'd72e69eacd4afeac33f71d07c484686084e55b9a',\n",
       "    'title': 'Unicorn: Continual Learning with a Universal, Off-policy Agent'},\n",
       "   {'paperId': '674255f825742477f71e3b6728c602214f46c69d',\n",
       "    'title': 'Learning High-level Representations from Demonstrations'},\n",
       "   {'paperId': '809f951c77b5a39e2a9d556e9cf9938de87f2393',\n",
       "    'title': 'An Inference-Based Policy Gradient Method for Learning Options'},\n",
       "   {'paperId': 'b80991d12b41a5a68dc14dd87b692c0f903ceb9c',\n",
       "    'title': 'Some Considerations on Learning to Explore via Meta-Reinforcement Learning'},\n",
       "   {'paperId': '7dd8fc595afdd2097b43a5af9a8d9f5e97a65ec1',\n",
       "    'title': 'Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration'},\n",
       "   {'paperId': '90d3f103b6b03accff2799cb2bf8ca95d3d71669',\n",
       "    'title': 'Hierarchical Learning for Modular Robots'},\n",
       "   {'paperId': '4e43d0365e4e922123de54c5e9a430bbced4a817',\n",
       "    'title': 'Learning Robust Options'},\n",
       "   {'paperId': '955b7140b08791f6a56772d0e25d34863cf9c59c',\n",
       "    'title': 'Interrupting behaviour: Minimizing decision costs via temporal commitment and low-level interrupts'},\n",
       "   {'paperId': '0b670a55f83a08d0f049b30f48450fe608f90613',\n",
       "    'title': 'Hierarchical and Interpretable Skill Acquisition in Multi-task Reinforcement Learning'},\n",
       "   {'paperId': 'a9d8c7bcbe7d6a46edacf6d8241c3a719e897b21',\n",
       "    'title': 'Crossmodal Attentive Skill Learner'},\n",
       "   {'paperId': '6938ddf69008ac0a13afd5855c854c8a7520adc5',\n",
       "    'title': 'Hierarchical Policy Search via Return-Weighted Density Estimation'},\n",
       "   {'paperId': '9975ca72a2507d5de379501a72a430ef31d6218b',\n",
       "    'title': 'Learning with Options that Terminate Off-Policy'},\n",
       "   {'paperId': '58fb60c5592224901a26dd84220a2f3332c1fcf5',\n",
       "    'title': 'Eigenoption Discovery through the Deep Successor Representation'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': 'bf48f1d556fdb85d5dbe8cfd93ef13c212635bcf',\n",
       "    'title': 'Neural Task Programming: Learning to Generalize Across Hierarchical Tasks'},\n",
       "   {'paperId': '55f730385016802dd6452a34d0271b5c47ded332',\n",
       "    'title': 'Deep Abstract Q-Networks'},\n",
       "   {'paperId': '96e81cabed55630f2ad3e1346300bd7a7a17f060',\n",
       "    'title': 'When Waiting is not an Option : Learning Options with a Deliberation Cost'},\n",
       "   {'paperId': 'a4a0287a3f88fe992f0f23f3f1bc7eec0c11d32c',\n",
       "    'title': 'Reinforcement Learning in POMDPs with Memoryless Options and Option-Observation Initiation Sets'},\n",
       "   {'paperId': '2b64a0a4c4ef3cde80c6ad49428648ef6e47cbfa',\n",
       "    'title': 'Learning to Multi-Task by Active Sampling'},\n",
       "   {'paperId': '718f0c24aa8ad8d7343fe45031bbcc1d1fc7efc8',\n",
       "    'title': 'Path Planning using Reinforcement Learning and Objective Data'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': '99bcbc1f2b2a563285dc473be7ee9d50721f5f53',\n",
       "    'title': 'Human-level performance in first-person multiplayer games with population-based deep reinforcement learning'},\n",
       "   {'paperId': '5beaeff056549019926075746f8c4f78e30494b0',\n",
       "    'title': 'EARNING AN E MBEDDING S PACE FOR T RANSFERABLE R OBOT S KILLS'},\n",
       "   {'paperId': '7af2944a2415f8e32edd27d9bc79ad8f0fc338c8',\n",
       "    'title': 'ADVANTAGE-WEIGHTED INFORMATION MAXIMIZA-'},\n",
       "   {'paperId': '2bd506e55ffe397c768b4f816e3027f5b9b632e1',\n",
       "    'title': 'Hierarchy-Driven Exploration for Reinforcement Learning'},\n",
       "   {'paperId': 'da7d67f9d3341754942a4c3f5da6eface2adf68e',\n",
       "    'title': 'Template Grounded Examples High level advice Key Elements VariablesSelector Reactive LearnerScriptLesson'},\n",
       "   {'paperId': '3c65e39113a5b6f109e19819a4ae4483b0d558c6',\n",
       "    'title': 'Transfer reinforcement learning for task-oriented dialogue systems'},\n",
       "   {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       "   {'paperId': '4d2683e8c2055e13e8739707428239c995c7a5e3',\n",
       "    'title': 'Learning a Semantic Prior for Guided Navigation'},\n",
       "   {'paperId': 'ebe84994fc3f617e41ae856b8df1e0a00a84142a',\n",
       "    'title': 'Towards learning to best respond when losing control'},\n",
       "   {'paperId': '7cba1fb6edb762c3db8d7b8ab169dbe9c12bb28b',\n",
       "    'title': 'The Importance of Sampling in Meta-Reinforcement Learning'},\n",
       "   {'paperId': '4155ecb89086261704bae0040abcf326c41c21f8',\n",
       "    'title': 'Extending the Hierarchical Deep Reinforcement Learning framework'},\n",
       "   {'paperId': '6ffb8d5b3b39f7c372c3909a34498b7d4afae9bc',\n",
       "    'title': 'Learning robust policies when losing control'},\n",
       "   {'paperId': '92b26d7673fd77d6be8da55f88506c268bae408f',\n",
       "    'title': 'PROXIMITY REWARD INDUCTION'},\n",
       "   {'paperId': '6dad90f530cb9aa0deec5fa232155dab539d1b49',\n",
       "    'title': 'Action Permissibility in Deep Reinforcement Learning and Application to Autonomous Driving'},\n",
       "   {'paperId': '3277611ef0bebf3bcfc694f152ed17567242f287',\n",
       "    'title': 'When Should a Service Robot Switch Tasks ? A Learning-Based Approach for Interrupting Task Execution'},\n",
       "   {'paperId': '44c0f72c0d6604482d9bd9607b30d831c7cde82c',\n",
       "    'title': 'Learning from Language'},\n",
       "   {'paperId': '77a31a4601444a3f7aeed15061b08684d0bea92b',\n",
       "    'title': 'Exploration-Exploitation Trade-off in Deep Reinforcement Learning'},\n",
       "   {'paperId': '0634e57ed2f5bb3d68692fa457ab1c4e86a53bc4',\n",
       "    'title': 'Nested LSTMs'},\n",
       "   {'paperId': '7f64121eaf74b8204e0445e804f93f3b53a0a64a',\n",
       "    'title': 'The Eigenoption-Critic Framework'},\n",
       "   {'paperId': 'ef06f7016db8bcd5c5a53442a4aba1d1911a26dd',\n",
       "    'title': 'Hierarchical Actor-Critic'},\n",
       "   {'paperId': '72e87d27e8b3493981daca533b3956fae8b4f316',\n",
       "    'title': 'Learning Robot Skill Embeddings'},\n",
       "   {'paperId': '7a5196d05b145ec552912dccedd16a42c88718f1',\n",
       "    'title': 'Learning Skill Embeddings for Transferable Robot Skills'},\n",
       "   {'paperId': '4adfe39f84cfda351f5acf49f846eab6f21ccb67',\n",
       "    'title': 'Learnings Options End-to-End for Continuous Action Tasks'},\n",
       "   {'paperId': '37b5980cf1a202fbec2fec28b831b0f90b8d217a',\n",
       "    'title': 'Learning to Compose Skills'},\n",
       "   {'paperId': 'c572fe4b80a21bff85c87844e02b6de6eaa603a5',\n",
       "    'title': 'Identifying Reusable Macros for Efficient Exploration via Policy Compression'},\n",
       "   {'paperId': 'bb35b5042ac824fb8a4c53b7932f6ee1c4cffc5e',\n",
       "    'title': 'Transferring Agent Behaviors from Videos via Motion GANs'},\n",
       "   {'paperId': 'dd4924ecd5ba0e7c3af5a677f1db50c4003e28c9',\n",
       "    'title': 'Situationally Aware Options'},\n",
       "   {'paperId': '05893041d24dd404963960e73220aca83d19add4',\n",
       "    'title': 'Deep Reinforcement Learning: A Brief Survey'},\n",
       "   {'paperId': 'ed9f58f4e8ee8dcacafdf06ffa58deaa6404ad69',\n",
       "    'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations'},\n",
       "   {'paperId': '0c1e215765a46eacd374e706b3bd6b2e4bdd47a6',\n",
       "    'title': 'Rethink ReLU to Training Better CNNs'},\n",
       "   {'paperId': 'a934d496a1daae7556b1a65e0c95496c5f71836e',\n",
       "    'title': 'Training Better CNNs Requires to Rethink ReLU'},\n",
       "   {'paperId': '3159165e8a454da67ca6a1c13118646225888286',\n",
       "    'title': 'Autonomous Extracting a Hierarchical Structure of Tasks in Reinforcement Learning and Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '498238a3bd5fd322fc3ce1572e33bbe3853a356f',\n",
       "    'title': 'A Brief Survey of Deep Reinforcement Learning'},\n",
       "   {'paperId': '69a4a3948141f630e9bd6ba3ef4ac637294a9136',\n",
       "    'title': 'Independently Controllable Factors'},\n",
       "   {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "    'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       "   {'paperId': '3c63f8b8263cd6cc4c8c7429d46bb656accddc49',\n",
       "    'title': 'Hybrid Reward Architecture for Reinforcement Learning'},\n",
       "   {'paperId': '699d3449b82a635d4fbdf3b0946d31b4d78f0cea',\n",
       "    'title': 'Learning to Factor Policies and Action-Value Functions: Factored Action Space Representations for Deep Reinforcement learning'},\n",
       "   {'paperId': '3693414d385401997c13a4faa39a8b6c6cd4a4dd',\n",
       "    'title': 'Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization'},\n",
       "   {'paperId': 'f0a074177409db16cf3919434b44ce1b6590871b',\n",
       "    'title': 'Exploration-Exploitation in MDPs with Options'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '86a213137106fa986fbbfaf5c669563a6b0bf13e',\n",
       "    'title': 'Independently Controllable Features'},\n",
       "   {'paperId': '66bf51dfb04c0c6b957fb079b5b47611887c273d',\n",
       "    'title': 'Micro-Objective Learning : Accelerating Deep Reinforcement Learning through the Discovery of Continuous Subgoals'},\n",
       "   {'paperId': '049c6e5736313374c6e594c34b9be89a3a09dced',\n",
       "    'title': 'FeUdal Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '8423cc50c18d68f797adaa4f571f5e4efbe325a5',\n",
       "    'title': 'A Laplacian Framework for Option Discovery in Reinforcement Learning'},\n",
       "   {'paperId': 'b0d9d9efb909bf61ea60afbd79cda33d069aa9bd',\n",
       "    'title': 'Online Multi-Task Learning Using Active Sampling'},\n",
       "   {'paperId': '9f1e9e56d80146766bc2316efbc54d8b770a23df',\n",
       "    'title': 'Deep Reinforcement Learning: An Overview'},\n",
       "   {'paperId': 'afb42208cc499ede10a65af0dbe598e08556370d',\n",
       "    'title': 'Variational Intrinsic Control'},\n",
       "   {'paperId': '3a13f7c43b767b1fb72ef107ef62a4ddd48dd2a7',\n",
       "    'title': 'Modular Multitask Reinforcement Learning with Policy Sketches'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '3c3861c607fb79f3fbf79552018724617fc8ba1b',\n",
       "    'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft'},\n",
       "   {'paperId': '1172510fbdeed860aad0d3a976d8739297a92c5c',\n",
       "    'title': 'AN INFERENCE-BASED POLICY GRADIENT METHOD'},\n",
       "   {'paperId': '34da1ee1aea9e7e0575a7dd7a065dd7ba1dd76f2',\n",
       "    'title': 'CS 234 Project Final Report : Approaches to Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '4fd9c795f950868ce58a506c38e195f83ccb74ac',\n",
       "    'title': 'Hierarchical Task Generalization with Neural Programs'},\n",
       "   {'paperId': 'ebc3bd9cd67193b3a8f84d43d5b4377107c680dc',\n",
       "    'title': 'SITION IN MULTI-TASK REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '97c634c3b85ef4d78451cca6efb8811617f4ff2f',\n",
       "    'title': 'Learning Goal-Directed Behaviour'},\n",
       "   {'paperId': 'e4bc529ced68fae154e125c72af5381b1185f34e',\n",
       "    'title': 'PERCEPTUAL GOAL SPECIFICATIONS FOR REINFORCEMENT LEARNING'},\n",
       "   {'paperId': '231a4bd2bbdad9896458feb6fa17480b4dcdb9f2',\n",
       "    'title': 'Separation of Concerns in Reinforcement Learning'},\n",
       "   {'paperId': 'ebe794413451fdbcc4b918d1d975b2eeb1d65e2e',\n",
       "    'title': 'Improving Scalability of Reinforcement Learning by Separation of Concerns'},\n",
       "   {'paperId': '22c51bf1cf3582f324702771194934b9b34eaf40',\n",
       "    'title': 'A Matrix Splitting Perspective on Planning with Options'},\n",
       "   {'paperId': '867cc0e7a99a9ea8b845a28ef87ee391d8f0aa4c',\n",
       "    'title': 'Hierarchical reinforcement learning as creative problem solving'},\n",
       "   {'paperId': 'f8a257006599de5899506959de5f4a8a1b2d2fec',\n",
       "    'title': 'Options Discovery with Budgeted Reinforcement Learning'},\n",
       "   {'paperId': '44d0732589ae6d7a0031906a0b7fc073668666da',\n",
       "    'title': 'Situational Awareness by Risk-Conscious Skills'},\n",
       "   {'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': 'edff4138184b4f37590649b9aadc9a627942a5d0',\n",
       "    'title': 'Adaptive Skills Adaptive Partitions (ASAP)'},\n",
       "   {'paperId': '2ca53cbb1473d7b3e1f0230a234be72c54b94b77',\n",
       "    'title': 'EARNING WITH P OLICY S KETCHES'},\n",
       "   {'paperId': '80a7541a6c8cb3ad955915da13b92044dbc9885b',\n",
       "    'title': 'Learning with options : Just deliberate and relax'}],\n",
       "  'citnuminlist': 30,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True},\n",
       " 'e2bd18c1039f27675bd64014117db648d969452e': {'title': 'Learning and Transfer of Modulated Locomotor Controllers',\n",
       "  'year': 2016,\n",
       "  'references': [{'paperId': '4ba25cb493ac7a03fc15d3b936257c9a6c689c1d',\n",
       "    'title': 'Strategic Attentive Writer for Learning Macro-Actions'},\n",
       "   {'paperId': 'd358d41c69450b171327ebd99462b6afef687269',\n",
       "    'title': 'Continuous Deep Q-Learning with Model-based Acceleration'},\n",
       "   {'paperId': '69e76e16740ed69f4dc55361a3d319ac2f1293dd',\n",
       "    'title': 'Asynchronous Methods for Deep Reinforcement Learning'},\n",
       "   {'paperId': '024006d4c2a89f7acacc6e4438d156525b60a98f',\n",
       "    'title': 'Continuous control with deep reinforcement learning'},\n",
       "   {'paperId': 'd316c82c12cf4c45f9e85211ef3d1fa62497bff8',\n",
       "    'title': 'High-Dimensional Continuous Control Using Generalized Advantage Estimation'},\n",
       "   {'paperId': 'b6b8a1b80891c96c28cc6340267b58186157e536',\n",
       "    'title': 'End-to-End Training of Deep Visuomotor Policies'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Alex Graves, Oriol Vinyals, and Koray Kavukcuoglu. Strategic attentive writer for learning macro-actions. CoRR, abs/1606'},\n",
       "   {'paperId': '6640f4e4beae786f301928d82a9f8eb037aa6935',\n",
       "    'title': 'Learning Continuous Control Policies by Stochastic Value Gradients'},\n",
       "   {'paperId': 'b349c0fb0ad22a6c613fcedb90d7dd6ce5dd1c2d',\n",
       "    'title': 'A neural circuitry that emphasizes spinal feedback generates diverse behaviours of human locomotion'},\n",
       "   {'paperId': '37b0d1436f4ae77dc5c69db0c7d5f42fee9f0fae',\n",
       "    'title': 'Hierarchical Control Using Networks Trained with Higher-Level Forward Models'},\n",
       "   {'paperId': '1616fcc95f5183209b80f71272d6649527dbccf4',\n",
       "    'title': 'Learning parameterized motor skills on a humanoid robot'},\n",
       "   {'paperId': '484ad17c926292fbe0d5211540832a8c8a8e958b',\n",
       "    'title': 'Stochastic Backpropagation and Approximate Inference in Deep Generative Models'},\n",
       "   {'paperId': '5f5dc5b9a2ba710937e2c413b37b053cd673df02',\n",
       "    'title': 'Auto-Encoding Variational Bayes'},\n",
       "   {'paperId': 'cc52044f319f0c5c7cb882cae5402ef6e32be7fd',\n",
       "    'title': 'Motor primitive discovery'},\n",
       "   {'paperId': '5baabc483ae025f577f0ff89ad75090d28924ae5',\n",
       "    'title': 'Locomotor Primitives in Newborn Babies and Their Development'},\n",
       "   {'paperId': '0784503da5d57aaad3fd20ec23e2a091db204f42',\n",
       "    'title': 'Efficient skill learning using abstraction selection'},\n",
       "   {'paperId': '5f6c9e4a90af83686290e65e99c1e954099f4364',\n",
       "    'title': 'Effective Control Knowledge Transfer through Learning Skill and Representation Hierarchies'},\n",
       "   {'paperId': '257b0d075102aa7715c8a55f4c7c4c0934c5979b',\n",
       "    'title': 'From task parameters to motor synergies: A hierarchical framework for approximately optimal control of redundant manipulators'},\n",
       "   {'paperId': 'a4c9a9e9533ce0daf0490687f9624718842c1917',\n",
       "    'title': 'Relativized Options: Choosing the Right Transformation'},\n",
       "   {'paperId': '156e32df7f0c149de468f7d5ed4d7c5e17eba7d2',\n",
       "    'title': 'State abstraction for programmable reinforcement learning agents'},\n",
       "   {'paperId': '38688edefc7591ea2fc7d4294070e8bfe9d9ac3d',\n",
       "    'title': 'Learning Attractor Landscapes for Learning Motor Primitives'},\n",
       "   {'paperId': '7523da3b7e22d5a37265431f2b11d13237bdb3d3',\n",
       "    'title': 'Motor learning through the combination of primitives.'},\n",
       "   {'paperId': '0e7638dc16a5e5e9e46c91272bfb9c3dd242ef6d',\n",
       "    'title': 'Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning'},\n",
       "   {'paperId': '6cbb29935b7087303a22c07334d3731dad2a08dc',\n",
       "    'title': 'A hierarchical foundation for models of sensorimotor control'},\n",
       "   {'paperId': '1678bd32846b1aded5b1e80a617170812e80f562',\n",
       "    'title': 'Feudal Reinforcement Learning'},\n",
       "   {'paperId': '3d6ae6e9b59a871fd9259836ac9b6b7628f697f2',\n",
       "    'title': 'Principles of Neural Science'},\n",
       "   {'paperId': '4af77aafa93f810e403461e5ee911287aa16d76e',\n",
       "    'title': 'A robust layered control system for a mobile robot'},\n",
       "   {'paperId': '32580f01d9b499f26876370b851bf0012992752a',\n",
       "    'title': 'The co-ordination and regulation of movements'},\n",
       "   {'paperId': None,\n",
       "    'title': 'Die Schreitbewegungen der Neugeborenen [The walking movements of newborns'},\n",
       "   {'paperId': None,\n",
       "    'title': 'On the comparative study of disease of the nervous system'}],\n",
       "  'citations': [{'paperId': 'ca5c11627c021302d3bd26a12395d15847df37db',\n",
       "    'title': 'Adaptive Skill Coordination for Robotic Mobile Manipulation'},\n",
       "   {'paperId': '74dd51db773ea883d9804d1845345a46ab908ccd',\n",
       "    'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd3aff40b7fe5f3df4acac4d5cc1a29d16dc627f3',\n",
       "    'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains'},\n",
       "   {'paperId': '1f346f74e8eabececa4896d734ab9b261f30830d',\n",
       "    'title': 'Modular Deep Learning'},\n",
       "   {'paperId': '6eba2f014a17b26e15d251463b8e9dd1dbda2d3d',\n",
       "    'title': 'Centralized Cooperative Exploration Policy for Continuous Control Tasks'},\n",
       "   {'paperId': 'c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a',\n",
       "    'title': 'An Information-Theoretic Perspective on Intrinsic Motivation in Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '18d02042724a7fe29ae6aa9460353d7e2425ca86',\n",
       "    'title': 'Matching options to tasks using Option-Indexed Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1c6435cb353271f3cb87b27ccc6df5b727d55f26',\n",
       "    'title': 'Model-based Reinforcement Learning: A Survey'},\n",
       "   {'paperId': '44e023e90cf3508e14fdf4fd9ca00da85874a780',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '9274ebb09a17afd4d5e80a714d8afbc7e21bec50',\n",
       "    'title': 'Planning Immediate Landmarks of Targets for Model-Free Skill Transfer across Agents'},\n",
       "   {'paperId': '0af6a63167df299a1556a560d6884ae38eda390d',\n",
       "    'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors'},\n",
       "   {'paperId': '776975b39a1672dad9203d9bc469d05cbe44d35e',\n",
       "    'title': 'Multiple Subgoals-guided Hierarchical Learning in Robot Navigation'},\n",
       "   {'paperId': '6a4c74430e9c9c165ef3321edf6b4e9bdc36dea9',\n",
       "    'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration'},\n",
       "   {'paperId': 'bd2ff852e86d16df09376f2dfdc934c533bb04a2',\n",
       "    'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics'},\n",
       "   {'paperId': 'ee5661886abd37718edd5efa38b8bb660e2a7204',\n",
       "    'title': 'Towards Versatile Embodied Navigation'},\n",
       "   {'paperId': 'a78248272e8b87e6f7650fdc29dbb77454c1a745',\n",
       "    'title': 'Simple Emergent Action Representations from Multi-Task Policy Training'},\n",
       "   {'paperId': 'eb904f6080e771abb808ecf4ae60d31668bdf0ed',\n",
       "    'title': 'Hierarchical Reinforcement Learning using Gaussian Random Trajectory Generation in Autonomous Furniture Assembly'},\n",
       "   {'paperId': 'b27fd9ea29cabe6afedd01e446b96c34e956ce84',\n",
       "    'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning'},\n",
       "   {'paperId': '874af8384fcd4fe2517c7b514a60533aca1d22fb',\n",
       "    'title': 'Hierarchical Decentralized Deep Reinforcement Learning Architecture for a Simulated Four-Legged Agent'},\n",
       "   {'paperId': '71b21d725c3614f3bb16cc670bc19a6547bc1bfe',\n",
       "    'title': 'What deep reinforcement learning tells us about human motor learning and vice-versa'},\n",
       "   {'paperId': '2a297b042702b89c6a0a3864215231c84d0f0e5e',\n",
       "    'title': 'RobustAnalog: Fast Variation-Aware Analog Circuit Design Via Multi-task RL'},\n",
       "   {'paperId': '6d846a7601c4be41034b9316d7c256f639085d9f',\n",
       "    'title': 'Deep Hierarchical Planning from Pixels'},\n",
       "   {'paperId': '3364e4473d8746eb7b36653ba29a8e24093cf056',\n",
       "    'title': 'Meta-Learning Transferable Parameterized Skills'},\n",
       "   {'paperId': '78839ec995beab7f5fa8ce8d549fb4cf04b33d45',\n",
       "    'title': 'Meta-Learning Parameterized Skills'},\n",
       "   {'paperId': '4ba973b38e448b2060bd6e2cbc0255d767ddaf98',\n",
       "    'title': 'ASE: Large-Scale Reusable Adversarial Skill Embeddings for Physically Simulated Characters'},\n",
       "   {'paperId': '60ceffa024d7e79b8defb356c6173ea3aebd7693',\n",
       "    'title': 'Forgetting and Imbalance in Robot Lifelong Learning with Off-policy Data'},\n",
       "   {'paperId': '5642f71270f1dcadf1e572d94dade012603ac52e',\n",
       "    'title': 'Learning Design and Construction with Varying-Sized Materials via Prioritized Memory Resets'},\n",
       "   {'paperId': '2d5817230748b286f15637f073957588872fa76d',\n",
       "    'title': 'Learning Pneumatic Non-Prehensile Manipulation With a Mobile Blower'},\n",
       "   {'paperId': '1a6c31839b5bfaecea832920dcad3c77bd68e320',\n",
       "    'title': 'Plan Your Target and Learn Your Skills: Transferable State-Only Imitation Learning via Decoupled Policy Optimization'},\n",
       "   {'paperId': '82938e991a4094022bc190714c5033df4c35aaf2',\n",
       "    'title': 'Retrieval-Augmented Reinforcement Learning'},\n",
       "   {'paperId': '06b10851b7a53316b3b6588017c9f3b9aae8c7cb',\n",
       "    'title': 'Hierarchical Reinforcement Learning: A Survey and Open Research Challenges'},\n",
       "   {'paperId': '64c141dbeecdca2c9e494aa2aa518b19e7f71d97',\n",
       "    'title': 'Rethinking Learning Dynamics in RL using Adversarial Networks'},\n",
       "   {'paperId': 'eac4172613328fa02dd8fa320112c4de3a5f280d',\n",
       "    'title': 'Boosting Exploration in Multi-Task Reinforcement Learning using Adversarial Networks'},\n",
       "   {'paperId': 'abccbc60b6e4dcc1e46baee34d01d4fb4abbff48',\n",
       "    'title': 'Neural Circuit Architectural Priors for Embodied Control'},\n",
       "   {'paperId': '31656c8e07e3ec76e349bc61fdcf6a8fb1bef5e8',\n",
       "    'title': 'Intelligent problem-solving as integrated hierarchical reinforcement learning'},\n",
       "   {'paperId': 'c85662dcd17eed4452019b640a30a323970472ef',\n",
       "    'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies'},\n",
       "   {'paperId': '13867baa329020ce45fe7f6cd887ba27e15bcfed',\n",
       "    'title': 'Towards autonomous artificial agents with an active self: Modeling sense of control in situated action'},\n",
       "   {'paperId': '8eb73addbf8c2b52637af040755cf3ca13cdbf40',\n",
       "    'title': 'Training Transition Policies via Distribution Matching for Complex Tasks'},\n",
       "   {'paperId': '41e43d9c766128cdd715c64fbd30e0c9fdf14652',\n",
       "    'title': 'From Motor Control to Team Play in Simulated Humanoid Football'},\n",
       "   {'paperId': '52eccf617a38092d126417de970b74824e8cfa5c',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Timed Subgoals'},\n",
       "   {'paperId': '8af5ae20eed248c3eee7b2086a8724ad11b078a1',\n",
       "    'title': 'Learning to Jump from Pixels'},\n",
       "   {'paperId': 'ccb01a90b16b119db0201ed012f989ed48c68d9f',\n",
       "    'title': 'Temporal Abstraction in Reinforcement Learning with the Successor Representation'},\n",
       "   {'paperId': '6d00cb0a0cd79e3ee2456715cca195152b5267bb',\n",
       "    'title': 'Flygenvectors: The spatial and temporal structure of neural activity across the fly brain'},\n",
       "   {'paperId': '02b123d9f870afe852ba69d8bc430c213ad2d230',\n",
       "    'title': 'Adaptive Frequency Hopping Policy for Fast Pose Estimation'},\n",
       "   {'paperId': 'a30904d356f61dea1a1966571dbec8d2375e862e',\n",
       "    'title': 'The Multi-Dimensional Actions Control Approach for Obstacle Avoidance Based on Reinforcement Learning'},\n",
       "   {'paperId': '0a50454605da864cc4e1ac949e7a43055be11717',\n",
       "    'title': 'Recent Advances in Deep Reinforcement Learning Applications for Solving Partially Observable Markov Decision Processes (POMDP) Problems: Part 1 - Fundamentals and Applications in Games, Robotics and Natural Language Processing'},\n",
       "   {'paperId': 'cfcbeec1ae2f7d13ec1576aa30cb98f5326ffa71',\n",
       "    'title': 'Towards Autonomous Pipeline Inspection with Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a8c753bbd960bbde37234217f387ca246fc2146c',\n",
       "    'title': 'Using Reinforcement Learning to Create Control Barrier Functions for Explicit Risk Mitigation in Adversarial Environments'},\n",
       "   {'paperId': '4f9f09d1ab684b145627f5cbad5560f364e51559',\n",
       "    'title': 'Composable Energy Policies for Reactive Motion Generation and Reinforcement Learning'},\n",
       "   {'paperId': '54633d26e23b7ba9dc8901d65215d1f888a64296',\n",
       "    'title': 'AMP: Adversarial Motion Priors for Stylized Physics-Based Character Control'},\n",
       "   {'paperId': '22304ba16b6aa95afc18af93ee812e0f31890978',\n",
       "    'title': 'Residual Model Learning for Microrobot Control'},\n",
       "   {'paperId': '80feebd81b0e017c73b43fc89b3434c6ec8ee2cc',\n",
       "    'title': 'Online Baum-Welch algorithm for Hierarchical Imitation Learning'},\n",
       "   {'paperId': '8343b6f3c8424ac1a8069d31b7a0de1e8f3c40b8',\n",
       "    'title': 'Discovery of Options via Meta-Learned Subgoals'},\n",
       "   {'paperId': '4481c2a4111b7722dce68608b3e7e580d5cca768',\n",
       "    'title': 'A hierarchical representation of behaviour supporting open ended development and progressive learning for artificial agents'},\n",
       "   {'paperId': 'f5275f5eb6569ddb5ba9a959ede09875d56e3bac',\n",
       "    'title': 'Parrot: Data-Driven Behavioral Priors for Reinforcement Learning'},\n",
       "   {'paperId': '0d6a4e45acde6f47d704ed0752f17f7ab52223af',\n",
       "    'title': 'Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning'},\n",
       "   {'paperId': '105f8677126012a6b3c63cc4fb6f485c6040b691',\n",
       "    'title': 'ReLMoGen: Integrating Motion Generation in Reinforcement Learning for Mobile Manipulation'},\n",
       "   {'paperId': '9e38bbf16a458f9101fab5cae39a4f49d35dcb51',\n",
       "    'title': 'Data-efficient Hindsight Off-policy Option Learning'},\n",
       "   {'paperId': 'ed580995e4a51424d8f1a20f5b64200fe227c2cd',\n",
       "    'title': 'Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control'},\n",
       "   {'paperId': '6c49d768cff533cb9cafca0c3c3a4081a024ba77',\n",
       "    'title': 'Neuroprospecting with DeepRL agents'},\n",
       "   {'paperId': '1916e94c64c3dc855e1779f662c1edc60e5c15e0',\n",
       "    'title': 'Overleaf Example'},\n",
       "   {'paperId': '0dc16391dd10379b3500ff183c98ea0d5a879d10',\n",
       "    'title': 'Entropic Desired Dynamics for Intrinsic Control'},\n",
       "   {'paperId': '69fcad2acfef5fa61547ae043f1257a9b7af662a',\n",
       "    'title': 'SHIRO: Soft Hierarchical Reinforcement Learning with Off-Policy Corrections'},\n",
       "   {'paperId': '49e73f28ff90a229ddd4d07e0f10380d50417b39',\n",
       "    'title': 'Hierarchical principles of embodied reinforcement learning: A review'},\n",
       "   {'paperId': '8b62d928a7be4a6f408cc7a433c215a749604a95',\n",
       "    'title': 'UniCon: Universal Neural Controller For Physics-based Character Motion'},\n",
       "   {'paperId': 'c5df3ec3ebdeb3636b217a725aef68a7f5e86e42',\n",
       "    'title': 'From Pixels to Legs: Hierarchical Learning of Quadruped Locomotion'},\n",
       "   {'paperId': 'd669358916608af804c20329b7287d02c75b1311',\n",
       "    'title': 'Behavior Priors for Efficient Reinforcement Learning'},\n",
       "   {'paperId': '3c279a4760315ca9ab29255ff7ff0a9c1717948b',\n",
       "    'title': 'Beyond Tabula-Rasa: a Modular Reinforcement Learning Approach for Physically Embedded 3D Sokoban'},\n",
       "   {'paperId': '7cd87634f87bab49268bfda9f2d0491bcea57aad',\n",
       "    'title': 'Reinforcement learning for quadrupedal locomotion with design of continual-hierarchical curriculum'},\n",
       "   {'paperId': '74f23063ca77f5b1caa3770a5957ae5fc565843e',\n",
       "    'title': 'Multi-Task Learning with Deep Neural Networks: A Survey'},\n",
       "   {'paperId': '0922fc9d7aa9610896890f4d417ca7e72e417cdf',\n",
       "    'title': 'Importance Weighted Policy Learning and Adaption'},\n",
       "   {'paperId': 'a60b2bbd511beb7123ff10a5928ad38d9be5952e',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning a latent space of robot skills'},\n",
       "   {'paperId': '9fbef1a676813d395ced6d5b7ba3c39f1db11e21',\n",
       "    'title': 'Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation'},\n",
       "   {'paperId': 'afeffb9e05d89b2ac806282d3ed4366d67e4392e',\n",
       "    'title': 'Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion'},\n",
       "   {'paperId': 'b95f3b2081a9974d39c9819c104ad374638cb260',\n",
       "    'title': 'Inferring DQN structure for high-dimensional continuous control'},\n",
       "   {'paperId': '4e61140e7c7e0bfa2522bb33d360ddbbd8f8e27c',\n",
       "    'title': 'Catch & Carry'},\n",
       "   {'paperId': 'aecb95605083c460feb289ec40901e328805fae5',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientific Implications'},\n",
       "   {'paperId': 'b78c3c878955ea09d288dd4aad8ebdd5490f31bf',\n",
       "    'title': 'Perception-Prediction-Reaction Agents for Deep Reinforcement Learning'},\n",
       "   {'paperId': '0f6d74ebfbf9265a72bafaab7eef6bac3b50e33f',\n",
       "    'title': 'From proprioception to long-horizon planning in novel environments: A hierarchical RL model'},\n",
       "   {'paperId': '19170e491443dda63691f03010700c820ddf6dec',\n",
       "    'title': 'Decentralized Deep Reinforcement Learning for a Distributed and Adaptive Locomotion Controller of a Hexapod Robot'},\n",
       "   {'paperId': '467ee120f3456542b338fcfaff6f258c1913c7d0',\n",
       "    'title': 'Learning Diverse Sub-Policies via a Task-Agnostic Regularization on Action Distributions'},\n",
       "   {'paperId': 'ae3b2768b0a3c73410bce0d2ae03feaf01f6f864',\n",
       "    'title': 'Dynamics-Aware Unsupervised Skill Discovery'},\n",
       "   {'paperId': 'af968252f7501317658b3c167beeb122b631a961',\n",
       "    'title': 'Following Instructions by Imagining and Reaching Visual Goals'},\n",
       "   {'paperId': 'ffb3886a253ff927bcc46b78e00409893865a68e',\n",
       "    'title': 'Dynamics-Aware Unsupervised Discovery of Skills'},\n",
       "   {'paperId': '49172458767567434abf60970a45e07948c30c66',\n",
       "    'title': 'Compositional Transfer in Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '2fed116dea9c36914b52b55e0f9688ccf641ee07',\n",
       "    'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd05353adb12e9f74504ef3cb4229ec7b4dcfe1a4',\n",
       "    'title': 'Multitask Soft Option Learning'},\n",
       "   {'paperId': 'b846f7eaab8f10e9e29a030d7a66e6fd3db12cfc',\n",
       "    'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills'},\n",
       "   {'paperId': '323590ce6ffe2fa3a9272ff5af33502b0855d1fd',\n",
       "    'title': 'Deep Reinforcement Learning and Its Neuroscientiﬁc Implications'},\n",
       "   {'paperId': '058034310453d97c5e517565906ab48d15cfa0fd',\n",
       "    'title': 'Algorithms for Multi-task Reinforcement Learning'},\n",
       "   {'paperId': '728cfe9697d7f7a9940dca17a4045fd10d6c0bf4',\n",
       "    'title': 'IHRL: Interactive Influence-based Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '496e0a3ce73b81bf49b7f8aeec9140e78b8f6ee2',\n",
       "    'title': 'Toward Synergism in Macro Action Ensembles'},\n",
       "   {'paperId': '55ec24632ccfe9d65b0762c43eb5d57514a044cc',\n",
       "    'title': 'Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'd802b949ff627d51fcb148685256f9fd25f848d5',\n",
       "    'title': 'Hindsight Planner'},\n",
       "   {'paperId': '522b36b65bb555a16a15cb305d1c425d956934a3',\n",
       "    'title': 'The Option Keyboard: Combining Skills in Reinforcement Learning'},\n",
       "   {'paperId': 'b65decc03155f2e88984e4fa16493f70e5413e4d',\n",
       "    'title': 'Hierarchical motor control in mammals and machines'},\n",
       "   {'paperId': '1d6d157f4586ee5fffa172b7198ecb8f7101f921',\n",
       "    'title': 'Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks'},\n",
       "   {'paperId': 'cf0c21194c897012a825ede9fda2601e0c5665c3',\n",
       "    'title': 'Reusable neural skill embeddings for vision-guided whole body movement and object manipulation'},\n",
       "   {'paperId': '1817dacc1c6f87d0969b14b73ac2855fc22dfdc4',\n",
       "    'title': 'Sim-to-real: Six-legged Robot Control with Deep Reinforcement Learning and Curriculum Learning'},\n",
       "   {'paperId': '36a1d4d7f96bd3946608f2de51ecef0000105ca6',\n",
       "    'title': 'Object-oriented state editing for HRL'},\n",
       "   {'paperId': '56c4712402e94ca770206b6a383b569f3ccf7809',\n",
       "    'title': 'Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control'},\n",
       "   {'paperId': '2bd423f7a15f28fdf59065df3c8b623fa7e74477',\n",
       "    'title': 'HRL4IN: Hierarchical Reinforcement Learning for Interactive Navigation with Mobile Manipulators'},\n",
       "   {'paperId': 'ba0bf2bae46a97a7615af0a74356d293db1bc23b',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards'},\n",
       "   {'paperId': '8da3031ccfaa92214890fb277a5dfd9eb0634122',\n",
       "    'title': 'Explaining and Interpreting LSTMs'},\n",
       "   {'paperId': '5a44e0877ae4d3b6322164a02d204429604e3daf',\n",
       "    'title': 'Construction of Macro Actions for Deep Reinforcement Learning'},\n",
       "   {'paperId': '895735cace0de940aa647dbafc046b7f30316fe5',\n",
       "    'title': 'A survey on intrinsic motivation in reinforcement learning'},\n",
       "   {'paperId': '16b79cf9e3ab1800f5d4e6ef956dc22b179034e7',\n",
       "    'title': 'Learning to Explore in Motion and Interaction Tasks'},\n",
       "   {'paperId': '127a8f944a8010f768aac9d01cdba5548456b217',\n",
       "    'title': 'Skill based transfer learning with domain adaptation for continuous reinforcement learning domains'},\n",
       "   {'paperId': 'a2f7c70629a3c48761d97918e7a6c23152887a29',\n",
       "    'title': 'Learning to Solve a Rubik’s Cube with a Dexterous Hand'},\n",
       "   {'paperId': '22005795db3a0e85c9091a855427a39b5f8bb33a',\n",
       "    'title': 'Neural Embedding for Physical Manipulations'},\n",
       "   {'paperId': '105ba7bd2659670009eb5eac4bdaaa144672c2e5',\n",
       "    'title': 'Regularized Hierarchical Policies for Compositional Transfer in Robotics'},\n",
       "   {'paperId': 'c2c8482c713b94073f3d59895b373db4398ddfbb',\n",
       "    'title': 'Language as an Abstraction for Hierarchical Deep Reinforcement Learning'},\n",
       "   {'paperId': '3ed3fd08d89d130e5b028f83e550d4fc8c5c177d',\n",
       "    'title': 'Hierarchical automatic curriculum learning: Converting a sparse reward navigation task into dense reward'},\n",
       "   {'paperId': '4167fbb4c9d3e4d0ab4982d4d102edc5c935ae1e',\n",
       "    'title': 'Towards concept based software engineering for intelligent agents'},\n",
       "   {'paperId': '7aea82f3b7726b0bd3bb3931dff10c93d1907abf',\n",
       "    'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies'},\n",
       "   {'paperId': '0d6c52ac0424321e08cee82dd2ccb1fe0e826c01',\n",
       "    'title': 'Hierarchical Reinforcement Learning for Quadruped Locomotion'},\n",
       "   {'paperId': '074b4702465630da81a3b40600183e58a42c404d',\n",
       "    'title': 'Apprentissage séquentiel budgétisé pour la classification extrême et la découverte de hiérarchie en apprentissage par renforcement. (Budgeted sequential learning for extreme classification and for the discovery of hierarchy in reinforcement learning)'},\n",
       "   {'paperId': '8513f667ab7438c11f58755cb87db2d9785952a4',\n",
       "    'title': 'Towards Concept Based Software Engineering for Intelligent Agents'},\n",
       "   {'paperId': '1773f2f389d41134acd80cac7cc58ccc3c371973',\n",
       "    'title': 'Hierarchical Intermittent Motor Control With Deterministic Policy Gradient'},\n",
       "   {'paperId': '77c4252ead63894ffee82c2c0e98c6d0080f9390',\n",
       "    'title': 'Exploiting Hierarchy for Learning and Transfer in KL-regularized RL'},\n",
       "   {'paperId': '34108fe028c7bd0571160edbc105bf50874f23ea',\n",
       "    'title': 'The Termination Critic'},\n",
       "   {'paperId': '8a5c62f9c49a943b66fb1ae379442497609c8596',\n",
       "    'title': 'Emergent Coordination Through Competition'},\n",
       "   {'paperId': 'b08256a6a3faba052bdd3445d917dd656d248477',\n",
       "    'title': 'Credit Assignment Techniques in Stochastic Computation Graphs'},\n",
       "   {'paperId': '894536f2ac4728850bc18705daeeda6e88f3d6f1',\n",
       "    'title': 'Universal Successor Features Approximators'},\n",
       "   {'paperId': '5d6222dd49c6229eed9b9ba991c5aa0d9ad410fd',\n",
       "    'title': 'Diversity-Driven Extensible Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'a8a7219ee83cfd7ca258e20b5826a0c0786dcb73',\n",
       "    'title': 'Hierarchical visuomotor control of humanoids'},\n",
       "   {'paperId': 'bad355642cd299caca2328dae02563278ea74e8c',\n",
       "    'title': 'RUDDER: Return Decomposition for Delayed Rewards'},\n",
       "   {'paperId': '5b01eaef54a653ba03ddd5a978690380fbc19bfc',\n",
       "    'title': 'Diversity is All You Need: Learning Skills without a Reward Function'},\n",
       "   {'paperId': '4c5dca886f1d5cc4213ece9cfb7895c7f48fd1be',\n",
       "    'title': 'Scalable deep reinforcement learning for physics-based motion control'},\n",
       "   {'paperId': '58751ad1dd047cd96285c90a99de8c3697c1bcd9',\n",
       "    'title': 'Soft Option Transfer'},\n",
       "   {'paperId': '87d84a825924444efd8f9d063dcdf3c2fb9904df',\n",
       "    'title': 'Multi-Task Reinforcement Learning without Interference'},\n",
       "   {'paperId': 'd0d26881327fa5afaefc9afad84b1ab1ff35cf7f',\n",
       "    'title': 'MACRO ACTION ENSEMBLE SEARCHING METHODOL-'},\n",
       "   {'paperId': 'e7069f324d16847e9939bded648032e54c8c9331',\n",
       "    'title': '10-708 Final Report:Investigating Max-Entropy Latent-Space Policiesfor Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '655cfe96e20675183dc8c2acbab659bce54fd6f5',\n",
       "    'title': 'Relative Entropy Regularized Policy Iteration'},\n",
       "   {'paperId': '6446be07aac1ce8ecee77adf17e5d30bd44246c9',\n",
       "    'title': 'Safer reinforcement learning for robotics'},\n",
       "   {'paperId': '86273009fc1f30758a87710add3c20cf885f558a',\n",
       "    'title': 'Modulated Policy Hierarchies'},\n",
       "   {'paperId': '24bc58833ae3404c03b55a4c10ea588fbdcb3183',\n",
       "    'title': 'Expanding Motor Skills using Relay Networks'},\n",
       "   {'paperId': '3996f47c119223fb547905784124a0a2aa09c9a4',\n",
       "    'title': 'Scaling simulation-to-real transfer by learning composable robot skills'},\n",
       "   {'paperId': '97b802a9b094594f5778682ccaa56863280a00d5',\n",
       "    'title': 'An Approach to Hierarchical Deep Reinforcement Learning for a Decentralized Walking Control Architecture'},\n",
       "   {'paperId': '9536e0f62f24b7e51c44e046aa26952b2891f882',\n",
       "    'title': 'Calibration Method to Improve Transfer from Simulation to Quadruped Robots'},\n",
       "   {'paperId': '963cc19101ac6409ba6db0bc07d8ce1baecaf84a',\n",
       "    'title': 'Adaptive Skip Intervals: Temporal Abstraction for Recurrent Dynamical Models'},\n",
       "   {'paperId': 'f650f1fd44ab0778d30577f8c2077b2ff58830da',\n",
       "    'title': 'Transfer in Deep Reinforcement Learning Using Successor Features and Generalised Policy Improvement'},\n",
       "   {'paperId': 'd2eaa230a68d38e9fe508dc8f2e712712e978cdc',\n",
       "    'title': 'Budgeted Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '0f710daa7bbba3350169f0bbb5d24f8db3e5199e',\n",
       "    'title': 'Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement Learning with Trajectory Embeddings'},\n",
       "   {'paperId': '2dae355d690827c26da2025a3d02edc0a98b21b4',\n",
       "    'title': 'Deep Hierarchical Reinforcement Learning for Autonomous Driving with Distinct Behaviors'},\n",
       "   {'paperId': '6ace67b82d5a09eeb3bb77c231da7bde61772618',\n",
       "    'title': 'Automated Deep Reinforcement Learning Environment for Hardware of a Modular Legged Robot'},\n",
       "   {'paperId': '39b7007e6f3dd0744833f292f07ed77973503bfd',\n",
       "    'title': 'Data-Efficient Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'f2fe9ceeb9f2d93d917dc491b4ac3c08561c588f',\n",
       "    'title': 'Parametrized Hierarchical Procedures for Neural Programming'},\n",
       "   {'paperId': '216b77da552b0ef1c1b795fef704071d55a68f12',\n",
       "    'title': 'Deep Reinforcement Learning to Acquire Navigation Skills for Wheel-Legged Robots in Complex Environments'},\n",
       "   {'paperId': 'ccf8dd6f5602d0c2be46eab1bd0d04424aa060ef',\n",
       "    'title': 'Latent Space Policies for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '1b9ce6abc0f3024b88fcd4dbd0c10cf5bcf7d38d',\n",
       "    'title': 'DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills'},\n",
       "   {'paperId': 'bfe12ed806f0bf9d17dc7e5bd5653294035d1caf',\n",
       "    'title': 'Learning to Run challenge solutions: Adapting reinforcement learning methods for neuromusculoskeletal environments'},\n",
       "   {'paperId': '3e1a140cff104984c0f0fb25ce838223dce4d892',\n",
       "    'title': 'Neuronal Circuit Policies'},\n",
       "   {'paperId': 'd356a5603f14c7a6873272774782d7812871f952',\n",
       "    'title': 'Reinforcement and Imitation Learning for Diverse Visuomotor Skills'},\n",
       "   {'paperId': '7cccc3b6c6d50bc9b75781573cb065e7b758c931',\n",
       "    'title': 'Progressive Reinforcement Learning with Distillation for Multi-Skilled Motion Control'},\n",
       "   {'paperId': '61527789b487ab2dc0155f6f274de7196908c57c',\n",
       "    'title': 'Transferring Task Goals via Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '485445ad19e79c9f81c2236105f47b1a7f85863c',\n",
       "    'title': 'Model-Based Action Exploration for Learning Dynamic Motion Skills'},\n",
       "   {'paperId': '468c522d7ee3fad66cdfd86ea95bc90351a87600',\n",
       "    'title': 'Model-Based Action Exploration'},\n",
       "   {'paperId': '4d2c4cbb535801549371d9783a98d1e43bddf4e5',\n",
       "    'title': 'Meta Learning Shared Hierarchies'},\n",
       "   {'paperId': '471f9742b4e32d8ee68f9ee493768ff0466a231d',\n",
       "    'title': 'Automatic Goal Generation for Reinforcement Learning Agents'},\n",
       "   {'paperId': '0718d725fd01fdf147cd7787fa62f814f5723053',\n",
       "    'title': 'Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning'},\n",
       "   {'paperId': 'ae1ecbfde00d841d9a35cf6f2239501713f517cc',\n",
       "    'title': 'Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration'},\n",
       "   {'paperId': 'f2043beaab45e7cd8a2b5af86dd7d5d663b5176f',\n",
       "    'title': \"The NIPS '17 Competition: Building Intelligent Systems\"},\n",
       "   {'paperId': '5beaeff056549019926075746f8c4f78e30494b0',\n",
       "    'title': 'EARNING AN E MBEDDING S PACE FOR T RANSFERABLE R OBOT S KILLS'},\n",
       "   {'paperId': '714bb2c21605cde034334aa9e345a9b19bc44cdf',\n",
       "    'title': 'MULTI-SKILLED MOTION CONTROL'},\n",
       "   {'paperId': '72e87d27e8b3493981daca533b3956fae8b4f316',\n",
       "    'title': 'Learning Robot Skill Embeddings'},\n",
       "   {'paperId': '7a5196d05b145ec552912dccedd16a42c88718f1',\n",
       "    'title': 'Learning Skill Embeddings for Transferable Robot Skills'},\n",
       "   {'paperId': 'e96614bf49ffcbde3a0ad458d317fe49f1914b9a',\n",
       "    'title': 'Emergence of human-comparable balancing behaviours by deep reinforcement learning'},\n",
       "   {'paperId': '9b75d487ac4bead09c7b887ea70187c7dd1364bf',\n",
       "    'title': 'Automata-Guided Hierarchical Reinforcement Learning for Skill Composition'},\n",
       "   {'paperId': '0dd87b0eca77dd9470ae41653ebaaa350c315336',\n",
       "    'title': 'Automata Guided Hierarchical Reinforcement Learning for Zero-shot Skill Composition'},\n",
       "   {'paperId': '52b18b9b31d942b8fc83dc69db097557c881a641',\n",
       "    'title': 'Hierarchical Reinforcement Learning with Parameters'},\n",
       "   {'paperId': 'd672baf56986a3bc5748c25362b2d2b4d65efcb8',\n",
       "    'title': 'Multi-task Learning with Gradient Guided Policy Specialization'},\n",
       "   {'paperId': '171d79216cd8d090f1e69623a3c1e19453cb8eb8',\n",
       "    'title': 'Expanding Motor Skills through Relay Neural Networks'},\n",
       "   {'paperId': '9917363277c783a01bff32af1c27fc9b373ad55d',\n",
       "    'title': 'DeepLoco: dynamic locomotion skills using hierarchical deep reinforcement learning'},\n",
       "   {'paperId': 'a762ae907b7dd71a59bd8bd98aba69dfe2de13a2',\n",
       "    'title': 'Emergence of Locomotion Behaviours in Rich Environments'},\n",
       "   {'paperId': 'e6e01f580c973d91f6445d839389f9f2d5efc78e',\n",
       "    'title': 'Learning human behaviors from motion capture by adversarial imitation'},\n",
       "   {'paperId': '30834ae1497c35d362eea14857d93c28d2d12b57',\n",
       "    'title': 'Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning'},\n",
       "   {'paperId': '1544925bf3407641a48621db5006336c3e2f120e',\n",
       "    'title': 'Multi-Level Discovery of Deep Options'},\n",
       "   {'paperId': '850d78496304829d16d14701e4d81692f088f47d',\n",
       "    'title': 'EX2: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       "   {'paperId': '9172cd6c253edf7c3a1568e03577db20648ad0c4',\n",
       "    'title': 'Reinforcement Learning with Deep Energy-Based Policies'},\n",
       "   {'paperId': '3deecaee4ec1a37de3cb10420eaabff067669e17',\n",
       "    'title': 'Stochastic Neural Networks for Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': '34da1ee1aea9e7e0575a7dd7a065dd7ba1dd76f2',\n",
       "    'title': 'CS 234 Project Final Report : Approaches to Hierarchical Reinforcement Learning'},\n",
       "   {'paperId': 'b189470ff3138cdcfd6a5613f9f7f1b80dd463a1',\n",
       "    'title': 'Edinburgh Research Explorer Emergence of Human-comparable Balancing Behaviors by Deep Reinforcement Learning'},\n",
       "   {'paperId': '4ff902d56471f6812d945e376f06e6aecc9b95b2',\n",
       "    'title': 'UAV Collision Avoidance Policy Optimization with Deep Reinforcement Learning'},\n",
       "   {'paperId': '4eb38b3460606a4042b04fc52d0044ab948b4a17',\n",
       "    'title': 'EX: Exploration with Exemplar Models for Deep Reinforcement Learning'},\n",
       "   {'paperId': '3512fe4e8d823320d7c13175588f463399b8b660',\n",
       "    'title': 'Modular Hopping and Running via Parallel Composition'},\n",
       "   {'paperId': '231a4bd2bbdad9896458feb6fa17480b4dcdb9f2',\n",
       "    'title': 'Separation of Concerns in Reinforcement Learning'},\n",
       "   {'paperId': 'ebe794413451fdbcc4b918d1d975b2eeb1d65e2e',\n",
       "    'title': 'Improving Scalability of Reinforcement Learning by Separation of Concerns'},\n",
       "   {'paperId': 'f8a257006599de5899506959de5f4a8a1b2d2fec',\n",
       "    'title': 'Options Discovery with Budgeted Reinforcement Learning'},\n",
       "   {'paperId': 'f621237cf2dbb4c94d70d75bc7f2ff48e2a327a9',\n",
       "    'title': 'Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics'},\n",
       "   {'paperId': '1af52747faf164a4d3ea4dee4180c0ab1780f602',\n",
       "    'title': 'Deep learning based motor control unit'}],\n",
       "  'citnuminlist': 15,\n",
       "  'refnuminlist': 0,\n",
       "  'isKeypaper': True}}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rwa.paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "0\n",
      "8\n",
      "Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "0\n",
      "3\n",
      "Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "0\n",
      "2\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "0\n",
      "8\n",
      "Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning\n",
      "0\n",
      "1\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "0\n",
      "10\n",
      "Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "0\n",
      "4\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "0\n",
      "7\n",
      "MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "0\n",
      "0\n",
      "Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space\n",
      "0\n",
      "1\n",
      "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning\n",
      "0\n",
      "10\n",
      "Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\n",
      "0\n",
      "1\n",
      "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "0\n",
      "19\n",
      "Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "0\n",
      "6\n",
      "Cache-Assisted Collaborative Task Offloading and Resource Allocation Strategy: A Metareinforcement Learning Approach\n",
      "0\n",
      "0\n",
      "STRUCTURAL ACTIVE CONTROL FRAMEWORK USING REINFORCEMENT LEARNING\n",
      "0\n",
      "0\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "0\n",
      "16\n",
      "Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback\n",
      "0\n",
      "0\n",
      "Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "3\n",
      "11\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "3\n",
      "3\n",
      "Demonstration-Guided Reinforcement Learning with Learned Skills\n",
      "5\n",
      "10\n",
      "Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "2\n",
      "3\n",
      "Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "0\n",
      "5\n",
      "Hierarchical Skills for Efficient Exploration\n",
      "2\n",
      "11\n",
      "Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space\n",
      "0\n",
      "0\n",
      "Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "3\n",
      "14\n",
      "Model Predictive Actor-Critic: Accelerating Robot Skill Acquisition with Deep Reinforcement Learning\n",
      "1\n",
      "0\n",
      "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "1\n",
      "14\n",
      "Accelerating Reinforcement Learning with Learned Skill Priors\n",
      "15\n",
      "8\n",
      "Behavior Priors for Efficient Reinforcement Learning\n",
      "2\n",
      "9\n",
      "Catch & Carry: Reusable Neural Controllers for Vision-Guided Whole-Body Tasks\n",
      "8\n",
      "4\n",
      "CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "0\n",
      "9\n",
      "Discovering Motor Programs by Recomposing Demonstrations\n",
      "9\n",
      "5\n",
      "Hierarchical reinforcement learning for efficient exploration and transfer\n",
      "1\n",
      "2\n",
      "Learning quadrupedal locomotion over challenging terrain\n",
      "2\n",
      "0\n",
      "Learning Robot Skills with Temporal Variational Inference\n",
      "4\n",
      "5\n",
      "Multi-expert learning of adaptive legged locomotion\n",
      "1\n",
      "2\n",
      "Multiplicative Controller Fusion: Leveraging Algorithmic Priors for Sample-efficient Reinforcement Learning and Safe Sim-To-Real Transfer\n",
      "3\n",
      "0\n",
      "Parrot: Data-Driven Behavioral Priors for Reinforcement Learning\n",
      "11\n",
      "11\n",
      "Directed-Info GAIL: Learning Hierarchical Policies from Unsegmented Demonstrations using Directed Information\n",
      "6\n",
      "1\n",
      "Dynamics-Aware Unsupervised Discovery of Skills\n",
      "7\n",
      "5\n",
      "Learning Latent Plans from Play\n",
      "15\n",
      "1\n",
      "Learning Multi-Level Hierarchies with Hindsight\n",
      "6\n",
      "2\n",
      "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "10\n",
      "9\n",
      "Neural probabilistic motor primitives for humanoid control\n",
      "13\n",
      "0\n",
      "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "8\n",
      "5\n",
      "Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "3\n",
      "9\n",
      "Joint Optimization of Quota Policy Design and Electric Market Behavior Based on Renewable Portfolio Standard in China\n",
      "0\n",
      "0\n",
      "CompILE: Compositional Imitation Learning and Execution\n",
      "9\n",
      "5\n",
      "Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "13\n",
      "6\n",
      "Learning an Embedding Space for Transferable Robot Skills\n",
      "20\n",
      "0\n",
      "A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "4\n",
      "1\n",
      "DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "9\n",
      "3\n",
      "FeUdal Networks for Hierarchical Reinforcement Learning\n",
      "15\n",
      "2\n",
      "Stochastic Neural Networks for Hierarchical Reinforcement Learning\n",
      "14\n",
      "2\n",
      "The Option-Critic Architecture\n",
      "30\n",
      "0\n",
      "Learning and Transfer of Modulated Locomotor Controllers\n",
      "15\n",
      "0\n",
      "update pkl\n"
     ]
    }
   ],
   "source": [
    "for paperId in rwa.paper_list['keypaper']:\n",
    "    rwa.checkCitationInList(paperId)\n",
    "    rwa.checkReferenceInList(paperId)\n",
    "    print(rwa.paper_list[paperId]['title'])\n",
    "    print(rwa.paper_list[paperId]['citnuminlist'])\n",
    "    print(rwa.paper_list[paperId]['refnuminlist'])\n",
    "rwa.update_pkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
