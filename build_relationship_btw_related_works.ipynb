{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d82db7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scholarly\n",
      "  Downloading scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Collecting bibtexparser\n",
      "  Downloading bibtexparser-1.4.0.tar.gz (51 kB)\n",
      "     -------------------------------------- 51.9/51.9 kB 886.0 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.5/50.5 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "     ---------------------------------------- 75.4/75.4 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (2.28.1)\n",
      "Collecting sphinx-rtd-theme\n",
      "  Downloading sphinx_rtd_theme-1.2.1-py2.py3-none-any.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting deprecated\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting free-proxy\n",
      "  Downloading free_proxy-1.1.1.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (4.11.1)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.9.1-py3-none-any.whl (6.6 MB)\n",
      "     ---------------------------------------- 6.6/6.6 MB 4.9 MB/s eta 0:00:00\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Collecting arrow\n",
      "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.4/66.4 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from arrow->scholarly) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from beautifulsoup4->scholarly) (2.3.2.post1)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bibtexparser->scholarly) (3.0.9)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.9.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "     ---------------------------------------- 3.8/3.8 MB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (1.3.0)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "     ---------------------------------------- 72.5/72.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: certifi in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (2022.9.24)\n",
      "Requirement already satisfied: idna in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->scholarly) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->scholarly) (1.26.12)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "     ------------------------------------- 384.9/384.9 kB 11.7 MB/s eta 0:00:00\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting docutils<0.19\n",
      "  Downloading docutils-0.18.1-py2.py3-none-any.whl (570 kB)\n",
      "     -------------------------------------- 570.0/570.0 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "     -------------------------------------- 121.1/121.1 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting sphinx<7,>=1.6\n",
      "  Downloading sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 10.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->scholarly) (3.6.2)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Requirement already satisfied: colorama>=0.4.5 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Collecting imagesize>=1.3\n",
      "  Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Collecting sphinxcontrib-htmlhelp>=2.0.0\n",
      "  Downloading sphinxcontrib_htmlhelp-2.0.1-py3-none-any.whl (99 kB)\n",
      "     ---------------------------------------- 99.8/99.8 kB 5.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pygments>=2.13 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.13.0)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (21.3)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 90.6/90.6 kB ? eta 0:00:00\n",
      "Collecting sphinxcontrib-serializinghtml>=1.1.5\n",
      "  Downloading sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "     ---------------------------------------- 94.0/94.0 kB ? eta 0:00:00\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (3.1.2)\n",
      "Collecting babel>=2.9\n",
      "  Downloading Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "     ---------------------------------------- 10.1/10.1 MB 7.3 MB/s eta 0:00:00\n",
      "Collecting snowballstemmer>=2.0\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "     ---------------------------------------- 93.0/93.0 kB 5.2 MB/s eta 0:00:00\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Downloading sphinxcontrib_applehelp-1.0.4-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.6/120.6 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Downloading alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.7/84.7 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (1.15.1)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (22.1.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from Jinja2>=3.0->sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.1.1)\n",
      "Using legacy 'setup.py install' for bibtexparser, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for free-proxy, since package 'wheel' is not installed.\n",
      "Installing collected packages: sortedcontainers, snowballstemmer, fake-useragent, wrapt, typing-extensions, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, python-dotenv, PySocks, outcome, lxml, imagesize, h11, exceptiongroup, docutils, bibtexparser, babel, async-generator, alabaster, wsproto, trio, sphinx, httpcore, free-proxy, deprecated, arrow, trio-websocket, sphinxcontrib-jquery, httpx, sphinx-rtd-theme, selenium, scholarly\n",
      "  Running setup.py install for bibtexparser: started\n",
      "  Running setup.py install for bibtexparser: finished with status 'done'\n",
      "  Running setup.py install for free-proxy: started\n",
      "  Running setup.py install for free-proxy: finished with status 'done'\n",
      "Successfully installed PySocks-1.7.1 alabaster-0.7.13 arrow-1.2.3 async-generator-1.10 babel-2.12.1 bibtexparser-1.4.0 deprecated-1.2.14 docutils-0.18.1 exceptiongroup-1.1.1 fake-useragent-1.1.3 free-proxy-1.1.1 h11-0.14.0 httpcore-0.17.2 httpx-0.24.1 imagesize-1.4.1 lxml-4.9.2 outcome-1.2.0 python-dotenv-1.0.0 scholarly-1.7.11 selenium-4.9.1 snowballstemmer-2.2.0 sortedcontainers-2.4.0 sphinx-6.2.1 sphinx-rtd-theme-1.2.1 sphinxcontrib-applehelp-1.0.4 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.1 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 sphinxcontrib-serializinghtml-1.1.5 trio-0.22.0 trio-websocket-0.10.2 typing-extensions-4.6.3 wrapt-1.15.0 wsproto-1.2.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\lhs\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "Collecting scholarly\n",
      "  Using cached scholarly-1.7.11-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: bibtexparser in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (1.4.0)\n",
      "Collecting httpx\n",
      "  Using cached httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "Collecting free-proxy\n",
      "  Using cached free_proxy-1.1.1.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting arrow\n",
      "  Using cached arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "Collecting sphinx-rtd-theme\n",
      "  Using cached sphinx_rtd_theme-1.2.1-py2.py3-none-any.whl (2.8 MB)\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.9.1-py3-none-any.whl (6.6 MB)\n",
      "Collecting deprecated\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (2.28.1)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (1.1.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (4.6.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (1.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scholarly) (4.11.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from arrow->scholarly) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from beautifulsoup4->scholarly) (2.3.2.post1)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from bibtexparser->scholarly) (3.0.9)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from deprecated->scholarly) (1.15.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from free-proxy->scholarly) (4.9.2)\n",
      "Requirement already satisfied: idna in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (3.4)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Using cached httpcore-0.17.2-py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (2022.9.24)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx->scholarly) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->scholarly) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->scholarly) (1.26.12)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.10.2-py3-none-any.whl (17 kB)\n",
      "Collecting sphinxcontrib-jquery!=3.0.0,>=2.0.0\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Requirement already satisfied: docutils<0.19 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx-rtd-theme->scholarly) (0.18.1)\n",
      "Collecting sphinx<7,>=1.6\n",
      "  Using cached sphinx-6.2.1-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->scholarly) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx->scholarly) (3.6.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7.0->arrow->scholarly) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (21.3)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.3)\n",
      "Collecting babel>=2.9\n",
      "  Using cached Babel-2.12.1-py3-none-any.whl (10.1 MB)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.0.1)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Using cached alabaster-0.7.13-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: Pygments>=2.13 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.13.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.2)\n",
      "Requirement already satisfied: colorama>=0.4.5 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.4)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.1.5)\n",
      "Requirement already satisfied: snowballstemmer>=2.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.2.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (3.1.2)\n",
      "Requirement already satisfied: outcome in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (1.2.0)\n",
      "Collecting async-generator>=1.9\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (1.1.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (22.1.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from trio~=0.17->selenium->scholarly) (1.15.1)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lhs\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from Jinja2>=3.0->sphinx<7,>=1.6->sphinx-rtd-theme->scholarly) (2.1.1)\n",
      "Using legacy 'setup.py install' for free-proxy, since package 'wheel' is not installed.\n",
      "Installing collected packages: wsproto, deprecated, babel, async-generator, alabaster, trio, sphinx, httpcore, free-proxy, arrow, trio-websocket, sphinxcontrib-jquery, httpx, sphinx-rtd-theme, selenium, scholarly\n",
      "  Running setup.py install for free-proxy: started\n",
      "  Running setup.py install for free-proxy: finished with status 'done'\n",
      "Successfully installed alabaster-0.7.13 arrow-1.2.3 async-generator-1.10 babel-2.12.1 deprecated-1.2.14 free-proxy-1.1.1 httpcore-0.17.2 httpx-0.24.1 scholarly-1.7.11 selenium-4.9.1 sphinx-6.2.1 sphinx-rtd-theme-1.2.1 sphinxcontrib-jquery-4.1 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 23.1.2\n",
      "[notice] To update, run: C:\\Users\\lhs\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scholarly\n",
    "%pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9cd7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholarly import scholarly, MaxTriesExceededException, ProxyGenerator\n",
    "import pickle\n",
    "import os\n",
    "import time, random\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88786cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelatedWork:\n",
    "    def __init__(self):\n",
    "        # set free proxy\n",
    "        #pg = ProxyGenerator()\n",
    "        #pg.FreeProxies()\n",
    "        #scholarly.use_proxy(pg)\n",
    "        \n",
    "        self.paper_list = {}\n",
    "        self.is_updated = False\n",
    "        self.load_paper_list()\n",
    "        \n",
    "    def build_paper_list_from_txt(self):\n",
    "        if os.path.exists('./paper_list.txt'): \n",
    "            f = open('paper_list.txt', 'r')\n",
    "            lines = f.readlines()\n",
    "            for i, line in enumerate(lines):\n",
    "                items = line.split('_')\n",
    "                year = items[0]\n",
    "                title = items[1].split('\\n')[0]\n",
    "                self.paper_list[i+1] = {'title': title, 'year': year}\n",
    "            self.is_updated = True\n",
    "        else: # txt 파일이 없으면 빈 딕셔너리 반환\n",
    "            print('No txt file')\n",
    "            \n",
    "        # Save data_dict\n",
    "        self.update_pkl()\n",
    "          \n",
    "    def load_pkl(self):\n",
    "        f =  open('paper_list.pkl','rb')\n",
    "        self.paper_list = pickle.load(f)\n",
    "    \n",
    "    def update_pkl(self):\n",
    "        if self.is_updated: # 업데이트 사항이 있는 경우에만\n",
    "            print('update pkl')\n",
    "            with open('paper_list.pkl','wb') as f:\n",
    "               pickle.dump(self.paper_list, f)\n",
    "            self.is_updated = False\n",
    "        else:\n",
    "            print('no update')\n",
    "    \n",
    "    def load_paper_list(self):\n",
    "        if os.path.exists('./paper_list.pkl'): # pkl 파일이 존재하면 불러옴\n",
    "            print('load from pkl')\n",
    "            self.load_pkl()\n",
    "        else: # pkl 파일이 없으면 \n",
    "            print('load from txt')\n",
    "            self.build_paper_list_from_txt()\n",
    "    \n",
    "    def update_paper_info_from_scholar(self, paper_num):\n",
    "        paper = self.paper_list[paper_num]\n",
    "        if 'info' not in paper.keys(): # info 정보가 없는 경우에만 업데이트\n",
    "            title = paper['title']\n",
    "            while True:\n",
    "                try:\n",
    "                    search_query = scholarly.search_pubs(title)\n",
    "                    break\n",
    "                except MaxTriesExceededException as err:\n",
    "                    print(\"Google Scholar is aggressively blocking us! Quitting for now.\")\n",
    "                    self.updateFreeProxy()\n",
    "            paper_info = next(search_query)\n",
    "            self.paper_list[paper_num]['info'] = paper_info\n",
    "            print(title, ' information updated')\n",
    "            self.is_updated = True\n",
    "        else:\n",
    "            print('info already exists')\n",
    "        \n",
    "    def update_all_info(self):\n",
    "        for key in self.paper_list.keys():\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            self.update_paper_info_from_scholar(key)\n",
    "            self.update_pkl()\n",
    "            print('paper {} updated\\n'.format(key))\n",
    "\n",
    "    def update_citedby(self, paper_num):\n",
    "        paper = self.paper_list[paper_num]\n",
    "        print(paper['title'])\n",
    "        if 'citedby' not in paper.keys(): # citedby 정보가 없는 경우에만 업데이트\n",
    "            while True:\n",
    "                try:\n",
    "                    citation_list = scholarly.citedby(paper['info'])\n",
    "                    cl = []\n",
    "                    for i, c in enumerate(citation_list):\n",
    "                        print(c['bib']['title'])\n",
    "                        cl.append(c['bib'])\n",
    "                    paper['citedby'] = cl            \n",
    "                    self.is_updated = True\n",
    "                    break\n",
    "                except MaxTriesExceededException as err:\n",
    "                    print(\"Google Scholar is aggressively blocking us! Quitting for now.\")\n",
    "                    self.updateFreeProxy()\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    if e == 'citedby_url': # cited by no paper\n",
    "                        paper['citedby'] = []\n",
    "                    break \n",
    "        else:\n",
    "            print('citedby already exists')\n",
    "            \n",
    "    def update_all_citedby(self):\n",
    "        for key in self.paper_list.keys():\n",
    "            #time.sleep(random.uniform(2, 5))\n",
    "            self.update_citedby(key)\n",
    "            self.update_pkl()\n",
    "            print('paper {} updated\\n'.format(key))\n",
    "            \n",
    "    def updateFreeProxy(self):\n",
    "        pg = ProxyGenerator()\n",
    "        pg.FreeProxies()\n",
    "        print(\"update FreeProxy!\")\n",
    "        scholarly.use_proxy(pg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6c04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from pkl\n"
     ]
    }
   ],
   "source": [
    "rw = RelatedWork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dc801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 1 updated\n",
      "\n",
      "Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 2 updated\n",
      "\n",
      "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "'citedby_url'\n",
      "no update\n",
      "paper 3 updated\n",
      "\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "citedby already exists\n",
      "no update\n",
      "paper 4 updated\n",
      "\n",
      "Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 5 updated\n",
      "\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "'citedby_url'\n",
      "no update\n",
      "paper 6 updated\n",
      "\n",
      "Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "citedby already exists\n",
      "no update\n",
      "paper 7 updated\n",
      "\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "'citedby_url'\n",
      "no update\n",
      "paper 8 updated\n",
      "\n",
      "MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "citedby already exists\n",
      "no update\n",
      "paper 9 updated\n",
      "\n",
      "Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space\n",
      "citedby already exists\n",
      "no update\n",
      "paper 10 updated\n",
      "\n",
      "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 11 updated\n",
      "\n",
      "Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\n",
      "citedby already exists\n",
      "no update\n",
      "paper 12 updated\n",
      "\n",
      "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "citedby already exists\n",
      "no update\n",
      "paper 13 updated\n",
      "\n",
      "Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "citedby already exists\n",
      "no update\n",
      "paper 14 updated\n",
      "\n",
      "Skill-based Meta-Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 15 updated\n",
      "\n",
      "Skill-based Model-based Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 16 updated\n",
      "\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "citedby already exists\n",
      "no update\n",
      "paper 17 updated\n",
      "\n",
      "Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback\n",
      "citedby already exists\n",
      "no update\n",
      "paper 18 updated\n",
      "\n",
      "Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "citedby already exists\n",
      "no update\n",
      "paper 19 updated\n",
      "\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "citedby already exists\n",
      "no update\n",
      "paper 20 updated\n",
      "\n",
      "Demonstration-Guided Reinforcement Learning with Learned Skills\n",
      "citedby already exists\n",
      "no update\n",
      "paper 21 updated\n",
      "\n",
      "Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "citedby already exists\n",
      "no update\n",
      "paper 22 updated\n",
      "\n",
      "Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "citedby already exists\n",
      "no update\n",
      "paper 23 updated\n",
      "\n",
      "Hierarchical Skills for Efficient Exploration\n",
      "citedby already exists\n",
      "no update\n",
      "paper 24 updated\n",
      "\n",
      "Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space\n",
      "citedby already exists\n",
      "no update\n",
      "paper 25 updated\n",
      "\n",
      "Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "Deep hierarchical planning from pixels\n",
      "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains\n",
      "DMAP: a Distributed Morphological Attention Policy for learning to locomote with a changing body\n",
      "Learning Options via Compression\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n",
      "Meta-Learning Transferable Parameterized Skills\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "Skill Learning for Long-Horizon Sequential Tasks\n",
      "Data efficiency in imitation learning with a focus on object manipulation\n",
      "update pkl\n",
      "paper 26 updated\n",
      "\n",
      "Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "Temporal difference learning for model predictive control\n",
      "Learn2assemble with structured representations and search for robotic architectural construction\n",
      "Learning dynamics models for model predictive agents\n",
      "Robot learning of mobile manipulation with reachability behavior priors\n",
      "Grid: Gpu-accelerated rigid body dynamics with analytical gradients\n",
      "Active exploration for robotic manipulation\n",
      "Data-Driven Risk-Sensitive Control for Personalized Lane Change Maneuvers\n",
      "On the feasibility of learning finger-gaiting in-hand manipulation with intrinsic sensing\n",
      "Force-based simultaneous mapping and object reconstruction for robotic manipulation\n",
      "Navigational behavior of humans and deep reinforcement learning agents\n",
      "Mjolnir: A framework agnostic auto-tuning system with deep reinforcement learning\n",
      "Scalable Model-based Policy Optimization for Decentralized Networked Systems\n",
      "Performance Analysis and Flexible Control of a Novel Ball Double-screw Hydraulic Robot Knee Joint\n",
      "Value Summation: A Novel Scoring Function for MPC-based Model-based Reinforcement Learning\n",
      "Dynamic Mirror Descent based Model Predictive Control for Accelerating Robot Learning\n",
      "Backward Imitation and Forward Reinforcement Learning via Bi-directional Model Rollouts\n",
      "Goal and Force Switching Policy for DMP-Based Manipulation\n",
      "Safety Aspects of Data-Driven Control in Contact-Rich Manipulation\n",
      "GPU Acceleration for Real-Time, Whole-Body, Nonlinear Model Predictive Controla\n",
      "Supplementary Document\n",
      "Learning to Visually Observe, Plan, and Control Compliant Robot In-Hand Manipulation\n",
      "Latent State-Space Models for Control\n",
      "A fault-tolerant and robust controller using model predictive path integral control for free-flying space robots\n",
      "update pkl\n",
      "paper 27 updated\n",
      "\n",
      "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "How to leverage unlabeled data in offline reinforcement learning\n",
      "Making linear mdps practical via contrastive representation learning\n",
      "Eliciting compatible demonstrations for multi-human imitation learning\n",
      "Interactive language: Talking to robots in real time\n",
      "Chain of thought imitation with procedure cloning\n",
      "Imitation learning by estimating expertise of demonstrators\n",
      "PI-ARS: Accelerating Evolution-Learned Visual-Locomotion with Predictive Information Representations\n",
      "Learning Options via Compression\n",
      "Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "Pretraining in Deep Reinforcement Learning: A Survey\n",
      "Learning and Retrieval from Prior Data for Skill-based Imitation Learning\n",
      "Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "Semi-Supervised Imitation Learning of Team Policies from Suboptimal Demonstrations\n",
      "Energy-based Predictive Representation for Reinforcement Learning\n",
      "Imitation learning with auxiliary, suboptimal, and task-agnostic data\n",
      "Building Versatile Reinforcement Learning Agents with Offline Data\n",
      "Generative Adversarial Imitation Learning from Human Behavior with Reward Shaping\n",
      "Pre-Training for Robots: Leveraging Diverse Multitask Data via Offline Reinforcement Learning\n",
      "Pre-Training for Robots: Leveraging Diverse Multitask Data via Offline Reinforcement Learning\n",
      "Expertise Combination with Spectral Clustering in Inverse Reinforcement Learning\n",
      "One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control\n",
      "update pkl\n",
      "paper 28 updated\n",
      "\n",
      "Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "Decision transformer: Reinforcement learning via sequence modeling\n",
      "Behavior Transformers: Cloning  modes with one stone\n",
      "What matters in learning from offline human demonstrations for robot manipulation\n",
      "Goal-conditioned reinforcement learning with imagined subgoals\n",
      "Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble\n",
      "Latent plans for task-agnostic offline reinforcement learning\n",
      "Hierarchical skills for efficient exploration\n",
      "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress\n",
      "IKEA furniture assembly environment for long-horizon complex manipulation tasks\n",
      "Learning to synthesize programs as interpretable and generalizable policies\n",
      "Transformers are adaptable task planners\n",
      "Retrieval-augmented reinforcement learning\n",
      "Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning\n",
      "Trail: Near-optimal imitation learning with suboptimal data\n",
      "Guided reinforcement learning with learned skills\n",
      "Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search\n",
      "Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks\n",
      "Imitating human behaviour with diffusion models\n",
      "Learning latent actions to control assistive robots\n",
      "Skill-based meta-reinforcement learning\n",
      "Skill preferences: Learning to extract and execute robotic skills from human feedback\n",
      "Retrospectives on the embodied ai workshop\n",
      "Learning neuro-symbolic skills for bilevel planning\n",
      "Learning transferable motor skills with hierarchical latent mixture policies\n",
      "Hierarchical few-shot imitation with skill transition models\n",
      "Dichotomy of control: Separating what you can control from what you cannot\n",
      "Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "Adversarial skill chaining for long-horizon robot manipulation via terminal state regularization\n",
      "Behavioral priors and dynamics models: Improving performance and domain transfer in offline rl\n",
      "MPR-RL: multi-prior regularized reinforcement learning for knowledge transfer\n",
      "Guided Reinforcement Learning: A Review and Evaluation for Efficient and Effective Real-World Robotics\n",
      "Robot learning of mobile manipulation with reachability behavior priors\n",
      "SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition\n",
      "Manipulation planning from demonstration via goal-conditioned prior action primitive decomposition and alignment\n",
      "Hyper-decision transformer for efficient online policy adaptation\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "Goal-conditioned imitation learning using score-based diffusion policies\n",
      "Action priors for large action spaces in robotics\n",
      "Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts\n",
      "Learning Options via Compression\n",
      "Mo2: Model-based offline options\n",
      "Towards deployment-efficient reinforcement learning: Lower bound and optimality\n",
      "Discovering generalizable skills via automated generation of diverse tasks\n",
      "Skill-based model-based reinforcement learning\n",
      "Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning\n",
      "Towards Physically Adversarial Intelligent Networks (PAINs) for Safer Self-Driving\n",
      "Priors, hierarchy, and information asymmetry for skill transfer in reinforcement learning\n",
      "Variable Impedance Skill Learning for Contact-Rich Manipulation\n",
      "Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning\n",
      "Guided Skill Learning and Abstraction for Long-Horizon Manipulation\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "to-Executable Trajectory Translation for One-Shot Task Generalization\n",
      "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation\n",
      "SFP: State-free priors for exploration in off-policy reinforcement learning\n",
      "Learning to share autonomy across repeated interaction\n",
      "Pretraining in Deep Reinforcement Learning: A Survey\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "TAPS: Task-Agnostic Policy Sequencing\n",
      "Self-Supervised Skill Learning for Semi-Supervised Long-Horizon Instruction Following\n",
      "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n",
      "Learning and Retrieval from Prior Data for Skill-based Imitation Learning\n",
      "Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction\n",
      "Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences\n",
      "Leveraging Demonstrations with Latent Space Priors\n",
      "FIRL: Fast Imitation and Policy Reuse Learning\n",
      "Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation\n",
      "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation\n",
      "Constrained Exploration in Reinforcement Learning with Optimality Preservation\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models\n",
      "Contrastive Language, Action, and State Pre-training for Robot Learning\n",
      "Efficient Learning of High Level Plans from Play\n",
      "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model\n",
      "Cross-Domain Transfer via Semantic Skill Imitation\n",
      "Chain-of-Thought Predictive Control\n",
      "A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization\n",
      "Learning Visuo-Motor Behaviours for Robot Locomotion Over Difficult Terrain\n",
      "Learning latent actions without human demonstrations\n",
      "Beyond Reward: Offline Preference-guided Policy Optimization\n",
      "Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning\n",
      "Stap: Sequencing task-agnostic policies\n",
      "Distance Weighted Supervised Learning for Offline Interaction Data\n",
      "Reinforcement learning for assembly robots: A review\n",
      "Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation\n",
      "Learning impedance actions for safe reinforcement learning in contact-rich tasks\n",
      "Learning Dynamic Manipulation Skills from Haptic-Play\n",
      "A Survey of Demonstration Learning\n",
      "Hierarchical Decision Transformer\n",
      "An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation\n",
      "Meta-Reinforcement Learning via Exploratory Task Clustering\n",
      "Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning\n",
      "Transferring Knowledge for Reinforcement Learning in Contact-Rich Manipulation\n",
      "Learning Setup Policies: Reliable Transition Between Locomotion Behaviours\n",
      "Robot target location based on the difference in monocular vision projection\n",
      "Stochastic Policy Optimization with Heuristic Information for Robot Learning\n",
      "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning\n",
      "Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment\n",
      "Learning to Drive Using Sparse Imitation Reinforcement Learning\n",
      "The StarCraft Multi-Agent Challenges+: Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions\n",
      "Uncertainty-Aware Hierarchical Reinforcement Learning Robust to Noisy Observations\n",
      "Generating Goal-conditioned Sub-goals for Hierarchical Learning\n",
      "Policy Transfer via Skill Adaptation and Composition\n",
      "Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data\n",
      "Pseudometric guided online query and update for offline reinforcement learning\n",
      "Pre-training Agents for Design Optimization and Control\n",
      "Broad Generalization Through Domain Transfer: Abstractions and Algorithms\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "Decision transformer: Reinforcement learning via sequence modeling\n",
      "Behavior Transformers: Cloning  modes with one stone\n",
      "What matters in learning from offline human demonstrations for robot manipulation\n",
      "Goal-conditioned reinforcement learning with imagined subgoals\n",
      "Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble\n",
      "Latent plans for task-agnostic offline reinforcement learning\n",
      "Hierarchical skills for efficient exploration\n",
      "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress\n",
      "IKEA furniture assembly environment for long-horizon complex manipulation tasks\n",
      "Learning to synthesize programs as interpretable and generalizable policies\n",
      "Transformers are adaptable task planners\n",
      "Retrieval-augmented reinforcement learning\n",
      "Surrol: An open-source reinforcement learning centered and dvrk compatible platform for surgical robot learning\n",
      "Trail: Near-optimal imitation learning with suboptimal data\n",
      "Guided reinforcement learning with learned skills\n",
      "Model-based meta reinforcement learning using graph structured surrogate models and amortized policy search\n",
      "Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks\n",
      "Imitating human behaviour with diffusion models\n",
      "Learning latent actions to control assistive robots\n",
      "Skill-based meta-reinforcement learning\n",
      "Skill preferences: Learning to extract and execute robotic skills from human feedback\n",
      "Retrospectives on the embodied ai workshop\n",
      "Learning neuro-symbolic skills for bilevel planning\n",
      "Learning transferable motor skills with hierarchical latent mixture policies\n",
      "Hierarchical few-shot imitation with skill transition models\n",
      "Dichotomy of control: Separating what you can control from what you cannot\n",
      "Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "Adversarial skill chaining for long-horizon robot manipulation via terminal state regularization\n",
      "Behavioral priors and dynamics models: Improving performance and domain transfer in offline rl\n",
      "MPR-RL: multi-prior regularized reinforcement learning for knowledge transfer\n",
      "Guided Reinforcement Learning: A Review and Evaluation for Efficient and Effective Real-World Robotics\n",
      "Robot learning of mobile manipulation with reachability behavior priors\n",
      "SAFER: Data-Efficient and Safe Reinforcement Learning via Skill Acquisition\n",
      "Manipulation planning from demonstration via goal-conditioned prior action primitive decomposition and alignment\n",
      "Hyper-decision transformer for efficient online policy adaptation\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "Goal-conditioned imitation learning using score-based diffusion policies\n",
      "Action priors for large action spaces in robotics\n",
      "Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts\n",
      "Learning Options via Compression\n",
      "Mo2: Model-based offline options\n",
      "Towards deployment-efficient reinforcement learning: Lower bound and optimality\n",
      "Discovering generalizable skills via automated generation of diverse tasks\n",
      "Skill-based model-based reinforcement learning\n",
      "Juewu-mc: Playing minecraft with sample-efficient hierarchical reinforcement learning\n",
      "Towards Physically Adversarial Intelligent Networks (PAINs) for Safer Self-Driving\n",
      "Priors, hierarchy, and information asymmetry for skill transfer in reinforcement learning\n",
      "Variable Impedance Skill Learning for Contact-Rich Manipulation\n",
      "Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning\n",
      "Guided Skill Learning and Abstraction for Long-Horizon Manipulation\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "to-Executable Trajectory Translation for One-Shot Task Generalization\n",
      "Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation\n",
      "SFP: State-free priors for exploration in off-policy reinforcement learning\n",
      "Learning to share autonomy across repeated interaction\n",
      "Pretraining in Deep Reinforcement Learning: A Survey\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "TAPS: Task-Agnostic Policy Sequencing\n",
      "Self-Supervised Skill Learning for Semi-Supervised Long-Horizon Instruction Following\n",
      "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n",
      "Learning and Retrieval from Prior Data for Skill-based Imitation Learning\n",
      "Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction\n",
      "Relative Behavioral Attributes: Filling the Gap between Symbolic Goal Specification and Reward Learning from Human Preferences\n",
      "Leveraging Demonstrations with Latent Space Priors\n",
      "FIRL: Fast Imitation and Policy Reuse Learning\n",
      "Decoupling Skill Learning from Robotic Control for Generalizable Object Manipulation\n",
      "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation\n",
      "Constrained Exploration in Reinforcement Learning with Optimality Preservation\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "Model-based Meta Reinforcement Learning using Graph Structured Surrogate Models\n",
      "Contrastive Language, Action, and State Pre-training for Robot Learning\n",
      "Efficient Learning of High Level Plans from Play\n",
      "EUCLID: Towards Efficient Unsupervised Reinforcement Learning with Multi-choice Dynamics Model\n",
      "Cross-Domain Transfer via Semantic Skill Imitation\n",
      "Chain-of-Thought Predictive Control\n",
      "A Survey on Deep Reinforcement Learning-based Approaches for Adaptation and Generalization\n",
      "Learning Visuo-Motor Behaviours for Robot Locomotion Over Difficult Terrain\n",
      "Learning latent actions without human demonstrations\n",
      "Beyond Reward: Offline Preference-guided Policy Optimization\n",
      "Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning\n",
      "Stap: Sequencing task-agnostic policies\n",
      "Distance Weighted Supervised Learning for Offline Interaction Data\n",
      "Reinforcement learning for assembly robots: A review\n",
      "Natural Language-conditioned Reinforcement Learning with Inside-out Task Language Development and Translation\n",
      "Learning impedance actions for safe reinforcement learning in contact-rich tasks\n",
      "Learning Dynamic Manipulation Skills from Haptic-Play\n",
      "A Survey of Demonstration Learning\n",
      "Hierarchical Decision Transformer\n",
      "An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation\n",
      "Meta-Reinforcement Learning via Exploratory Task Clustering\n",
      "Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning\n",
      "Transferring Knowledge for Reinforcement Learning in Contact-Rich Manipulation\n",
      "Learning Setup Policies: Reliable Transition Between Locomotion Behaviours\n",
      "Robot target location based on the difference in monocular vision projection\n",
      "Stochastic Policy Optimization with Heuristic Information for Robot Learning\n",
      "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning\n",
      "Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment\n",
      "Learning to Drive Using Sparse Imitation Reinforcement Learning\n",
      "The StarCraft Multi-Agent Challenges+: Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions\n",
      "Uncertainty-Aware Hierarchical Reinforcement Learning Robust to Noisy Observations\n",
      "Generating Goal-conditioned Sub-goals for Hierarchical Learning\n",
      "Policy Transfer via Skill Adaptation and Composition\n",
      "Boosting the Convergence of Reinforcement Learning-based Auto-pruning Using Historical Data\n",
      "Pseudometric guided online query and update for offline reinforcement learning\n",
      "Pre-training Agents for Design Optimization and Control\n",
      "Broad Generalization Through Domain Transfer: Abstractions and Algorithms\n",
      "DISCO-DANCE: Learning to Discover Skills with Guidance\n",
      "Self-taught Robots: Autonomous and Weakly-Supervised Learning for Robotic Manipulation\n",
      "Relative Entropy Regularized Sample Efficient Reinforcement Learning with Continuous Actions\n",
      "Unravel Structured Heterogeneity of Tasks in Meta-Reinforcement Learning via Exploratory Clustering\n",
      "Enabling Human-Robot Partnerships in Digitally-Driven Construction Work through Integration of Building Information Models, Interactive Virtual Reality, and Process …\n",
      "Discovering knowledge abstractions for sample efficient embodied transfer learning\n",
      "Learning from Demonstration using Hierarchical Reinforcement Learning for Robotic Skill transferability.\n",
      "Of Priors and Particles: Structured and Distributed Approaches to Robot Perception and Control\n",
      "Gene Regulatory Network control with Reinforcement Learning and Optimal Control/Author Ionella Buzatu, BSc\n",
      "VARIATIONAL REPARAMETRIZED POLICY LEARNING WITH DIFFERENTIABLE PHYSICS\n",
      "SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling\n",
      "Optimization Frameworks for Compact DNN Acceleration Based on Collaborative Modeling and History Data\n",
      "Structured and Distributed Representations for Perception and Control\n",
      "Inducing Reusable Skills From Demonstrations with Option-Controller Network\n",
      "Decoupling skill learning from robotic control for generalizable manipulation\n",
      "SAFER: Data-Efficient and Safe Reinforcement Learning Through Skill Acquisition\n",
      "OPTIDICE: OFFLINE POLICY OPTIMIZATION VIA STATIONARY DISTRIBUTION CORRECTION ESTIMA\n",
      "One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control\n",
      "계층적 에이전트의 목표 조건부 하위 목표 학습을 위한 커리큘럼 학습\n",
      "Evaluating Generalization of Policy Learning Under Domain Shifts\n",
      "로봇 물따르기 조작 스킬 학습을 위한 시뮬레이션 기반 훈련 프레임워크 구현\n",
      "update pkl\n",
      "paper 29 updated\n",
      "\n",
      "Behavior Priors for Efficient Reinforcement Learning\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "Can wikipedia help offline reinforcement learning?\n",
      "Reward is enough for convex MDPs\n",
      "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors\n",
      "How to spend your robot time: Bridging kickstarting and offline reinforcement learning for vision-based robotic manipulation\n",
      "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning\n",
      "Learning agile soccer skills for a bipedal robot with deep reinforcement learning\n",
      "From motor control to team play in simulated humanoid football\n",
      "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains\n",
      "Humans account for cognitive costs when finding shortcuts: An information-theoretic analysis of navigation\n",
      "Generative Planning for Temporally Coordinated Exploration in Reinforcement Learning\n",
      "Hierarchical clustering optimizes the tradeoff between compositionality and expressivity of task structures for flexible reinforcement learning\n",
      "Coordinating Policies Among Multiple Agents via an Intelligent Communication Channel\n",
      "A Unified Theory of Dual-Process Control\n",
      "Leveraging Demonstrations with Latent Space Priors\n",
      "Minimum Description Length Control\n",
      "Efficient Learning of High Level Plans from Play\n",
      "Data augmentation for efficient learning from parametric experts\n",
      "TempoRL: Temporal Priors for Exploration in Off-Policy Reinforcement Learning\n",
      "A Simple Approach to Continual Learning by Transferring Skill Parameters\n",
      "Reinforcement Learning for Vision-based Object Manipulation with Non-parametric Policy and Action Primitives\n",
      "Discovering knowledge abstractions for sample efficient embodied transfer learning\n",
      "Reinforcement learning for charged-particle tracking\n",
      "Incentivizing User-centric Resource Allocation in Wireless Networks in Realtime\n",
      "update pkl\n",
      "paper 30 updated\n",
      "\n",
      "Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "Amp: Adversarial motion priors for stylized physics-based character control\n",
      "Accelerating reinforcement learning with learned skill priors\n",
      "dm_control: Software and tasks for continuous control\n",
      "Simpoe: Simulated character control for 3d human pose estimation\n",
      "Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters\n",
      "A survey on reinforcement learning methods in character animation\n",
      "Stochastic scene-aware motion prediction\n",
      "Goal: Generating 4d whole-body motion for hand-object grasping\n",
      "Hierarchical skills for efficient exploration\n",
      "Deepphase: Periodic autoencoders for learning motion phase manifolds\n",
      "Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers\n",
      "From motor control to team play in simulated humanoid football\n",
      "A Survey on Deep Learning for Skeleton‐Based Human Animation\n",
      "Physics-based character controllers using conditional vaes\n",
      "Comic: Complementary task learning & mimicry for reusable skills\n",
      "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors\n",
      "Learning a family of motor skills from a single motion clip\n",
      "Guided reinforcement learning with learned skills\n",
      "Manipnet: neural manipulation synthesis with a hand-object spatial representation\n",
      "Human dynamics from monocular video with dynamic camera movements\n",
      "Skill-based meta-reinforcement learning\n",
      "Learning to use chopsticks in diverse gripping styles\n",
      "Learning transferable motor skills with hierarchical latent mixture policies\n",
      "Unicon: Universal neural controller for physics-based character motion\n",
      "Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "ControlVAE: Model-Based Learning of Generative Controllers for Physics-Based Characters\n",
      "From motor control to team play in simulated humanoid football\n",
      "Vision-guided quadrupedal locomotion in the wild with multi-modal delay randomization\n",
      "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control\n",
      "Through hawks' eyes: synthetically reconstructing the visual field of a bird in flight\n",
      "Pose2room: understanding 3D scenes from human activities\n",
      "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains\n",
      "Mo2: Model-based offline options\n",
      "QuestSim: Human Motion Tracking from Sparse Sensors with Simulated Avatars\n",
      "Flexible motion optimization with modulated assistive forces\n",
      "Generative gaitnet\n",
      "Efficient hyperparameter optimization for physics-based character animation\n",
      "Learning Virtual Chimeras by Dynamic Motion Reassembly\n",
      "Learning to Transfer In‐Hand Manipulations Using a Greedy Shape Curriculum\n",
      "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n",
      "Beyond tabula-rasa: a modular reinforcement learning approach for physically embedded 3d sokoban\n",
      "Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments\n",
      "Neural3Points: Learning to Generate Physically Realistic Full-body Motion for Virtual Reality Users\n",
      "RoboPianist: A Benchmark for High-Dimensional Robot Control\n",
      "Learning human search behavior from egocentric visual inputs\n",
      "Synthesizing Get‐Up Motions for Physics‐based Characters\n",
      "Efficient Multi-Task Learning via Iterated Single-Task Transfer\n",
      "Synthesizing Physical Character-Scene Interactions\n",
      "Composite Motion Learning with Task Control\n",
      "Learning High-Risk High-Precision Motion Control\n",
      "Learning Task-Agnostic Action Spaces for Movement Optimization\n",
      "Interactive characters for virtual reality stories\n",
      "Dash: Modularized human manipulation simulation with vision and language for embodied ai\n",
      "PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors\n",
      "Simulation and Retargeting of Complex Multi-Character Interactions\n",
      "Perpetual Humanoid Control for Real-time Simulated Avatars\n",
      "Efficient Trust Region-Based Safe Reinforcement Learning with Low-Bias Distributional Actor-Critic\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "A Simple Approach to Continual Learning by Transferring Skill Parameters\n",
      "Deep reinforcement learning for task planning of virtual characters\n",
      "Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XX\n",
      "Techniques in learning-based approaches for character animation\n",
      "Physics-based character animation for Virtual Reality\n",
      "SDAC: Efficient Safe Reinforcement Learning with Low-Biased Distributional Actor-Critic\n",
      "Design and training of deep reinforcement learning agents\n",
      "Differentiable Robotics: Compositional Deep Learning with Differentiable Algorithm Networks\n",
      "Discovering knowledge abstractions for sample efficient embodied transfer learning\n",
      "Creating Deep Learning-based Acrobatic Videos Using Imitation Videos.\n",
      "Directable physics-based character animation\n",
      "Acquiring Motor Skills Through Motion Imitation and Reinforcement Learning\n",
      "A Supervised Learning Framework for Physics-based Controllers Using Stochastic Model Predictive Control\n",
      "Multiple Dynamic Pivots Rig for 3D Biped Character Designed along Human-Computer Interaction Principles\n",
      "Synthétisation de mouvements de lever du sol pour les personnages basés sur la physique\n",
      "CoMic: Complementary Task Learning & Mimicry for Reusable Skills Supplementary Material\n",
      "Laboratorio VISGRAF\n",
      "확률적 모델예측제어를 이용한 물리기반 제어기 지도 학습 프레임워크\n",
      "update pkl\n",
      "paper 31 updated\n",
      "\n",
      "CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "Hierarchical reinforcement learning: A comprehensive survey\n",
      "Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters\n",
      "A distributional view on multi-objective policy optimization\n",
      "From motor control to team play in simulated humanoid football\n",
      "Physics-based character controllers using conditional vaes\n",
      "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors\n",
      "Transferable and adaptable driving behavior prediction\n",
      "From motor control to team play in simulated humanoid football\n",
      "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control\n",
      "Ostrichrl: A musculoskeletal ostrich simulation to study bio-mechanical locomotion\n",
      "Learning coordinated terrain-adaptive locomotion by imitating a centroidal dynamics planner\n",
      "Hierarchical adaptable and transferable networks (hatn) for driving behavior prediction\n",
      "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains\n",
      "Unsupervised domain adaptation with dynamics-aware rewards in reinforcement learning\n",
      "Visual pre-training for navigation: What can we learn from noise?\n",
      "Manipulator-independent representations for visual imitation\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Hierarchical primitive composition: Simultaneous activation of skills with inconsistent action dimensions in multiple hierarchies\n",
      "User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks\n",
      "Leveraging Demonstrations with Latent Space Priors\n",
      "Efficient Multi-Task Learning via Iterated Single-Task Transfer\n",
      "DiffMimic: Efficient Motion Mimicking with Differentiable Physics\n",
      "Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control\n",
      "Learning Task-Agnostic Action Spaces for Movement Optimization\n",
      "PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors\n",
      "Perpetual Humanoid Control for Real-time Simulated Avatars\n",
      "Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach\n",
      "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation\n",
      "A Simple Approach to Continual Learning by Transferring Skill Parameters\n",
      "Design and training of deep reinforcement learning agents\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "Hierarchical reinforcement learning: A comprehensive survey\n",
      "Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters\n",
      "A distributional view on multi-objective policy optimization\n",
      "From motor control to team play in simulated humanoid football\n",
      "Physics-based character controllers using conditional vaes\n",
      "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors\n",
      "Transferable and adaptable driving behavior prediction\n",
      "From motor control to team play in simulated humanoid football\n",
      "MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control\n",
      "Ostrichrl: A musculoskeletal ostrich simulation to study bio-mechanical locomotion\n",
      "Learning coordinated terrain-adaptive locomotion by imitating a centroidal dynamics planner\n",
      "Hierarchical adaptable and transferable networks (hatn) for driving behavior prediction\n",
      "Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains\n",
      "Unsupervised domain adaptation with dynamics-aware rewards in reinforcement learning\n",
      "Visual pre-training for navigation: What can we learn from noise?\n",
      "Manipulator-independent representations for visual imitation\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Hierarchical primitive composition: Simultaneous activation of skills with inconsistent action dimensions in multiple hierarchies\n",
      "User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks\n",
      "Leveraging Demonstrations with Latent Space Priors\n",
      "Efficient Multi-Task Learning via Iterated Single-Task Transfer\n",
      "DiffMimic: Efficient Motion Mimicking with Differentiable Physics\n",
      "Reinforcement Learning for Legged Robots: Motion Imitation from Model-Based Optimal Control\n",
      "Learning Task-Agnostic Action Spaces for Movement Optimization\n",
      "PMP: Learning to Physically Interact with Environments using Part-wise Motion Priors\n",
      "Perpetual Humanoid Control for Real-time Simulated Avatars\n",
      "Learning Multimodal Bipedal Locomotion and Implicit Transitions: A Versatile Policy Approach\n",
      "Towards Context-Aware Human-like Pointing Gestures with RL Motion Imitation\n",
      "A Simple Approach to Continual Learning by Transferring Skill Parameters\n",
      "Design and training of deep reinforcement learning agents\n",
      "Visual Pre-training for Navigation: What Can We Learn from Noise?\n",
      "update pkl\n",
      "paper 32 updated\n",
      "\n",
      "Discovering Motor Programs by Recomposing Demonstrations\n",
      "Intelligent problem-solving as integrated hierarchical reinforcement learning\n",
      "Accelerating reinforcement learning with learned skill priors\n",
      "Mt-opt: Continuous multi-task robotic reinforcement learning at scale\n",
      "Parrot: Data-driven behavioral priors for reinforcement learning\n",
      "Accelerating robotic reinforcement learning via parameterized action primitives\n",
      "Learning robot skills with temporal variational inference\n",
      "Rapid adaptation of brain–computer interfaces to new neuronal ensembles or participants via generative modelling\n",
      "Trail: Near-optimal imitation learning with suboptimal data\n",
      "Guided reinforcement learning with learned skills\n",
      "Learning neuro-symbolic skills for bilevel planning\n",
      "Hierarchical principles of embodied reinforcement learning: A review\n",
      "Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "Hierarchically decoupled imitation for morphological transfer\n",
      "Scaling up multi-task robotic reinforcement learning\n",
      "Learning Options via Compression\n",
      "Skill-based model-based reinforcement learning\n",
      "Augmenting policy learning with routines discovered from a single demonstration\n",
      "Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis\n",
      "Assistive Teaching of Motor Control Tasks to Humans\n",
      "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey\n",
      "Pretraining in Deep Reinforcement Learning: A Survey\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "TAPS: Task-Agnostic Policy Sequencing\n",
      "NetHack is Hard to Hack\n",
      "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors\n",
      "One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation\n",
      "Learning and Retrieval from Prior Data for Skill-based Imitation Learning\n",
      "Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction\n",
      "Translating Robot Skills: Learning Unsupervised Skill Correspondences Across Robots\n",
      "Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation\n",
      "One After Another: Learning Incremental Skills for a Changing World\n",
      "Stap: Sequencing task-agnostic policies\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "SHERLock: Self-Supervised Hierarchical Event Representation Learning\n",
      "Continuous Control with Action Quantization from Demonstrations\n",
      "Self-Organization of Action Hierarchy and Inferring Latent States in Deep Reinforcement Learning with Stochastic Recurrent Neural Networks\n",
      "Unsupervised Hierarchical Concept Learning\n",
      "Real World Robot Learning: Learned Rewards, Offline Datasets and Skill Re-Use\n",
      "Weakly-Supervised Learning of Disentangled and Interpretable Skills for Hierarchical Reinforcement Learning\n",
      "update pkl\n",
      "paper 33 updated\n",
      "\n",
      "Hierarchical reinforcement learning for efficent exploration and transfer\n",
      "Disentangling transfer in continual reinforcement learning\n",
      "Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "Reinforcement learning vibration control of a multi-flexible beam coupling system\n",
      "Learning Robust Real-Time Cultural Transmission without Human Data\n",
      "Efficient exploration for multi-agent reinforcement learning via transferable successor features\n",
      "Challenging social media threats using collective well-being aware recommendation algorithms and an educational virtual companion\n",
      "Toward an Adaptive Threshold on Cooperative Bandwidth Management Based on Hierarchical Reinforcement Learning\n",
      "Efficient Search of Active Inference Policy Spaces Using k-Means\n",
      "update pkl\n",
      "paper 34 updated\n",
      "\n",
      "Learning quadrupedal locomotion over challenging terrain\n",
      "How to train your robot with deep reinforcement learning: lessons we have learned\n",
      "Perspectives in machine learning for wildlife conservation\n",
      "Learning robust perceptive locomotion for quadrupedal robots in the wild\n",
      "Daydreamer: World models for physical robot learning\n",
      "Rma: Rapid motor adaptation for legged robots\n",
      "Augmented language models: a survey\n",
      "Legged locomotion in challenging terrains using egocentric vision\n",
      "A system for general in-hand object re-orientation\n",
      "Policy finetuning: Bridging sample-efficient offline and online reinforcement learning\n",
      "What matters in learning from offline human demonstrations for robot manipulation\n",
      "Reinforcement learning for robust parameterized locomotion control of bipedal robots\n",
      "d3rlpy: An offline deep reinforcement learning library\n",
      "Autonomous drone racing with deep reinforcement learning\n",
      "Deep whole-body control: learning a unified policy for manipulation and locomotion\n",
      "Learning to drive from a world on rails\n",
      "Blind bipedal stair traversal via sim-to-real reinforcement learning\n",
      "In-hand object rotation via rapid motor adaptation\n",
      "Machine intelligence for chemical reaction space\n",
      "Neural-fly enables rapid learning for agile flight in strong winds\n",
      "Neuromorphic computing hardware and neural architectures for robotics\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n",
      "How to train your robot with deep reinforcement learning: lessons we have learned\n",
      "Perspectives in machine learning for wildlife conservation\n",
      "Learning robust perceptive locomotion for quadrupedal robots in the wild\n",
      "Daydreamer: World models for physical robot learning\n",
      "Rma: Rapid motor adaptation for legged robots\n",
      "Augmented language models: a survey\n",
      "Legged locomotion in challenging terrains using egocentric vision\n",
      "A system for general in-hand object re-orientation\n",
      "Policy finetuning: Bridging sample-efficient offline and online reinforcement learning\n",
      "What matters in learning from offline human demonstrations for robot manipulation\n",
      "Reinforcement learning for robust parameterized locomotion control of bipedal robots\n",
      "d3rlpy: An offline deep reinforcement learning library\n",
      "Autonomous drone racing with deep reinforcement learning\n",
      "Deep whole-body control: learning a unified policy for manipulation and locomotion\n",
      "Learning to drive from a world on rails\n",
      "Blind bipedal stair traversal via sim-to-real reinforcement learning\n",
      "In-hand object rotation via rapid motor adaptation\n",
      "Machine intelligence for chemical reaction space\n",
      "Neural-fly enables rapid learning for agile flight in strong winds\n",
      "Neuromorphic computing hardware and neural architectures for robotics\n",
      "Learning visible connectivity dynamics for cloth smoothing\n",
      "i-sim2real: Reinforcement learning of robotic policies in tight human-robot interaction loops\n",
      "Perceptive Locomotion Through Nonlinear Model-Predictive Control\n",
      "Rloc: Terrain-aware legged locomotion using reinforcement learning and optimal control\n",
      "Genloco: Generalized locomotion controllers for quadrupedal robots\n",
      "Self-reconfigurable multilegged robot swarms collectively accomplish challenging terradynamic tasks\n",
      "Fast and efficient locomotion via learned gait transitions\n",
      "Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review\n",
      "Causal navigation by continuous-time neural networks\n",
      "Learning where to trust unreliable models in an unstructured world for deformable object manipulation\n",
      "Rapid locomotion via reinforcement learning\n",
      "Super-human performance in gran turismo sport using deep reinforcement learning\n",
      "Policy search for model predictive control with application to agile drone flight\n",
      "Real-world embodied AI through a morphologically adaptive quadruped robot\n",
      "Is bang-bang control all you need? solving continuous control with bernoulli policies\n",
      "Auto-tuned sim-to-real transfer\n",
      "Walk these ways: Tuning robot control for generalization with multiplicity of behavior\n",
      "Deep reinforcement learning with shallow controllers: An experimental application to PID tuning\n",
      "Concurrent training of a control policy and a state estimator for dynamic and robust legged locomotion\n",
      "Legged robots that keep on learning: Fine-tuning locomotion policies in the real world\n",
      "Glide: Generalizable quadrupedal locomotion in diverse environments with a centroidal model\n",
      "A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning\n",
      "Learning quadrupedal locomotion on deformable terrain\n",
      "Minimizing energy consumption leads to the emergence of gaits in legged robots\n",
      "Learning vision-guided quadrupedal locomotion end-to-end with cross-modal transformers\n",
      "Measuring and modeling the motor system with machine learning\n",
      "From motor control to team play in simulated humanoid football\n",
      "Parallel learning: Overview and perspective for computational learning across Syn2Real and Sim2Real\n",
      "Dynamics randomization revisited: A case study for quadrupedal locomotion\n",
      "Neural Volumetric Memory for Visual Locomotion Control\n",
      "Secant: Self-expert cloning for zero-shot generalization of visual policies\n",
      "Learning to jump from pixels\n",
      "Autonomous overtaking in gran turismo sport using curriculum reinforcement learning\n",
      "Imitate and repurpose: Learning reusable robot movement skills from human and animal behaviors\n",
      "Dojo: A differentiable simulator for robotics\n",
      "Development of a small-sized quadruped robotic rat capable of multimodal motions\n",
      "Visual-locomotion: Learning to walk on complex terrains with vision\n",
      "What is an artificial muscle? A comparison of soft actuators to biological muscles\n",
      "Diverse auto-curriculum is critical for successful real-world multiagent learning systems\n",
      "Advanced skills through multiple adversarial motion priors in reinforcement learning\n",
      "Tactile sim-to-real policy transfer via real-to-sim image translation\n",
      "Learning free gait transition for quadruped robots via phase-guided controller\n",
      "Fast contact-implicit model-predictive control\n",
      "Biconmp: A nonlinear model predictive control framework for whole body motion planning\n",
      "Adversarial motion priors make good substitutes for complex reward functions\n",
      "Lifelong robotic reinforcement learning by retaining experiences\n",
      "Learning agile skills via adversarial imitation of rough partial demonstrations\n",
      "Learning minimum-time flight in cluttered environments\n",
      "Factory: Fast contact for robotic assembly\n",
      "Robust feedback motion policy design using reinforcement learning on a 3d digit bipedal robot\n",
      "Chain of thought imitation with procedure cloning\n",
      "Accelerated policy learning with parallel differentiable simulation\n",
      "Meta-adaptive nonlinear control: Theory and algorithms\n",
      "Towards real robot learning in the wild: A case study in bipedal locomotion\n",
      "Safe reinforcement learning for legged locomotion\n",
      "A survey of the development of biomimetic intelligence and robotics\n",
      "Real-time multi-contact model predictive control via admm\n",
      "Locomotor transitions in the potential energy landscape-dominated regime\n",
      "Combining learning-based locomotion policy with model-based manipulation for legged mobile manipulators\n",
      "Real-time trajectory adaptation for quadrupedal locomotion using deep reinforcement learning\n",
      "Reinforcement learning with evolutionary trajectory generator: A general approach for quadrupedal locomotion\n",
      "Neural scene representation for locomotion on structured terrain\n",
      "Robust predictive control for quadrupedal locomotion: Learning to close the gap between reduced-and full-order models\n",
      "Robust quadruped jumping via deep reinforcement learning\n",
      "Lyapunov-stable neural-network control\n",
      "Toward a data-driven template model for quadrupedal locomotion\n",
      "Circus anymal: A quadruped learning dexterous manipulation with its limbs\n",
      "Learning Visual Locomotion with Cross-Modal Supervision\n",
      "Rough terrain navigation for legged robots using reachability planning and template learning\n",
      "Context is everything: Implicit identification for dynamics adaptation\n",
      "Trajectory optimization of contact-rich motions using implicit differential dynamic programming\n",
      "Collision-free trajectory planning for a 6-DoF free-floating space robot via hierarchical decoupling optimization\n",
      "Self-supervised learning of lidar odometry for robotic applications\n",
      "Traversing steep and granular martian analog slopes with a dynamic quadrupedal robot\n",
      "Learning to navigate sidewalks in outdoor environments\n",
      "Navigating by touch: haptic Monte Carlo localization via geometric sensing and terrain classification\n",
      "Episodic learning for safe bipedal locomotion with control barrier functions and projection-to-state safety\n",
      "From motor control to team play in simulated humanoid football\n",
      "Contextualize Me--The Case for Context in Reinforcement Learning\n",
      "Towards robust data-driven control synthesis for nonlinear systems with actuation uncertainty\n",
      "Vision-guided quadrupedal locomotion in the wild with multi-modal delay randomization\n",
      "Quadruped capturability and push recovery via a switched-systems characterization of dynamic balance\n",
      "Recent progress in legged robots locomotion control\n",
      "Legged locomotion over irregular terrains: State of the art of human and robot performance\n",
      "Robust and versatile bipedal jumping control through multi-task reinforcement learning\n",
      "Sideways crab-walking is faster and more efficient than forward walking for a hexapod robot\n",
      "Mechanism design and workspace analysis of a hexapod robot\n",
      "Learning torque control for quadrupedal locomotion\n",
      "Learning deep sensorimotor policies for vision-based autonomous drone racing\n",
      "Optimization-based control for dynamic legged robots\n",
      "Learning physical characteristics like animals for legged robots\n",
      "Real-time optimal navigation planning using learned motion costs\n",
      "Backward reachability analysis of neural feedback loops: Techniques for linear and nonlinear systems\n",
      "An efficient locally reactive controller for safe navigation in visual teach and repeat missions\n",
      "Virtual to real-world transfer learning: A systematic review\n",
      "Robot learning of mobile manipulation with reachability behavior priors\n",
      "Automated dynamic algorithm configuration\n",
      "Learning and adapting agile locomotion skills by transferring experience\n",
      "The neuromechanics of animal locomotion: From biology to robotics and back\n",
      "Bridging Model-based Safety and Model-free Reinforcement Learning through System Identification of Low Dimensional Linear Models\n",
      "CPG-RL: Learning central pattern generators for quadruped locomotion\n",
      "A hierarchical framework for quadruped locomotion based on reinforcement learning\n",
      "Adapting rapid motor adaptation for bipedal robots\n",
      "A survey of traversability estimation for mobile robots\n",
      "The need for and feasibility of alternative ground robots to traverse sandy and rocky extraterrestrial terrain\n",
      "Learning Humanoid Locomotion with Transformers\n",
      "Carl: A benchmark for contextual and adaptive reinforcement learning\n",
      "Learning to walk autonomously via reset-free quality-diversity\n",
      "Learning coordinated terrain-adaptive locomotion by imitating a centroidal dynamics planner\n",
      "AdaptiveON: Adaptive outdoor navigation method for stable and reliable motions\n",
      "Robust high-speed running for quadruped robots via deep reinforcement learning\n",
      "Nerf2real: Sim2real transfer of vision-guided bipedal motion skills using neural radiance fields\n",
      "How to pick a mobile robot simulator: A quantitative comparison of CoppeliaSim, Gazebo, MORSE and Webots with a focus on accuracy of motion\n",
      "Robust adversarial reinforcement learning with dissipation inequation constraint\n",
      "High-speed quadrupedal locomotion by imitation-relaxation reinforcement learning\n",
      "Versatile modular neural locomotion control with fast learning\n",
      "Computational design of reconfigurable underactuated linkages for adaptive grippers\n",
      "Sequence model imitation learning with unobserved contexts\n",
      "Structured learning of rigid‐body dynamics: A survey and unified view from a robotics perspective\n",
      "Meta-reinforcement learning for the tuning of PI controllers: An offline approach\n",
      "Real-time optimal landing control of the mit mini cheetah\n",
      "Cocoi: Contact-aware online context inference for generalizable non-planar pushing\n",
      "Human Motion Control of Quadrupedal Robots using Deep Reinforcement Learning\n",
      "Auto-tuning of controller and online trajectory planner for legged robots\n",
      "Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe Quadruped Navigation\n",
      "Achieving natural behavior in a robot using neurally inspired hierarchical perceptual control\n",
      "Learning navigation skills for legged robots with learned robot embeddings\n",
      "A Learning-based Iterative Control Framework for Controlling a Robot Arm with Pneumatic Artificial Muscles\n",
      "Dynamic Bipedal Maneuvers through Sim-to-Real Reinforcement Learning\n",
      "Learning Robust Real-Time Cultural Transmission without Human Data\n",
      "Natural walking with musculoskeletal models using deep reinforcement learning\n",
      "Valuenetqp: Learned one-step optimal control for legged locomotion\n",
      "Control of rough terrain vehicles using deep reinforcement learning\n",
      "Meta reinforcement learning for optimal design of legged robots\n",
      "Prismatic Quasi-Direct-Drives for dynamic quadruped locomotion with high payload capacity\n",
      "DribbleBot: Dynamic legged manipulation in the wild\n",
      "Using physics knowledge for learning rigid-body forward dynamics with gaussian process force priors\n",
      "Learning agile locomotion skills with a mentor\n",
      "A survey of sim-to-real transfer techniques applied to reinforcement learning for bioinspired robots\n",
      "Model-free dynamic control of robotic joints with integrated elastic ligaments\n",
      "Optimizing bipedal maneuvers of single rigid-body models for reinforcement learning\n",
      "Learning inertial odometry for dynamic legged robot state estimation\n",
      "Tiny robot learning: challenges and directions for machine learning in resource-constrained robots\n",
      "Deep reinforcement learning: Opportunities and challenges\n",
      "Multifingered grasping based on multimodal reinforcement learning\n",
      "Unsupervised Learning of Terrain Representations for Haptic Monte Carlo Localization\n",
      "Adaptive motion skill learning of quadruped robot on slopes based on augmented random search algorithm\n",
      "Strength through diversity: Robust behavior learning via mixture policies\n",
      "On Slip Detection for Quadruped Robots\n",
      "Adaptive actuation of magnetic soft robots using deep reinforcement learning\n",
      "Google Scholar is aggressively blocking us! Quitting for now.\n",
      "update FreeProxy!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rw\u001b[39m.\u001b[39;49mupdate_all_citedby()\n",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_all_citedby\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaper_list\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m         \u001b[39m#time.sleep(random.uniform(2, 5))\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_citedby(key)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_pkl()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpaper \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m updated\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(key))\n",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 5\u001b[0m in \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m         citation_list \u001b[39m=\u001b[39m scholarly\u001b[39m.\u001b[39;49mcitedby(paper[\u001b[39m'\u001b[39;49m\u001b[39minfo\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         cl \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#W4sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         \u001b[39mfor\u001b[39;00m i, c \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(citation_list):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\_scholarly.py:287\u001b[0m, in \u001b[0;36m_Scholarly.citedby\u001b[1;34m(self, object)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnum_citations\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m PublicationParser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__nav)\u001b[39m.\u001b[39;49mcitedby(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mSince the paper titled \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m citations (>1000), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mfetching it on an annual basis.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mbib\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnum_citations\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    292\u001b[0m year_end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(datetime\u001b[39m.\u001b[39mdate\u001b[39m.\u001b[39mtoday()\u001b[39m.\u001b[39myear)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\publication_parser.py:389\u001b[0m, in \u001b[0;36mPublicationParser.citedby\u001b[1;34m(self, publication)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m publication[\u001b[39m'\u001b[39m\u001b[39mfilled\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    388\u001b[0m     publication \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill(publication)\n\u001b[1;32m--> 389\u001b[0m \u001b[39mreturn\u001b[39;00m _SearchScholarIterator(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnav, publication[\u001b[39m'\u001b[39;49m\u001b[39mcitedby_url\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\publication_parser.py:53\u001b[0m, in \u001b[0;36m_SearchScholarIterator.__init__\u001b[1;34m(self, nav, url)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pubtype \u001b[39m=\u001b[39m PublicationSource\u001b[39m.\u001b[39mPUBLICATION_SEARCH_SNIPPET \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/scholar?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m url \u001b[39melse\u001b[39;00m PublicationSource\u001b[39m.\u001b[39mJOURNAL_CITATION_LIST\n\u001b[0;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nav \u001b[39m=\u001b[39m nav\n\u001b[1;32m---> 53\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_url(url)\n\u001b[0;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_total_results()\n\u001b[0;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_parser \u001b[39m=\u001b[39m PublicationParser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_nav)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\publication_parser.py:59\u001b[0m, in \u001b[0;36m_SearchScholarIterator._load_url\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_url\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m):\n\u001b[0;32m     58\u001b[0m     \u001b[39m# this is temporary until setup json file\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_nav\u001b[39m.\u001b[39;49m_get_soup(url)\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pos \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     61\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_rows \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgs_r gs_or gs_scl\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m, class_\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgsc_mpat_ttl\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\_navigator.py:239\u001b[0m, in \u001b[0;36mNavigator._get_soup\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_soup\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BeautifulSoup:\n\u001b[0;32m    238\u001b[0m     \u001b[39m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     html \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_page(\u001b[39m'\u001b[39;49m\u001b[39mhttps://scholar.google.com\u001b[39;49m\u001b[39m{0}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(url))\n\u001b[0;32m    240\u001b[0m     html \u001b[39m=\u001b[39m html\u001b[39m.\u001b[39mreplace(\u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\xa0\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39mu\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    241\u001b[0m     res \u001b[39m=\u001b[39m BeautifulSoup(html, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\_navigator.py:113\u001b[0m, in \u001b[0;36mNavigator._get_page\u001b[1;34m(self, pagerequest, premium)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     w \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39muniform(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m     time\u001b[39m.\u001b[39;49msleep(w)\n\u001b[0;32m    114\u001b[0m     resp \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mget(pagerequest, timeout\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m premium \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:  \u001b[39m# premium methods may contain sensitive information\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rw.update_all_citedby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34ded59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'container_type': 'Publication',\n",
       " 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>,\n",
       " 'bib': {'title': 'Skill-based meta-reinforcement learning',\n",
       "  'author': ['T Nam', 'SH Sun', 'K Pertsch', 'SJ Hwang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form'},\n",
       " 'filled': False,\n",
       " 'gsrank': 1,\n",
       " 'pub_url': 'https://arxiv.org/abs/2204.11828',\n",
       " 'author_id': ['', 'uXsfnaQAAAAJ', '3oe0I0QAAAAJ', 'RP4Qx3QAAAAJ'],\n",
       " 'url_scholarbib': '/scholar?hl=en&q=info:ZUkyd0TIIHQJ:scholar.google.com/&output=cite&scirp=0&hl=en',\n",
       " 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkill-based%2BMeta-Reinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZUkyd0TIIHQJ&ei=fSx9ZPWuCqKbywSeir5w&json=',\n",
       " 'num_citations': 19,\n",
       " 'citedby_url': '/scholar?cites=8367908304037497189&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       " 'url_related_articles': '/scholar?q=related:ZUkyd0TIIHQJ:scholar.google.com/&scioq=Skill-based+Meta-Reinforcement+Learning&hl=en&as_sdt=0,33',\n",
       " 'eprint_url': 'https://arxiv.org/pdf/2204.11828'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[15]['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca17a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_list = scholarly.citedby(rw.paper_list[15]['info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46f825c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill-based Meta-Reinforcement Learning\n",
      "1\n",
      "2\n",
      "Towards continual reinforcement learning: A review and perspectives\n",
      "How to reuse and compose knowledge for a lifetime of tasks: A survey on continual learning and functional composition\n",
      "Hyper-decision transformer for efficient online policy adaptation\n",
      "Learning Options via Compression\n",
      "Efficient Multi-Task Reinforcement Learning via Selective Behavior Sharing\n",
      "Guided Skill Learning and Abstraction for Long-Horizon Manipulation\n",
      "Meta-Learning Transferable Parameterized Skills\n",
      "A Survey of Meta-Reinforcement Learning\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "Uncertainty-based Meta-Reinforcement Learning for Robust Radar Tracking\n",
      "Skills Regularized Task Decomposition for Multi-task Offline Reinforcement Learning\n",
      "SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments\n",
      "Meta-Reinforcement Learning via Exploratory Task Clustering\n",
      "The StarCraft Multi-Agent Challenges+: Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions\n",
      "Toward Effective Deep Reinforcement Learning for 3D Robotic Manipulation: End-to-End Learning from Multimodal Raw Sensory Data\n",
      "-Invariant Hierarchical Reinforcement Learning for Building Generalizable Policy\n",
      "Unravel Structured Heterogeneity of Tasks in Meta-Reinforcement Learning via Exploratory Clustering\n",
      "SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling\n",
      "Toward Effective Deep Reinforcement Learning for 3D Robotic Manipulation: Multimodal End-to-End Reinforcement Learning from Visual and Proprioceptive …\n"
     ]
    }
   ],
   "source": [
    "rw.update_citedby(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a55566db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'title': 'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning', 'year': '2023', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'A task-agnostic regularizer for diverse subpolicy discovery in hierarchical reinforcement learning', 'author': 'Huo, Liangyu and Wang, Zulin and Xu, Mai and Song, Yuhang', 'pub_year': '2022', 'venue': 'IEEE Transactions on Systems …', 'abstract': 'The automatic subpolicy discovery approach in hierarchical reinforcement learning (HRL) has recently achieved promising performance on sparse reward tasks. This accelerates transfer learning and unsupervised intelligent creatures while eliminating the domain-specific knowledge constraint. Most previously developed approaches are demonstrated to suffer from collapsing into the situation where one subpolicy dominates the whole task, since they cannot ensure the diversity of different subpolicies. In contrast, this article proposes a', 'publisher': 'IEEE', 'journal': 'IEEE Transactions on Systems, Man, and Cybernetics: Systems', 'pub_type': 'article', 'bib_id': 'huo2022task'}, 'filled': True, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9906972/', 'author_id': ['', '', 'JdhDuXAAAAAJ', 'cyd3EsgAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:FJRPp-6cRQcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BTask-Agnostic%2BRegularizer%2Bfor%2BDiverse%2BSubpolicy%2BDiscovery%2Bin%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FJRPp-6cRQcJ&ei=-SV9ZP34FMWsmgHHtLzoDQ&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=523997480481690644&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:FJRPp-6cRQcJ:scholar.google.com/&scioq=A+Task-Agnostic+Regularizer+for+Diverse+Subpolicy+Discovery+in+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33'}, 'citedby': [{'title': 'Hierarchical deep reinforcement learning with experience sharing for metaverse in education', 'author': ['R Hare', 'Y Tang'], 'pub_year': '2022', 'venue': 'IEEE Transactions on Systems, Man, and …', 'abstract': 'Metaverse has gained increasing interest in education, with much of literature focusing on its great potential to enhance both individual and social aspects of learning. However, little'}, {'title': 'A Metaverse-Based Teaching Building Evacuation Training System With Deep Reinforcement Learning', 'author': ['J Gu', 'J Wang', 'X Guo', 'G Liu', 'S Qin'], 'pub_year': '2023', 'venue': 'IEEE Transactions on …', 'abstract': 'With the development of IoT, virtual reality, cloud computing, and digital twin technologies, the advent of metaverse has attracted increasing world attention. Metaverse integrates and'}]}, 2: {'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning', 'year': '2023', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical kickstarting for skill transfer in reinforcement learning', 'author': ['M Matthews', 'M Samvelyan', 'J Parker-Holder'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Practising and honing skills forms a fundamental component of how humans learn, yet artificial agents are rarely specifically trained to perform them. Instead, they are usually trained end-to-end, with the hope being that useful skills will be implicitly learned in order to maximise discounted return of some extrinsic reward function. In this paper, we investigate how skills can be incorporated into the training of reinforcement learning (RL) agents in complex environments with large state-action spaces and sparse rewards. To this end, we'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2207.11584', 'author_id': ['ubOhfnIAAAAJ', '2Qs19WAAAAAJ', 'J3VCfPYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:s00WZ4tVYfoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&oe=ASCII&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BKickstarting%2Bfor%2BSkill%2BTransfer%2Bin%2BReinforcement%2BLearning%26hl%3Den%26oe%3DASCII%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s00WZ4tVYfoJ&ei=7SZ9ZMLFLP2M6rQPrYemoAo&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=18041795639441247667&as_sdt=5,33&sciodt=0,33&hl=en&oe=ASCII', 'url_related_articles': '/scholar?q=related:s00WZ4tVYfoJ:scholar.google.com/&scioq=Hierarchical+Kickstarting+for+Skill+Transfer+in+Reinforcement+Learning&hl=en&oe=ASCII&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2207.11584'}}, 3: {'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning', 'year': '2023', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning', 'author': ['J Chen', 'D Tamboli', 'T Lan', 'V Aggarwal'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2305.12633', 'abstract': 'Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2305.12633', 'author_id': ['k0KJm7kAAAAJ', 'H8YEc-gAAAAJ', '', 'Tu4lmGwAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:FFxbIHggUdcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-task%2BHierarchical%2BAdversarial%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FFxbIHggUdcJ&ei=9CZ9ZNvaI8CQ6rQP9LmDmAo&json=', 'num_citations': 0, 'eprint_url': 'https://arxiv.org/pdf/2305.12633'}}, 4: {'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills', 'author': ['T Zhou', 'L Wang', 'R Chen', 'W Wang', 'Y Liu'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2209.12072', 'abstract': 'Efficient and effective exploration in continuous space is a central problem in applying reinforcement learning (RL) to autonomous driving. Skills learned from expert demonstrations or designed for specific tasks can benefit the exploration, but they are usually costly-collected, unbalanced/sub-optimal, or failing to transfer to diverse tasks. However, human drivers can adapt to varied driving tasks without demonstrations by taking efficient and structural explorations in the entire skill space rather than a limited space with'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2209.12072', 'author_id': ['', 'HEzCWisAAAAJ', '', 'vQ1dKQwAAAAJ', ''], 'url_scholarbib': '/scholar?hl=en&q=info:98DXNds4AkMJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BReinforcement%2BLearning%2Bfor%2BAutonomous%2BDriving%2Busing%2BTask-Agnostic%2Band%2BEgo-Centric%2BMotion%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=98DXNds4AkMJ&ei=Eyd9ZPSSMcb2mgHK57_YCA&json=', 'num_citations': 1, 'citedby_url': '/scholar?cites=4828484264646918391&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:98DXNds4AkMJ:scholar.google.com/&scioq=Accelerating+Reinforcement+Learning+for+Autonomous+Driving+using+Task-Agnostic+and+Ego-Centric+Motion+Skills&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2209.12072'}}, 5: {'title': 'Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Advanced skills through multiple adversarial motion priors in reinforcement learning', 'author': ['E Vollenweider', 'M Bjelonic', 'V Klemm', 'N Rudin'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In recent years, reinforcement learning (RL) has shown outstanding performance for locomotion control of highly articulated robotic systems. Such approaches typically involve tedious reward function tuning to achieve the desired motion style. Imitation learning approaches such as adversarial motion priors aim to reduce this problem by encouraging a pre-defined motion style. In this work, we present an approach to augment the concept of adversarial motion prior-based RL to allow for multiple, discretely switchable styles. We'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2203.14912', 'author_id': ['7-5HN8oAAAAJ', 'W7PnUfUAAAAJ', '-3pMVPUAAAAJ', '1kKJYVIAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:skJw7zfckFEJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdvanced%2BSkills%2Bthrough%2BMultiple%2BAdversarial%2BMotion%2BPriors%2Bin%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=skJw7zfckFEJ&ei=YCd9ZJ6jN7iBy9YP0MKi-AY&json=', 'num_citations': 16, 'citedby_url': '/scholar?cites=5877439646516921010&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:skJw7zfckFEJ:scholar.google.com/&scioq=Advanced+Skills+through+Multiple+Adversarial+Motion+Priors+in+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2203.14912'}}, 6: {'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'ASPiRe: Adaptive Skill Priors for Reinforcement Learning', 'author': ['M Xu', 'M Veloso', 'S Song'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2209.15205', 'abstract': 'We introduce ASPiRe (Adaptive Skill Prior for RL), a new approach that leverages prior experience to accelerate reinforcement learning. Unlike existing methods that learn a single skill prior from a large and diverse dataset, our framework learns a library of different distinction skill priors (ie, behavior priors) from a collection of specialized datasets, and learns how to combine them to solve a new task. This formulation allows the algorithm to acquire a set of specialized skill priors that are more reusable for downstream tasks;'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2209.15205', 'author_id': ['K1_MygkAAAAJ', '2FbkAzYAAAAJ', '5031vK4AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:NhnjZMyXRdIJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&oe=ASCII&xsrf=&continue=/scholar%3Fq%3DASPiRe:%2BAdaptive%2BSkill%2BPriors%2Bfor%2BReinforcement%2BLearning%26hl%3Den%26oe%3DASCII%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=NhnjZMyXRdIJ&ei=bCd9ZIOwAYvwyAS0sqboDg&json=', 'num_citations': 0, 'url_related_articles': '/scholar?q=related:NhnjZMyXRdIJ:scholar.google.com/&scioq=ASPiRe:+Adaptive+Skill+Priors+for+Reinforcement+Learning&hl=en&oe=ASCII&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2209.15205'}}, 7: {'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Cascaded Compositional Residual Learning for Complex Interactive Behaviors', 'author': ['KN Kumar', 'I Essa', 'S Ha'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2212.08954', 'abstract': 'Real-world autonomous missions often require rich interaction with nearby objects, such as doors or switches, along with effective navigation. However, such complex behaviors are difficult to learn because they involve both high-level planning and low-level motor control. We present a novel framework, Cascaded Compositional Residual Learning (CCRL), which learns composite skills by recursively leveraging a library of previously learned control policies. Our framework learns multiplicative policy composition, task-specific residual'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2212.08954', 'author_id': ['UD7d2NEAAAAJ', 'XM97iScAAAAJ', 'Q6F3O0sAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:rZEO510O57IJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCascaded%2BCompositional%2BResidual%2BLearning%2Bfor%2BComplex%2BInteractive%2BBehaviors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rZEO510O57IJ&ei=9Cd9ZJ78NYb4yASAjZegAQ&json=', 'num_citations': 1, 'citedby_url': '/scholar?cites=12891288254842573229&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:rZEO510O57IJ:scholar.google.com/&scioq=Cascaded+Compositional+Residual+Learning+for+Complex+Interactive+Behaviors&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2212.08954'}}, 8: {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations', 'author': ['K Yan', 'A Schwing', 'YX Wang'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'Although reinforcement learning has found widespread use in dense reward settings, training autonomous agents with sparse rewards remains challenging. To address this difficulty, prior work has shown promising results when using not only task-specific demonstrations but also task-agnostic albeit somewhat related demonstrations. In most cases, the available demonstrations are distilled into an implicit prior, commonly represented via a single deep net. Explicit priors in the form of a database that can be'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/322e4a595afd9442a89f0bfaa441871e-Abstract-Conference.html', 'author_id': ['KElKfgQAAAAJ', '3B2c31wAAAAJ', 'T_Q-xDkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:OxNYT1yjoKsJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCEIP:%2BCombining%2BExplicit%2Band%2BImplicit%2BPriors%2Bfor%2BReinforcement%2BLearning%2Bwith%2BDemonstrations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=OxNYT1yjoKsJ&ei=Iih9ZI7UIO3AsQKExJC4Cg&json=', 'num_citations': 0, 'url_related_articles': '/scholar?q=related:OxNYT1yjoKsJ:scholar.google.com/&scioq=CEIP:+Combining+Explicit+and+Implicit+Priors+for+Reinforcement+Learning+with+Demonstrations&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/322e4a595afd9442a89f0bfaa441871e-Paper-Conference.pdf'}}, 9: {'title': 'MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'MPR-RL: multi-prior regularized reinforcement learning for knowledge transfer', 'author': ['Q Yang', 'JA Stork', 'T Stoyanov'], 'pub_year': '2022', 'venue': 'IEEE Robotics and Automation …', 'abstract': 'In manufacturing, assembly tasks have been a challenge for learning algorithms due to variant dynamics of different environments. Reinforcement learning (RL) is a promising framework to automatically learn these tasks, yet it is still not easy to apply a learned policy or skill, that is the ability of solving a task, to a similar environment even if the deployment conditions are only slightly different. In this letter, we address the challenge of transferring knowledge within a family of similar tasks by leveraging multiple skill priors. We propose to'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9803274/', 'author_id': ['8UrHg8AAAAAJ', '4HtJjKAAAAAJ', 'GJIaXm4AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:GTys22hxK38J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMPR-RL:%2BMulti-Prior%2BRegularized%2BReinforcement%2BLearning%2Bfor%2BKnowledge%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=GTys22hxK38J&ei=1Ch9ZIOOEb2P6rQP85mLmAc&json=', 'num_citations': 5, 'citedby_url': '/scholar?cites=9163542561991441433&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:GTys22hxK38J:scholar.google.com/&scioq=MPR-RL:+Multi-Prior+Regularized+Reinforcement+Learning+for+Knowledge+Transfer&hl=en&as_sdt=0,33', 'eprint_url': 'https://ieeexplore.ieee.org/iel7/7083369/7339444/09803274.pdf'}}, 10: {'title': 'Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Planning to practice: Efficient online fine-tuning by composing goals in latent space', 'author': ['K Fang', 'P Yin', 'A Nair', 'S Levine'], 'pub_year': '2022', 'venue': '2022 IEEE/RSJ International …', 'abstract': 'General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9981999/', 'author_id': ['BZBkjNYAAAAJ', 'sNOIBrkAAAAJ', 'BsOkXDsAAAAJ', '8R35rCwAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:7bzpmXeIXnsJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlanning%2Bto%2BPractice:%2BEfficient%2BOnline%2BFine-Tuning%2Bby%2BComposing%2BGoals%2Bin%2BLatent%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=7bzpmXeIXnsJ&ei=Hyl9ZNe_NO3AsQKExJC4Cg&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=8889692761740655853&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:7bzpmXeIXnsJ:scholar.google.com/&scioq=Planning+to+Practice:+Efficient+Online+Fine-Tuning+by+Composing+Goals+in+Latent+Space&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2205.08129'}}, 11: {'title': 'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Priors, hierarchy, and information asymmetry for skill transfer in reinforcement learning', 'author': ['S Salter', 'K Hartikainen', 'W Goodwin'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized RL individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry to bias which skills are learnt. While asymmetric choice'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2201.08115', 'author_id': ['2m-p8FcAAAAJ', 'eVYhlDQAAAAJ', '9b64ixMAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:Z6M3XQudPokJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPriors,%2BHierarchy,%2Band%2BInformation%2BAsymmetry%2Bfor%2BSkill%2BTransfer%2Bin%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Z6M3XQudPokJ&ei=RCl9ZNyGEcCQ6rQP9LmDmAo&json=', 'num_citations': 3, 'citedby_url': '/scholar?cites=9889514503886316391&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:Z6M3XQudPokJ:scholar.google.com/&scioq=Priors,+Hierarchy,+and+Information+Asymmetry+for+Skill+Transfer+in+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2201.08115'}}, 12: {'title': 'Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Deep reinforcement learning versus evolution strategies: a comparative survey', 'author': ['AY Majid', 'S Saaybi', 'V Francois-Lavet'], 'pub_year': '2023', 'venue': '… and Learning …', 'abstract': '’s performance on RL tasks with sparse rewards as they help  ] proposed self-guided ESs  (SGES). This work is inspired by  Furthermore, it conflicts with the usage of experience replay'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/10114063/', 'author_id': ['hVezSVIAAAAJ', '', 'n12uNYcAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:3denzQsorQ4J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&oe=ASCII&xsrf=&continue=/scholar%3Fq%3DRelay%2BHindsight%2BExperience%2BReplay:%2BSelf-Guided%2BContinual%2BReinforcement%2BLearning%2Bfor%2BSequential%2BObject%2BManipulation%2BTasks%2Bwith%2BSparse%2BRewards%26hl%3Den%26oe%3DASCII%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=3denzQsorQ4J&ei=XCl9ZMHROIb4yASAjZegAQ&json=', 'num_citations': 10, 'citedby_url': '/scholar?cites=1057545518662014941&as_sdt=5,33&sciodt=0,33&hl=en&oe=ASCII', 'url_related_articles': '/scholar?q=related:3denzQsorQ4J:scholar.google.com/&scioq=Relay+Hindsight+Experience+Replay:+Self-Guided+Continual+Reinforcement+Learning+for+Sequential+Object+Manipulation+Tasks+with+Sparse+Rewards&hl=en&oe=ASCII&as_sdt=0,33', 'eprint_url': 'https://ieeexplore.ieee.org/iel7/5962385/6104215/10114063.pdf'}}, 13: {'title': 'Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics', 'author': ['K Rana', 'M Xu', 'B Tidd', 'M Milford'], 'pub_year': '2023', 'venue': 'Conference on Robot …', 'abstract': 'Skill-based reinforcement learning (RL) has emerged as a promising strategy to leverage prior knowledge for accelerated robot learning. Skills are typically extracted from expert demonstrations and are embedded into a latent space from which they can be sampled as actions by a high-level RL agent. However, this\\\\textit {skill space} is expansive, and not all skills are relevant for a given robot state, making exploration difficult. Furthermore, the downstream RL agent is limited to learning structurally similar tasks to those used to'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v205/rana23a.html', 'author_id': ['-hYjPxsAAAAJ', 'Zs15GgMAAAAJ', '-oTAAHQAAAAJ', 'TDSmCKgAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:kbsHhmwYDgEJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BSkill%2BPolicies:%2BLearning%2Ban%2BAdaptable%2BSkill-based%2BAction%2BSpace%2Bfor%2BReinforcement%2BLearning%2Bfor%2BRobotics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kbsHhmwYDgEJ&ei=eCx9ZIOkOIvwyAS0sqboDg&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=76025098096065425&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:kbsHhmwYDgEJ:scholar.google.com/&scioq=Residual+Skill+Policies:+Learning+an+Adaptable+Skill-based+Action+Space+for+Reinforcement+Learning+for+Robotics&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.mlr.press/v205/rana23a/rana23a.pdf'}}, 14: {'title': 'Robot Learning of Mobile Manipulation With Reachability Behavior Priors', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Robot learning of mobile manipulation with reachability behavior priors', 'author': ['S Jauhri', 'J Peters', 'G Chalvatzaki'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': \"Mobile Manipulation (MM) systems are ideal candidates for taking up the role of personal assistants in unstructured real-world environments. Among other challenges, Mobile Manipulation (MM) requires effective coordination of the robot's embodiments for executing tasks that require both mobility and manipulation. Reinforcement Learning (RL) holds the promise of endowing robots with adaptive behaviors, but most methods require prohibitively large amounts of data for learning a useful control policy. In this work, we study the\"}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9813580/', 'author_id': ['Hx_XWGEAAAAJ', '-kIVAcAAAAAJ', 'mlho5FkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:esQ8cPGxmt0J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobot%2BLearning%2Bof%2BMobile%2BManipulation%2BWith%2BReachability%2BBehavior%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=esQ8cPGxmt0J&ei=eix9ZK_BJsCQ6rQP9LmDmAo&json=', 'num_citations': 7, 'citedby_url': '/scholar?cites=15968271079323780218&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:esQ8cPGxmt0J:scholar.google.com/&scioq=Robot+Learning+of+Mobile+Manipulation+With+Reachability+Behavior+Priors&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2203.04051'}}, 15: {'title': 'Skill-based Meta-Reinforcement Learning', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Skill-based meta-reinforcement learning', 'author': ['T Nam', 'SH Sun', 'K Pertsch', 'SJ Hwang'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors with real robot systems infeasible. To mitigate this issue, meta-reinforcement learning methods aim to enable fast learning on novel tasks by learning how to learn. Yet, the application has been limited to short-horizon tasks with dense rewards. To enable learning long-horizon behaviors, recent works have explored leveraging prior experience in the form'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2204.11828', 'author_id': ['', 'uXsfnaQAAAAJ', '3oe0I0QAAAAJ', 'RP4Qx3QAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:ZUkyd0TIIHQJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkill-based%2BMeta-Reinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=ZUkyd0TIIHQJ&ei=fSx9ZPWuCqKbywSeir5w&json=', 'num_citations': 19, 'citedby_url': '/scholar?cites=8367908304037497189&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:ZUkyd0TIIHQJ:scholar.google.com/&scioq=Skill-based+Meta-Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2204.11828'}}, 16: {'title': 'Skill-based Model-based Reinforcement Learning', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Skill-based model-based reinforcement learning', 'author': ['LX Shi', 'JJ Lim', 'Y Lee'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2207.07560', 'abstract': 'Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in imagination. However, planning every action for long-horizon tasks is not practical, akin to a human planning out every muscle movement. Instead, humans efficiently plan with high-level skills to solve complex tasks. From this intuition, we propose a Skill-based Model-based RL framework (SkiMo) that enables planning in the skill space using a skill dynamics'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2207.07560', 'author_id': ['KUc7JJoAAAAJ', 'jTnQTBoAAAAJ', 'CDPa3AgAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:CNtlP3bPULoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkill-based%2BModel-based%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=CNtlP3bPULoJ&ei=fyx9ZMynLJCP6rQP6Iq6sAY&json=', 'num_citations': 4, 'citedby_url': '/scholar?cites=13425458595968178952&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:CNtlP3bPULoJ:scholar.google.com/&scioq=Skill-based+Model-based+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2207.07560'}}, 17: {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration', 'author': ['G Vezzani', 'D Tirumala', 'M Wulfmeier', 'D Rao'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common approaches, but current methods have considerable limitations. For example, fine-tuning an existing policy frequently fails, as the policy can degrade rapidly early in training. In a similar vein, distillation of expert behavior can lead to poor results when given sub-optimal experts. We compare several common approaches for skill transfer on multiple domains including'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2211.13743', 'author_id': ['Zlpuln8AAAAJ', 'q-06TwoAAAAJ', 'YCO3WQsAAAAJ', '1t3Fi2AAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:YB2Q7Z5HeJcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkillS:%2BAdaptive%2BSkill%2BSequencing%2Bfor%2BEfficient%2BTemporally-Extended%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=YB2Q7Z5HeJcJ&ei=gSx9ZKrTHMKM6rQPyqqYyAU&json=', 'num_citations': 1, 'citedby_url': '/scholar?cites=10914552444848446816&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:YB2Q7Z5HeJcJ:scholar.google.com/&scioq=SkillS:+Adaptive+Skill+Sequencing+for+Efficient+Temporally-Extended+Exploration&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2211.13743'}}, 18: {'title': 'Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback', 'year': '2022', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback', 'author': ['Y Chen', 'C Zeng', 'Z Wang', 'P Lu', 'C Yang'], 'pub_year': '2023', 'venue': 'Robotica', 'abstract': 'In the field of robot reinforcement learning (RL), the reality gap has always been a problem that restricts the robustness and generalization of algorithms. We propose Simulation Twin (SimTwin): a deep RL framework that can help directly transfer the model from simulation to reality without any real-world training. SimTwin consists of a RL module and an adaptive correct module. We train the policy using the soft actor-critic algorithm only in a simulator with demonstration and domain randomization. In the adaptive correct module, we design'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://www.cambridge.org/core/journals/robotica/article/zeroshot-simtoreal-transfer-of-reinforcement-learning-framework-for-robotics-manipulation-with-demonstration-and-force-feedback/8F7630528F363B7624020FEA4A94B06B', 'author_id': ['LR4CHSkAAAAJ', '', '', 'ts7ItWgAAAAJ', 'e8io0fYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:XClHHJx7xNwJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DZero-shot%2Bsim-to-real%2Btransfer%2Bof%2Breinforcement%2Blearning%2Bframework%2Bfor%2Brobotics%2Bmanipulation%2Bwith%2Bdemonstration%2Band%2Bforce%2Bfeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XClHHJx7xNwJ&ei=gyx9ZIzoHIHuygS37r6oCA&json=', 'num_citations': 1, 'citedby_url': '/scholar?cites=15907975694198974812&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:XClHHJx7xNwJ:scholar.google.com/&scioq=Zero-shot+sim-to-real+transfer+of+reinforcement+learning+framework+for+robotics+manipulation+with+demonstration+and+force+feedback&hl=en&as_sdt=0,33'}}, 19: {'title': 'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Accelerating robotic reinforcement learning via parameterized action primitives', 'author': ['M Dalal', 'D Pathak'], 'pub_year': '2021', 'venue': 'Advances in Neural …', 'abstract': 'Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the difficulty of exploration in purely continuous action spaces. Addressing this problem is an active area of research with the majority of focus on improving RL methods via better optimization or more efficient exploration. An alternate but important component to consider improving is the interface of the RL algorithm with the robot. In this work, we manually'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.neurips.cc/paper/2021/hash/b6846b0186a035fcc76b1b1d26fd42fa-Abstract.html', 'author_id': ['5dBp2f4AAAAJ', 'AEsPCAUAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:RlAKYe7lPMcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BRobotic%2BReinforcement%2BLearning%2Bvia%2BParameterized%2BAction%2BPrimitives%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=RlAKYe7lPMcJ&ei=hSx9ZJm1IL2P6rQP85mLmAc&json=', 'num_citations': 31, 'citedby_url': '/scholar?cites=14356602524143341638&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:RlAKYe7lPMcJ:scholar.google.com/&scioq=Accelerating+Robotic+Reinforcement+Learning+via+Parameterized+Action+Primitives&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.neurips.cc/paper/2021/file/b6846b0186a035fcc76b1b1d26fd42fa-Paper.pdf'}}, 20: {'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics', 'author': ['K Rana', 'V Dasagi', 'J Haviland', 'B Talbot'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. We study BCF on two real-world robotics tasks'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2107.09822', 'author_id': ['-hYjPxsAAAAJ', 'BrhJ6-EAAAAJ', 'IIhSLvgAAAAJ', 'oDCvYTEAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:vF79EAeHhdcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2Bcontroller%2Bfusion:%2BLeveraging%2Bcontrol%2Bpriors%2Bin%2Bdeep%2Breinforcement%2Blearning%2Bfor%2Brobotics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vF79EAeHhdcJ&ei=hyx9ZNGRIob4yASAjZegAQ&json=', 'num_citations': 10, 'citedby_url': '/scholar?cites=15529967354476584636&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:vF79EAeHhdcJ:scholar.google.com/&scioq=Bayesian+controller+fusion:+Leveraging+control+priors+in+deep+reinforcement+learning+for+robotics&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2107.09822'}}, 21: {'title': 'Demonstration-Guided Reinforcement Learning with Learned Skills', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Guided reinforcement learning with learned skills', 'author': ['K Pertsch', 'Y Lee', 'Y Wu', 'JJ Lim'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2107.10253', 'abstract': 'We detail our full SkiLD algorithm for demonstration-guided RL with learned skills in Algorithm  1. It is based on the SPiRL algorithm for RL with learned skills [16] which in turn builds on'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2107.10253', 'author_id': ['3oe0I0QAAAAJ', 'CDPa3AgAAAAJ', '', 'jTnQTBoAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:Iur7-t7_kRsJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDemonstration-Guided%2BReinforcement%2BLearning%2Bwith%2BLearned%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Iur7-t7_kRsJ&ei=iSx9ZOmxHP2M6rQPrYemoAo&json=', 'num_citations': 34, 'citedby_url': '/scholar?cites=1986650243805735458&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:Iur7-t7_kRsJ:scholar.google.com/&scioq=Demonstration-Guided+Reinforcement+Learning+with+Learned+Skills&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2107.10253'}}, 22: {'title': 'Hierarchical Few-Shot Imitation with Skill Transition Models', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical few-shot imitation with skill transition models', 'author': ['K Hakhamaneshi', 'R Zhao', 'A Zhan', 'P Abbeel'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'A desirable property of autonomous agents is the ability to both solve long-horizon problems and generalize to unseen tasks. Recent advances in data-driven skill learning have shown that extracting behavioral priors from offline data can enable agents to solve challenging long-horizon tasks with reinforcement learning. However, generalization to tasks unseen during behavioral prior training remains an outstanding challenge. To this end, we present Few-shot Imitation with Skill Transition Models (FIST), an algorithm that extracts skills from'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2107.08981', 'author_id': ['kub9RqMAAAAJ', 'dxco-1UAAAAJ', 'P356Id4AAAAJ', 'vtwH6GkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:cjgrn0k-BJ0J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BFew-Shot%2BImitation%2Bwith%2BSkill%2BTransition%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cjgrn0k-BJ0J&ei=iix9ZLHqPIvwyAS0sqboDg&json=', 'num_citations': 17, 'citedby_url': '/scholar?cites=11314236649785473138&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:cjgrn0k-BJ0J:scholar.google.com/&scioq=Hierarchical+Few-Shot+Imitation+with+Skill+Transition+Models&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2107.08981'}}, 23: {'title': 'Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical policies for cluttered-scene grasping with latent plans', 'author': ['L Wang', 'X Meng', 'Y Xiang', 'D Fox'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': '6D grasping in cluttered scenes is a longstanding problem in robotic manipulation. Open-loop manipulation pipelines may fail due to inaccurate state estimation, while most end-to-end grasping methods have not yet scaled to complex scenes with obstacles. In this work, we propose a new method for end-to-end learning of 6D grasping in cluttered scenes. Our hierarchical framework learns collision-free target-driven grasping based on partial point cloud observations. We learn an embedding space to encode expert grasping plans during'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9682579/', 'author_id': ['EM9YhH0AAAAJ', 'JUuickkAAAAJ', '9cZUlEYAAAAJ', 'DqXsbPAAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:vSslJkotDAYJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BPolicies%2Bfor%2BCluttered-Scene%2BGrasping%2Bwith%2BLatent%2BPlans%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vSslJkotDAYJ&ei=jCx9ZLnMK8CQ6rQP9LmDmAo&json=', 'num_citations': 14, 'citedby_url': '/scholar?cites=435773060438895549&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:vSslJkotDAYJ:scholar.google.com/&scioq=Hierarchical+Policies+for+Cluttered-Scene+Grasping+with+Latent+Plans&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2107.01518'}}, 24: {'title': 'Hierarchical Skills for Efficient Exploration', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical skills for efficient exploration', 'author': ['J Gehring', 'G Synnaeve', 'A Krause'], 'pub_year': '2021', 'venue': 'Advances in Neural …', 'abstract': 'In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.neurips.cc/paper/2021/hash/60106888f8977b71e1f15db7bc9a88d1-Abstract.html', 'author_id': ['jOwTwm4AAAAJ', 'wN9rBkcAAAAJ', 'eDHv58AAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:qtVcuql1kdYJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BSkills%2Bfor%2BEfficient%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=qtVcuql1kdYJ&ei=jix9ZJr3C6KbywSeir5w&json=', 'num_citations': 16, 'citedby_url': '/scholar?cites=15461268367576192426&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:qtVcuql1kdYJ:scholar.google.com/&scioq=Hierarchical+Skills+for+Efficient+Exploration&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.neurips.cc/paper/2021/file/60106888f8977b71e1f15db7bc9a88d1-Paper.pdf'}}, 25: {'title': 'Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space', 'author': ['M Ulmer', 'E Aljalbout', 'S Schwarz'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Intelligent agents must be able to think fast and slow to perform elaborate manipulation tasks. Reinforcement Learning (RL) has led to many promising results on a range of challenging decision-making tasks. However, in real-world robotics, these methods still struggle, as they require large amounts of expensive interactions and have slow feedback loops. On the other hand, fast human-like adaptive control methods can optimize complex robotic interactions, yet fail to integrate multimodal feedback needed for unstructured tasks'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2110.09904', 'author_id': ['FgFPar8AAAAJ', 'b8l7jMwAAAAJ', '_hS_cAQAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:djmkAZngJxcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobotic%2BManipulation%2BSkills%2BUsing%2Ban%2BAdaptive%2BForce-Impedance%2BAction%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=djmkAZngJxcJ&ei=kCx9ZIiMFpCP6rQP6Iq6sAY&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=1668549134726216054&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:djmkAZngJxcJ:scholar.google.com/&scioq=Learning+Robotic+Manipulation+Skills+Using+an+Adaptive+Force-Impedance+Action+Space&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2110.09904'}}, 26: {'title': 'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning transferable motor skills with hierarchical latent mixture policies', 'author': ['D Rao', 'F Sadeghi', 'L Hasenclever', 'M Wulfmeier'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'For robots operating in the real world, it is desirable to learn reusable behaviours that can effectively be transferred and adapted to numerous tasks and scenarios. We propose an approach to learn abstract motor skills from data using a hierarchical mixture latent variable model. In contrast to existing work, our method exploits a three-level hierarchy of both discrete and continuous latent variables, to capture a set of high-level behaviours while allowing for variance in how they are executed. We demonstrate in manipulation domains'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2112.05062', 'author_id': ['1t3Fi2AAAAAJ', 'vS8b6GwAAAAJ', 'dD-3S4QAAAAJ', 'YCO3WQsAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:Qmt5lbIgu7gJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTransferable%2BMotor%2BSkills%2Bwith%2BHierarchical%2BLatent%2BMixture%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Qmt5lbIgu7gJ&ei=kix9ZL_EHsKM6rQPyqqYyAU&json=', 'num_citations': 12, 'citedby_url': '/scholar?cites=13311269075007662914&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:Qmt5lbIgu7gJ:scholar.google.com/&scioq=Learning+Transferable+Motor+Skills+with+Hierarchical+Latent+Mixture+Policies&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2112.05062'}}, 27: {'title': 'Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning', 'author': ['AS Morgan', 'D Nandha', 'G Chalvatzaki'], 'pub_year': '2021', 'venue': '… on Robotics and …', 'abstract': 'Substantial advancements to model-based reinforcement learning algorithms have been impeded by the model-bias induced by the collected data, which generally hurts performance. Meanwhile, their inherent sample efficiency warrants utility for most robot applications, limiting potential damage to the robot and its environment during training. Inspired by information theoretic model predictive control and advances in deep reinforcement learning, we introduce Model Predictive Actor-Critic (MoPAC)†, a hybrid'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9561298/', 'author_id': ['vYCtlrMAAAAJ', '_e0H23UAAAAJ', 'mlho5FkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:bSDMHVZZ3lgJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel%2Bpredictive%2Bactor-critic:%2BAccelerating%2Brobot%2Bskill%2Bacquisition%2Bwith%2Bdeep%2Breinforcement%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bSDMHVZZ3lgJ&ei=lCx9ZOKSAoHuygS37r6oCA&json=', 'num_citations': 23, 'citedby_url': '/scholar?cites=6403653946569400429&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:bSDMHVZZ3lgJ:scholar.google.com/&scioq=Model+predictive+actor-critic:+Accelerating+robot+skill+acquisition+with+deep+reinforcement+learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2103.13842'}}, 28: {'title': 'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data', 'year': '2021', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Trail: Near-optimal imitation learning with suboptimal data', 'author': ['M Yang', 'S Levine', 'O Nachum'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2110.14770', 'abstract': 'The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be expensive to obtain in large numbers. On the other hand, it is often much easier to obtain large quantities of suboptimal or task-agnostic trajectories, which are not useful for direct imitation, but can nevertheless provide insight into the dynamical structure of the environment, showing what could be done in the environment even if not what should be'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2110.14770', 'author_id': ['7c1B_fIAAAAJ', '8R35rCwAAAAJ', 'C-ZlBWMAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:6nhGLGiE2rQJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTRAIL:%2BNear-Optimal%2BImitation%2BLearning%2Bwith%2BSuboptimal%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=6nhGLGiE2rQJ&ei=lix9ZL6wDr2P6rQP85mLmAc&json=', 'num_citations': 23, 'citedby_url': '/scholar?cites=13031874054704232682&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:6nhGLGiE2rQJ:scholar.google.com/&scioq=TRAIL:+Near-Optimal+Imitation+Learning+with+Suboptimal+Data&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2110.14770'}}, 29: {'title': 'Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Accelerating reinforcement learning with learned skill priors', 'author': ['K Pertsch', 'Y Lee', 'J Lim'], 'pub_year': '2021', 'venue': 'Conference on robot learning', 'abstract': 'Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One approach for leveraging prior knowledge is to transfer skills learned on prior tasks to the new task. However, as the amount of prior experience increases, the number of transferable skills grows too, making it challenging to explore the full set of available skills during downstream learning. Yet, intuitively, not all skills should be explored with equal probability; for example'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v155/pertsch21a.html', 'author_id': ['3oe0I0QAAAAJ', 'CDPa3AgAAAAJ', 'jTnQTBoAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:MdyXqaJ5FLEJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BReinforcement%2BLearning%2Bwith%2BLearned%2BSkill%2BPriors%2B(SPiRL)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MdyXqaJ5FLEJ&ei=lyx9ZL3TO4b4yASAjZegAQ&json=', 'num_citations': 131, 'citedby_url': '/scholar?cites=12759957383784422449&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:MdyXqaJ5FLEJ:scholar.google.com/&scioq=Accelerating+Reinforcement+Learning+with+Learned+Skill+Priors+(SPiRL)&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.mlr.press/v155/pertsch21a/pertsch21a.pdf'}}, 30: {'title': 'Behavior Priors for Efficient Reinforcement Learning', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Behavior priors for efficient reinforcement learning', 'author': ['D Tirumala', 'A Galashov', 'H Noh'], 'pub_year': '2022', 'venue': 'The Journal of Machine …', 'abstract': 'As we deploy reinforcement learning agents to solve increasingly challenging problems, methods that allow us to inject prior knowledge about the structure of the world and effective solution strategies becomes increasingly important. In this work we consider how information and architectural constraints can be combined with ideas from the probabilistic modeling literature to learn behavior priors that capture the common movement and interaction patterns that are shared across a set of related tasks or contexts. For example the'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://dl.acm.org/doi/abs/10.5555/3586589.3586810', 'author_id': ['q-06TwoAAAAJ', 'kIpoNtcAAAAJ', '_ycM9GAAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:rDrtYuIsGKgJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBehavior%2BPriors%2Bfor%2BEfficient%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=rDrtYuIsGKgJ&ei=mSx9ZI7NLf2M6rQPrYemoAo&json=', 'num_citations': 23, 'citedby_url': '/scholar?cites=12112480548646894252&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:rDrtYuIsGKgJ:scholar.google.com/&scioq=Behavior+Priors+for+Efficient+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.jmlr.org/papers/volume23/20-1038/20-1038.pdf'}}, 31: {'title': 'Catch and carry: Reusable neural controllers for vision-guided whole-body tasks', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Catch & carry: reusable neural controllers for vision-guided whole-body tasks', 'author': ['J Merel', 'S Tunyasuvunakool', 'A Ahuja', 'Y Tassa'], 'pub_year': '2020', 'venue': 'ACM Transactions on …', 'abstract': 'We address the longstanding challenge of producing flexible, realistic humanoid character controllers that can perform diverse whole-body tasks involving object interactions. This challenge is central to a variety of fields, from graphics and animation to robotics and motor neuroscience. Our physics-based environment uses realistic actuation and first-person perception-including touch sensors and egocentric vision-with a view to producing active-sensing behaviors (eg gaze direction), transferability to real robots, and comparisons to the'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://dl.acm.org/doi/abs/10.1145/3386569.3392474', 'author_id': ['K4OcFXUAAAAJ', '9dAjSlYAAAAJ', 'HFV9GmMAAAAJ', 'CjOTm_4AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:2LUfRG4rk84J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCatch%2Band%2Bcarry:%2BReusable%2Bneural%2Bcontrollers%2Bfor%2Bvision-guided%2Bwhole-body%2Btasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2LUfRG4rk84J&ei=myx9ZPuTBYvwyAS0sqboDg&json=', 'num_citations': 76, 'citedby_url': '/scholar?cites=14885288945978947032&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:2LUfRG4rk84J:scholar.google.com/&scioq=Catch+and+carry:+Reusable+neural+controllers+for+vision-guided+whole-body+tasks&hl=en&as_sdt=0,33', 'eprint_url': 'https://dl.acm.org/doi/pdf/10.1145/3386569.3392474'}}, 32: {'title': 'CoMic: Complementary Task Learning & Mimicry for Reusable Skills', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Comic: Complementary task learning & mimicry for reusable skills', 'author': ['L Hasenclever', 'F Pardo', 'R Hadsell'], 'pub_year': '2020', 'venue': 'International …', 'abstract': 'Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills by imitating motion capture data and joint training with complementary tasks. We show that it is possible to learn reusable skills through reinforcement learning on 50 times more motion capture data than prior work. We systematically compare a variety of different network architectures across different data regimes both in terms of imitation performance as well as'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v119/hasenclever20a.html', 'author_id': ['dD-3S4QAAAAJ', 'LDpuKxkAAAAJ', 'Q0YEc-QAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:yM1TfLrnlqYJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoMic:%2BComplementary%2BTask%2BLearning%2B%2526%2BMimicry%2Bfor%2BReusable%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=yM1TfLrnlqYJ&ei=nSx9ZNigEsCQ6rQP9LmDmAo&json=', 'num_citations': 31, 'citedby_url': '/scholar?cites=12004036644938436040&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:yM1TfLrnlqYJ:scholar.google.com/&scioq=CoMic:+Complementary+Task+Learning+%26+Mimicry+for+Reusable+Skills&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v119/hasenclever20a/hasenclever20a.pdf'}}, 33: {'title': 'Discovering Motor Programs by Recomposing Demonstrations', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Discovering motor programs by recomposing demonstrations', 'author': ['T Shankar', 'S Tulsiani', 'L Pinto'], 'pub_year': '2020', 'venue': '… Conference on Learning …', 'abstract': 'In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing demonstrations into primitives often assume manually defined primitives and bypass the difficulty of discovering these primitives. On the other hand, approaches in primitive discovery put restrictive assumptions on the complexity of a primitive, which limit applicability to narrow tasks. Our approach attempts to circumvent these challenges by jointly learning'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://openreview.net/forum?id=rkgHY0NYwr', 'author_id': ['0k1qcvgAAAAJ', '06rffEkAAAAJ', 'pmVPj94AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:IaYonpojJUYJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscovering%2BMotor%2BPrograms%2Bby%2BRecomposing%2BDemonstrations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=IaYonpojJUYJ&ei=nyx9ZJqJF6KbywSeir5w&json=', 'num_citations': 39, 'citedby_url': '/scholar?cites=5054485303778649633&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:IaYonpojJUYJ:scholar.google.com/&scioq=Discovering+Motor+Programs+by+Recomposing+Demonstrations&hl=en&as_sdt=0,33', 'eprint_url': 'https://openreview.net/pdf?id=rkgHY0NYwr'}}, 34: {'title': 'Hierarchical reinforcement learning for efficent exploration and transfer', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical reinforcement learning for efficient exploration and transfer', 'author': ['L Steccanella', 'S Totaro', 'D Allonsius'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Sparse-reward domains are challenging for reinforcement learning algorithms since significant exploration is needed before encountering reward for the first time. Hierarchical reinforcement learning can facilitate exploration by reducing the number of decisions necessary before obtaining a reward. In this paper, we present a novel hierarchical reinforcement learning framework based on the compression of an invariant state space that is common to a range of tasks. The algorithm introduces subtasks which consist of moving'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2011.06335', 'author_id': ['ZcxHOUQAAAAJ', 'D4ESIykAAAAJ', ''], 'url_scholarbib': '/scholar?hl=en&q=info:03rgN4S4GqEJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2Breinforcement%2Blearning%2Bfor%2Befficent%2Bexploration%2Band%2Btransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=03rgN4S4GqEJ&ei=oSx9ZK6mAZCP6rQP6Iq6sAY&json=', 'num_citations': 8, 'citedby_url': '/scholar?cites=11608793867513526995&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:03rgN4S4GqEJ:scholar.google.com/&scioq=Hierarchical+reinforcement+learning+for+efficent+exploration+and+transfer&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2011.06335'}}, 35: {'title': 'Learning quadrupedal locomotion over challenging terrain', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning quadrupedal locomotion over challenging terrain', 'author': ['J Lee', 'J Hwangbo', 'L Wellhausen', 'V Koltun', 'M Hutter'], 'pub_year': '2020', 'venue': 'Science robotics', 'abstract': 'Legged locomotion can extend the operational domain of robots to some of the most challenging environments on Earth. However, conventional controllers for legged locomotion are based on elaborate state machines that explicitly trigger the execution of motion primitives and reflexes. These designs have increased in complexity but fallen short of the generality and robustness of animal locomotion. Here, we present a robust controller for blind quadrupedal locomotion in challenging natural environments. Our approach'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://www.science.org/doi/abs/10.1126/scirobotics.abc5986', 'author_id': ['6Htb7swAAAAJ', 'Uam1ZB8AAAAJ', '0otNjsQAAAAJ', 'kg4bCpgAAAAJ', 'DO3quJYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:xlhKqu4B9FoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bquadrupedal%2Blocomotion%2Bover%2Bchallenging%2Bterrain%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=xlhKqu4B9FoJ&ei=oyx9ZMubCcKM6rQPyqqYyAU&json=', 'num_citations': 526, 'citedby_url': '/scholar?cites=6553865482301757638&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:xlhKqu4B9FoJ:scholar.google.com/&scioq=Learning+quadrupedal+locomotion+over+challenging+terrain&hl=en&as_sdt=0,33', 'eprint_url': 'https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/448343/1/2020_science_robotics_lee_locomotion.pdf'}}, 36: {'title': 'Learning Robot Skills with Temporal Variational Inference', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning robot skills with temporal variational inference', 'author': ['T Shankar', 'A Gupta'], 'pub_year': '2020', 'venue': 'International Conference on Machine …', 'abstract': 'In this paper, we address the discovery of robotic options from demonstrations in an unsupervised manner. Specifically, we present a framework to jointly learn low-level control policies and higher-level policies of how to use them from demonstrations of a robot performing various tasks. By representing options as continuous latent variables, we frame the problem of learning these options as latent variable inference. We then present a temporally causal variant of variational inference based on a temporal factorization of'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://proceedings.mlr.press/v119/shankar20b.html', 'author_id': ['0k1qcvgAAAAJ', 'bqL73OkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:DTujsMcYmbgJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobot%2BSkills%2Bwith%2BTemporal%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=DTujsMcYmbgJ&ei=pCx9ZLy6LoHuygS37r6oCA&json=', 'num_citations': 39, 'citedby_url': '/scholar?cites=13301690220356516621&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:DTujsMcYmbgJ:scholar.google.com/&scioq=Learning+Robot+Skills+with+Temporal+Variational+Inference&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v119/shankar20b/shankar20b.pdf'}}, 37: {'title': 'Multi-expert learning of adaptive legged locomotion', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Multi-expert learning of adaptive legged locomotion', 'author': ['C Yang', 'K Yuan', 'Q Zhu', 'W Yu', 'Z Li'], 'pub_year': '2020', 'venue': 'Science Robotics', 'abstract': 'Achieving versatile robot locomotion requires motor skills that can adapt to previously unseen situations. We propose a multi-expert learning architecture (MELA) that learns to generate adaptive skills from a group of representative expert skills. During training, MELA is first initialized by a distinct set of pretrained experts, each in a separate deep neural network (DNN). Then, by learning the combination of these DNNs using a gating neural network (GNN), MELA can acquire more specialized experts and transitional skills across various'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://www.science.org/doi/abs/10.1126/scirobotics.abb2174', 'author_id': ['R2qREd0AAAAJ', '8eLlbhMAAAAJ', '', 'J66YKBwAAAAJ', 'PrJjrjIAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:L0XO-wp3fbcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-expert%2Blearning%2Bof%2Badaptive%2Blegged%2Blocomotion%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=L0XO-wp3fbcJ&ei=pix9ZPbROr2P6rQP85mLmAc&json=', 'num_citations': 99, 'citedby_url': '/scholar?cites=13221854970087621935&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:L0XO-wp3fbcJ:scholar.google.com/&scioq=Multi-expert+learning+of+adaptive+legged+locomotion&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2012.05810'}}, 38: {'title': 'Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer', 'author': ['K Rana', 'V Dasagi', 'B Talbot', 'M Milford'], 'pub_year': '2020', 'venue': '2020 IEEE/RSJ …', 'abstract': 'Learning-based approaches often outperform hand-coded algorithmic solutions for many problems in robotics. However, learning long-horizon tasks on real robot hardware can be intractable, and transferring a learned policy from simulation to reality is still extremely challenging. We present a novel approach to model-free reinforcement learning that can leverage existing sub-optimal solutions as an algorithmic prior during training and deployment. During training, our gated fusion approach enables the prior to guide the initial'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ieeexplore.ieee.org/abstract/document/9341372/', 'author_id': ['-hYjPxsAAAAJ', 'BrhJ6-EAAAAJ', 'oDCvYTEAAAAJ', 'TDSmCKgAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:G-iXIk_dT10J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiplicative%2Bcontroller%2Bfusion:%2BLeveraging%2Balgorithmic%2Bpriors%2Bfor%2Bsample-efficient%2Breinforcement%2Blearning%2Band%2Bsafe%2Bsim-to-real%2Btransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=G-iXIk_dT10J&ei=qCx9ZIzgM4b4yASAjZegAQ&json=', 'num_citations': 8, 'citedby_url': '/scholar?cites=6723836100639975451&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:G-iXIk_dT10J:scholar.google.com/&scioq=Multiplicative+controller+fusion:+Leveraging+algorithmic+priors+for+sample-efficient+reinforcement+learning+and+safe+sim-to-real+transfer&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2003.05117'}}, 39: {'title': 'PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING', 'year': '2020', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Parrot: Data-driven behavioral priors for reinforcement learning', 'author': ['A Singh', 'H Liu', 'G Zhou', 'A Yu', 'N Rhinehart'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to learn. In other machine learning fields, such as natural language processing or computer vision, pre-training on large, previously collected datasets to bootstrap learning for new tasks has emerged as a powerful paradigm to reduce data requirements when learning a new task. In this paper, we ask the following question: how can we enable similarly useful'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2011.10024', 'author_id': ['C2_ZXdcAAAAJ', 'mPabwyYAAAAJ', '-1iyBukAAAAJ', 'ZzURcb4AAAAJ', 'xUGZX_MAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:EzS89Q8xnwsJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPARROT:%2BDATA-DRIVEN%2BBEHAVIORAL%2BPRIORS%2BFOR%2BREINFORCEMENT%2BLEARNING%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=EzS89Q8xnwsJ&ei=qix9ZMnnLf2M6rQPrYemoAo&json=', 'num_citations': 85, 'citedby_url': '/scholar?cites=837442000331224083&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:EzS89Q8xnwsJ:scholar.google.com/&scioq=PARROT:+DATA-DRIVEN+BEHAVIORAL+PRIORS+FOR+REINFORCEMENT+LEARNING&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2011.10024'}}, 40: {'title': 'Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Directed-info gail: Learning hierarchical policies from unsegmented demonstrations using directed information', 'author': ['A Sharma', 'M Sharma', 'N Rhinehart'], 'pub_year': '2018', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The use of imitation learning to learn a single policy for a complex task that has multiple modes or hierarchical structure can be challenging. In fact, previous work has shown that when the modes are known, learning separate policies for each mode or sub-task can greatly improve the performance of imitation learning. In this work, we discover the interaction between sub-tasks from their resulting state-action trajectory sequences using a directed graphical model. We propose a new algorithm based on the generative adversarial'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1810.01266', 'author_id': ['uOSmj2MAAAAJ', 'FBVwO5wAAAAJ', 'xUGZX_MAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:pugvoGcMYkoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDirected-info%2BGAIL:%2BLearning%2Bhierarchical%2Bpolicies%2Bfrom%2Bunsegmented%2Bdemonstrations%2Busing%2Bdirected%2Binformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=pugvoGcMYkoJ&ei=rCx9ZPbEFYvwyAS0sqboDg&json=', 'num_citations': 57, 'citedby_url': '/scholar?cites=5359860145732970662&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:pugvoGcMYkoJ:scholar.google.com/&scioq=Directed-info+GAIL:+Learning+hierarchical+policies+from+unsegmented+demonstrations+using+directed+information&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1810.01266'}}, 41: {'title': 'Dynamics-Aware Unsupervised Discovery of Skills', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Dynamics-aware unsupervised discovery of skills', 'author': ['A Sharma', 'S Gu', 'S Levine', 'V Kumar'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Conventionally, model-based reinforcement learning (MBRL) aims to learn a global model for the dynamics of the environment. A good model can potentially enable planning algorithms to generate a large variety of behaviors and solve diverse tasks. However, learning an accurate model for complex dynamical systems is difficult, and even then, the model might not generalize well outside the distribution of states on which it was trained. In this work, we combine model-based learning with model-free learning of primitives that'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1907.01657', 'author_id': ['_0IIzxgAAAAJ', 'B8wslVsAAAAJ', '8R35rCwAAAAJ', 'nu3W--sAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:kJKBrRGuQfMJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamics-Aware%2BUnsupervised%2BDiscovery%2Bof%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kJKBrRGuQfMJ&ei=rix9ZOSqB8CQ6rQP9LmDmAo&json=', 'num_citations': 273, 'citedby_url': '/scholar?cites=17528482615651308176&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:kJKBrRGuQfMJ:scholar.google.com/&scioq=Dynamics-Aware+Unsupervised+Discovery+of+Skills&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1907.01657'}}, 42: {'title': 'Learning Latent Plans from Play', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning latent plans from play', 'author': 'Lynch, Corey and Khansari, Mohi and Xiao, Ted and Kumar, Vikash and Tompson, Jonathan and Levine, Sergey and Sermanet, Pierre', 'pub_year': '2020', 'venue': '… on robot learning', 'abstract': 'Unsupervised task discovery: We investigate the latent plan spaced learned by Play-LMP,  seeing whether or not it is capable of encoding task information despite not being trained with', 'organization': 'PMLR', 'pages': '1113--1132', 'booktitle': 'Conference on robot learning', 'pub_type': 'inproceedings', 'bib_id': 'lynch2020learning'}, 'filled': True, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v100/lynch20a.html', 'author_id': ['CYWO-oAAAAAJ', 'Z3dxz9IAAAAJ', 'LIJQ_ZYAAAAJ', 'nu3W--sAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:_So_NjYYD4YJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BPlans%2Bfrom%2BPlay%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_So_NjYYD4YJ&ei=sCx9ZKfeDKKbywSeir5w&json=', 'num_citations': 245, 'citedby_url': '/scholar?cites=9659966346850413309&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:_So_NjYYD4YJ:scholar.google.com/&scioq=Learning+Latent+Plans+from+Play&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v100/lynch20a/lynch20a.pdf'}, 'citedby': [{'title': 'A review of robot learning for manipulation: Challenges, representations, and algorithms', 'author': ['O Kroemer', 'S Niekum', 'G Konidaris'], 'pub_year': '2021', 'venue': 'The Journal of Machine Learning …', 'abstract': 'A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen'}, {'title': \"Solving rubik's cube with a robot hand\", 'author': ['I Akkaya', 'M Andrychowicz', 'M Chociej', 'M Litwin'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key'}, {'title': 'Play, curiosity, and cognition', 'author': ['J Chu', 'LE Schulz'], 'pub_year': '2020', 'venue': 'Annual Review of Developmental …', 'abstract': 'Few phenomena in childhood are as compelling—and mystifying—as play. We review five proposals about the relationship between play and development. We believe each captures'}, {'title': 'Amp: Adversarial motion priors for stylized physics-based character control', 'author': ['XB Peng', 'Z Ma', 'P Abbeel', 'S Levine'], 'pub_year': '2021', 'venue': 'ACM Transactions on …', 'abstract': 'Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion'}, {'title': 'Accelerating reinforcement learning with learned skill priors', 'author': ['K Pertsch', 'Y Lee', 'J Lim'], 'pub_year': '2021', 'venue': 'Conference on robot learning', 'abstract': 'Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One'}, {'title': 'Episodic transformer for vision-and-language navigation', 'author': ['A Pashevich', 'C Schmid', 'C Sun'], 'pub_year': '2021', 'venue': 'Proceedings of the IEEE …', 'abstract': 'Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on'}, {'title': 'Behavior Transformers: Cloning  modes with one stone', 'author': ['NM Shafiullah', 'Z Cui'], 'pub_year': '2022', 'venue': 'Advances in neural …', 'abstract': 'While behavior learning has made impressive progress in recent times, it lags behind computer vision and natural language processing due to its inability to leverage large'}, {'title': 'Search on the replay buffer: Bridging planning and reinforcement learning', 'author': ['B Eysenbach', 'RR Salakhutdinov'], 'pub_year': '2019', 'venue': 'Advances in Neural …', 'abstract': 'The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively'}, {'title': 'Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning', 'author': ['A Gupta', 'V Kumar', 'C Lynch', 'S Levine'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two'}, {'title': 'Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters', 'author': ['XB Peng', 'Y Guo', 'L Halper', 'S Levine'], 'pub_year': '2022', 'venue': 'ACM Transactions On …', 'abstract': 'The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and'}, {'title': 'Robonet: Large-scale multi-robot learning', 'author': ['S Dasari', 'F Ebert', 'S Tian', 'S Nair', 'B Bucher'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the'}, {'title': 'Instruction-driven history-aware policies for robotic manipulations', 'author': ['PL Guhur', 'S Chen', 'RG Pinel'], 'pub_year': '2023', 'venue': '… on Robot Learning', 'abstract': 'In human environments, robots are expected to accomplish a variety of manipulation tasks given simple natural language instructions. Yet, robotic manipulation is extremely'}, {'title': 'Contrastive learning as goal-conditioned reinforcement learning', 'author': ['B Eysenbach', 'T Zhang', 'S Levine'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often'}, {'title': 'Parrot: Data-driven behavioral priors for reinforcement learning', 'author': ['A Singh', 'H Liu', 'G Zhou', 'A Yu', 'N Rhinehart'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to'}, {'title': 'Actionable models: Unsupervised offline reinforcement learning of robotic skills', 'author': ['Y Chebotar', 'K Hausman', 'Y Lu', 'T Xiao'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting'}, {'title': 'Explore, discover and learn: Unsupervised discovery of state-covering skills', 'author': ['V Campos', 'A Trott', 'C Xiong', 'R Socher'], 'pub_year': '2020', 'venue': 'International …', 'abstract': 'Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of'}, {'title': 'Accelerating robotic reinforcement learning via parameterized action primitives', 'author': ['M Dalal', 'D Pathak'], 'pub_year': '2021', 'venue': 'Advances in Neural …', 'abstract': 'Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the'}, {'title': 'Language conditioned imitation learning over unstructured data', 'author': ['C Lynch', 'P Sermanet'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2005.07648', 'abstract': 'Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with'}, {'title': 'RvS: What is Essential for Offline RL via Supervised Learning?', 'author': ['S Emmons', 'B Eysenbach', 'I Kostrikov'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which'}, {'title': 'Transformers for one-shot visual imitation', 'author': ['S Dasari', 'A Gupta'], 'pub_year': '2021', 'venue': 'Conference on Robot Learning', 'abstract': 'Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex'}, {'title': 'Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation', 'author': ['S Nair', 'C Finn'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv:1909.05829', 'abstract': 'Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision'}, {'title': 'Latent plans for task-agnostic offline reinforcement learning', 'author': ['E Rosete-Beas', 'O Mees', 'G Kalweit'], 'pub_year': '2023', 'venue': '… on Robot Learning', 'abstract': 'Everyday tasks of long-horizon and comprising a sequence of multiple implicit subtasks still impose a major challenge in offline robot control. While a number of prior methods aimed to'}, {'title': 'Hierarchical planning for long-horizon manipulation with geometric and symbolic scene graphs', 'author': ['Y Zhu', 'J Tremblay', 'S Birchfield'], 'pub_year': '2021', 'venue': '2021 IEEE International …', 'abstract': 'We present a visually grounded hierarchical planning algorithm for long-horizon manipulation tasks. Our algorithm offers a joint framework of neuro-symbolic task planning'}, {'title': 'Generalized decision transformer for offline hindsight information matching', 'author': ['H Furuta', 'Y Matsuo', 'SS Gu'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2111.10364', 'abstract': 'How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for'}, {'title': 'Rewriting history with inverse rl: Hindsight inference for policy improvement', 'author': ['B Eysenbach', 'X Geng', 'S Levine'], 'pub_year': '2020', 'venue': 'Advances in neural …', 'abstract': 'Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward'}, {'title': 'Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system', 'author': ['A Handa', 'K Van Wyk', 'W Yang', 'J Liang'], 'pub_year': '2020', 'venue': '… on Robotics and …', 'abstract': 'Teleoperation offers the possibility of imparting robotic systems with sophisticated reasoning skills, intuition, and creativity to perform tasks. However, teleoperation solutions for high'}, {'title': 'Iris: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data', 'author': ['A Mandlekar', 'F Ramos', 'B Boots'], 'pub_year': '2020', 'venue': '… on Robotics and …', 'abstract': 'Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline'}, {'title': 'Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks', 'author': ['O Mees', 'L Hermann', 'E Rosete-Beas'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': 'General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks'}, {'title': 'Learning mobile manipulation through deep reinforcement learning', 'author': ['C Wang', 'Q Zhang', 'Q Tian', 'S Li', 'X Wang', 'D Lane'], 'pub_year': '2020', 'venue': 'Sensors', 'abstract': 'Mobile manipulation has a broad range of applications in robotics. However, it is usually more challenging than fixed-base manipulation due to the complex coordination of a mobile'}, {'title': 'Learning to reach goals via iterated supervised learning', 'author': ['D Ghosh', 'A Gupta', 'A Reddy', 'J Fu', 'C Devin'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation'}, {'title': 'Learning affordance landscapes for interaction exploration in 3d environments', 'author': ['T Nagarajan', 'K Grauman'], 'pub_year': '2020', 'venue': 'Advances in Neural Information …', 'abstract': 'Embodied agents operating in human spaces must be able to master how their environment works: what objects can the agent use, and how can it use them? We introduce a'}, {'title': 'Dall-e-bot: Introducing web-scale diffusion models to robotics', 'author': ['I Kapelyukh', 'V Vosylius', 'E Johns'], 'pub_year': '2023', 'venue': 'IEEE Robotics and …', 'abstract': 'We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those'}, {'title': 'Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation', 'author': ['SP Arunachalam', 'S Silwal', 'B Evans', 'L Pinto'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement'}, {'title': 'C-learning: Learning to achieve goals via recursive classification', 'author': ['B Eysenbach', 'R Salakhutdinov', 'S Levine'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned'}, {'title': 'Learning to generalize across long-horizon tasks from human demonstrations', 'author': ['A Mandlekar', 'D Xu', 'R Martín-Martín', 'S Savarese'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Imitation learning is an effective and safe technique to train robot policies in the real world because it does not depend on an expensive random exploration process. However, due to'}, {'title': 'Hierarchical reinforcement learning by discovering intrinsic options', 'author': ['J Zhang', 'H Yu', 'W Xu'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2101.06521', 'abstract': 'We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve'}, {'title': 'Grounding language in play', 'author': ['C Lynch', 'P Sermanet'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2005.07648', 'abstract': 'Natural language is perhaps the most versatile and intuitive way for humans to communicate tasks to a robot. Prior work on Learning from Play (LfP)(Lynch et al., 2019) provides a simple'}, {'title': 'Comic: Complementary task learning & mimicry for reusable skills', 'author': ['L Hasenclever', 'F Pardo', 'R Hadsell'], 'pub_year': '2020', 'venue': 'International …', 'abstract': 'Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills'}, {'title': 'Trail: Near-optimal imitation learning with suboptimal data', 'author': ['M Yang', 'S Levine', 'O Nachum'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2110.14770', 'abstract': 'The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be'}, {'title': 'Guided reinforcement learning with learned skills', 'author': ['K Pertsch', 'Y Lee', 'Y Wu', 'JJ Lim'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2107.10253', 'abstract': 'Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task'}, {'title': 'Controlling assistive robots with learned latent actions', 'author': ['DP Losey', 'K Srinivasan', 'A Mandlekar'], 'pub_year': '2020', 'venue': '… on Robotics and …', 'abstract': 'Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful'}, {'title': 'Universal manipulation policy network for articulated objects', 'author': ['Z Xu', 'Z He', 'S Song'], 'pub_year': '2022', 'venue': 'IEEE Robotics and Automation Letters', 'abstract': 'We introduce the Universal Manipulation Policy Network (UMPNet)–a single image-based policy network that infers closed-loop action sequences for manipulating articulated objects'}, {'title': 'What matters in language conditioned robotic imitation learning', 'author': ['O Mees', 'L Hermann', 'W Burgard'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2204.06252', 'abstract': 'A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural'}, {'title': 'The differentiable cross-entropy method', 'author': ['B Amos', 'D Yarats'], 'pub_year': '2020', 'venue': 'International Conference on Machine …', 'abstract': 'We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that'}, {'title': 'Learning latent actions to control assistive robots', 'author': ['DP Losey', 'HJ Jeon', 'M Li', 'K Srinivasan', 'A Mandlekar'], 'pub_year': '2022', 'venue': 'Autonomous …', 'abstract': 'Assistive robot arms enable people with disabilities to conduct everyday tasks on their own. These arms are dexterous and high-dimensional; however, the interfaces people must use'}, {'title': 'Affordance learning from play for sample-efficient policy learning', 'author': ['J Borja-Diaz', 'O Mees', 'G Kalweit'], 'pub_year': '2022', 'venue': '… on Robotics and …', 'abstract': 'Robots operating in human-centered environments should have the ability to understand how objects function: what can be done with each object, where this interaction may occur'}, {'title': 'Intra-agent speech permits zero-shot task acquisition', 'author': ['C Yan', 'F Carnevale', 'PI Georgiev'], 'pub_year': '2022', 'venue': 'Advances in …', 'abstract': 'Human language learners are exposed to a trickle of informative, context-sensitive language, but a flood of raw sensory data. Through both social language use and internal'}, {'title': 'Skill-based meta-reinforcement learning', 'author': ['T Nam', 'SH Sun', 'K Pertsch', 'SJ Hwang'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors'}, {'title': 'What matters in language conditioned robotic imitation learning over unstructured data', 'author': ['O Mees', 'L Hermann', 'W Burgard'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': 'A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural'}, {'title': 'Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning', 'author': ['Y Li', 'T Gao', 'J Yang', 'H Xu', 'Y Wu'], 'pub_year': '2022', 'venue': '… Conference on Machine …', 'abstract': 'It has been a recent trend to leverage the power of supervised learning (SL) towards more effective reinforcement learning (RL) methods. We propose a novel phasic solution by'}, {'title': 'Model-based visual planning with self-supervised functional distances', 'author': ['S Tian', 'S Nair', 'F Ebert', 'S Dasari', 'B Eysenbach'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal'}, {'title': 'Intrinsically motivated goal-conditioned reinforcement learning: a short survey', 'author': ['C Colas', 'T Karch', 'O Sigaud', 'PY Oudeyer'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Building autonomous machines that can explore open-ended environments, discover possible interactions and autonomously build repertoires of skills is a general objective of'}, {'title': 'Eliciting compatible demonstrations for multi-human imitation learning', 'author': ['K Gandhi', 'S Karamcheti', 'M Liao'], 'pub_year': '2023', 'venue': 'Conference on Robot …', 'abstract': 'Imitation learning from human-provided demonstrations is a strong approach for learning policies for robot manipulation. While the ideal dataset for imitation learning is homogenous'}, {'title': 'Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey', 'author': ['C Colas', 'T Karch', 'O Sigaud', 'PY Oudeyer'], 'pub_year': '2022', 'venue': 'Journal of Artificial Intelligence …', 'abstract': 'Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial'}, {'title': 'Latent-variable advantage-weighted policy optimization for offline rl', 'author': ['X Chen', 'A Ghadirzadeh', 'T Yu', 'Y Gao', 'J Wang'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Offline reinforcement learning methods hold the promise of learning policies from pre-collected datasets without the need to query the environment for new transitions. This setting'}, {'title': 'What can i do here? learning new skills by imagining visual affordances', 'author': ['A Khazatsky', 'A Nair', 'D Jing'], 'pub_year': '2021', 'venue': '2021 IEEE International …', 'abstract': 'A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always'}, {'title': 'Interactive language: Talking to robots in real time', 'author': ['C Lynch', 'A Wahid', 'J Tompson', 'T Ding', 'J Betker'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We present a framework for building interactive, real-time, natural language-instructable robots in the real world, and we open source related assets (dataset, environment'}, {'title': 'Imitation learning: Progress, taxonomies and challenges', 'author': ['B Zheng', 'S Verma', 'J Zhou', 'IW Tsang'], 'pub_year': '2022', 'venue': 'IEEE Transactions on …', 'abstract': \"Imitation learning (IL) aims to extract knowledge from human experts' demonstrations or artificially created agents to replicate their behaviors. It promotes interdisciplinary\"}, {'title': 'Rapid exploration for open-world navigation with latent goal models', 'author': ['D Shah', 'B Eysenbach', 'N Rhinehart'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable'}, {'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation', 'author': ['OAI OpenAI', 'M Plappert', 'R Sampedro', 'T Xu'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We train a single, goal-conditioned policy that can solve many robotic manipulation tasks, including tasks with previously unseen goals and objects. We rely on asymmetric self-play'}, {'title': 'Learning geometric reasoning and control for long-horizon tasks from visual input', 'author': ['D Driess', 'JS Ha', 'R Tedrake'], 'pub_year': '2021', 'venue': '2021 IEEE International …', 'abstract': 'Long-horizon manipulation tasks require joint reasoning over a sequence of discrete actions and their associated continuous control parameters. While Task and Motion Planning'}, {'title': 'Discovering motor programs by recomposing demonstrations', 'author': ['T Shankar', 'S Tulsiani', 'L Pinto'], 'pub_year': '2020', 'venue': '… Conference on Learning …', 'abstract': 'In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing'}, {'title': 'Foundation models for decision making: Problems, methods, and opportunities', 'author': ['S Yang', 'O Nachum', 'Y Du', 'J Wei', 'P Abbeel'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed'}, {'title': 'Receding-horizon perceptive trajectory optimization for dynamic legged locomotion with learned initialization', 'author': ['O Melon', 'R Orsolino', 'D Surovik'], 'pub_year': '2021', 'venue': '… on Robotics and …', 'abstract': 'To dynamically traverse challenging terrain, legged robots need to continually perceive and reason about upcoming features, adjust the locations and timings of future footfalls and'}, {'title': 'Learning transferable motor skills with hierarchical latent mixture policies', 'author': ['D Rao', 'F Sadeghi', 'L Hasenclever', 'M Wulfmeier'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'For robots operating in the real world, it is desirable to learn reusable behaviours that can effectively be transferred and adapted to numerous tasks and scenarios. We propose an'}, {'title': 'Avlen: Audio-visual-language embodied navigation in 3d environments', 'author': ['S Paul', 'A Roy-Chowdhury'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'Recent years have seen embodied visual navigation advance in two distinct directions:(i) in equipping the AI agent to follow natural language instructions, and (ii) in making the'}, {'title': 'Towards more generalizable one-shot visual imitation learning', 'author': ['Z Mandi', 'F Liu', 'K Lee', 'P Abbeel'], 'pub_year': '2022', 'venue': '… International Conference on …', 'abstract': 'A general-purpose robot should be able to master a wide range of tasks and quickly learn a novel one by leveraging past experiences. One-shot imitation learning (OSIL) approaches'}, {'title': 'Broadly-exploring, local-policy trees for long-horizon task planning', 'author': ['B Ichter', 'P Sermanet', 'C Lynch'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2010.06491', 'abstract': 'Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion'}, {'title': 'Dichotomy of control: Separating what you can control from what you cannot', 'author': ['M Yang', 'D Schuurmans', 'P Abbeel'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Future-or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), where the future outcome (ie, return) associated with an'}, {'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics', 'author': ['K Rana', 'M Xu', 'B Tidd', 'M Milford'], 'pub_year': '2023', 'venue': 'Conference on Robot …', 'abstract': 'Skill-based reinforcement learning (RL) has emerged as a promising strategy to leverage prior knowledge for accelerated robot learning. Skills are typically extracted from expert'}, {'title': 'Bootstrapped autonomous practicing via multi-task reinforcement learning', 'author': ['A Gupta', 'C Lynch', 'B Kinman', 'G Peake', 'S Levine'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice'}, {'title': 'ASHA: Assistive teleoperation via human-in-the-loop reinforcement learning', 'author': ['S Chen', 'J Gao', 'S Reddy', 'G Berseth'], 'pub_year': '2022', 'venue': '… on Robotics and …', 'abstract': 'Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (eg, webcam images of eye gaze) can be challenging, especially when it involves'}, {'title': 'Is curiosity all you need? on the utility of emergent behaviours from curious exploration', 'author': ['O Groth', 'M Wulfmeier', 'G Vezzani', 'V Dasagi'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Curiosity-based reward schemes can present powerful exploration mechanisms which facilitate the discovery of solutions for complex, sparse or long-horizon tasks. However, as'}, {'title': 'Learning robotic navigation from experience: principles, methods and recent results', 'author': ['S Levine', 'D Shah'], 'pub_year': '2023', 'venue': '… Transactions of the Royal Society B', 'abstract': 'Navigation is one of the most heavily studied problems in robotics and is conventionally approached as a geometric mapping and planning problem. However, real-world navigation'}, {'title': 'Hierarchical policies for cluttered-scene grasping with latent plans', 'author': ['L Wang', 'X Meng', 'Y Xiang', 'D Fox'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': '6D grasping in cluttered scenes is a longstanding problem in robotic manipulation. Open-loop manipulation pipelines may fail due to inaccurate state estimation, while most end-to'}, {'title': 'Lapo: Latent-variable advantage-weighted policy optimization for offline reinforcement learning', 'author': ['X Chen', 'A Ghadirzadeh', 'T Yu', 'J Wang'], 'pub_year': '2022', 'venue': 'Advances in …', 'abstract': 'Offline reinforcement learning methods hold the promise of learning policies from pre-collected datasets without the need to query the environment for new samples. This setting'}, {'title': 'A policy-guided imitation approach for offline reinforcement learning', 'author': ['H Xu', 'L Jiang', 'L Jianxiong'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution'}, {'title': 'Offline imitation learning with a misspecified simulator', 'author': ['S Jiang', 'J Pang', 'Y Yu'], 'pub_year': '2020', 'venue': 'Advances in neural information …', 'abstract': 'In real-world decision-making tasks, learning an optimal policy without a trial-and-error process is an appealing challenge. When expert demonstrations are available, imitation'}, {'title': 'Mimicplay: Long-horizon imitation learning by watching human play', 'author': ['C Wang', 'L Fan', 'J Sun', 'R Zhang', 'L Fei-Fei', 'D Xu'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Imitation Learning from human demonstrations is a promising paradigm to teach robots manipulation skills in the real world, but learning complex long-horizon tasks often requires'}, {'title': 'Plato: Predicting latent affordances through object-centric play', 'author': ['S Belkhale', 'D Sadigh'], 'pub_year': '2023', 'venue': 'Conference on Robot Learning', 'abstract': 'Constructing a diverse repertoire of manipulation skills in a scalable fashion remains an unsolved challenge in robotics. One way to address this challenge is with unstructured'}, {'title': 'Offline Goal-Conditioned Reinforcement Learning via -Advantage Regression', 'author': ['JY Ma', 'J Yan', 'D Jayaraman'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'Offline goal-conditioned reinforcement learning (GCRL) promises general-purpose skill learning in the form of reaching diverse goals from purely offline datasets. We propose'}, {'title': 'Hybrid trajectory and force learning of complex assembly tasks: A combined learning framework', 'author': ['Y Wang', 'CC Beltran-Hernandez', 'W Wan'], 'pub_year': '2021', 'venue': 'IEEE Access', 'abstract': 'Complex assembly tasks involve nonlinear and low-clearance insertion trajectories with varying contact forces at different stages. For a robot to solve these tasks, it requires a'}, {'title': 'Robust flight navigation out of distribution with liquid neural networks', 'author': ['M Chahine', 'R Hasani', 'P Kao', 'A Ray', 'R Shubert'], 'pub_year': '2023', 'venue': 'Science Robotics', 'abstract': 'Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations and generalize well to online and unseen scenarios within the same'}, {'title': 'Solving compositional reinforcement learning problems via task reduction', 'author': ['Y Li', 'Y Wu', 'H Xu', 'X Wang', 'Y Wu'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2103.07607', 'abstract': 'We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task'}, {'title': 'Grounding language with visual affordances over unstructured data', 'author': ['O Mees', 'J Borja-Diaz', 'W Burgard'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2210.01911', 'abstract': 'Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task'}, {'title': 'Hindsight for foresight: Unsupervised structured dynamics models from physical interaction', 'author': ['I Nematollahi', 'O Mees', 'L Hermann'], 'pub_year': '2020', 'venue': '2020 IEEE/RSJ …', 'abstract': 'A key challenge for an agent learning to interact with the world is to reason about physical properties of objects and to foresee their dynamics under the effect of applied forces. In'}, {'title': 'Task-oriented motion mapping on robots of various configuration using body role division', 'author': ['K Sasabuchi', 'N Wake', 'K Ikeuchi'], 'pub_year': '2020', 'venue': 'IEEE Robotics and …', 'abstract': 'Many works in robot teaching either focus only on teaching task knowledge, such as geometric constraints, or motion knowledge, such as the motion for accomplishing a task'}, {'title': 'Bits: Bi-level imitation for traffic simulation', 'author': ['D Xu', 'Y Chen', 'B Ivanovic', 'M Pavone'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2208.12403', 'abstract': 'Simulation is the key to scaling up validation and verification for robotic systems such as autonomous vehicles. Despite advances in high-fidelity physics and sensor simulation, a'}, {'title': 'Manipulation planning from demonstration via goal-conditioned prior action primitive decomposition and alignment', 'author': ['N Lin', 'Y Li', 'K Tang', 'Y Zhu', 'X Zhang'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': 'Manipulation plays a vital role in robotics but is left unsolved. Recent work attempts to leverage the hierarchical structure of tasks via using action primitives. However, due to'}, {'title': 'A motion capture and imitation learning based approach to Robot Control', 'author': ['P Racinskis', 'J Arents', 'M Greitans'], 'pub_year': '2022', 'venue': 'Applied Sciences', 'abstract': 'Imitation learning is a discipline of machine learning primarily concerned with replicating observed behavior of agents known to perform well on a given task, collected in'}, {'title': 'Learning multi-stage tasks with one demonstration via self-replay', 'author': ['N Di Palo', 'E Johns'], 'pub_year': '2022', 'venue': 'Conference on Robot Learning', 'abstract': 'In this work, we introduce a novel method to learn everyday-like multi-stage tasks from a single human demonstration, without requiring any prior object knowledge. Inspired by the'}, {'title': 'Learning an expert skill-space for replanning dynamic quadruped locomotion over obstacles', 'author': ['D Surovik', 'O Melon', 'M Geisert'], 'pub_year': '2021', 'venue': '… on Robot Learning', 'abstract': 'Function approximators are increasingly being considered as a tool for generating robot motions that are temporally extended and express foresight about the scenario at hand'}, {'title': 'Learning task decomposition with ordered memory policy network', 'author': ['Y Lu', 'Y Shen', 'S Zhou', 'A Courville'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Many complex real-world tasks are composed of several levels of sub-tasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better'}, {'title': \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via -Advantage Regression\", 'author': ['YJ Ma', 'J Yan', 'D Jayaraman', 'O Bastani'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2206.03023', 'abstract': 'Offline goal-conditioned reinforcement learning (GCRL) promises general-purpose skill learning in the form of reaching diverse goals from purely offline datasets. We propose'}, {'title': 'ToolTango: Common sense Generalization in Predicting Sequential Tool Interactions for Robot Plan Synthesis', 'author': ['S Tuli', 'R Bansal', 'R Paul'], 'pub_year': '2022', 'venue': 'Journal of Artificial Intelligence Research', 'abstract': 'Robots assisting us in environments such as factories or homes must learn to make use of objects as tools to perform tasks, for instance, using a tray to carry objects. We consider the'}, {'title': 'Causal inference q-network: Toward resilient reinforcement learning', 'author': ['CHH Yang', 'I Hung', 'T Danny', 'Y Ouyang'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may'}, {'title': 'Self-supervised reinforcement learning with independently controllable subgoals', 'author': ['A Zadaianchuk', 'G Martius'], 'pub_year': '2022', 'venue': 'Conference on Robot …', 'abstract': 'To successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them. Recently, self-supervised agents that set their'}, {'title': 'Playful interactions for representation learning', 'author': ['S Young', 'J Pari', 'P Abbeel'], 'pub_year': '2022', 'venue': '2022 IEEE/RSJ …', 'abstract': 'Ahstract-One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations'}, {'title': 'Braxlines: Fast and interactive toolkit for rl-driven behavior engineering beyond reward maximization', 'author': ['SS Gu', 'M Diaz', 'DC Freeman', 'H Furuta'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The goal of continuous control is to synthesize desired behaviors. In reinforcement learning (RL)-driven approaches, this is often accomplished through careful task reward engineering'}, {'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains', 'author': ['J Zhang', 'JT Springenberg', 'A Byravan'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In this paper we study the problem of learning multi-step dynamics prediction models (jumpy models) from unlabeled experience and their utility for fast inference of (high-level) plans in'}, {'title': 'Enabling visual action planning for object manipulation through latent space roadmap', 'author': ['M Lippi', 'P Poklukar', 'MC Welle', 'A Varava'], 'pub_year': '2022', 'venue': 'IEEE Transactions …', 'abstract': 'In this article, we present a framework for visual action planning of complex manipulation tasks with high-dimensional state spaces, focusing on manipulation of deformable objects'}, {'title': 'Learning visually guided latent actions for assistive teleoperation', 'author': ['S Karamcheti', 'AJ Zhai', 'DP Losey'], 'pub_year': '2021', 'venue': 'Learning for Dynamics …', 'abstract': 'It is challenging for humans—particularly people living with physical disabilities—to control high-dimensional and dexterous robots. Prior work explores how robots can learn'}, {'title': 'Virtual reality teleoperation of a humanoid robot using markerless human upper body pose imitation', 'author': ['M Hirschmanner', 'C Tsiourti', 'T Patten'], 'pub_year': '2019', 'venue': '2019 IEEE-RAS 19th …', 'abstract': 'Teleoperation of robots with traditional input devices (joysticks, keyboard, etc.) is often difficult and cumbersome especially for novice users. We introduce an intuitive virtual reality'}, {'title': 'Goal-conditioned imitation learning using score-based diffusion policies', 'author': ['M Reuss', 'M Li', 'X Jia', 'R Lioutikov'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2304.02532', 'abstract': 'We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation'}, {'title': 'Learning periodic tasks from human demonstrations', 'author': ['J Yang', 'J Zhang', 'C Settle', 'A Rai'], 'pub_year': '2022', 'venue': '… on Robotics and …', 'abstract': 'We develop a method for learning periodic tasks from visual demonstrations. The core idea is to leverage periodicity in the policy structure to model periodic aspects of the tasks. We'}, {'title': 'Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl', 'author': ['T Yamagata', 'A Khalil', 'R Santos-Rodriguez'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional'}, {'title': 'Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts', 'author': ['E Xing', 'A Gupta', 'S Powers', 'V Dean'], 'pub_year': '2021', 'venue': 'NeurIPS 2021 Workshop on …', 'abstract': 'Humans are remarkably capable of zero-shot generalizing while performing tasks in new settings, even when the task is learned entirely from observing others. In this work, we show'}, {'title': 'Grimgep: learning progress for robust goal sampling in visual deep reinforcement learning', 'author': ['G Kovač', 'A Laversanne-Finot'], 'pub_year': '2022', 'venue': 'IEEE Transactions on …', 'abstract': 'Autotelic RL agents sample their own goals, and try to reach them. They often prioritize goal sampling according to some intrinsic reward, ex. novelty or absolute learning progress'}, {'title': 'Using human gaze to improve robustness against irrelevant objects in robot manipulation tasks', 'author': ['H Kim', 'Y Ohmura', 'Y Kuniyoshi'], 'pub_year': '2020', 'venue': 'IEEE Robotics and Automation …', 'abstract': 'Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images'}, {'title': 'Transferring Hierarchical Structures with Dual Meta Imitation Learning', 'author': ['C Gao', 'Y Jiang', 'F Chen'], 'pub_year': '2023', 'venue': 'Conference on Robot Learning', 'abstract': 'Hierarchical Imitation Learning (HIL) is an effective way for robots to learn sub-skills from long-horizon unsegmented demonstrations. However, the learned hierarchical'}, {'title': 'Geometric task networks: Learning efficient and explainable skill coordination for object manipulation', 'author': ['M Guo', 'M Bürger'], 'pub_year': '2021', 'venue': 'IEEE Transactions on Robotics', 'abstract': 'Complex manipulation tasks can contain various execution branches of primitive skills in sequence or in parallel under different scenarios. Manual specifications of such branching'}, {'title': 'It takes four to tango: Multiagent selfplay for automatic curriculum generation', 'author': ['Y Du', 'P Abbeel', 'A Grover'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2202.10608', 'abstract': 'We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a'}, {'title': 'Trass: Time reversal as self-supervision', 'author': ['S Nair', 'M Babaeizadeh', 'C Finn'], 'pub_year': '2020', 'venue': '… on Robotics and …', 'abstract': 'A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning'}, {'title': 'The embodied crossmodal self forms language and interaction: a computational cognitive review', 'author': ['F Röder', 'O Özdemir', 'PDH Nguyen', 'S Wermter'], 'pub_year': '2021', 'venue': 'Frontiers in …', 'abstract': 'Human language is inherently embodied and grounded in sensorimotor representations of the self and the world around it. This suggests that the body schema and ideomotor action'}, {'title': 'Context-aware language modeling for goal-oriented dialogue systems', 'author': ['C Snell', 'S Yang', 'J Fu', 'Y Su', 'S Levine'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2204.10198', 'abstract': 'Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of'}, {'title': 'Robotic imitation of human assembly skills using hybrid trajectory and force learning', 'author': ['Y Wang', 'CC Beltran-Hernandez'], 'pub_year': '2021', 'venue': '… on Robotics and …', 'abstract': 'Robotic assembly tasks involve complex and low-clearance insertion trajectories with varying contact forces at different stages. While the nominal motion trajectory can be easily'}, {'title': 'LISA: Learning interpretable skill abstractions from language', 'author': ['D Garg', 'S Vaidyanath', 'K Kim', 'J Song'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Learning policies that effectually utilize language instructions in complex, multi-task environments is an important problem in imitation learning. While it is possible to condition'}, {'title': 'Shared autonomy with learned latent actions', 'author': ['HJ Jeon', 'DP Losey', 'D Sadigh'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2005.03210', 'abstract': 'Assistive robots enable people with disabilities to conduct everyday tasks on their own. However, these tasks can be complex, containing both coarse reaching motions and fine'}, {'title': 'Skill-based model-based reinforcement learning', 'author': ['LX Shi', 'JJ Lim', 'Y Lee'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2207.07560', 'abstract': 'Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in'}, {'title': 'StructDiffusion: Object-centric diffusion for semantic rearrangement of novel objects', 'author': ['W Liu', 'T Hermans', 'S Chernova', 'C Paxton'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this'}, {'title': 'Aligning Robot and Human Representations', 'author': ['A Bobu', 'A Peng', 'P Agrawal', 'J Shah'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'To act in the world, robots rely on a representation of salient task aspects: for example, to carry a cup of coffee, a robot must consider movement efficiency and cup orientation in its'}, {'title': 'Detecting Incorrect Visual Demonstrations for Improved Policy Learning', 'author': ['M Hussein', 'M Begum'], 'pub_year': '2023', 'venue': 'Conference on Robot Learning', 'abstract': 'Learning tasks only from raw video demonstrations is the current state of the art in robotics visual imitation learning research. The implicit assumption here is that all video'}, {'title': 'SAFARI: Safe and active robot imitation learning with imagination', 'author': ['N Di Palo', 'E Johns'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2011.09586', 'abstract': 'One of the main issues in Imitation Learning is the erroneous behavior of an agent when facing out-of-distribution situations, not covered by the set of demonstrations given by the'}, {'title': 'Intrinsically motivated exploration of learned goal spaces', 'author': ['A Laversanne-Finot', 'A Péré', 'PY Oudeyer'], 'pub_year': '2021', 'venue': 'Frontiers in neurorobotics', 'abstract': 'Finding algorithms that allow agents to discover a wide variety of skills efficiently and autonomously, remains a challenge of Artificial Intelligence. Intrinsically Motivated Goal'}, {'title': 'Training a resilient q-network against observational interference', 'author': ['CHH Yang', 'ITD Hung', 'Y Ouyang'], 'pub_year': '2022', 'venue': 'Proceedings of the AAAI …', 'abstract': 'Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may'}, {'title': 'C-planning: An automatic curriculum for learning goal-reaching tasks', 'author': ['T Zhang', 'B Eysenbach', 'R Salakhutdinov'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central'}, {'title': 'Variable Impedance Skill Learning for Contact-Rich Manipulation', 'author': ['Q Yang', 'A Dürr', 'EA Topp', 'JA Stork'], 'pub_year': '2022', 'venue': 'IEEE Robotics and …', 'abstract': 'Contact-rich manipulation tasks remain a hard problem in robotics that requires interaction with unstructured environments. Reinforcement Learning (RL) is one potential solution to'}, {'title': 'Meta-Reinforcement Learning via Language Instructions', 'author': ['Z Bing', 'A Koch', 'X Yao', 'FO Morin', 'K Huang'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires tremendous amount of data to learn a task, let alone being'}, {'title': 'Manipulator-independent representations for visual imitation', 'author': ['Y Zhou', 'Y Aytar', 'K Bousmalis'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2103.09016', 'abstract': 'Imitation learning is an effective tool for robotic learning tasks where specifying a reinforcement learning (RL) reward is not feasible or where the exploration problem is'}, {'title': 'Exploring with sticky mittens: Reinforcement learning with expert interventions via option templates', 'author': ['S Dutta', 'K Sridhar', 'O Bastani'], 'pub_year': '2023', 'venue': '… on Robot Learning', 'abstract': 'Long horizon robot learning tasks with sparse rewards pose a significant challenge for current reinforcement learning algorithms. A key feature enabling humans to learn'}, {'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis', 'author': ['S Omidshafiei', 'A Kapishnikov', 'Y Assogba'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Each year, expert-level performance is attained in increasingly-complex multiagent domains, notable examples including Go, Poker, and StarCraft II. This rapid progression is'}, {'title': 'Evaluating agents without rewards', 'author': ['B Matusch', 'J Ba', 'D Hafner'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2012.11538', 'abstract': 'Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming'}, {'title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models', 'author': ['T Xiao', 'H Chan', 'P Sermanet', 'A Wahid'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot'}, {'title': 'Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization', 'author': ['L Zhang', 'BC Stadie'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2209.13046', 'abstract': 'Hindsight goal relabeling has become a foundational technique for multi-goal reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory can be seen as an expert'}, {'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments', 'author': ['BA Pires', 'F Behbahani', 'H Soyer', 'K Nikiforou'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Hierarchical Reinforcement Learning (HRL) agents have the potential to demonstrate appealing capabilities such as planning and exploration with abstraction, transfer, and skill'}, {'title': 'Learning Video-Conditioned Policies for Unseen Manipulation Tasks', 'author': ['E Chane-Sane', 'C Schmid', 'I Laptev'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2305.06289', 'abstract': 'The ability to specify robot commands by a non-expert user is critical for building generalist agents capable of solving a large variety of tasks. One convenient way to specify the'}, {'title': 'Learning to share autonomy across repeated interaction', 'author': ['A Jonnavittula', 'DP Losey'], 'pub_year': '2021', 'venue': '2021 IEEE/RSJ International …', 'abstract': 'Wheelchair-mounted robotic arms (and other assistive robots) should help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy'}, {'title': 'Pretraining in Deep Reinforcement Learning: A Survey', 'author': ['Z Xie', 'Z Lin', 'J Li', 'S Li', 'D Ye'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2211.03959', 'abstract': 'The past few years have seen rapid progress in combining reinforcement learning (RL) with deep learning. Various breakthroughs ranging from games to robotics have spurred the'}, {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration', 'author': ['G Vezzani', 'D Tirumala', 'M Wulfmeier', 'D Rao'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common'}, {'title': 'Learning High Speed Precision Table Tennis on a Physical Robot', 'author': ['T Ding', 'L Graesser', 'S Abeyruwan'], 'pub_year': '2022', 'venue': '2022 IEEE/RSJ …', 'abstract': 'Learning goal conditioned control in the real world is a challenging open problem in robotics. Reinforcement learning systems have the potential to learn autonomously via trial'}, {'title': 'Squirl: Robust and efficient learning from video demonstration of long-horizon robotic manipulation tasks', 'author': ['B Wu', 'F Xu', 'Z He', 'A Gupta'], 'pub_year': '2020', 'venue': '2020 IEEE/RSJ …', 'abstract': 'Recent advances in deep reinforcement learning (RL) have demonstrated its potential to learn complex robotic manipulation tasks. However, RL still requires the robot to collect a'}, {'title': 'DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics', 'author': ['S Li', 'Z Huang', 'T Chen', 'T Du', 'H Su'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation'}, {'title': 'Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets', 'author': ['M Du', 'S Nair', 'D Sadigh', 'C Finn'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2304.08742', 'abstract': 'Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is'}, {'title': 'Robot program parameter inference via differentiable shadow program inversion', 'author': ['B Alt', 'D Katic', 'R Jäkel', 'AK Bozcuoglu'], 'pub_year': '2021', 'venue': '… on Robotics and …', 'abstract': 'Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand'}, {'title': 'Syntactic Inductive Biases for Deep Learning Methods', 'author': ['Y Shen'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2206.04806', 'abstract': 'In this thesis, we try to build a connection between the two schools by introducing syntactic inductive biases for deep learning models. We propose two families of inductive biases, one'}, {'title': 'Imitating Task and Motion Planning with Visuomotor Transformers', 'author': ['M Dalal', 'A Mandlekar', 'C Garrett', 'A Handa'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However'}, {'title': 'Leveraging motor babbling for efficient robot learning', 'author': ['K Kase', 'N Matsumoto', 'T Ogata'], 'pub_year': '2021', 'venue': 'Journal of Robotics and …', 'abstract': 'Deep robotic learning by learning from demonstration allows robots to mimic a given demonstration and generalize their performance to unknown task setups. However, this'}, {'title': 'CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning', 'author': ['Z Mandi', 'H Bharadhwaj', 'V Moens', 'S Song'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Developing robots that are capable of many skills and generalization to unseen scenarios requires progress on two fronts: efficient collection of large and diverse datasets, and'}, {'title': 'Visual perspective taking for opponent behavior modeling', 'author': ['B Chen', 'Y Hu', 'R Kwiatkowski', 'S Song'], 'pub_year': '2021', 'venue': '… on Robotics and …', 'abstract': \"In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others' plans\"}, {'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning', 'author': ['S Nasiriany', 'T Gao', 'A Mandlekar', 'Y Zhu'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2210.11435', 'abstract': 'Imitation learning offers a promising path for robots to learn general-purpose behaviors, but traditionally has exhibited limited scalability due to high data supervision requirements and'}, {'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction', 'author': ['M Tavassoli', 'S Katyara', 'M Pozzi', 'N Deshpande'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The uses of robots are changing from static environments in factories to encompass novel concepts such as Human-Robot Collaboration in unstructured settings. Pre-programming all'}, {'title': 'Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability', 'author': ['H Zhu', 'A Zhang'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2302.03770', 'abstract': 'Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills which aim to reach diverse goals. In particular, offline GCRL only requires purely pre'}, {'title': 'Evaluating Multimodal Interactive Agents', 'author': ['J Abramson', 'A Ahuja', 'F Carnevale', 'P Georgiev'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Creating agents that can interact naturally with humans is a common goal in artificial intelligence (AI) research. However, evaluating these interactions is challenging: collecting'}, {'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation', 'author': ['D Brandfonbrener', 'O Nachum', 'J Bruna'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2305.16985', 'abstract': 'In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be'}, {'title': 'Learning to Share Autonomy from Repeated Human-Robot Interaction', 'author': ['A Jonnavittula', 'SA Mehta', 'DP Losey'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2205.09795', 'abstract': 'Assistive robot arms try to help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy. Within shared autonomy, both the human and'}, {'title': 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection', 'author': ['S Dass', 'K Pertsch', 'H Zhang', 'Y Lee', 'JJ Lim'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However'}, {'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation', 'author': ['X Meng', 'Y Xiang', 'D Fox'], 'pub_year': '2021', 'venue': 'IEEE Robotics and Automation …', 'abstract': 'Learning high-level navigation behaviors has important implications: it enables robots to build compact visual memory for repeating demonstrations and to build sparse topological'}, {'title': 'H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions', 'author': ['K Ota', 'HY Tung', 'KA Smith', 'A Cherian', 'TK Marks'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The world is filled with articulated objects that are difficult to determine how to use from vision alone, eg, a door might open inwards or outwards. Humans handle these objects with'}, {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations', 'author': ['K Yan', 'A Schwing', 'YX Wang'], 'pub_year': '2022', 'venue': 'Advances in Neural …', 'abstract': 'Although reinforcement learning has found widespread use in dense reward settings, training autonomous agents with sparse rewards remains challenging. To address this'}, {'title': 'A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation', 'author': ['H Furuta', 'Y Iwasawa', 'Y Matsuo', 'SS Gu'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2211.14296', 'abstract': 'The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other'}, {'title': 'Manipulate by Seeing: Creating Manipulation Controllers', 'author': ['J Wang', 'S Dasari', 'MK Srirama', 'S Tulsiani'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The field of visual representation learning has seen explosive growth in the past years, but its benefits in robotics have been surprisingly limited so far. Prior work uses generic visual'}, {'title': 'Efficient Learning of High Level Plans from Play', 'author': ['NA Urpí', 'M Bagatella', 'O Hilliges', 'G Martius'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Real-world robotic manipulation tasks remain an elusive challenge, since they involve both fine-grained environment interaction, as well as the ability to plan for long-horizon goals'}, {'title': 'Curriculum Goal-Conditioned Imitation for Offline Reinforcement Learning', 'author': ['X Feng', 'L Jiang', 'X Yu', 'H Xu', 'X Sun'], 'pub_year': '2022', 'venue': 'IEEE Transactions …', 'abstract': 'Offline reinforcement learning (RL) enables learning policies from pre-collected datasets without online data collection. Although it offers the possibility to surpass the performance of'}, {'title': 'Robot peels banana with goal-conditioned dual-action deep imitation learning', 'author': ['H Kim', 'Y Ohmura', 'Y Kuniyoshi'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2203.09749', 'abstract': 'A long-horizon dexterous robot manipulation task of deformable objects, such as banana peeling, is problematic because of difficulties in object modeling and a lack of knowledge'}, {'title': 'Chain-of-Thought Predictive Control', 'author': ['Z Jia', 'F Liu', 'V Thumuluri', 'L Chen', 'Z Huang'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We study generalizable policy learning from demonstrations for complex low-level control tasks (eg, contact-rich object manipulations). We propose an imitation learning method that'}, {'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures', 'author': ['K Kutsuzawa', 'M Hayashibe'], 'pub_year': '2023', 'venue': 'IEEE Access', 'abstract': 'Imitation learning is a promising approach for robots to learn complex motor skills. Recent techniques allow robots to learn long-term movements comprising multiple sub-behaviors'}, {'title': 'PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining', 'author': ['G Thomas', 'CA Cheng', 'R Loynd', 'V Vineet'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which'}, {'title': 'Towards Vygotskian Autotelic Agents: Learning Skills with Goals, Language and Intrinsically Motivated Deep Reinforcement Learning', 'author': ['C Colas'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Building autonomous machines that can explore large environments, discover interesting interactions and learn open-ended repertoires of skills is a long-standing goal in artificial'}, {'title': 'Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning', 'author': ['D Brandfonbrener', 'S Tu', 'A Singh', 'S Welker'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To'}, {'title': 'Learning latent actions without human demonstrations', 'author': ['SA Mehta', 'S Parekh', 'DP Losey'], 'pub_year': '2022', 'venue': '… International Conference on …', 'abstract': \"We can make it easier for disabled users to control assistive robots by mapping the user's low-dimensional joystick inputs to high-dimensional, complex actions. Prior works learn\"}, {'title': 'Explore, discover and learn: Unsupervised discovery of state-covering skills', 'author': ['V Campos Camúñez', 'A Trott', 'C Xiong'], 'pub_year': '2020', 'venue': 'ICML 2020, Thirty …', 'abstract': 'Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of'}, {'title': 'Learning to play by imitating humans', 'author': ['R Dinyari', 'P Sermanet', 'C Lynch'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2006.06874', 'abstract': 'Acquiring multiple skills has commonly involved collecting a large number of expert demonstrations per task or engineering custom reward functions. Recently it has been'}, {'title': 'Get Back Here: Robust Imitation by Return-to-Distribution Planning', 'author': ['G Cideron', 'B Tabanpour', 'S Curi', 'S Girgin'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting'}, {'title': 'A formal methods approach to interpretability, safety and composability for reinforcement learning', 'author': ['X Li'], 'pub_year': '2020', 'venue': 'NA', 'abstract': 'Robotic systems that are capable of learning from experience have recently become more common place. These systems have demonstrated success in learning difficult control tasks'}, {'title': 'Distance Weighted Supervised Learning for Offline Interaction Data', 'author': ['J Hejna', 'J Gao', 'D Sadigh'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv:2304.13774', 'abstract': 'Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised'}, {'title': 'Datasets for data-driven reinforcement learning', 'author': ['J Fu', 'A Kumar', 'O Nachum', 'G Tucker'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without'}, {'title': 'Learning impedance actions for safe reinforcement learning in contact-rich tasks', 'author': ['Q Yang', 'A Dürr', 'EA Topp', 'JA Stork'], 'pub_year': '2021', 'venue': 'NeurIPS 2021 Workshop …', 'abstract': 'Reinforcement Learning (RL) has the potential of solving complex continuous control tasks, with direct applications to robotics. Nevertheless, current stateof-the-art methods are'}, {'title': 'Dances with robots: Choreographing, correcting, and performing with moving machines', 'author': ['C Cuan'], 'pub_year': '2021', 'venue': 'TDR', 'abstract': 'What does it feel like to dance with a robot? How do you choreograph one? Working with robots during three artistic residencies and two research projects has raised questions about'}, {'title': 'Learning Dynamic Manipulation Skills from Haptic-Play', 'author': ['T Lee', 'D Sung', 'K Choi', 'C Lee', 'C Park'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In this paper, we propose a data-driven skill learning approach to solve highly dynamic manipulation tasks entirely from offline teleoperated play data. We use a bilateral'}, {'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data', 'author': ['H Zhou', 'Z Bing', 'X Yao', 'X Su', 'C Yang', 'K Huang'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots'}, {'title': 'A Proprioceptive Haptic Device Design for Teaching Bimanual Manipulation', 'author': ['C Lee', 'T Lee', 'JK Min', 'A Wang', 'SP Lee'], 'pub_year': '2022', 'venue': '… on Robotics and …', 'abstract': 'Manipulation involves a broad spectrum of skills, eg, polishing, peeling, flipping, screwing, etc., requiring complex and delicate control over both force and position. This paper aims at'}, {'title': 'Learn proportional derivative controllable latent space from pixels', 'author': ['W Wang', 'M Kobilarov', 'GD Hager'], 'pub_year': '2022', 'venue': '2022 IEEE 18th …', 'abstract': 'Recent advances in latent space dynamics model from pixels show promising progress in vision-based model predictive control (MPC). However, executing MPC in real time can be'}, {'title': 'A Glimpse in ChatGPT Capabilities and its impact for AI research', 'author': ['F Joublin', 'A Ceravola', 'J Deigmoeller'], 'pub_year': '2023', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon'}, {'title': 'Distilling a hierarchical policy for planning and control via representation and reinforcement learning', 'author': ['JS Ha', 'YJ Park', 'HJ Chae', 'SS Park'], 'pub_year': '2021', 'venue': '2021 IEEE International …', 'abstract': 'We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for'}, {'title': 'Machine Learning for Robotic Manipulation', 'author': ['Q Vuong'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2101.00755', 'abstract': 'The past decade has witnessed the tremendous successes of machine learning techniques in the supervised learning paradigm, where there is a clear demarcation between training'}, {'title': 'Multi-goal Reinforcement Learning via Exploring Successor Matching', 'author': ['X Feng'], 'pub_year': '2022', 'venue': '2022 IEEE Conference on Games (CoG)', 'abstract': 'Multi-goal reinforcement learning (RL) agent aims at achieving and generalizing over various goals. Due to the sparsity of goal-reaching rewards, it suffers from unreliable value'}, {'title': 'Time reversal as self-supervision', 'author': ['S Nair', 'M Babaeizadeh', 'C Finn', 'S Levine'], 'pub_year': '2018', 'venue': 'arXiv preprint arXiv …', 'abstract': 'A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning'}, {'title': 'Continuous Control with Action Quantization from Demonstrations', 'author': ['R Dadashi', 'L Hussenot', 'D Vincent', 'S Girgin'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv …', 'abstract': 'In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problems and the immediate computation of the maximum of the'}, {'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning', 'author': ['T Ablett', 'B Chan', 'J Kelly'], 'pub_year': '2021', 'venue': 'arXiv preprint arXiv:2112.08932', 'abstract': 'Effective exploration continues to be a significant challenge that prevents the deployment of reinforcement learning for many physical systems. This is particularly true for systems with'}, {'title': 'A development cycle for automated self-exploration of robot behaviors', 'author': ['TM Roehr', 'D Harnack', 'H Wöhrle'], 'pub_year': '2021', 'venue': 'AI …', 'abstract': 'In this paper we introduce Q-Rock, a development cycle for the automated self-exploration and qualification of robot behaviors. With Q-Rock, we suggest a novel, integrative approach'}, {'title': 'ESportsU digital warrior camp: Creating an esports-based culturally relevant computing living learning camp', 'author': ['JA Engerman', 'RF Otto', 'M VanAuken'], 'pub_year': '2021', 'venue': 'Handbook of research on …', 'abstract': 'The authors share two case studies that provide preliminary data for a National Science Foundation Innovative Technology Experiences for Students and Teachers award at the'}, {'title': 'Aligning Robot Representations with Humans', 'author': ['A Bobu', 'A Peng'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv:2205.07882', 'abstract': 'As robots are increasingly deployed in real-world scenarios, a key question is how to best transfer knowledge learned in one environment to another, where shifting constraints and'}, {'title': 'Skill Learning for Long-Horizon Sequential Tasks', 'author': ['J Alves', 'N Lau', 'F Silva'], 'pub_year': '2022', 'venue': 'Progress in Artificial Intelligence: 21st EPIA …', 'abstract': 'Solving long-horizon problems is a desirable property in autonomous agents. Learning reusable behaviours can equip the agent with this property, allowing it to adapt them when'}, {'title': 'ToolNet: Using Commonsense Generalization for Predicting Tool Use for Robot Plan Synthesis', 'author': ['R Bansal', 'S Tuli', 'R Paul'], 'pub_year': '2020', 'venue': 'arXiv preprint arXiv:2006.05478', 'abstract': 'A robot working in a physical environment (like home or factory) needs to learn to use various available tools for accomplishing different tasks, for instance, a mop for cleaning and'}, {'title': 'Trajectory adjustment for nonprehensile manipulation using latent space of trained sequence-to-sequence model', 'author': ['K Kutsuzawa', 'S Sakaino', 'T Tsuji'], 'pub_year': '2019', 'venue': 'Advanced Robotics', 'abstract': 'When robots are used to manipulate objects in various ways, they often have to consider the dynamic constraint. Machine learning is a good candidate for such complex trajectory'}, {'title': 'Latent Hierarchical Imitation Learning for Stochastic Environments', 'author': ['M Igl', 'P Shah', 'P Mougin', 'S Srinivasan', 'T Gupta'], 'pub_year': '2023', 'venue': 'NA', 'abstract': 'Many applications of imitation learning require the agent to avoid mode collapse and mirrorthe full distribution of observed behaviors. Existing methods improving this'}, {'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning', 'author': ['P Sermanet', 'C Lynch'], 'pub_year': '2022', 'venue': '5th Annual Conference on Robot …', 'abstract': 'Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion'}, {'title': 'Improving Meta-imitation Learning with Focused Task Embedding', 'author': ['YF Lin', 'CK Ho', 'CT King'], 'pub_year': '2022', 'venue': '… Systems and Applications: Proceedings of the …', 'abstract': 'Meta-imitation learning has been applied to enable robots to quickly generalize the learned tasks to perform new tasks. The basic idea is to encode tasks into meaningful embeddings'}, {'title': 'Visual Prediction of Priors for Articulated Object Interaction', 'author': ['C Moses', 'M Noseworthy', 'LP Kaelbling'], 'pub_year': '2020', 'venue': '… on Robotics and …', 'abstract': 'Exploration in novel settings can be challenging without prior experience in similar domains. However, humans are able to build on prior experience quickly and efficiently. Children'}, {'title': 'Compositional Reasoning in Robot Learning', 'author': ['D Xu'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'To carry out diverse tasks in everyday human environments, future robots must generalize beyond the knowledge they are equipped with. However, despite recent advances in\" end-to'}, {'title': 'Structured Motion Generation with Predictive Learning: Proposing Subgoal for Long-Horizon Manipulation', 'author': ['N Saito', 'J Moura', 'T Ogata'], 'pub_year': '2023', 'venue': '… on Robotics and …', 'abstract': 'For assisting humans in their daily lives, robots need to perform long-horizon tasks, such as tidying up a room or preparing a meal. One effective strategy for handling a long-horizon'}, {'title': 'Design and training of deep reinforcement learning agents', 'author': ['F Pardo'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'Deep reinforcement learning is a field of research at the intersection of reinforcement learning and deep learning. On one side, the problem that researchers address is the one of'}, {'title': 'Learning, Planning, and Acting with Models', 'author': ['T Kurutach'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'While the classical approach to planning and control has enabled robots to achieve various challenging control tasks, it requires domain experts to specify transition dynamics as well'}, {'title': 'Self-taught Robots: Autonomous and Weakly-Supervised Learning for Robotic Manipulation', 'author': ['M Alakuijala'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'Despite significant advances in machine learning in recent years, robotic control learned from data has yet to show large-scale impact in the real world. One of the main limitations is'}, {'title': 'Leveraging Humans to Detect and Fix Representation Misalignment', 'author': ['A Peng'], 'pub_year': '2023', 'venue': 'NA', 'abstract': 'As robots are increasingly deployed in real-world environments, a key question be-comes how to best teach them to accomplish tasks that end users want. A critical problem suffered'}, {'title': 'Robust Flight Navigation with Liquid Neural Networks', 'author': ['P Kao'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations, and generalize well to online and unseen scenarios within the same'}, {'title': 'Goal-Directed Exploration and Skill Reuse', 'author': ['V Pong'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'General-purpose robots that exhibit a broad range of capabilities in unstructured environments would greatly advance the utility of robotics. However, robot applications have'}, {'title': 'Task-Oriented Manipulation Planning: Teaching Robot Manipulators to Learn Trajectory Tasks', 'author': ['Y Li'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'As robot manipulator applications are conducted in more complex tasks and unstructured environments, traditional manual programming cannot match the growing requirements'}, {'title': 'How to Train Your Robot: Techniques for Enabling Robotic Learning in the Real World', 'author': ['A Gupta'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'The ultimate goal for many roboticists is to build robotic systems that are able to master complex skills involving object interaction, contact rich manipulation and unmapped'}, {'title': 'Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks', 'author': ['T Ablett', 'B Chan', 'J Kelly'], 'pub_year': '2023', 'venue': 'IEEE Robotics and Automation …', 'abstract': 'Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires'}, {'title': 'Real World Robot Learning: Learned Rewards, Offline Datasets and Skill Re-Use', 'author': ['A Singh'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'The last decade witnessed an unprecedented explosion in AI-powered applications: our phones unlock via face recognition, can recognize our speech with near-perfection, and are'}, {'title': 'Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces', 'author': ['J Gao'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'Figure 1.1: We formulate assistive typing as a human-in-the-loop decision-making problem, in which the interface observes user inputs (eg, neural activity measured by a brain implant)'}, {'title': 'Learning to Align Multimodal Data for Static and Dynamic Tasks', 'author': ['S Paul'], 'pub_year': '2022', 'venue': 'NA', 'abstract': 'Our experience of the world is multimodal-we see objects, hear sounds, and read texts to perceive information. In order for Artificial Intelligence to make progress in understanding the'}, {'title': 'Robots that can see: Learning visually guided behavior', 'author': ['A Pashevich'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Recently, vision and learning made significant progress that could improve robot control policies for complex environments. In this thesis, we introduce novel methods for learning'}, {'title': 'Using Google Classroom to Increase Students Motivation to Learn English at SMP Negeri 1 Limboto Gorontalo', 'author': ['M Muziatun', 'H Fatsah'], 'pub_year': '2022', 'venue': '… , Sosial, dan Budaya', 'abstract': 'Teachers experiences might restrict their utilization of technology in the classroom, making learning challenging. Teachers must improve learning performance and motivation to'}, {'title': 'The Nature of Information, Semantics, and Effectiveness for Artificial Intelligence and Cognition', 'author': ['K Brown'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'This manuscript puts forward claims to help address foundational gaps in understanding Cognition and Artificial General Intelligence (AGI), including the nature of Emergence'}, {'title': 'Graph-based distributied motion planning intightmulti-lane platoons', 'author': ['GE Pizarro Lorca'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Platooning of connected and automated vehicles (CAVs) has received extensive interest due to its potential to improve road traffic. In this context, multi-lane platoons have appeared'}, {'title': 'Discovery and learning of navigation goals from pixels in Minecraft', 'author': ['JJ Nieto Salas'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Pre-training Reinforcement Learning (RL) agents in a task-agnostic manner has shown promising results. However, previous works still struggle to learn and discover meaningful'}, {'title': 'Learning multi-stage tasks with one demonstration via self-replay', 'author': ['E Johns', 'N Di Palo'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': 'In this work, we introduce a novel method to learn everyday-like multistage tasks from a single human demonstration, without requiring any prior object knowledge. Inspired by the'}, {'title': 'VARIATIONAL REPARAMETRIZED POLICY LEARNING WITH DIFFERENTIABLE PHYSICS', 'author': ['Z Huang', 'L Liang', 'Z Ling', 'X Li', 'C Gan', 'H Su'], 'pub_year': 'NA', 'venue': '… Learning Workshop NeurIPS …', 'abstract': 'We study the problem of policy parameterization for reinforcement learning (RL) with high-dimensional continuous action space. Our goal is to find a good way to parameterize the'}, {'title': 'Skill Acquisition by Instruction Augmentation on Offline Datasets', 'author': ['T Xiao', 'H Chan', 'P Sermanet', 'A Wahid', 'A Brohan'], 'pub_year': 'NA', 'venue': '… Foundation Models for …', 'abstract': 'In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot'}, {'title': 'Acquiring Motor Skills Through Motion Imitation and Reinforcement Learning', 'author': ['XB Peng'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Humans are capable of performing awe-inspiring feats of agility by drawing from a vast repertoire of diverse and sophisticated motor skills. This dynamism is in sharp contrast to the'}, {'title': 'Deep learning that scales: leveraging compute and data', 'author': ['V Campos Camúñez'], 'pub_year': '2020', 'venue': 'NA', 'abstract': 'Deep learning has revolutionized the field of artificial intelligence in the past decade. Although the development of these techniques spans over several years, the recent advent'}, {'title': 'INTEGRATING USABILITY WITH TASK PERFORMANCE FOR SHARED AUTONOMY', 'author': ['B Paulhamus'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'As robots become more complex, the degrees-of-freedom (DoF) for controlling them is rapidly outpacing the degrees-of-control that can be supplied by humans via conventional'}, {'title': 'SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling', 'author': ['J Zhang', 'K Pertsch', 'J Zhang', 'T Nam', 'SJ Hwang'], 'pub_year': 'NA', 'venue': '… Workshop NeurIPS 2022', 'abstract': \"We propose SPRINT, an approach for scalable offline policy pre-training based on natural language instructions. SPRINT pre-trains an agent's policy to execute a diverse set of\"}, {'title': 'Graph-Based Distributied Motion Planning in Tight Multi-Lane Platoons', 'author': ['GEP Lorca'], 'pub_year': '2021', 'venue': 'NA', 'abstract': 'Platooning of connected and automated vehicles (CAVs) has received extensive interest due to its potential to improve road traffic. In this context, multi-lane platoons have appeared'}, {'title': 'An adaptive imitation learning framework for robotic complex contact-rich insertion tasks', 'author': ['Y Wang', 'CC Beltran-Hernandez', 'W Wan'], 'pub_year': '2022', 'venue': 'Frontiers in Robotics …', 'abstract': 'Complex contact-rich insertion is a ubiquitous robotic manipulation skill and usually involves nonlinear and low-clearance insertion trajectories as well as varying force requirements. A'}, {'title': 'Learning to generalise through features', 'author': ['D Grebenyuk'], 'pub_year': '2020', 'venue': 'NA', 'abstract': ''}, {'title': 'Inducing Reusable Skills From Demonstrations with Option-Controller Network', 'author': ['S Zhou', 'Y Shen', 'Y Lu', 'A Courville', 'JB Tenenbaum'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': 'Humans can decompose previous experiences into skills and reuse them to enable fast learning in the future. Inspired by this process, we propose a new model called Option'}, {'title': 'Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based RL', 'author': ['D Brandfonbrener', 'S Tu', 'A Singh', 'S Welker'], 'pub_year': 'NA', 'venue': '3rd Offline RL Workshop …', 'abstract': 'We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To'}, {'title': 'Learning from Play for Deformable Object Manipulation', 'author': ['A Bahety'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': 'Deformable object manipulation is a challenging task in robotics due to the complex dynamics of deformable objects as compared to rigid objects and the unlimited degrees of'}, {'title': 'Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks', 'author': ['O Mees', 'W Burgard'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': 'I. INTRODUCTION “... spend the summer linking a camera to a computer and getting the computer to describe what it saw.” Marvin Minsky on the goal of a 1966 undergraduate'}, {'title': 'Self-Generating Data for Goal-Conditioned Compositional Problems', 'author': ['Y Yuan', 'Y Li', 'Y Wu'], 'pub_year': '2023', 'venue': 'Workshop on Reincarnating Reinforcement …', 'abstract': 'Building reinforcement learning agents that are generalizable to compositional problems has long been a research challenge. Recent success relies on a pre-existing dataset of rich'}, {'title': 'One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control', 'author': ['H Watahiki', 'Y Tsuruoka'], 'pub_year': 'NA', 'venue': 'ICLR 2022 Workshop on Generalizable Policy …', 'abstract': 'Recent advances in skill learning from a task-agnostic offline dataset enable the agent to acquire various skills that can be used as primitives to perform long-horizon imitation'}, {'title': 'データ中心の視点から捉える深層強化学習', 'author': ['古田拓毅'], 'pub_year': '2022', 'venue': '人工知能', 'abstract': '図 1 AI や機械学習によるシステムは, 大きく分けてアルゴリズム (モデル) とデータセットという二つの要素からなる. データ中心の AI (Data-Centric AI) とは, アルゴリズムやモデルを最適化する'}, {'title': 'Task Focused Robotic Imitation Learning', 'author': ['P Abolghasemi'], 'pub_year': '2019', 'venue': 'NA', 'abstract': 'For many years, successful applications of robotics were the domain of controlled environments, such as industrial assembly lines. Such environments are custom designed'}, {'title': 'Representation learning of scene images for task and motion planning', 'author': ['ST Nguyen'], 'pub_year': '2020', 'venue': 'NA', 'abstract': 'This thesis investigates two different methods to learn a state representation from only image observations for task and motion planning (TAMP) problems. Our first method integrates a'}, {'title': 'Hierarchical adversarial imitation learning from motion capture data', 'author': ['S Hagenmayer'], 'pub_year': '2020', 'venue': 'NA', 'abstract': 'A lot of problems in Imitation Learning can be split into multiple low-level tasks that need to be executed in sequence or parallel. Likewise in the area of human motion prediction, the'}, {'title': 'Not playing by the rules: Exploratory play, rational action, and efficient search', 'author': ['J Chu', 'LE Schulz'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': \"Recent studies suggest children's exploratory play is consistent with formal accounts of rational learning. Here we focus on the tension between this view and a nearly ubiquitous\"}, {'title': 'Exploration of Teacher-Centered and Task-Centered paradigms for efficient transfer of skills between morphologically distinct robots', 'author': ['M Mounsif'], 'pub_year': '2020', 'venue': 'NA', 'abstract': 'Recently, it has been possible to observe the acceleration of robot deployment in domains beyondthe usual industrial and manufacturing framework. However, for the majority of'}, {'title': 'A reduced-order approach to assist with reinforcement learning for underactuated robotics', 'author': ['J Augot', 'AJ Snoswell', 'SPN Singh'], 'pub_year': '2020', 'venue': 'Proceedings of the …', 'abstract': 'Underactuated robot designs are enticing due to their electromechanical simplicity; but, their operation is complex especially for compliant designs that may not have an explicit model'}, {'title': 'Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning', 'author': ['JS Ha', 'YJ Park', 'HJ Chae', 'SS Park', 'HL Choi'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': 'We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for'}, {'title': '深層学習を用いた協働ロボットによる作業目標の推論と動的修正の実現', 'author': ['平松駿， 村田真悟'], 'pub_year': '2022', 'venue': '人工知能学会全国大会論文集 第 36 回 (2022)', 'abstract': '抄録 ロボットが人と協働作業をするには, 作業目標が共有されている必要がある. 目標の共有方法には, 画像等であらかじめ伝える方法, ロボットが自ら推論する方法がある'}, {'title': 'Des robots qui voient: apprentissage de comportements guidés par la vision', 'author': ['A Pashevich'], 'pub_year': '2021', 'venue': 'NA', 'abstract': \"Résumé Récemment, la vision par ordinateur et l'apprentissage automatique ont fait des progrès significatifs qui pourraient améliorer le contrôle des robots dans les environnements\"}, {'title': 'Evaluating Generalization of Policy Learning Under Domain Shifts', 'author': ['E Xing', 'A Gupta', 'S Powers', 'V Dean'], 'venue': 'NA', 'pub_year': 'NA', 'abstract': ''}]}, 43: {'title': 'LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning multi-level hierarchies with hindsight', 'author': ['A Levy', 'G Konidaris', 'R Platt', 'K Saenko'], 'pub_year': '2017', 'venue': 'arXiv preprint arXiv:1712.00948', 'abstract': 'Hierarchical agents have the potential to solve sequential decision making tasks with greater sample efficiency than their non-hierarchical counterparts because hierarchical agents can break down tasks into sets of subtasks that only require short sequences of decisions. In order to realize this potential of faster learning, hierarchical agents need to be able to learn their multiple levels of policies in parallel so these simpler subproblems can be solved simultaneously. Yet, learning multiple levels of policies in parallel is hard because it'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1712.00948', 'author_id': ['t2eNzb8AAAAJ', '9UERvVEAAAAJ', 'Z4Y5S2oAAAAJ', '9xDADY4AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:XuoYvSr0ZqAJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLEARNING%2BMULTI-LEVEL%2BHIERARCHIES%2BWITH%2BHINDSIGHT%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=XuoYvSr0ZqAJ&ei=six9ZIb5HpCP6rQP6Iq6sAY&json=', 'num_citations': 198, 'citedby_url': '/scholar?cites=11558193958091287134&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:XuoYvSr0ZqAJ:scholar.google.com/&scioq=LEARNING+MULTI-LEVEL+HIERARCHIES+WITH+HINDSIGHT&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1712.00948'}}, 44: {'title': 'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Mcp: Learning composable hierarchical control with multiplicative compositional policies', 'author': ['XB Peng', 'M Chang', 'G Zhang'], 'pub_year': '2019', 'venue': 'Advances in Neural …', 'abstract': 'Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills quickly becomes prohibitive'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.neurips.cc/paper/2019/hash/95192c98732387165bf8e396c0f2dad2-Abstract.html', 'author_id': ['FwxfQosAAAAJ', 'vgfGtykAAAAJ', 'n3frhFEAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:_jx1Zvx4Ya0J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMCP:%2BLearning%2BComposable%2BHierarchical%2BControl%2Bwith%2BMultiplicative%2BCompositional%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_jx1Zvx4Ya0J&ei=tCx9ZLe5JsKM6rQPyqqYyAU&json=', 'num_citations': 152, 'citedby_url': '/scholar?cites=12493399866748517630&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:_jx1Zvx4Ya0J:scholar.google.com/&scioq=MCP:+Learning+Composable+Hierarchical+Control+with+Multiplicative+Compositional+Policies&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.neurips.cc/paper/2019/file/95192c98732387165bf8e396c0f2dad2-Paper.pdf'}}, 45: {'title': 'Neural probabilistic motor primitives for humanoid control', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Neural probabilistic motor primitives for humanoid control', 'author': ['J Merel', 'L Hasenclever', 'A Galashov', 'A Ahuja'], 'pub_year': '2018', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We focus on the problem of learning a single motor module that can flexibly express a range of behaviors for the control of high-dimensional physically simulated humanoids. To do this, we propose a motor architecture that has the general structure of an inverse model with a latent-variable bottleneck. We show that it is possible to train this model entirely offline to compress thousands of expert policies and learn a motor primitive embedding space. The trained neural probabilistic motor primitive system can perform one-shot imitation of whole'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1811.11711', 'author_id': ['K4OcFXUAAAAJ', 'dD-3S4QAAAAJ', 'kIpoNtcAAAAJ', 'HFV9GmMAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:avdpR2OPC5sJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2Bprobabilistic%2Bmotor%2Bprimitives%2Bfor%2Bhumanoid%2Bcontrol%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=avdpR2OPC5sJ&ei=tix9ZKzsH4HuygS37r6oCA&json=', 'num_citations': 112, 'citedby_url': '/scholar?cites=11172180957185308522&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:avdpR2OPC5sJ:scholar.google.com/&scioq=Neural+probabilistic+motor+primitives+for+humanoid+control&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1811.11711'}}, 46: {'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning', 'author': ['A Gupta', 'V Kumar', 'C Lynch', 'S Levine'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1910.11956', 'author_id': ['1wLVDP4AAAAJ', 'nu3W--sAAAAJ', 'CYWO-oAAAAAJ', '8R35rCwAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:kBR-7NAzJj4J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelay%2BPolicy%2BLearning:%2BSolving%2BLong-Horizon%2BTasks%2Bvia%2BImitation%2Band%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kBR-7NAzJj4J&ei=uCx9ZMiIH72P6rQP85mLmAc&json=', 'num_citations': 202, 'citedby_url': '/scholar?cites=4478323851880436880&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:kBR-7NAzJj4J:scholar.google.com/&scioq=Relay+Policy+Learning:+Solving+Long-Horizon+Tasks+via+Imitation+and+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1910.11956'}}, 47: {'title': 'Sub-policy Adaptation for Hierarchical Reinforcement Learning', 'year': '2019', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Sub-policy adaptation for hierarchical reinforcement learning', 'author': ['AC Li', 'C Florensa', 'I Clavera', 'P Abbeel'], 'pub_year': '2019', 'venue': 'arXiv preprint arXiv:1906.05862', 'abstract': 'Hierarchical reinforcement learning is a promising approach to tackle long-horizon decision-making problems with sparse rewards. Unfortunately, most methods still decouple the lower-level skill acquisition process and the training of a higher level that controls the skills in a new task. Leaving the skills fixed can lead to significant sub-optimality in the transfer setting. In this work, we propose a novel algorithm to discover a set of skills, and continuously adapt them along with the higher level even when training on a new task. Our main contributions'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1906.05862', 'author_id': ['bOitqMUAAAAJ', '7_7op_IAAAAJ', 'yABlzrsAAAAJ', 'vtwH6GkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:VJblSozHuYcJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSub-policy%2BAdaptation%2Bfor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=VJblSozHuYcJ&ei=uSx9ZOnpNIb4yASAjZegAQ&json=', 'num_citations': 67, 'citedby_url': '/scholar?cites=9780067471177651796&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:VJblSozHuYcJ:scholar.google.com/&scioq=Sub-policy+Adaptation+for+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1906.05862'}}, 48: {'title': 'Data-Efficient Hierarchical Reinforcement Learning (HIRO)', 'year': '2018', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Data-efficient hierarchical reinforcement learning', 'author': ['O Nachum', 'SS Gu', 'H Lee'], 'pub_year': '2018', 'venue': 'Advances in neural …', 'abstract': 'Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.neurips.cc/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html', 'author_id': ['C-ZlBWMAAAAJ', 'B8wslVsAAAAJ', 'fmSHtE8AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:bysPMNcGMXIJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData-Efficient%2BHierarchical%2BReinforcement%2BLearning%2B(HIRO)%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=bysPMNcGMXIJ&ei=vCx9ZJbOBP2M6rQPrYemoAo&json=', 'num_citations': 685, 'citedby_url': '/scholar?cites=8228365515476642671&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:bysPMNcGMXIJ:scholar.google.com/&scioq=Data-Efficient+Hierarchical+Reinforcement+Learning+(HIRO)&hl=en&as_sdt=0,33', 'eprint_url': 'https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf'}}, 49: {'title': 'CompILE: Compositional Imitation Learning and Execution', 'year': '2018', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Compile: Compositional imitation learning and execution', 'author': ['T Kipf', 'Y Li', 'H Dai', 'V Zambaldi'], 'pub_year': '2019', 'venue': 'International …', 'abstract': 'We introduce Compositional Imitation Learning and Execution (CompILE): a framework for learning reusable, variable-length segments of hierarchically-structured behavior from demonstration data. CompILE uses a novel unsupervised, fully-differentiable sequence segmentation module to learn latent encodings of sequential data that can be re-composed and executed to perform new tasks. Once trained, our model generalizes to sequences of longer length and from environment instances not seen during training. We'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://proceedings.mlr.press/v97/kipf19a.html', 'author_id': ['83HL5FwAAAAJ', 'UY7CtEwAAAAJ', 'obpl7GQAAAAJ', 'Iyc-xXkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:2G3T9VsuvKoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompILE:%2BCompositional%2BImitation%2BLearning%2Band%2BExecution%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=2G3T9VsuvKoJ&ei=vSx9ZJ_FF4vwyAS0sqboDg&json=', 'num_citations': 88, 'citedby_url': '/scholar?cites=12302759254570528216&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:2G3T9VsuvKoJ:scholar.google.com/&scioq=CompILE:+Compositional+Imitation+Learning+and+Execution&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v97/kipf19a/kipf19a.pdf'}}, 50: {'title': 'Latent Space Policies for Hierarchical Reinforcement Learning', 'year': '2018', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Latent space policies for hierarchical reinforcement learning', 'author': ['T Haarnoja', 'K Hartikainen', 'P Abbeel'], 'pub_year': '2018', 'venue': '… on Machine Learning', 'abstract': 'We address the problem of learning hierarchical deep neural network policies for reinforcement learning. In contrast to methods that explicitly restrict or cripple lower layers of a hierarchy to force them to use higher-level modulating signals, each layer in our framework is trained to directly solve the task, but acquires a range of diverse strategies via a maximum entropy reinforcement learning objective. Each layer is also augmented with latent random variables, which are sampled from a prior distribution during the training of'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v80/haarnoja18a.html', 'author_id': ['VT7peyEAAAAJ', 'eVYhlDQAAAAJ', 'vtwH6GkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:Li0iEetwj-QJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BSpace%2BPolicies%2Bfor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=Li0iEetwj-QJ&ei=vyx9ZMnOG8CQ6rQP9LmDmAo&json=', 'num_citations': 176, 'citedby_url': '/scholar?cites=16469506517224271150&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:Li0iEetwj-QJ:scholar.google.com/&scioq=Latent+Space+Policies+for+Hierarchical+Reinforcement+Learning&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v80/haarnoja18a/haarnoja18a.pdf'}}, 51: {'title': 'Learning an embedding space for transferable robot skills', 'year': '2018', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning an embedding space for transferable robot skills', 'author': ['K Hausman', 'JT Springenberg', 'Z Wang'], 'pub_year': '2018', 'venue': 'International …', 'abstract': 'We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://openreview.net/forum?id=rk07ZXZRb', 'author_id': ['yy0UFOwAAAAJ', 'MGXJkIAAAAAJ', 'Rne0FzEAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:g_jZX3_UNBkJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ban%2Bembedding%2Bspace%2Bfor%2Btransferable%2Brobot%2Bskills%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=g_jZX3_UNBkJ&ei=wSx9ZM3ZEKKbywSeir5w&json=', 'num_citations': 278, 'citedby_url': '/scholar?cites=1816310193271208067&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:g_jZX3_UNBkJ:scholar.google.com/&scioq=Learning+an+embedding+space+for+transferable+robot+skills&hl=en&as_sdt=0,33', 'eprint_url': 'https://openreview.net/pdf?id=rk07ZXZRb'}}, 52: {'title': 'A Deep Hierarchical Approach to Lifelong Learning in Minecraft', 'year': '2017', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'A deep hierarchical approach to lifelong learning in minecraft', 'author': ['C Tessler', 'S Givony', 'T Zahavy', 'D Mankowitz'], 'pub_year': '2017', 'venue': 'Proceedings of the …', 'abstract': 'We propose a lifelong learning system that has the ability to reuse and transfer knowledge from one task to another while efficiently retaining the previously learned knowledge-base. Knowledge is transferred by learning reusable skills to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem. These reusable skills, which we refer to as Deep Skill Networks, are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ojs.aaai.org/index.php/AAAI/article/view/10744', 'author_id': ['7eLKa3IAAAAJ', 'nlVsO4YAAAAJ', '9dXN6cMAAAAJ', 'v84tWxsAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:a_Er9i3hDtUJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDeep%2BHierarchical%2BApproach%2Bto%2BLifelong%2BLearning%2Bin%2BMinecraft%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=a_Er9i3hDtUJ&ei=wix9ZIeHMpCP6rQP6Iq6sAY&json=', 'num_citations': 378, 'citedby_url': '/scholar?cites=15352455767272452459&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:a_Er9i3hDtUJ:scholar.google.com/&scioq=A+Deep+Hierarchical+Approach+to+Lifelong+Learning+in+Minecraft&hl=en&as_sdt=0,33', 'eprint_url': 'https://ojs.aaai.org/index.php/AAAI/article/view/10744/10603'}}, 53: {'title': 'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations', 'year': '2017', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Ddco: Discovery of deep continuous options for robot learning from demonstrations', 'author': ['S Krishnan', 'R Fox', 'I Stoica'], 'pub_year': '2017', 'venue': 'Conference on robot …', 'abstract': 'An option is a short-term skill consisting of a control policy for a specified region of the state space, and a termination condition recognizing leaving that region. In prior work, we proposed an algorithm called Deep Discovery of Options (DDO) to discover options to accelerate reinforcement learning in Atari games. This paper studies an extension to robot imitation learning, called Discovery of Deep Continuous Options (DDCO), where low-level continuous control skills parametrized by deep neural networks are learned from'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://proceedings.mlr.press/v78/krishnan17a.html', 'author_id': ['Yxh9WWoAAAAJ', 'FH9nKOAAAAAJ', 'vN-is70AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:p-I34QYiRdYJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDDCO:%2BDiscovery%2Bof%2BDeep%2BContinuous%2BOptions%2Bfor%2BRobot%2BLearning%2Bfrom%2BDemonstrations%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=p-I34QYiRdYJ&ei=xCx9ZOLmNsKM6rQPyqqYyAU&json=', 'num_citations': 76, 'citedby_url': '/scholar?cites=15439784310453297831&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:p-I34QYiRdYJ:scholar.google.com/&scioq=DDCO:+Discovery+of+Deep+Continuous+Options+for+Robot+Learning+from+Demonstrations&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v78/krishnan17a/krishnan17a.pdf'}}, 54: {'title': 'Feudal networks for hierarchical reinforcement learning', 'year': '2017', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Feudal networks for hierarchical reinforcement learning', 'author': ['AS Vezhnevets', 'S Osindero', 'T Schaul'], 'pub_year': '2017', 'venue': '… Machine Learning', 'abstract': 'introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learn  This paper introduced FeUdal Networks, a novel architecture that formulates sub-goals as'}, 'filled': False, 'gsrank': 1, 'pub_url': 'http://proceedings.mlr.press/v70/vezhnevets17a.html', 'author_id': ['vo1zs4sAAAAJ', 'Jq8ZS5kAAAAJ', 'vDimc-4AAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:LtoRi8c0yRwJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeudal%2Bnetworks%2Bfor%2Bhierarchical%2Breinforcement%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=LtoRi8c0yRwJ&ei=xix9ZKepF4HuygS37r6oCA&json=', 'num_citations': 832, 'citedby_url': '/scholar?cites=2074247135017163310&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:LtoRi8c0yRwJ:scholar.google.com/&scioq=Feudal+networks+for+hierarchical+reinforcement+learning&hl=en&as_sdt=0,33', 'eprint_url': 'http://proceedings.mlr.press/v70/vezhnevets17a/vezhnevets17a.pdf'}}, 55: {'title': 'Stochastic neural networks for hierarchical reinforcement learning', 'year': '2017', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Stochastic neural networks for hierarchical reinforcement learning', 'author': ['C Florensa', 'Y Duan', 'P Abbeel'], 'pub_year': '2017', 'venue': 'arXiv preprint arXiv:1704.03012', 'abstract': 'Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks. Our approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1704.03012', 'author_id': ['7_7op_IAAAAJ', 'EMDboA4AAAAJ', 'vtwH6GkAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:cLCsdpbOXNkJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2Bneural%2Bnetworks%2Bfor%2Bhierarchical%2Breinforcement%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=cLCsdpbOXNkJ&ei=yCx9ZK_vIb2P6rQP85mLmAc&json=', 'num_citations': 365, 'citedby_url': '/scholar?cites=15662620749719187568&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:cLCsdpbOXNkJ:scholar.google.com/&scioq=Stochastic+neural+networks+for+hierarchical+reinforcement+learning&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1704.03012'}}, 56: {'title': 'The option-critic architecture', 'year': '2017', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'The option-critic architecture', 'author': ['PL Bacon', 'J Harb', 'D Precup'], 'pub_year': '2017', 'venue': 'Proceedings of the AAAI conference on …', 'abstract': 'propose a new option-critic architecture capable of learning both the internal policies and   the resulting system as an option-critic architecture, in reference to the actor-critic architectures'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://ojs.aaai.org/index.php/AAAI/article/view/10916', 'author_id': ['9H77FYYAAAAJ', '5Qp_0TUAAAAJ', 'j54VcVEAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:_7_eu82KfgEJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Boption-critic%2Barchitecture%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=_7_eu82KfgEJ&ei=yix9ZOa6J4b4yASAjZegAQ&json=', 'num_citations': 981, 'citedby_url': '/scholar?cites=107676057328336895&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:_7_eu82KfgEJ:scholar.google.com/&scioq=The+option-critic+architecture&hl=en&as_sdt=0,33', 'eprint_url': 'https://ojs.aaai.org/index.php/AAAI/article/download/10916/10775'}}, 57: {'title': 'Learning and Transfer of Modulated Locomotor Controllers', 'year': '2016', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Learning and transfer of modulated locomotor controllers', 'author': ['N Heess', 'G Wayne', 'Y Tassa', 'T Lillicrap'], 'pub_year': '2016', 'venue': 'arXiv preprint arXiv …', 'abstract': 'We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level\" spinal\" network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level\" cortical\" network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/1610.05182', 'author_id': ['79k7bGEAAAAJ', '', 'CjOTm_4AAAAJ', 'htPVdRMAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:MWycmPiHyb8J:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Band%2BTransfer%2Bof%2BModulated%2BLocomotor%2BControllers%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=MWycmPiHyb8J&ei=zCx9ZOv3CP2M6rQPrYemoAo&json=', 'num_citations': 200, 'citedby_url': '/scholar?cites=13819726433345104945&as_sdt=5,33&sciodt=0,33&hl=en', 'url_related_articles': '/scholar?q=related:MWycmPiHyb8J:scholar.google.com/&scioq=Learning+and+Transfer+of+Modulated+Locomotor+Controllers&hl=en&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/1610.05182'}}}\n"
     ]
    }
   ],
   "source": [
    "print(rw.paper_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e5cbecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info already exists\n"
     ]
    }
   ],
   "source": [
    "rw.update_paper_info_from_scholar(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e83271bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info already exists\n",
      "no update\n",
      "paper 1 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 2 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 3 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 4 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 5 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 6 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 7 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 8 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 9 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 10 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 11 updated\n",
      "\n",
      "info already exists\n",
      "no update\n",
      "paper 12 updated\n",
      "\n",
      "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics  information updated\n",
      "update pkl\n",
      "paper 13 updated\n",
      "\n",
      "Robot Learning of Mobile Manipulation With Reachability Behavior Priors  information updated\n",
      "update pkl\n",
      "paper 14 updated\n",
      "\n",
      "Skill-based Meta-Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 15 updated\n",
      "\n",
      "Skill-based Model-based Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 16 updated\n",
      "\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration  information updated\n",
      "update pkl\n",
      "paper 17 updated\n",
      "\n",
      "Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback  information updated\n",
      "update pkl\n",
      "paper 18 updated\n",
      "\n",
      "Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives  information updated\n",
      "update pkl\n",
      "paper 19 updated\n",
      "\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics  information updated\n",
      "update pkl\n",
      "paper 20 updated\n",
      "\n",
      "Demonstration-Guided Reinforcement Learning with Learned Skills  information updated\n",
      "update pkl\n",
      "paper 21 updated\n",
      "\n",
      "Hierarchical Few-Shot Imitation with Skill Transition Models  information updated\n",
      "update pkl\n",
      "paper 22 updated\n",
      "\n",
      "Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans  information updated\n",
      "update pkl\n",
      "paper 23 updated\n",
      "\n",
      "Hierarchical Skills for Efficient Exploration  information updated\n",
      "update pkl\n",
      "paper 24 updated\n",
      "\n",
      "Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space  information updated\n",
      "update pkl\n",
      "paper 25 updated\n",
      "\n",
      "Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies  information updated\n",
      "update pkl\n",
      "paper 26 updated\n",
      "\n",
      "Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning  information updated\n",
      "update pkl\n",
      "paper 27 updated\n",
      "\n",
      "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data  information updated\n",
      "update pkl\n",
      "paper 28 updated\n",
      "\n",
      "Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)  information updated\n",
      "update pkl\n",
      "paper 29 updated\n",
      "\n",
      "Behavior Priors for Efficient Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 30 updated\n",
      "\n",
      "Catch and carry: Reusable neural controllers for vision-guided whole-body tasks  information updated\n",
      "update pkl\n",
      "paper 31 updated\n",
      "\n",
      "CoMic: Complementary Task Learning & Mimicry for Reusable Skills  information updated\n",
      "update pkl\n",
      "paper 32 updated\n",
      "\n",
      "Discovering Motor Programs by Recomposing Demonstrations  information updated\n",
      "update pkl\n",
      "paper 33 updated\n",
      "\n",
      "Hierarchical reinforcement learning for efficent exploration and transfer  information updated\n",
      "update pkl\n",
      "paper 34 updated\n",
      "\n",
      "Learning quadrupedal locomotion over challenging terrain  information updated\n",
      "update pkl\n",
      "paper 35 updated\n",
      "\n",
      "Learning Robot Skills with Temporal Variational Inference  information updated\n",
      "update pkl\n",
      "paper 36 updated\n",
      "\n",
      "Multi-expert learning of adaptive legged locomotion  information updated\n",
      "update pkl\n",
      "paper 37 updated\n",
      "\n",
      "Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer  information updated\n",
      "update pkl\n",
      "paper 38 updated\n",
      "\n",
      "PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING  information updated\n",
      "update pkl\n",
      "paper 39 updated\n",
      "\n",
      "Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information  information updated\n",
      "update pkl\n",
      "paper 40 updated\n",
      "\n",
      "Dynamics-Aware Unsupervised Discovery of Skills  information updated\n",
      "update pkl\n",
      "paper 41 updated\n",
      "\n",
      "Learning Latent Plans from Play  information updated\n",
      "update pkl\n",
      "paper 42 updated\n",
      "\n",
      "LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT  information updated\n",
      "update pkl\n",
      "paper 43 updated\n",
      "\n",
      "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies  information updated\n",
      "update pkl\n",
      "paper 44 updated\n",
      "\n",
      "Neural probabilistic motor primitives for humanoid control  information updated\n",
      "update pkl\n",
      "paper 45 updated\n",
      "\n",
      "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 46 updated\n",
      "\n",
      "Sub-policy Adaptation for Hierarchical Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 47 updated\n",
      "\n",
      "Data-Efficient Hierarchical Reinforcement Learning (HIRO)  information updated\n",
      "update pkl\n",
      "paper 48 updated\n",
      "\n",
      "CompILE: Compositional Imitation Learning and Execution  information updated\n",
      "update pkl\n",
      "paper 49 updated\n",
      "\n",
      "Latent Space Policies for Hierarchical Reinforcement Learning  information updated\n",
      "update pkl\n",
      "paper 50 updated\n",
      "\n",
      "Learning an embedding space for transferable robot skills  information updated\n",
      "update pkl\n",
      "paper 51 updated\n",
      "\n",
      "A Deep Hierarchical Approach to Lifelong Learning in Minecraft  information updated\n",
      "update pkl\n",
      "paper 52 updated\n",
      "\n",
      "DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations  information updated\n",
      "update pkl\n",
      "paper 53 updated\n",
      "\n",
      "Feudal networks for hierarchical reinforcement learning  information updated\n",
      "update pkl\n",
      "paper 54 updated\n",
      "\n",
      "Stochastic neural networks for hierarchical reinforcement learning  information updated\n",
      "update pkl\n",
      "paper 55 updated\n",
      "\n",
      "The option-critic architecture  information updated\n",
      "update pkl\n",
      "paper 56 updated\n",
      "\n",
      "Learning and Transfer of Modulated Locomotor Controllers  information updated\n",
      "update pkl\n",
      "paper 57 updated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rw.update_all_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5065b315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ieeexplore.ieee.org/abstract/document/9906972/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[1]['info']['pub_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c455ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'container_type': 'Publication',\n",
       " 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>,\n",
       " 'bib': {'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning',\n",
       "  'author': ['J Chen', 'D Tamboli', 'T Lan', 'V Aggarwal'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2305.12633',\n",
       "  'abstract': 'Multi-task Imitation Learning (MIL) aims to train a policy capable of performing a distribution of tasks based on multi-task expert demonstrations, which is essential for general-purpose robots. Existing MIL algorithms suffer from low data efficiency and poor performance on complex long-horizontal tasks. We develop Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (MH-AIRL) to learn hierarchically-structured multi-task policies, which is more beneficial for compositional tasks with long horizons and has higher expert'},\n",
       " 'filled': False,\n",
       " 'gsrank': 1,\n",
       " 'pub_url': 'https://arxiv.org/abs/2305.12633',\n",
       " 'author_id': ['k0KJm7kAAAAJ', 'H8YEc-gAAAAJ', '', 'Tu4lmGwAAAAJ'],\n",
       " 'url_scholarbib': '/scholar?hl=en&q=info:FFxbIHggUdcJ:scholar.google.com/&output=cite&scirp=0&hl=en',\n",
       " 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-task%2BHierarchical%2BAdversarial%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=FFxbIHggUdcJ&ei=9CZ9ZNvaI8CQ6rQP9LmDmAo&json=',\n",
       " 'num_citations': 0,\n",
       " 'eprint_url': 'https://arxiv.org/pdf/2305.12633'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[3]['info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68775933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://scholar.google.co.kr//scholar?cites=523997480481690644&as_sdt=5,33&sciodt=0,33&hl=en\n"
     ]
    }
   ],
   "source": [
    "scholar_url = 'https://scholar.google.co.kr/'\n",
    "citedby_url = scholar_url + rw.paper_list[1]['info']['citedby_url']\n",
    "print(citedby_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816bbe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning', 'year': '2023', 'info': {'container_type': 'Publication', 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>, 'bib': {'title': 'Hierarchical kickstarting for skill transfer in reinforcement learning', 'author': ['M Matthews', 'M Samvelyan', 'J Parker-Holder'], 'pub_year': '2022', 'venue': 'arXiv preprint arXiv …', 'abstract': 'Practising and honing skills forms a fundamental component of how humans learn, yet artificial agents are rarely specifically trained to perform them. Instead, they are usually trained end-to-end, with the hope being that useful skills will be implicitly learned in order to maximise discounted return of some extrinsic reward function. In this paper, we investigate how skills can be incorporated into the training of reinforcement learning (RL) agents in complex environments with large state-action spaces and sparse rewards. To this end, we'}, 'filled': False, 'gsrank': 1, 'pub_url': 'https://arxiv.org/abs/2207.11584', 'author_id': ['ubOhfnIAAAAJ', '2Qs19WAAAAAJ', 'J3VCfPYAAAAJ'], 'url_scholarbib': '/scholar?hl=en&q=info:s00WZ4tVYfoJ:scholar.google.com/&output=cite&scirp=0&hl=en', 'url_add_sclib': '/citations?hl=en&oe=ASCII&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BKickstarting%2Bfor%2BSkill%2BTransfer%2Bin%2BReinforcement%2BLearning%26hl%3Den%26oe%3DASCII%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=s00WZ4tVYfoJ&ei=7SZ9ZMLFLP2M6rQPrYemoAo&json=', 'num_citations': 2, 'citedby_url': '/scholar?cites=18041795639441247667&as_sdt=5,33&sciodt=0,33&hl=en&oe=ASCII', 'url_related_articles': '/scholar?q=related:s00WZ4tVYfoJ:scholar.google.com/&scioq=Hierarchical+Kickstarting+for+Skill+Transfer+in+Reinforcement+Learning&hl=en&oe=ASCII&as_sdt=0,33', 'eprint_url': 'https://arxiv.org/pdf/2207.11584'}}\n"
     ]
    }
   ],
   "source": [
    "rw.update_citedby(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbafc9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'A review of robot learning for manipulation: Challenges, representations, and algorithms',\n",
       "  'author': ['O Kroemer', 'S Niekum', 'G Konidaris'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'The Journal of Machine Learning …',\n",
       "  'abstract': 'A key challenge in intelligent robotics is creating robots that are capable of directly interacting with the world around them to achieve their goals. The last decade has seen'},\n",
       " {'title': \"Solving rubik's cube with a robot hand\",\n",
       "  'author': ['I Akkaya', 'M Andrychowicz', 'M Chociej', 'M Litwin'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We demonstrate that models trained only in simulation can be used to solve a manipulation problem of unprecedented complexity on a real robot. This is made possible by two key'},\n",
       " {'title': 'Play, curiosity, and cognition',\n",
       "  'author': ['J Chu', 'LE Schulz'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Annual Review of Developmental …',\n",
       "  'abstract': 'Few phenomena in childhood are as compelling—and mystifying—as play. We review five proposals about the relationship between play and development. We believe each captures'},\n",
       " {'title': 'Amp: Adversarial motion priors for stylized physics-based character control',\n",
       "  'author': ['XB Peng', 'Z Ma', 'P Abbeel', 'S Levine'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'ACM Transactions on …',\n",
       "  'abstract': 'Synthesizing graceful and life-like behaviors for physically simulated characters has been a fundamental challenge in computer animation. Data-driven methods that leverage motion'},\n",
       " {'title': 'Accelerating reinforcement learning with learned skill priors',\n",
       "  'author': ['K Pertsch', 'Y Lee', 'J Lim'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Conference on robot learning',\n",
       "  'abstract': 'Intelligent agents rely heavily on prior experience when learning a new task, yet most modern reinforcement learning (RL) approaches learn every task from scratch. One'},\n",
       " {'title': 'Episodic transformer for vision-and-language navigation',\n",
       "  'author': ['A Pashevich', 'C Schmid', 'C Sun'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Proceedings of the IEEE …',\n",
       "  'abstract': 'Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on'},\n",
       " {'title': 'Behavior Transformers: Cloning  modes with one stone',\n",
       "  'author': ['NM Shafiullah', 'Z Cui'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in neural …',\n",
       "  'abstract': 'While behavior learning has made impressive progress in recent times, it lags behind computer vision and natural language processing due to its inability to leverage large'},\n",
       " {'title': 'Search on the replay buffer: Bridging planning and reinforcement learning',\n",
       "  'author': ['B Eysenbach', 'RR Salakhutdinov'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively'},\n",
       " {'title': 'Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning',\n",
       "  'author': ['A Gupta', 'V Kumar', 'C Lynch', 'S Levine'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two'},\n",
       " {'title': 'Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters',\n",
       "  'author': ['XB Peng', 'Y Guo', 'L Halper', 'S Levine'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'ACM Transactions On …',\n",
       "  'abstract': 'The incredible feats of athleticism demonstrated by humans are made possible in part by a vast repertoire of general-purpose motor skills, acquired through years of practice and'},\n",
       " {'title': 'Robonet: Large-scale multi-robot learning',\n",
       "  'author': ['S Dasari', 'F Ebert', 'S Tian', 'S Nair', 'B Bucher'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Robot learning has emerged as a promising tool for taming the complexity and diversity of the real world. Methods based on high-capacity models, such as deep networks, hold the'},\n",
       " {'title': 'Instruction-driven history-aware policies for robotic manipulations',\n",
       "  'author': ['PL Guhur', 'S Chen', 'RG Pinel'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': '… on Robot Learning',\n",
       "  'abstract': 'In human environments, robots are expected to accomplish a variety of manipulation tasks given simple natural language instructions. Yet, robotic manipulation is extremely'},\n",
       " {'title': 'Contrastive learning as goal-conditioned reinforcement learning',\n",
       "  'author': ['B Eysenbach', 'T Zhang', 'S Levine'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'In reinforcement learning (RL), it is easier to solve a task if given a good representation. While deep RL should automatically acquire such good representations, prior work often'},\n",
       " {'title': 'Parrot: Data-driven behavioral priors for reinforcement learning',\n",
       "  'author': ['A Singh', 'H Liu', 'G Zhou', 'A Yu', 'N Rhinehart'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Reinforcement learning provides a general framework for flexible decision making and control, but requires extensive data collection for each new task that an agent needs to'},\n",
       " {'title': 'Actionable models: Unsupervised offline reinforcement learning of robotic skills',\n",
       "  'author': ['Y Chebotar', 'K Hausman', 'Y Lu', 'T Xiao'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We consider the problem of learning useful robotic skills from previously collected offline data without access to manually specified rewards or additional online exploration, a setting'},\n",
       " {'title': 'Explore, discover and learn: Unsupervised discovery of state-covering skills',\n",
       "  'author': ['V Campos', 'A Trott', 'C Xiong', 'R Socher'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'International …',\n",
       "  'abstract': 'Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of'},\n",
       " {'title': 'Accelerating robotic reinforcement learning via parameterized action primitives',\n",
       "  'author': ['M Dalal', 'D Pathak'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the'},\n",
       " {'title': 'Language conditioned imitation learning over unstructured data',\n",
       "  'author': ['C Lynch', 'P Sermanet'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2005.07648',\n",
       "  'abstract': 'Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with'},\n",
       " {'title': 'RvS: What is Essential for Offline RL via Supervised Learning?',\n",
       "  'author': ['S Emmons', 'B Eysenbach', 'I Kostrikov'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Recent work has shown that supervised learning alone, without temporal difference (TD) learning, can be remarkably effective for offline RL. When does this hold true, and which'},\n",
       " {'title': 'Transformers for one-shot visual imitation',\n",
       "  'author': ['S Dasari', 'A Gupta'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'abstract': 'Humans are able to seamlessly visually imitate others, by inferring their intentions and using past experience to achieve the same end goal. In other words, we can parse complex'},\n",
       " {'title': 'Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation',\n",
       "  'author': ['S Nair', 'C Finn'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'arXiv preprint arXiv:1909.05829',\n",
       "  'abstract': 'Video prediction models combined with planning algorithms have shown promise in enabling robots to learn to perform many vision-based tasks through only self-supervision'},\n",
       " {'title': 'Latent plans for task-agnostic offline reinforcement learning',\n",
       "  'author': ['E Rosete-Beas', 'O Mees', 'G Kalweit'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': '… on Robot Learning',\n",
       "  'abstract': 'Everyday tasks of long-horizon and comprising a sequence of multiple implicit subtasks still impose a major challenge in offline robot control. While a number of prior methods aimed to'},\n",
       " {'title': 'Hierarchical planning for long-horizon manipulation with geometric and symbolic scene graphs',\n",
       "  'author': ['Y Zhu', 'J Tremblay', 'S Birchfield'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '2021 IEEE International …',\n",
       "  'abstract': 'We present a visually grounded hierarchical planning algorithm for long-horizon manipulation tasks. Our algorithm offers a joint framework of neuro-symbolic task planning'},\n",
       " {'title': 'Generalized decision transformer for offline hindsight information matching',\n",
       "  'author': ['H Furuta', 'Y Matsuo', 'SS Gu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2111.10364',\n",
       "  'abstract': 'How to extract as much learning signal from each trajectory data has been a key problem in reinforcement learning (RL), where sample inefficiency has posed serious challenges for'},\n",
       " {'title': 'Rewriting history with inverse rl: Hindsight inference for policy improvement',\n",
       "  'author': ['B Eysenbach', 'X Geng', 'S Levine'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Advances in neural …',\n",
       "  'abstract': 'Multi-task reinforcement learning (RL) aims to simultaneously learn policies for solving many tasks. Several prior works have found that relabeling past experience with different reward'},\n",
       " {'title': 'Dexpilot: Vision-based teleoperation of dexterous robotic hand-arm system',\n",
       "  'author': ['A Handa', 'K Van Wyk', 'W Yang', 'J Liang'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Teleoperation offers the possibility of imparting robotic systems with sophisticated reasoning skills, intuition, and creativity to perform tasks. However, teleoperation solutions for high'},\n",
       " {'title': 'Iris: Implicit reinforcement without interaction at scale for learning control from offline robot manipulation data',\n",
       "  'author': ['A Mandlekar', 'F Ramos', 'B Boots'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline'},\n",
       " {'title': 'Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks',\n",
       "  'author': ['O Mees', 'L Hermann', 'E Rosete-Beas'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks'},\n",
       " {'title': 'Learning mobile manipulation through deep reinforcement learning',\n",
       "  'author': ['C Wang', 'Q Zhang', 'Q Tian', 'S Li', 'X Wang', 'D Lane'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Sensors',\n",
       "  'abstract': 'Mobile manipulation has a broad range of applications in robotics. However, it is usually more challenging than fixed-base manipulation due to the complex coordination of a mobile'},\n",
       " {'title': 'Learning to reach goals via iterated supervised learning',\n",
       "  'author': ['D Ghosh', 'A Gupta', 'A Reddy', 'J Fu', 'C Devin'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Current reinforcement learning (RL) algorithms can be brittle and difficult to use, especially when learning goal-reaching behaviors from sparse rewards. Although supervised imitation'},\n",
       " {'title': 'Learning affordance landscapes for interaction exploration in 3d environments',\n",
       "  'author': ['T Nagarajan', 'K Grauman'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Advances in Neural Information …',\n",
       "  'abstract': 'Embodied agents operating in human spaces must be able to master how their environment works: what objects can the agent use, and how can it use them? We introduce a'},\n",
       " {'title': 'Dall-e-bot: Introducing web-scale diffusion models to robotics',\n",
       "  'author': ['I Kapelyukh', 'V Vosylius', 'E Johns'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'We introduce the first work to explore web-scale diffusion models for robotics. DALL-E-Bot enables a robot to rearrange objects in a scene, by first inferring a text description of those'},\n",
       " {'title': 'Dexterous imitation made easy: A learning-based framework for efficient dexterous manipulation',\n",
       "  'author': ['SP Arunachalam', 'S Silwal', 'B Evans', 'L Pinto'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Optimizing behaviors for dexterous manipulation has been a longstanding challenge in robotics, with a variety of methods from model-based control to model-free reinforcement'},\n",
       " {'title': 'C-learning: Learning to achieve goals via recursive classification',\n",
       "  'author': ['B Eysenbach', 'R Salakhutdinov', 'S Levine'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned'},\n",
       " {'title': 'Learning to generalize across long-horizon tasks from human demonstrations',\n",
       "  'author': ['A Mandlekar', 'D Xu', 'R Martín-Martín', 'S Savarese'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Imitation learning is an effective and safe technique to train robot policies in the real world because it does not depend on an expensive random exploration process. However, due to'},\n",
       " {'title': 'Hierarchical reinforcement learning by discovering intrinsic options',\n",
       "  'author': ['J Zhang', 'H Yu', 'W Xu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2101.06521',\n",
       "  'abstract': 'We propose a hierarchical reinforcement learning method, HIDIO, that can learn task-agnostic options in a self-supervised manner while jointly learning to utilize them to solve'},\n",
       " {'title': 'Grounding language in play',\n",
       "  'author': ['C Lynch', 'P Sermanet'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2005.07648',\n",
       "  'abstract': 'Natural language is perhaps the most versatile and intuitive way for humans to communicate tasks to a robot. Prior work on Learning from Play (LfP)(Lynch et al., 2019) provides a simple'},\n",
       " {'title': 'Comic: Complementary task learning & mimicry for reusable skills',\n",
       "  'author': ['L Hasenclever', 'F Pardo', 'R Hadsell'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'International …',\n",
       "  'abstract': 'Learning to control complex bodies and reuse learned behaviors is a longstanding challenge in continuous control. We study the problem of learning reusable humanoid skills'},\n",
       " {'title': 'Trail: Near-optimal imitation learning with suboptimal data',\n",
       "  'author': ['M Yang', 'S Levine', 'O Nachum'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2110.14770',\n",
       "  'abstract': 'The aim in imitation learning is to learn effective policies by utilizing near-optimal expert demonstrations. However, high-quality demonstrations from human experts can be'},\n",
       " {'title': 'Guided reinforcement learning with learned skills',\n",
       "  'author': ['K Pertsch', 'Y Lee', 'Y Wu', 'JJ Lim'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2107.10253',\n",
       "  'abstract': 'Demonstration-guided reinforcement learning (RL) is a promising approach for learning complex behaviors by leveraging both reward feedback and a set of target task'},\n",
       " {'title': 'Controlling assistive robots with learned latent actions',\n",
       "  'author': ['DP Losey', 'K Srinivasan', 'A Mandlekar'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Assistive robotic arms enable users with physical disabilities to perform everyday tasks without relying on a caregiver. Unfortunately, the very dexterity that makes these arms useful'},\n",
       " {'title': 'Universal manipulation policy network for articulated objects',\n",
       "  'author': ['Z Xu', 'Z He', 'S Song'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and Automation Letters',\n",
       "  'abstract': 'We introduce the Universal Manipulation Policy Network (UMPNet)–a single image-based policy network that infers closed-loop action sequences for manipulating articulated objects'},\n",
       " {'title': 'What matters in language conditioned robotic imitation learning',\n",
       "  'author': ['O Mees', 'L Hermann', 'W Burgard'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2204.06252',\n",
       "  'abstract': 'A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural'},\n",
       " {'title': 'The differentiable cross-entropy method',\n",
       "  'author': ['B Amos', 'D Yarats'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'International Conference on Machine …',\n",
       "  'abstract': 'We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that'},\n",
       " {'title': 'Learning latent actions to control assistive robots',\n",
       "  'author': ['DP Losey', 'HJ Jeon', 'M Li', 'K Srinivasan', 'A Mandlekar'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Autonomous …',\n",
       "  'abstract': 'Assistive robot arms enable people with disabilities to conduct everyday tasks on their own. These arms are dexterous and high-dimensional; however, the interfaces people must use'},\n",
       " {'title': 'Affordance learning from play for sample-efficient policy learning',\n",
       "  'author': ['J Borja-Diaz', 'O Mees', 'G Kalweit'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Robots operating in human-centered environments should have the ability to understand how objects function: what can be done with each object, where this interaction may occur'},\n",
       " {'title': 'Intra-agent speech permits zero-shot task acquisition',\n",
       "  'author': ['C Yan', 'F Carnevale', 'PI Georgiev'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in …',\n",
       "  'abstract': 'Human language learners are exposed to a trickle of informative, context-sensitive language, but a flood of raw sensory data. Through both social language use and internal'},\n",
       " {'title': 'Skill-based meta-reinforcement learning',\n",
       "  'author': ['T Nam', 'SH Sun', 'K Pertsch', 'SJ Hwang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'While deep reinforcement learning methods have shown impressive results in robot learning, their sample inefficiency makes the learning of complex, long-horizon behaviors'},\n",
       " {'title': 'What matters in language conditioned robotic imitation learning over unstructured data',\n",
       "  'author': ['O Mees', 'L Hermann', 'W Burgard'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'A long-standing goal in robotics is to build robots that can perform a wide range of daily tasks from perceptions obtained with their onboard sensors and specified only via natural'},\n",
       " {'title': 'Phasic Self-Imitative Reduction for Sparse-Reward Goal-Conditioned Reinforcement Learning',\n",
       "  'author': ['Y Li', 'T Gao', 'J Yang', 'H Xu', 'Y Wu'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… Conference on Machine …',\n",
       "  'abstract': 'It has been a recent trend to leverage the power of supervised learning (SL) towards more effective reinforcement learning (RL) methods. We propose a novel phasic solution by'},\n",
       " {'title': 'Model-based visual planning with self-supervised functional distances',\n",
       "  'author': ['S Tian', 'S Nair', 'F Ebert', 'S Dasari', 'B Eysenbach'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'A generalist robot must be able to complete a variety of tasks in its environment. One appealing way to specify each task is in terms of a goal observation. However, learning goal'},\n",
       " {'title': 'Intrinsically motivated goal-conditioned reinforcement learning: a short survey',\n",
       "  'author': ['C Colas', 'T Karch', 'O Sigaud', 'PY Oudeyer'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Building autonomous machines that can explore open-ended environments, discover possible interactions and autonomously build repertoires of skills is a general objective of'},\n",
       " {'title': 'Eliciting compatible demonstrations for multi-human imitation learning',\n",
       "  'author': ['K Gandhi', 'S Karamcheti', 'M Liao'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Conference on Robot …',\n",
       "  'abstract': 'Imitation learning from human-provided demonstrations is a strong approach for learning policies for robot manipulation. While the ideal dataset for imitation learning is homogenous'},\n",
       " {'title': 'Autotelic agents with intrinsically motivated goal-conditioned reinforcement learning: a short survey',\n",
       "  'author': ['C Colas', 'T Karch', 'O Sigaud', 'PY Oudeyer'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Journal of Artificial Intelligence …',\n",
       "  'abstract': 'Building autonomous machines that can explore open-ended environments, discover possible interactions and build repertoires of skills is a general objective of artificial'},\n",
       " {'title': 'Latent-variable advantage-weighted policy optimization for offline rl',\n",
       "  'author': ['X Chen', 'A Ghadirzadeh', 'T Yu', 'Y Gao', 'J Wang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Offline reinforcement learning methods hold the promise of learning policies from pre-collected datasets without the need to query the environment for new transitions. This setting'},\n",
       " {'title': 'What can i do here? learning new skills by imagining visual affordances',\n",
       "  'author': ['A Khazatsky', 'A Nair', 'D Jing'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '2021 IEEE International …',\n",
       "  'abstract': 'A generalist robot equipped with learned skills must be able to perform many tasks in many different environments. However, zero-shot generalization to new settings is not always'},\n",
       " {'title': 'Interactive language: Talking to robots in real time',\n",
       "  'author': ['C Lynch', 'A Wahid', 'J Tompson', 'T Ding', 'J Betker'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We present a framework for building interactive, real-time, natural language-instructable robots in the real world, and we open source related assets (dataset, environment'},\n",
       " {'title': 'Imitation learning: Progress, taxonomies and challenges',\n",
       "  'author': ['B Zheng', 'S Verma', 'J Zhou', 'IW Tsang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Transactions on …',\n",
       "  'abstract': \"Imitation learning (IL) aims to extract knowledge from human experts' demonstrations or artificially created agents to replicate their behaviors. It promotes interdisciplinary\"},\n",
       " {'title': 'Rapid exploration for open-world navigation with latent goal models',\n",
       "  'author': ['D Shah', 'B Eysenbach', 'N Rhinehart'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We describe a robotic learning system for autonomous exploration and navigation in diverse, open-world environments. At the core of our method is a learned latent variable'},\n",
       " {'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation',\n",
       "  'author': ['OAI OpenAI', 'M Plappert', 'R Sampedro', 'T Xu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We train a single, goal-conditioned policy that can solve many robotic manipulation tasks, including tasks with previously unseen goals and objects. We rely on asymmetric self-play'},\n",
       " {'title': 'Learning geometric reasoning and control for long-horizon tasks from visual input',\n",
       "  'author': ['D Driess', 'JS Ha', 'R Tedrake'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '2021 IEEE International …',\n",
       "  'abstract': 'Long-horizon manipulation tasks require joint reasoning over a sequence of discrete actions and their associated continuous control parameters. While Task and Motion Planning'},\n",
       " {'title': 'Discovering motor programs by recomposing demonstrations',\n",
       "  'author': ['T Shankar', 'S Tulsiani', 'L Pinto'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… Conference on Learning …',\n",
       "  'abstract': 'In this paper, we present an approach to learn recomposable motor primitives across large-scale and diverse manipulation demonstrations. Current approaches to decomposing'},\n",
       " {'title': 'Foundation models for decision making: Problems, methods, and opportunities',\n",
       "  'author': ['S Yang', 'O Nachum', 'Y Du', 'J Wei', 'P Abbeel'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed'},\n",
       " {'title': 'Receding-horizon perceptive trajectory optimization for dynamic legged locomotion with learned initialization',\n",
       "  'author': ['O Melon', 'R Orsolino', 'D Surovik'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'To dynamically traverse challenging terrain, legged robots need to continually perceive and reason about upcoming features, adjust the locations and timings of future footfalls and'},\n",
       " {'title': 'Learning transferable motor skills with hierarchical latent mixture policies',\n",
       "  'author': ['D Rao', 'F Sadeghi', 'L Hasenclever', 'M Wulfmeier'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'For robots operating in the real world, it is desirable to learn reusable behaviours that can effectively be transferred and adapted to numerous tasks and scenarios. We propose an'},\n",
       " {'title': 'Avlen: Audio-visual-language embodied navigation in 3d environments',\n",
       "  'author': ['S Paul', 'A Roy-Chowdhury'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'Recent years have seen embodied visual navigation advance in two distinct directions:(i) in equipping the AI agent to follow natural language instructions, and (ii) in making the'},\n",
       " {'title': 'Towards more generalizable one-shot visual imitation learning',\n",
       "  'author': ['Z Mandi', 'F Liu', 'K Lee', 'P Abbeel'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… International Conference on …',\n",
       "  'abstract': 'A general-purpose robot should be able to master a wide range of tasks and quickly learn a novel one by leveraging past experiences. One-shot imitation learning (OSIL) approaches'},\n",
       " {'title': 'Broadly-exploring, local-policy trees for long-horizon task planning',\n",
       "  'author': ['B Ichter', 'P Sermanet', 'C Lynch'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2010.06491',\n",
       "  'abstract': 'Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion'},\n",
       " {'title': 'Dichotomy of control: Separating what you can control from what you cannot',\n",
       "  'author': ['M Yang', 'D Schuurmans', 'P Abbeel'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Future-or return-conditioned supervised learning is an emerging paradigm for offline reinforcement learning (RL), where the future outcome (ie, return) associated with an'},\n",
       " {'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics',\n",
       "  'author': ['K Rana', 'M Xu', 'B Tidd', 'M Milford'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Conference on Robot …',\n",
       "  'abstract': 'Skill-based reinforcement learning (RL) has emerged as a promising strategy to leverage prior knowledge for accelerated robot learning. Skills are typically extracted from expert'},\n",
       " {'title': 'Bootstrapped autonomous practicing via multi-task reinforcement learning',\n",
       "  'author': ['A Gupta', 'C Lynch', 'B Kinman', 'G Peake', 'S Levine'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Reinforcement learning systems have the potential to enable continuous improvement in unstructured environments, leveraging data collected autonomously. However, in practice'},\n",
       " {'title': 'ASHA: Assistive teleoperation via human-in-the-loop reinforcement learning',\n",
       "  'author': ['S Chen', 'J Gao', 'S Reddy', 'G Berseth'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Building assistive interfaces for controlling robots through arbitrary, high-dimensional, noisy inputs (eg, webcam images of eye gaze) can be challenging, especially when it involves'},\n",
       " {'title': 'Is curiosity all you need? on the utility of emergent behaviours from curious exploration',\n",
       "  'author': ['O Groth', 'M Wulfmeier', 'G Vezzani', 'V Dasagi'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Curiosity-based reward schemes can present powerful exploration mechanisms which facilitate the discovery of solutions for complex, sparse or long-horizon tasks. However, as'},\n",
       " {'title': 'Learning robotic navigation from experience: principles, methods and recent results',\n",
       "  'author': ['S Levine', 'D Shah'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': '… Transactions of the Royal Society B',\n",
       "  'abstract': 'Navigation is one of the most heavily studied problems in robotics and is conventionally approached as a geometric mapping and planning problem. However, real-world navigation'},\n",
       " {'title': 'Hierarchical policies for cluttered-scene grasping with latent plans',\n",
       "  'author': ['L Wang', 'X Meng', 'Y Xiang', 'D Fox'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': '6D grasping in cluttered scenes is a longstanding problem in robotic manipulation. Open-loop manipulation pipelines may fail due to inaccurate state estimation, while most end-to'},\n",
       " {'title': 'Lapo: Latent-variable advantage-weighted policy optimization for offline reinforcement learning',\n",
       "  'author': ['X Chen', 'A Ghadirzadeh', 'T Yu', 'J Wang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in …',\n",
       "  'abstract': 'Offline reinforcement learning methods hold the promise of learning policies from pre-collected datasets without the need to query the environment for new samples. This setting'},\n",
       " {'title': 'A policy-guided imitation approach for offline reinforcement learning',\n",
       "  'author': ['H Xu', 'L Jiang', 'L Jianxiong'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution'},\n",
       " {'title': 'Offline imitation learning with a misspecified simulator',\n",
       "  'author': ['S Jiang', 'J Pang', 'Y Yu'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Advances in neural information …',\n",
       "  'abstract': 'In real-world decision-making tasks, learning an optimal policy without a trial-and-error process is an appealing challenge. When expert demonstrations are available, imitation'},\n",
       " {'title': 'Mimicplay: Long-horizon imitation learning by watching human play',\n",
       "  'author': ['C Wang', 'L Fan', 'J Sun', 'R Zhang', 'L Fei-Fei', 'D Xu'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Imitation Learning from human demonstrations is a promising paradigm to teach robots manipulation skills in the real world, but learning complex long-horizon tasks often requires'},\n",
       " {'title': 'Plato: Predicting latent affordances through object-centric play',\n",
       "  'author': ['S Belkhale', 'D Sadigh'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'abstract': 'Constructing a diverse repertoire of manipulation skills in a scalable fashion remains an unsolved challenge in robotics. One way to address this challenge is with unstructured'},\n",
       " {'title': 'Offline Goal-Conditioned Reinforcement Learning via -Advantage Regression',\n",
       "  'author': ['JY Ma', 'J Yan', 'D Jayaraman'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'Offline goal-conditioned reinforcement learning (GCRL) promises general-purpose skill learning in the form of reaching diverse goals from purely offline datasets. We propose'},\n",
       " {'title': 'Hybrid trajectory and force learning of complex assembly tasks: A combined learning framework',\n",
       "  'author': ['Y Wang', 'CC Beltran-Hernandez', 'W Wan'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'IEEE Access',\n",
       "  'abstract': 'Complex assembly tasks involve nonlinear and low-clearance insertion trajectories with varying contact forces at different stages. For a robot to solve these tasks, it requires a'},\n",
       " {'title': 'Robust flight navigation out of distribution with liquid neural networks',\n",
       "  'author': ['M Chahine', 'R Hasani', 'P Kao', 'A Ray', 'R Shubert'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Science Robotics',\n",
       "  'abstract': 'Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations and generalize well to online and unseen scenarios within the same'},\n",
       " {'title': 'Solving compositional reinforcement learning problems via task reduction',\n",
       "  'author': ['Y Li', 'Y Wu', 'H Xu', 'X Wang', 'Y Wu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2103.07607',\n",
       "  'abstract': 'We propose a novel learning paradigm, Self-Imitation via Reduction (SIR), for solving compositional reinforcement learning problems. SIR is based on two core ideas: task'},\n",
       " {'title': 'Grounding language with visual affordances over unstructured data',\n",
       "  'author': ['O Mees', 'J Borja-Diaz', 'W Burgard'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2210.01911',\n",
       "  'abstract': 'Recent works have shown that Large Language Models (LLMs) can be applied to ground natural language to a wide variety of robot skills. However, in practice, learning multi-task'},\n",
       " {'title': 'Hindsight for foresight: Unsupervised structured dynamics models from physical interaction',\n",
       "  'author': ['I Nematollahi', 'O Mees', 'L Hermann'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '2020 IEEE/RSJ …',\n",
       "  'abstract': 'A key challenge for an agent learning to interact with the world is to reason about physical properties of objects and to foresee their dynamics under the effect of applied forces. In'},\n",
       " {'title': 'Task-oriented motion mapping on robots of various configuration using body role division',\n",
       "  'author': ['K Sasabuchi', 'N Wake', 'K Ikeuchi'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'Many works in robot teaching either focus only on teaching task knowledge, such as geometric constraints, or motion knowledge, such as the motion for accomplishing a task'},\n",
       " {'title': 'Bits: Bi-level imitation for traffic simulation',\n",
       "  'author': ['D Xu', 'Y Chen', 'B Ivanovic', 'M Pavone'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2208.12403',\n",
       "  'abstract': 'Simulation is the key to scaling up validation and verification for robotic systems such as autonomous vehicles. Despite advances in high-fidelity physics and sensor simulation, a'},\n",
       " {'title': 'Manipulation planning from demonstration via goal-conditioned prior action primitive decomposition and alignment',\n",
       "  'author': ['N Lin', 'Y Li', 'K Tang', 'Y Zhu', 'X Zhang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'Manipulation plays a vital role in robotics but is left unsolved. Recent work attempts to leverage the hierarchical structure of tasks via using action primitives. However, due to'},\n",
       " {'title': 'A motion capture and imitation learning based approach to Robot Control',\n",
       "  'author': ['P Racinskis', 'J Arents', 'M Greitans'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Applied Sciences',\n",
       "  'abstract': 'Imitation learning is a discipline of machine learning primarily concerned with replicating observed behavior of agents known to perform well on a given task, collected in'},\n",
       " {'title': 'Learning multi-stage tasks with one demonstration via self-replay',\n",
       "  'author': ['N Di Palo', 'E Johns'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'abstract': 'In this work, we introduce a novel method to learn everyday-like multi-stage tasks from a single human demonstration, without requiring any prior object knowledge. Inspired by the'},\n",
       " {'title': 'Learning an expert skill-space for replanning dynamic quadruped locomotion over obstacles',\n",
       "  'author': ['D Surovik', 'O Melon', 'M Geisert'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '… on Robot Learning',\n",
       "  'abstract': 'Function approximators are increasingly being considered as a tool for generating robot motions that are temporally extended and express foresight about the scenario at hand'},\n",
       " {'title': 'Learning task decomposition with ordered memory policy network',\n",
       "  'author': ['Y Lu', 'Y Shen', 'S Zhou', 'A Courville'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Many complex real-world tasks are composed of several levels of sub-tasks. Humans leverage these hierarchical structures to accelerate the learning process and achieve better'},\n",
       " {'title': \"How Far I'll Go: Offline Goal-Conditioned Reinforcement Learning via -Advantage Regression\",\n",
       "  'author': ['YJ Ma', 'J Yan', 'D Jayaraman', 'O Bastani'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2206.03023',\n",
       "  'abstract': 'Offline goal-conditioned reinforcement learning (GCRL) promises general-purpose skill learning in the form of reaching diverse goals from purely offline datasets. We propose'},\n",
       " {'title': 'ToolTango: Common sense Generalization in Predicting Sequential Tool Interactions for Robot Plan Synthesis',\n",
       "  'author': ['S Tuli', 'R Bansal', 'R Paul'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Journal of Artificial Intelligence Research',\n",
       "  'abstract': 'Robots assisting us in environments such as factories or homes must learn to make use of objects as tools to perform tasks, for instance, using a tray to carry objects. We consider the'},\n",
       " {'title': 'Causal inference q-network: Toward resilient reinforcement learning',\n",
       "  'author': ['CHH Yang', 'I Hung', 'T Danny', 'Y Ouyang'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may'},\n",
       " {'title': 'Self-supervised reinforcement learning with independently controllable subgoals',\n",
       "  'author': ['A Zadaianchuk', 'G Martius'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Conference on Robot …',\n",
       "  'abstract': 'To successfully tackle challenging manipulation tasks, autonomous agents must learn a diverse set of skills and how to combine them. Recently, self-supervised agents that set their'},\n",
       " {'title': 'Playful interactions for representation learning',\n",
       "  'author': ['S Young', 'J Pari', 'P Abbeel'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '2022 IEEE/RSJ …',\n",
       "  'abstract': 'Ahstract-One of the key challenges in visual imitation learning is collecting large amounts of expert demonstrations for a given task. While methods for collecting human demonstrations'},\n",
       " {'title': 'Braxlines: Fast and interactive toolkit for rl-driven behavior engineering beyond reward maximization',\n",
       "  'author': ['SS Gu', 'M Diaz', 'DC Freeman', 'H Furuta'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The goal of continuous control is to synthesize desired behaviors. In reinforcement learning (RL)-driven approaches, this is often accomplished through careful task reward engineering'},\n",
       " {'title': 'Leveraging Jumpy Models for Planning and Fast Learning in Robotic Domains',\n",
       "  'author': ['J Zhang', 'JT Springenberg', 'A Byravan'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'In this paper we study the problem of learning multi-step dynamics prediction models (jumpy models) from unlabeled experience and their utility for fast inference of (high-level) plans in'},\n",
       " {'title': 'Enabling visual action planning for object manipulation through latent space roadmap',\n",
       "  'author': ['M Lippi', 'P Poklukar', 'MC Welle', 'A Varava'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Transactions …',\n",
       "  'abstract': 'In this article, we present a framework for visual action planning of complex manipulation tasks with high-dimensional state spaces, focusing on manipulation of deformable objects'},\n",
       " {'title': 'Learning visually guided latent actions for assistive teleoperation',\n",
       "  'author': ['S Karamcheti', 'AJ Zhai', 'DP Losey'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Learning for Dynamics …',\n",
       "  'abstract': 'It is challenging for humans—particularly people living with physical disabilities—to control high-dimensional and dexterous robots. Prior work explores how robots can learn'},\n",
       " {'title': 'Virtual reality teleoperation of a humanoid robot using markerless human upper body pose imitation',\n",
       "  'author': ['M Hirschmanner', 'C Tsiourti', 'T Patten'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': '2019 IEEE-RAS 19th …',\n",
       "  'abstract': 'Teleoperation of robots with traditional input devices (joysticks, keyboard, etc.) is often difficult and cumbersome especially for novice users. We introduce an intuitive virtual reality'},\n",
       " {'title': 'Goal-conditioned imitation learning using score-based diffusion policies',\n",
       "  'author': ['M Reuss', 'M Li', 'X Jia', 'R Lioutikov'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2304.02532',\n",
       "  'abstract': 'We propose a new policy representation based on score-based diffusion models (SDMs). We apply our new policy representation in the domain of Goal-Conditioned Imitation'},\n",
       " {'title': 'Learning periodic tasks from human demonstrations',\n",
       "  'author': ['J Yang', 'J Zhang', 'C Settle', 'A Rai'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'We develop a method for learning periodic tasks from visual demonstrations. The core idea is to leverage periodicity in the policy structure to model periodic aspects of the tasks. We'},\n",
       " {'title': 'Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl',\n",
       "  'author': ['T Yamagata', 'A Khalil', 'R Santos-Rodriguez'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Recent works have shown that tackling offline reinforcement learning (RL) with a conditional policy produces promising results. The Decision Transformer (DT) combines the conditional'},\n",
       " {'title': 'Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts',\n",
       "  'author': ['E Xing', 'A Gupta', 'S Powers', 'V Dean'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NeurIPS 2021 Workshop on …',\n",
       "  'abstract': 'Humans are remarkably capable of zero-shot generalizing while performing tasks in new settings, even when the task is learned entirely from observing others. In this work, we show'},\n",
       " {'title': 'Grimgep: learning progress for robust goal sampling in visual deep reinforcement learning',\n",
       "  'author': ['G Kovač', 'A Laversanne-Finot'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Transactions on …',\n",
       "  'abstract': 'Autotelic RL agents sample their own goals, and try to reach them. They often prioritize goal sampling according to some intrinsic reward, ex. novelty or absolute learning progress'},\n",
       " {'title': 'Using human gaze to improve robustness against irrelevant objects in robot manipulation tasks',\n",
       "  'author': ['H Kim', 'Y Ohmura', 'Y Kuniyoshi'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'IEEE Robotics and Automation …',\n",
       "  'abstract': 'Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images'},\n",
       " {'title': 'Transferring Hierarchical Structures with Dual Meta Imitation Learning',\n",
       "  'author': ['C Gao', 'Y Jiang', 'F Chen'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'abstract': 'Hierarchical Imitation Learning (HIL) is an effective way for robots to learn sub-skills from long-horizon unsegmented demonstrations. However, the learned hierarchical'},\n",
       " {'title': 'Geometric task networks: Learning efficient and explainable skill coordination for object manipulation',\n",
       "  'author': ['M Guo', 'M Bürger'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'IEEE Transactions on Robotics',\n",
       "  'abstract': 'Complex manipulation tasks can contain various execution branches of primitive skills in sequence or in parallel under different scenarios. Manual specifications of such branching'},\n",
       " {'title': 'It takes four to tango: Multiagent selfplay for automatic curriculum generation',\n",
       "  'author': ['Y Du', 'P Abbeel', 'A Grover'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2202.10608',\n",
       "  'abstract': 'We are interested in training general-purpose reinforcement learning agents that can solve a wide variety of goals. Training such agents efficiently requires automatic generation of a'},\n",
       " {'title': 'Trass: Time reversal as self-supervision',\n",
       "  'author': ['S Nair', 'M Babaeizadeh', 'C Finn'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning'},\n",
       " {'title': 'The embodied crossmodal self forms language and interaction: a computational cognitive review',\n",
       "  'author': ['F Röder', 'O Özdemir', 'PDH Nguyen', 'S Wermter'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Frontiers in …',\n",
       "  'abstract': 'Human language is inherently embodied and grounded in sensorimotor representations of the self and the world around it. This suggests that the body schema and ideomotor action'},\n",
       " {'title': 'Context-aware language modeling for goal-oriented dialogue systems',\n",
       "  'author': ['C Snell', 'S Yang', 'J Fu', 'Y Su', 'S Levine'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2204.10198',\n",
       "  'abstract': 'Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of'},\n",
       " {'title': 'Robotic imitation of human assembly skills using hybrid trajectory and force learning',\n",
       "  'author': ['Y Wang', 'CC Beltran-Hernandez'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Robotic assembly tasks involve complex and low-clearance insertion trajectories with varying contact forces at different stages. While the nominal motion trajectory can be easily'},\n",
       " {'title': 'LISA: Learning interpretable skill abstractions from language',\n",
       "  'author': ['D Garg', 'S Vaidyanath', 'K Kim', 'J Song'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Learning policies that effectually utilize language instructions in complex, multi-task environments is an important problem in imitation learning. While it is possible to condition'},\n",
       " {'title': 'Shared autonomy with learned latent actions',\n",
       "  'author': ['HJ Jeon', 'DP Losey', 'D Sadigh'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2005.03210',\n",
       "  'abstract': 'Assistive robots enable people with disabilities to conduct everyday tasks on their own. However, these tasks can be complex, containing both coarse reaching motions and fine'},\n",
       " {'title': 'Skill-based model-based reinforcement learning',\n",
       "  'author': ['LX Shi', 'JJ Lim', 'Y Lee'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2207.07560',\n",
       "  'abstract': 'Model-based reinforcement learning (RL) is a sample-efficient way of learning complex behaviors by leveraging a learned single-step dynamics model to plan actions in'},\n",
       " {'title': 'StructDiffusion: Object-centric diffusion for semantic rearrangement of novel objects',\n",
       "  'author': ['W Liu', 'T Hermans', 'S Chernova', 'C Paxton'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this'},\n",
       " {'title': 'Aligning Robot and Human Representations',\n",
       "  'author': ['A Bobu', 'A Peng', 'P Agrawal', 'J Shah'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'To act in the world, robots rely on a representation of salient task aspects: for example, to carry a cup of coffee, a robot must consider movement efficiency and cup orientation in its'},\n",
       " {'title': 'Detecting Incorrect Visual Demonstrations for Improved Policy Learning',\n",
       "  'author': ['M Hussein', 'M Begum'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Conference on Robot Learning',\n",
       "  'abstract': 'Learning tasks only from raw video demonstrations is the current state of the art in robotics visual imitation learning research. The implicit assumption here is that all video'},\n",
       " {'title': 'SAFARI: Safe and active robot imitation learning with imagination',\n",
       "  'author': ['N Di Palo', 'E Johns'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2011.09586',\n",
       "  'abstract': 'One of the main issues in Imitation Learning is the erroneous behavior of an agent when facing out-of-distribution situations, not covered by the set of demonstrations given by the'},\n",
       " {'title': 'Intrinsically motivated exploration of learned goal spaces',\n",
       "  'author': ['A Laversanne-Finot', 'A Péré', 'PY Oudeyer'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Frontiers in neurorobotics',\n",
       "  'abstract': 'Finding algorithms that allow agents to discover a wide variety of skills efficiently and autonomously, remains a challenge of Artificial Intelligence. Intrinsically Motivated Goal'},\n",
       " {'title': 'Training a resilient q-network against observational interference',\n",
       "  'author': ['CHH Yang', 'ITD Hung', 'Y Ouyang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Proceedings of the AAAI …',\n",
       "  'abstract': 'Deep reinforcement learning (DRL) has demonstrated impressive performance in various gaming simulators and real-world applications. In practice, however, a DRL agent may'},\n",
       " {'title': 'C-planning: An automatic curriculum for learning goal-reaching tasks',\n",
       "  'author': ['T Zhang', 'B Eysenbach', 'R Salakhutdinov'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Goal-conditioned reinforcement learning (RL) can solve tasks in a wide range of domains, including navigation and manipulation, but learning to reach distant goals remains a central'},\n",
       " {'title': 'Variable Impedance Skill Learning for Contact-Rich Manipulation',\n",
       "  'author': ['Q Yang', 'A Dürr', 'EA Topp', 'JA Stork'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Robotics and …',\n",
       "  'abstract': 'Contact-rich manipulation tasks remain a hard problem in robotics that requires interaction with unstructured environments. Reinforcement Learning (RL) is one potential solution to'},\n",
       " {'title': 'Meta-Reinforcement Learning via Language Instructions',\n",
       "  'author': ['Z Bing', 'A Koch', 'X Yao', 'FO Morin', 'K Huang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires tremendous amount of data to learn a task, let alone being'},\n",
       " {'title': 'Manipulator-independent representations for visual imitation',\n",
       "  'author': ['Y Zhou', 'Y Aytar', 'K Bousmalis'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2103.09016',\n",
       "  'abstract': 'Imitation learning is an effective tool for robotic learning tasks where specifying a reinforcement learning (RL) reward is not feasible or where the exploration problem is'},\n",
       " {'title': 'Exploring with sticky mittens: Reinforcement learning with expert interventions via option templates',\n",
       "  'author': ['S Dutta', 'K Sridhar', 'O Bastani'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': '… on Robot Learning',\n",
       "  'abstract': 'Long horizon robot learning tasks with sparse rewards pose a significant challenge for current reinforcement learning algorithms. A key feature enabling humans to learn'},\n",
       " {'title': 'Beyond Rewards: a Hierarchical Perspective on Offline Multiagent Behavioral Analysis',\n",
       "  'author': ['S Omidshafiei', 'A Kapishnikov', 'Y Assogba'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Each year, expert-level performance is attained in increasingly-complex multiagent domains, notable examples including Go, Poker, and StarCraft II. This rapid progression is'},\n",
       " {'title': 'Evaluating agents without rewards',\n",
       "  'author': ['B Matusch', 'J Ba', 'D Hafner'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2012.11538',\n",
       "  'abstract': 'Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming'},\n",
       " {'title': 'Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models',\n",
       "  'author': ['T Xiao', 'H Chan', 'P Sermanet', 'A Wahid'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot'},\n",
       " {'title': 'Understanding Hindsight Goal Relabeling Requires Rethinking Divergence Minimization',\n",
       "  'author': ['L Zhang', 'BC Stadie'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2209.13046',\n",
       "  'abstract': 'Hindsight goal relabeling has become a foundational technique for multi-goal reinforcement learning (RL). The idea is quite simple: any arbitrary trajectory can be seen as an expert'},\n",
       " {'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments',\n",
       "  'author': ['BA Pires', 'F Behbahani', 'H Soyer', 'K Nikiforou'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Hierarchical Reinforcement Learning (HRL) agents have the potential to demonstrate appealing capabilities such as planning and exploration with abstraction, transfer, and skill'},\n",
       " {'title': 'Learning Video-Conditioned Policies for Unseen Manipulation Tasks',\n",
       "  'author': ['E Chane-Sane', 'C Schmid', 'I Laptev'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2305.06289',\n",
       "  'abstract': 'The ability to specify robot commands by a non-expert user is critical for building generalist agents capable of solving a large variety of tasks. One convenient way to specify the'},\n",
       " {'title': 'Learning to share autonomy across repeated interaction',\n",
       "  'author': ['A Jonnavittula', 'DP Losey'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '2021 IEEE/RSJ International …',\n",
       "  'abstract': 'Wheelchair-mounted robotic arms (and other assistive robots) should help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy'},\n",
       " {'title': 'Pretraining in Deep Reinforcement Learning: A Survey',\n",
       "  'author': ['Z Xie', 'Z Lin', 'J Li', 'S Li', 'D Ye'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2211.03959',\n",
       "  'abstract': 'The past few years have seen rapid progress in combining reinforcement learning (RL) with deep learning. Various breakthroughs ranging from games to robotics have spurred the'},\n",
       " {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration',\n",
       "  'author': ['G Vezzani', 'D Tirumala', 'M Wulfmeier', 'D Rao'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The ability to effectively reuse prior knowledge is a key requirement when building general and flexible Reinforcement Learning (RL) agents. Skill reuse is one of the most common'},\n",
       " {'title': 'Learning High Speed Precision Table Tennis on a Physical Robot',\n",
       "  'author': ['T Ding', 'L Graesser', 'S Abeyruwan'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '2022 IEEE/RSJ …',\n",
       "  'abstract': 'Learning goal conditioned control in the real world is a challenging open problem in robotics. Reinforcement learning systems have the potential to learn autonomously via trial'},\n",
       " {'title': 'Squirl: Robust and efficient learning from video demonstration of long-horizon robotic manipulation tasks',\n",
       "  'author': ['B Wu', 'F Xu', 'Z He', 'A Gupta'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '2020 IEEE/RSJ …',\n",
       "  'abstract': 'Recent advances in deep reinforcement learning (RL) have demonstrated its potential to learn complex robotic manipulation tasks. However, RL still requires the robot to collect a'},\n",
       " {'title': 'DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics',\n",
       "  'author': ['S Li', 'Z Huang', 'T Chen', 'T Du', 'H Su'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'In this work, we aim to learn dexterous manipulation of deformable objects using multi-fingered hands. Reinforcement learning approaches for dexterous rigid object manipulation'},\n",
       " {'title': 'Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets',\n",
       "  'author': ['M Du', 'S Nair', 'D Sadigh', 'C Finn'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2304.08742',\n",
       "  'abstract': 'Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is'},\n",
       " {'title': 'Robot program parameter inference via differentiable shadow program inversion',\n",
       "  'author': ['B Alt', 'D Katic', 'R Jäkel', 'AK Bozcuoglu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Challenging manipulation tasks can be solved effectively by combining individual robot skills, which must be parameterized for the concrete physical environment and task at hand'},\n",
       " {'title': 'Syntactic Inductive Biases for Deep Learning Methods',\n",
       "  'author': ['Y Shen'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2206.04806',\n",
       "  'abstract': 'In this thesis, we try to build a connection between the two schools by introducing syntactic inductive biases for deep learning models. We propose two families of inductive biases, one'},\n",
       " {'title': 'Imitating Task and Motion Planning with Visuomotor Transformers',\n",
       "  'author': ['M Dalal', 'A Mandlekar', 'C Garrett', 'A Handa'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However'},\n",
       " {'title': 'Leveraging motor babbling for efficient robot learning',\n",
       "  'author': ['K Kase', 'N Matsumoto', 'T Ogata'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Journal of Robotics and …',\n",
       "  'abstract': 'Deep robotic learning by learning from demonstration allows robots to mimic a given demonstration and generalize their performance to unknown task setups. However, this'},\n",
       " {'title': 'CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning',\n",
       "  'author': ['Z Mandi', 'H Bharadhwaj', 'V Moens', 'S Song'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Developing robots that are capable of many skills and generalization to unseen scenarios requires progress on two fronts: efficient collection of large and diverse datasets, and'},\n",
       " {'title': 'Visual perspective taking for opponent behavior modeling',\n",
       "  'author': ['B Chen', 'Y Hu', 'R Kwiatkowski', 'S Song'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': \"In order to engage in complex social interaction, humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict others' plans\"},\n",
       " {'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning',\n",
       "  'author': ['S Nasiriany', 'T Gao', 'A Mandlekar', 'Y Zhu'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2210.11435',\n",
       "  'abstract': 'Imitation learning offers a promising path for robots to learn general-purpose behaviors, but traditionally has exhibited limited scalability due to high data supervision requirements and'},\n",
       " {'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction',\n",
       "  'author': ['M Tavassoli', 'S Katyara', 'M Pozzi', 'N Deshpande'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The uses of robots are changing from static environments in factories to encompass novel concepts such as Human-Robot Collaboration in unstructured settings. Pre-programming all'},\n",
       " {'title': 'Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability',\n",
       "  'author': ['H Zhu', 'A Zhang'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2302.03770',\n",
       "  'abstract': 'Goal-conditioned reinforcement learning (GCRL) refers to learning general-purpose skills which aim to reach diverse goals. In particular, offline GCRL only requires purely pre'},\n",
       " {'title': 'Evaluating Multimodal Interactive Agents',\n",
       "  'author': ['J Abramson', 'A Ahuja', 'F Carnevale', 'P Georgiev'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Creating agents that can interact naturally with humans is a common goal in artificial intelligence (AI) research. However, evaluating these interactions is challenging: collecting'},\n",
       " {'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation',\n",
       "  'author': ['D Brandfonbrener', 'O Nachum', 'J Bruna'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2305.16985',\n",
       "  'abstract': 'In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be'},\n",
       " {'title': 'Learning to Share Autonomy from Repeated Human-Robot Interaction',\n",
       "  'author': ['A Jonnavittula', 'SA Mehta', 'DP Losey'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2205.09795',\n",
       "  'abstract': 'Assistive robot arms try to help their users perform everyday tasks. One way robots can provide this assistance is shared autonomy. Within shared autonomy, both the human and'},\n",
       " {'title': 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection',\n",
       "  'author': ['S Dass', 'K Pertsch', 'H Zhang', 'Y Lee', 'JJ Lim'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Large-scale data is an essential component of machine learning as demonstrated in recent advances in natural language processing and computer vision research. However'},\n",
       " {'title': 'Learning Composable Behavior Embeddings for Long-Horizon Visual Navigation',\n",
       "  'author': ['X Meng', 'Y Xiang', 'D Fox'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'IEEE Robotics and Automation …',\n",
       "  'abstract': 'Learning high-level navigation behaviors has important implications: it enables robots to build compact visual memory for repeating demonstrations and to build sparse topological'},\n",
       " {'title': 'H-SAUR: Hypothesize, Simulate, Act, Update, and Repeat for Understanding Object Articulations from Interactions',\n",
       "  'author': ['K Ota', 'HY Tung', 'KA Smith', 'A Cherian', 'TK Marks'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The world is filled with articulated objects that are difficult to determine how to use from vision alone, eg, a door might open inwards or outwards. Humans handle these objects with'},\n",
       " {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations',\n",
       "  'author': ['K Yan', 'A Schwing', 'YX Wang'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Advances in Neural …',\n",
       "  'abstract': 'Although reinforcement learning has found widespread use in dense reward settings, training autonomous agents with sparse rewards remains challenging. To address this'},\n",
       " {'title': 'A System for Morphology-Task Generalization via Unified Representation and Behavior Distillation',\n",
       "  'author': ['H Furuta', 'Y Iwasawa', 'Y Matsuo', 'SS Gu'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2211.14296',\n",
       "  'abstract': 'The rise of generalist large-scale models in natural language and vision has made us expect that a massive data-driven approach could achieve broader generalization in other'},\n",
       " {'title': 'Manipulate by Seeing: Creating Manipulation Controllers',\n",
       "  'author': ['J Wang', 'S Dasari', 'MK Srirama', 'S Tulsiani'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The field of visual representation learning has seen explosive growth in the past years, but its benefits in robotics have been surprisingly limited so far. Prior work uses generic visual'},\n",
       " {'title': 'Efficient Learning of High Level Plans from Play',\n",
       "  'author': ['NA Urpí', 'M Bagatella', 'O Hilliges', 'G Martius'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Real-world robotic manipulation tasks remain an elusive challenge, since they involve both fine-grained environment interaction, as well as the ability to plan for long-horizon goals'},\n",
       " {'title': 'Curriculum Goal-Conditioned Imitation for Offline Reinforcement Learning',\n",
       "  'author': ['X Feng', 'L Jiang', 'X Yu', 'H Xu', 'X Sun'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'IEEE Transactions …',\n",
       "  'abstract': 'Offline reinforcement learning (RL) enables learning policies from pre-collected datasets without online data collection. Although it offers the possibility to surpass the performance of'},\n",
       " {'title': 'Robot peels banana with goal-conditioned dual-action deep imitation learning',\n",
       "  'author': ['H Kim', 'Y Ohmura', 'Y Kuniyoshi'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2203.09749',\n",
       "  'abstract': 'A long-horizon dexterous robot manipulation task of deformable objects, such as banana peeling, is problematic because of difficulties in object modeling and a lack of knowledge'},\n",
       " {'title': 'Chain-of-Thought Predictive Control',\n",
       "  'author': ['Z Jia', 'F Liu', 'V Thumuluri', 'L Chen', 'Z Huang'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We study generalizable policy learning from demonstrations for complex low-level control tasks (eg, contact-rich object manipulations). We propose an imitation learning method that'},\n",
       " {'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures',\n",
       "  'author': ['K Kutsuzawa', 'M Hayashibe'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'IEEE Access',\n",
       "  'abstract': 'Imitation learning is a promising approach for robots to learn complex motor skills. Recent techniques allow robots to learn long-term movements comprising multiple sub-behaviors'},\n",
       " {'title': 'PLEX: Making the Most of the Available Data for Robotic Manipulation Pretraining',\n",
       "  'author': ['G Thomas', 'CA Cheng', 'R Loynd', 'V Vineet'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'A rich representation is key to general robotic manipulation, but existing model architectures require a lot of data to learn it. Unfortunately, ideal robotic manipulation training data, which'},\n",
       " {'title': 'Towards Vygotskian Autotelic Agents: Learning Skills with Goals, Language and Intrinsically Motivated Deep Reinforcement Learning',\n",
       "  'author': ['C Colas'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Building autonomous machines that can explore large environments, discover interesting interactions and learn open-ended repertoires of skills is a long-standing goal in artificial'},\n",
       " {'title': 'Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based Reinforcement Learning',\n",
       "  'author': ['D Brandfonbrener', 'S Tu', 'A Singh', 'S Welker'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To'},\n",
       " {'title': 'Learning latent actions without human demonstrations',\n",
       "  'author': ['SA Mehta', 'S Parekh', 'DP Losey'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… International Conference on …',\n",
       "  'abstract': \"We can make it easier for disabled users to control assistive robots by mapping the user's low-dimensional joystick inputs to high-dimensional, complex actions. Prior works learn\"},\n",
       " {'title': 'Explore, discover and learn: Unsupervised discovery of state-covering skills',\n",
       "  'author': ['V Campos Camúñez', 'A Trott', 'C Xiong'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'ICML 2020, Thirty …',\n",
       "  'abstract': 'Acquiring abilities in the absence of a task-oriented reward function is at the frontier of reinforcement learning research. This problem has been studied through the lens of'},\n",
       " {'title': 'Learning to play by imitating humans',\n",
       "  'author': ['R Dinyari', 'P Sermanet', 'C Lynch'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2006.06874',\n",
       "  'abstract': 'Acquiring multiple skills has commonly involved collecting a large number of expert demonstrations per task or engineering custom reward functions. Recently it has been'},\n",
       " {'title': 'Get Back Here: Robust Imitation by Return-to-Distribution Planning',\n",
       "  'author': ['G Cideron', 'B Tabanpour', 'S Curi', 'S Girgin'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting'},\n",
       " {'title': 'A formal methods approach to interpretability, safety and composability for reinforcement learning',\n",
       "  'author': ['X Li'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Robotic systems that are capable of learning from experience have recently become more common place. These systems have demonstrated success in learning difficult control tasks'},\n",
       " {'title': 'Distance Weighted Supervised Learning for Offline Interaction Data',\n",
       "  'author': ['J Hejna', 'J Gao', 'D Sadigh'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv:2304.13774',\n",
       "  'abstract': 'Sequential decision making algorithms often struggle to leverage different sources of unstructured offline interaction data. Imitation learning (IL) methods based on supervised'},\n",
       " {'title': 'Datasets for data-driven reinforcement learning',\n",
       "  'author': ['J Fu', 'A Kumar', 'O Nachum', 'G Tucker'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The offline reinforcement learning (RL) problem, also referred to as batch RL, refers to the setting where a policy must be learned from a dataset of previously collected data, without'},\n",
       " {'title': 'Learning impedance actions for safe reinforcement learning in contact-rich tasks',\n",
       "  'author': ['Q Yang', 'A Dürr', 'EA Topp', 'JA Stork'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NeurIPS 2021 Workshop …',\n",
       "  'abstract': 'Reinforcement Learning (RL) has the potential of solving complex continuous control tasks, with direct applications to robotics. Nevertheless, current stateof-the-art methods are'},\n",
       " {'title': 'Dances with robots: Choreographing, correcting, and performing with moving machines',\n",
       "  'author': ['C Cuan'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'TDR',\n",
       "  'abstract': 'What does it feel like to dance with a robot? How do you choreograph one? Working with robots during three artistic residencies and two research projects has raised questions about'},\n",
       " {'title': 'Learning Dynamic Manipulation Skills from Haptic-Play',\n",
       "  'author': ['T Lee', 'D Sung', 'K Choi', 'C Lee', 'C Park'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'In this paper, we propose a data-driven skill learning approach to solve highly dynamic manipulation tasks entirely from offline teleoperated play data. We use a bilateral'},\n",
       " {'title': 'Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data',\n",
       "  'author': ['H Zhou', 'Z Bing', 'X Yao', 'X Su', 'C Yang', 'K Huang'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots'},\n",
       " {'title': 'A Proprioceptive Haptic Device Design for Teaching Bimanual Manipulation',\n",
       "  'author': ['C Lee', 'T Lee', 'JK Min', 'A Wang', 'SP Lee'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Manipulation involves a broad spectrum of skills, eg, polishing, peeling, flipping, screwing, etc., requiring complex and delicate control over both force and position. This paper aims at'},\n",
       " {'title': 'Learn proportional derivative controllable latent space from pixels',\n",
       "  'author': ['W Wang', 'M Kobilarov', 'GD Hager'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '2022 IEEE 18th …',\n",
       "  'abstract': 'Recent advances in latent space dynamics model from pixels show promising progress in vision-based model predictive control (MPC). However, executing MPC in real time can be'},\n",
       " {'title': 'A Glimpse in ChatGPT Capabilities and its impact for AI research',\n",
       "  'author': ['F Joublin', 'A Ceravola', 'J Deigmoeller'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon'},\n",
       " {'title': 'Distilling a hierarchical policy for planning and control via representation and reinforcement learning',\n",
       "  'author': ['JS Ha', 'YJ Park', 'HJ Chae', 'SS Park'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': '2021 IEEE International …',\n",
       "  'abstract': 'We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for'},\n",
       " {'title': 'Machine Learning for Robotic Manipulation',\n",
       "  'author': ['Q Vuong'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2101.00755',\n",
       "  'abstract': 'The past decade has witnessed the tremendous successes of machine learning techniques in the supervised learning paradigm, where there is a clear demarcation between training'},\n",
       " {'title': 'Multi-goal Reinforcement Learning via Exploring Successor Matching',\n",
       "  'author': ['X Feng'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '2022 IEEE Conference on Games (CoG)',\n",
       "  'abstract': 'Multi-goal reinforcement learning (RL) agent aims at achieving and generalizing over various goals. Due to the sparsity of goal-reaching rewards, it suffers from unreliable value'},\n",
       " {'title': 'Time reversal as self-supervision',\n",
       "  'author': ['S Nair', 'M Babaeizadeh', 'C Finn', 'S Levine'],\n",
       "  'pub_year': '2018',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'A longstanding challenge in robot learning for manipulation tasks has been the ability to generalize to varying initial conditions, diverse objects, and changing objectives. Learning'},\n",
       " {'title': 'Continuous Control with Action Quantization from Demonstrations',\n",
       "  'author': ['R Dadashi', 'L Hussenot', 'D Vincent', 'S Girgin'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'In Reinforcement Learning (RL), discrete actions, as opposed to continuous actions, result in less complex exploration problems and the immediate computation of the maximum of the'},\n",
       " {'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning',\n",
       "  'author': ['T Ablett', 'B Chan', 'J Kelly'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv:2112.08932',\n",
       "  'abstract': 'Effective exploration continues to be a significant challenge that prevents the deployment of reinforcement learning for many physical systems. This is particularly true for systems with'},\n",
       " {'title': 'A development cycle for automated self-exploration of robot behaviors',\n",
       "  'author': ['TM Roehr', 'D Harnack', 'H Wöhrle'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'AI …',\n",
       "  'abstract': 'In this paper we introduce Q-Rock, a development cycle for the automated self-exploration and qualification of robot behaviors. With Q-Rock, we suggest a novel, integrative approach'},\n",
       " {'title': 'ESportsU digital warrior camp: Creating an esports-based culturally relevant computing living learning camp',\n",
       "  'author': ['JA Engerman', 'RF Otto', 'M VanAuken'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'Handbook of research on …',\n",
       "  'abstract': 'The authors share two case studies that provide preliminary data for a National Science Foundation Innovative Technology Experiences for Students and Teachers award at the'},\n",
       " {'title': 'Aligning Robot Representations with Humans',\n",
       "  'author': ['A Bobu', 'A Peng'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'arXiv preprint arXiv:2205.07882',\n",
       "  'abstract': 'As robots are increasingly deployed in real-world scenarios, a key question is how to best transfer knowledge learned in one environment to another, where shifting constraints and'},\n",
       " {'title': 'Skill Learning for Long-Horizon Sequential Tasks',\n",
       "  'author': ['J Alves', 'N Lau', 'F Silva'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Progress in Artificial Intelligence: 21st EPIA …',\n",
       "  'abstract': 'Solving long-horizon problems is a desirable property in autonomous agents. Learning reusable behaviours can equip the agent with this property, allowing it to adapt them when'},\n",
       " {'title': 'ToolNet: Using Commonsense Generalization for Predicting Tool Use for Robot Plan Synthesis',\n",
       "  'author': ['R Bansal', 'S Tuli', 'R Paul'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'arXiv preprint arXiv:2006.05478',\n",
       "  'abstract': 'A robot working in a physical environment (like home or factory) needs to learn to use various available tools for accomplishing different tasks, for instance, a mop for cleaning and'},\n",
       " {'title': 'Trajectory adjustment for nonprehensile manipulation using latent space of trained sequence-to-sequence model',\n",
       "  'author': ['K Kutsuzawa', 'S Sakaino', 'T Tsuji'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'Advanced Robotics',\n",
       "  'abstract': 'When robots are used to manipulate objects in various ways, they often have to consider the dynamic constraint. Machine learning is a good candidate for such complex trajectory'},\n",
       " {'title': 'Latent Hierarchical Imitation Learning for Stochastic Environments',\n",
       "  'author': ['M Igl', 'P Shah', 'P Mougin', 'S Srinivasan', 'T Gupta'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Many applications of imitation learning require the agent to avoid mode collapse and mirrorthe full distribution of observed behaviors. Existing methods improving this'},\n",
       " {'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning',\n",
       "  'author': ['P Sermanet', 'C Lynch'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '5th Annual Conference on Robot …',\n",
       "  'abstract': 'Long-horizon planning in realistic environments requires the ability to reason over sequential tasks in high-dimensional state spaces with complex dynamics. Classical motion'},\n",
       " {'title': 'Improving Meta-imitation Learning with Focused Task Embedding',\n",
       "  'author': ['YF Lin', 'CK Ho', 'CT King'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… Systems and Applications: Proceedings of the …',\n",
       "  'abstract': 'Meta-imitation learning has been applied to enable robots to quickly generalize the learned tasks to perform new tasks. The basic idea is to encode tasks into meaningful embeddings'},\n",
       " {'title': 'Visual Prediction of Priors for Articulated Object Interaction',\n",
       "  'author': ['C Moses', 'M Noseworthy', 'LP Kaelbling'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'Exploration in novel settings can be challenging without prior experience in similar domains. However, humans are able to build on prior experience quickly and efficiently. Children'},\n",
       " {'title': 'Compositional Reasoning in Robot Learning',\n",
       "  'author': ['D Xu'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'To carry out diverse tasks in everyday human environments, future robots must generalize beyond the knowledge they are equipped with. However, despite recent advances in\" end-to'},\n",
       " {'title': 'Structured Motion Generation with Predictive Learning: Proposing Subgoal for Long-Horizon Manipulation',\n",
       "  'author': ['N Saito', 'J Moura', 'T Ogata'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': '… on Robotics and …',\n",
       "  'abstract': 'For assisting humans in their daily lives, robots need to perform long-horizon tasks, such as tidying up a room or preparing a meal. One effective strategy for handling a long-horizon'},\n",
       " {'title': 'Design and training of deep reinforcement learning agents',\n",
       "  'author': ['F Pardo'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Deep reinforcement learning is a field of research at the intersection of reinforcement learning and deep learning. On one side, the problem that researchers address is the one of'},\n",
       " {'title': 'Learning, Planning, and Acting with Models',\n",
       "  'author': ['T Kurutach'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'While the classical approach to planning and control has enabled robots to achieve various challenging control tasks, it requires domain experts to specify transition dynamics as well'},\n",
       " {'title': 'Self-taught Robots: Autonomous and Weakly-Supervised Learning for Robotic Manipulation',\n",
       "  'author': ['M Alakuijala'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Despite significant advances in machine learning in recent years, robotic control learned from data has yet to show large-scale impact in the real world. One of the main limitations is'},\n",
       " {'title': 'Leveraging Humans to Detect and Fix Representation Misalignment',\n",
       "  'author': ['A Peng'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'As robots are increasingly deployed in real-world environments, a key question be-comes how to best teach them to accomplish tasks that end users want. A critical problem suffered'},\n",
       " {'title': 'Robust Flight Navigation with Liquid Neural Networks',\n",
       "  'author': ['P Kao'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Autonomous robots can learn to perform visual navigation tasks from offline human demonstrations, and generalize well to online and unseen scenarios within the same'},\n",
       " {'title': 'Goal-Directed Exploration and Skill Reuse',\n",
       "  'author': ['V Pong'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'General-purpose robots that exhibit a broad range of capabilities in unstructured environments would greatly advance the utility of robotics. However, robot applications have'},\n",
       " {'title': 'Task-Oriented Manipulation Planning: Teaching Robot Manipulators to Learn Trajectory Tasks',\n",
       "  'author': ['Y Li'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'As robot manipulator applications are conducted in more complex tasks and unstructured environments, traditional manual programming cannot match the growing requirements'},\n",
       " {'title': 'How to Train Your Robot: Techniques for Enabling Robotic Learning in the Real World',\n",
       "  'author': ['A Gupta'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'The ultimate goal for many roboticists is to build robotic systems that are able to master complex skills involving object interaction, contact rich manipulation and unmapped'},\n",
       " {'title': 'Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks',\n",
       "  'author': ['T Ablett', 'B Chan', 'J Kelly'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'IEEE Robotics and Automation …',\n",
       "  'abstract': 'Adversarial imitation learning (AIL) has become a popular alternative to supervised imitation learning that reduces the distribution shift suffered by the latter. However, AIL requires'},\n",
       " {'title': 'Real World Robot Learning: Learned Rewards, Offline Datasets and Skill Re-Use',\n",
       "  'author': ['A Singh'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'The last decade witnessed an unprecedented explosion in AI-powered applications: our phones unlock via face recognition, can recognize our speech with near-perfection, and are'},\n",
       " {'title': 'Human-in-the-Loop Reinforcement Learning for Adaptive Assistive Interfaces',\n",
       "  'author': ['J Gao'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Figure 1.1: We formulate assistive typing as a human-in-the-loop decision-making problem, in which the interface observes user inputs (eg, neural activity measured by a brain implant)'},\n",
       " {'title': 'Learning to Align Multimodal Data for Static and Dynamic Tasks',\n",
       "  'author': ['S Paul'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Our experience of the world is multimodal-we see objects, hear sounds, and read texts to perceive information. In order for Artificial Intelligence to make progress in understanding the'},\n",
       " {'title': 'Robots that can see: Learning visually guided behavior',\n",
       "  'author': ['A Pashevich'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Recently, vision and learning made significant progress that could improve robot control policies for complex environments. In this thesis, we introduce novel methods for learning'},\n",
       " {'title': 'Using Google Classroom to Increase Students Motivation to Learn English at SMP Negeri 1 Limboto Gorontalo',\n",
       "  'author': ['M Muziatun', 'H Fatsah'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '… , Sosial, dan Budaya',\n",
       "  'abstract': 'Teachers experiences might restrict their utilization of technology in the classroom, making learning challenging. Teachers must improve learning performance and motivation to'},\n",
       " {'title': 'The Nature of Information, Semantics, and Effectiveness for Artificial Intelligence and Cognition',\n",
       "  'author': ['K Brown'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'This manuscript puts forward claims to help address foundational gaps in understanding Cognition and Artificial General Intelligence (AGI), including the nature of Emergence'},\n",
       " {'title': 'Graph-based distributied motion planning intightmulti-lane platoons',\n",
       "  'author': ['GE Pizarro Lorca'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Platooning of connected and automated vehicles (CAVs) has received extensive interest due to its potential to improve road traffic. In this context, multi-lane platoons have appeared'},\n",
       " {'title': 'Discovery and learning of navigation goals from pixels in Minecraft',\n",
       "  'author': ['JJ Nieto Salas'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Pre-training Reinforcement Learning (RL) agents in a task-agnostic manner has shown promising results. However, previous works still struggle to learn and discover meaningful'},\n",
       " {'title': 'Learning multi-stage tasks with one demonstration via self-replay',\n",
       "  'author': ['E Johns', 'N Di Palo'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': 'In this work, we introduce a novel method to learn everyday-like multistage tasks from a single human demonstration, without requiring any prior object knowledge. Inspired by the'},\n",
       " {'title': 'VARIATIONAL REPARAMETRIZED POLICY LEARNING WITH DIFFERENTIABLE PHYSICS',\n",
       "  'author': ['Z Huang', 'L Liang', 'Z Ling', 'X Li', 'C Gan', 'H Su'],\n",
       "  'pub_year': 'NA',\n",
       "  'venue': '… Learning Workshop NeurIPS …',\n",
       "  'abstract': 'We study the problem of policy parameterization for reinforcement learning (RL) with high-dimensional continuous action space. Our goal is to find a good way to parameterize the'},\n",
       " {'title': 'Skill Acquisition by Instruction Augmentation on Offline Datasets',\n",
       "  'author': ['T Xiao', 'H Chan', 'P Sermanet', 'A Wahid', 'A Brohan'],\n",
       "  'pub_year': 'NA',\n",
       "  'venue': '… Foundation Models for …',\n",
       "  'abstract': 'In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot'},\n",
       " {'title': 'Acquiring Motor Skills Through Motion Imitation and Reinforcement Learning',\n",
       "  'author': ['XB Peng'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Humans are capable of performing awe-inspiring feats of agility by drawing from a vast repertoire of diverse and sophisticated motor skills. This dynamism is in sharp contrast to the'},\n",
       " {'title': 'Deep learning that scales: leveraging compute and data',\n",
       "  'author': ['V Campos Camúñez'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Deep learning has revolutionized the field of artificial intelligence in the past decade. Although the development of these techniques spans over several years, the recent advent'},\n",
       " {'title': 'INTEGRATING USABILITY WITH TASK PERFORMANCE FOR SHARED AUTONOMY',\n",
       "  'author': ['B Paulhamus'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'As robots become more complex, the degrees-of-freedom (DoF) for controlling them is rapidly outpacing the degrees-of-control that can be supplied by humans via conventional'},\n",
       " {'title': 'SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling',\n",
       "  'author': ['J Zhang', 'K Pertsch', 'J Zhang', 'T Nam', 'SJ Hwang'],\n",
       "  'pub_year': 'NA',\n",
       "  'venue': '… Workshop NeurIPS 2022',\n",
       "  'abstract': \"We propose SPRINT, an approach for scalable offline policy pre-training based on natural language instructions. SPRINT pre-trains an agent's policy to execute a diverse set of\"},\n",
       " {'title': 'Graph-Based Distributied Motion Planning in Tight Multi-Lane Platoons',\n",
       "  'author': ['GEP Lorca'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Platooning of connected and automated vehicles (CAVs) has received extensive interest due to its potential to improve road traffic. In this context, multi-lane platoons have appeared'},\n",
       " {'title': 'An adaptive imitation learning framework for robotic complex contact-rich insertion tasks',\n",
       "  'author': ['Y Wang', 'CC Beltran-Hernandez', 'W Wan'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': 'Frontiers in Robotics …',\n",
       "  'abstract': 'Complex contact-rich insertion is a ubiquitous robotic manipulation skill and usually involves nonlinear and low-clearance insertion trajectories as well as varying force requirements. A'},\n",
       " {'title': 'Learning to generalise through features',\n",
       "  'author': ['D Grebenyuk'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': ''},\n",
       " {'title': 'Inducing Reusable Skills From Demonstrations with Option-Controller Network',\n",
       "  'author': ['S Zhou', 'Y Shen', 'Y Lu', 'A Courville', 'JB Tenenbaum'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': 'Humans can decompose previous experiences into skills and reuse them to enable fast learning in the future. Inspired by this process, we propose a new model called Option'},\n",
       " {'title': 'Visual Backtracking Teleoperation: A Data Collection Protocol for Offline Image-Based RL',\n",
       "  'author': ['D Brandfonbrener', 'S Tu', 'A Singh', 'S Welker'],\n",
       "  'pub_year': 'NA',\n",
       "  'venue': '3rd Offline RL Workshop …',\n",
       "  'abstract': 'We consider how to most efficiently leverage teleoperator time to collect data for learning robust image-based value functions and policies for sparse reward robotic tasks. To'},\n",
       " {'title': 'Learning from Play for Deformable Object Manipulation',\n",
       "  'author': ['A Bahety'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': 'Deformable object manipulation is a challenging task in robotics due to the complex dynamics of deformable objects as compared to rigid objects and the unlimited degrees of'},\n",
       " {'title': 'Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks',\n",
       "  'author': ['O Mees', 'W Burgard'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': 'I. INTRODUCTION “... spend the summer linking a camera to a computer and getting the computer to describe what it saw.” Marvin Minsky on the goal of a 1966 undergraduate'},\n",
       " {'title': 'Self-Generating Data for Goal-Conditioned Compositional Problems',\n",
       "  'author': ['Y Yuan', 'Y Li', 'Y Wu'],\n",
       "  'pub_year': '2023',\n",
       "  'venue': 'Workshop on Reincarnating Reinforcement …',\n",
       "  'abstract': 'Building reinforcement learning agents that are generalizable to compositional problems has long been a research challenge. Recent success relies on a pre-existing dataset of rich'},\n",
       " {'title': 'One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control',\n",
       "  'author': ['H Watahiki', 'Y Tsuruoka'],\n",
       "  'pub_year': 'NA',\n",
       "  'venue': 'ICLR 2022 Workshop on Generalizable Policy …',\n",
       "  'abstract': 'Recent advances in skill learning from a task-agnostic offline dataset enable the agent to acquire various skills that can be used as primitives to perform long-horizon imitation'},\n",
       " {'title': 'データ中心の視点から捉える深層強化学習',\n",
       "  'author': ['古田拓毅'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '人工知能',\n",
       "  'abstract': '図 1 AI や機械学習によるシステムは, 大きく分けてアルゴリズム (モデル) とデータセットという二つの要素からなる. データ中心の AI (Data-Centric AI) とは, アルゴリズムやモデルを最適化する'},\n",
       " {'title': 'Task Focused Robotic Imitation Learning',\n",
       "  'author': ['P Abolghasemi'],\n",
       "  'pub_year': '2019',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'For many years, successful applications of robotics were the domain of controlled environments, such as industrial assembly lines. Such environments are custom designed'},\n",
       " {'title': 'Representation learning of scene images for task and motion planning',\n",
       "  'author': ['ST Nguyen'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'This thesis investigates two different methods to learn a state representation from only image observations for task and motion planning (TAMP) problems. Our first method integrates a'},\n",
       " {'title': 'Hierarchical adversarial imitation learning from motion capture data',\n",
       "  'author': ['S Hagenmayer'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'A lot of problems in Imitation Learning can be split into multiple low-level tasks that need to be executed in sequence or parallel. Likewise in the area of human motion prediction, the'},\n",
       " {'title': 'Not playing by the rules: Exploratory play, rational action, and efficient search',\n",
       "  'author': ['J Chu', 'LE Schulz'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': \"Recent studies suggest children's exploratory play is consistent with formal accounts of rational learning. Here we focus on the tension between this view and a nearly ubiquitous\"},\n",
       " {'title': 'Exploration of Teacher-Centered and Task-Centered paradigms for efficient transfer of skills between morphologically distinct robots',\n",
       "  'author': ['M Mounsif'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'NA',\n",
       "  'abstract': 'Recently, it has been possible to observe the acceleration of robot deployment in domains beyondthe usual industrial and manufacturing framework. However, for the majority of'},\n",
       " {'title': 'A reduced-order approach to assist with reinforcement learning for underactuated robotics',\n",
       "  'author': ['J Augot', 'AJ Snoswell', 'SPN Singh'],\n",
       "  'pub_year': '2020',\n",
       "  'venue': 'Proceedings of the …',\n",
       "  'abstract': 'Underactuated robot designs are enticing due to their electromechanical simplicity; but, their operation is complex especially for compliant designs that may not have an explicit model'},\n",
       " {'title': 'Learning to Reason: Distilling Hierarchy via Self-Supervision and Reinforcement Learning',\n",
       "  'author': ['JS Ha', 'YJ Park', 'HJ Chae', 'SS Park', 'HL Choi'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': 'We present a hierarchical planning and control framework that enables an agent to perform various tasks and adapt to a new task flexibly. Rather than learning an individual policy for'},\n",
       " {'title': '深層学習を用いた協働ロボットによる作業目標の推論と動的修正の実現',\n",
       "  'author': ['平松駿， 村田真悟'],\n",
       "  'pub_year': '2022',\n",
       "  'venue': '人工知能学会全国大会論文集 第 36 回 (2022)',\n",
       "  'abstract': '抄録 ロボットが人と協働作業をするには, 作業目標が共有されている必要がある. 目標の共有方法には, 画像等であらかじめ伝える方法, ロボットが自ら推論する方法がある'},\n",
       " {'title': 'Des robots qui voient: apprentissage de comportements guidés par la vision',\n",
       "  'author': ['A Pashevich'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'NA',\n",
       "  'abstract': \"Résumé Récemment, la vision par ordinateur et l'apprentissage automatique ont fait des progrès significatifs qui pourraient améliorer le contrôle des robots dans les environnements\"},\n",
       " {'title': 'Evaluating Generalization of Policy Learning Under Domain Shifts',\n",
       "  'author': ['E Xing', 'A Gupta', 'S Powers', 'V Dean'],\n",
       "  'venue': 'NA',\n",
       "  'pub_year': 'NA',\n",
       "  'abstract': ''}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[42]['citedby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94840900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "citedby already exists\n",
      "no update\n",
      "paper 1 updated\n",
      "\n",
      "Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress\n",
      "Hierarchical Learning with Unsupervised Skill Discovery for Highway Merging Applications\n",
      "update pkl\n",
      "paper 2 updated\n",
      "\n",
      "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'citedby_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rw\u001b[39m.\u001b[39;49mupdate_all_citedby()\n",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_all_citedby\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpaper_list\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_citedby(key)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_pkl()\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mpaper \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m updated\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(key))\n",
      "\u001b[1;32md:\\OneDrive - GIST\\Code\\paper_info_crawl\\build_relationship_btw_related_works.ipynb Cell 17\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         citation_list \u001b[39m=\u001b[39m scholarly\u001b[39m.\u001b[39;49mcitedby(paper[\u001b[39m'\u001b[39;49m\u001b[39minfo\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive%20-%20GIST/Code/paper_info_crawl/build_relationship_btw_related_works.ipynb#X15sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m MaxTriesExceededException \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\_scholarly.py:287\u001b[0m, in \u001b[0;36m_Scholarly.citedby\u001b[1;34m(self, object)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnum_citations\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m PublicationParser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__nav)\u001b[39m.\u001b[39;49mcitedby(\u001b[39mobject\u001b[39;49m)\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mSince the paper titled \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m has \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m citations (>1000), \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mfetching it on an annual basis.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mbib\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39mobject\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mnum_citations\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    292\u001b[0m year_end \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(datetime\u001b[39m.\u001b[39mdate\u001b[39m.\u001b[39mtoday()\u001b[39m.\u001b[39myear)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\scholarly\\publication_parser.py:389\u001b[0m, in \u001b[0;36mPublicationParser.citedby\u001b[1;34m(self, publication)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m publication[\u001b[39m'\u001b[39m\u001b[39mfilled\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    388\u001b[0m     publication \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill(publication)\n\u001b[1;32m--> 389\u001b[0m \u001b[39mreturn\u001b[39;00m _SearchScholarIterator(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnav, publication[\u001b[39m'\u001b[39;49m\u001b[39mcitedby_url\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'citedby_url'"
     ]
    }
   ],
   "source": [
    "rw.update_all_citedby()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b7034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75ab878",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_list = {}\n",
    "f = open('paper_list.txt', 'r')\n",
    "lines = f.readlines()\n",
    "\n",
    "for i, line in enumerate(lines):\n",
    "    items = line.split('_')\n",
    "    year = items[0]\n",
    "    title = items[1].split('\\n')[0]\n",
    "    #print(i+1, year, title)\n",
    "    paper_list[i+1] = {'title': title, 'year': year}\n",
    "    #print(paper_list[i+1])\n",
    "    #search_query = scholarly.search_pubs(title)\n",
    "    #first_author_result = next(search_query)\n",
    "    #print(first_author_result)\n",
    "    #citation_list = scholarly.citedby(first_author_result)\n",
    "    #cl = []\n",
    "    #for c in citation_list:\n",
    "    #    title = c['bib']['title']\n",
    "    #    year = c['bib']['pub_year']\n",
    "    #    cl.append({'title': title, 'year': year})\n",
    "    #paper_list[i+1]['cited_by'] = cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0827412f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'container_type': 'Publication',\n",
       " 'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>,\n",
       " 'bib': {'title': 'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics',\n",
       "  'author': ['K Rana', 'V Dasagi', 'J Haviland', 'B Talbot'],\n",
       "  'pub_year': '2021',\n",
       "  'venue': 'arXiv preprint arXiv …',\n",
       "  'abstract': 'We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. We study BCF on two real-world robotics tasks'},\n",
       " 'filled': False,\n",
       " 'gsrank': 1,\n",
       " 'pub_url': 'https://arxiv.org/abs/2107.09822',\n",
       " 'author_id': ['-hYjPxsAAAAJ', 'BrhJ6-EAAAAJ', 'IIhSLvgAAAAJ', 'oDCvYTEAAAAJ'],\n",
       " 'url_scholarbib': '/scholar?hl=en&q=info:vF79EAeHhdcJ:scholar.google.com/&output=cite&scirp=0&hl=en',\n",
       " 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2Bcontroller%2Bfusion:%2BLeveraging%2Bcontrol%2Bpriors%2Bin%2Bdeep%2Breinforcement%2Blearning%2Bfor%2Brobotics%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=vF79EAeHhdcJ&ei=CCR8ZOEovY_qtA_zmYuYBw&json=',\n",
       " 'num_citations': 10,\n",
       " 'citedby_url': '/scholar?cites=15529967354476584636&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       " 'url_related_articles': '/scholar?q=related:vF79EAeHhdcJ:scholar.google.com/&scioq=Bayesian+controller+fusion:+Leveraging+control+priors+in+deep+reinforcement+learning+for+robotics&hl=en&as_sdt=0,33',\n",
       " 'eprint_url': 'https://arxiv.org/pdf/2107.09822'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_query = scholarly.search_pubs('Learning Latent Plans from Play')\n",
    "first_author_result = next(search_query)\n",
    "print(first_author_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5795c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_list = scholarly.citedby(first_author_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "83663088",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = []\n",
    "for i, c in enumerate(citation_list):\n",
    "    cl.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4abfc5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cited by\n",
      "2023 Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics\n",
      "2022 Robot learning of mobile manipulation with reachability behavior priors\n"
     ]
    }
   ],
   "source": [
    "print('cited by')\n",
    "for c in cl:\n",
    "    title = c['bib']['title']\n",
    "    year = c['bib']['pub_year']\n",
    "    #print(title)\n",
    "    for key in paper_list.keys():\n",
    "        if paper_list[key]['title'].lower() == title.lower():\n",
    "            print(year, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c38ad63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 관련 논문 딕셔너리 작성\n",
    "# 각 논문별 어떤 논문으로부터 인용되었는지 찾기 -> 딕셔너리에 저장\n",
    "# 마인드맵에 반영\n",
    "# 핵심 논문 선정 -> 내용 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d325a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from pkl\n"
     ]
    }
   ],
   "source": [
    "rw = RelatedWork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb35fc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "6 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "8 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "35 Learning quadrupedal locomotion over challenging terrain\n",
      "37 Multi-expert learning of adaptive legged locomotion\n",
      "38 Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer\n",
      "39 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "40 Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information\n",
      "41 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "43 LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "44 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "45 Neural probabilistic motor primitives for humanoid control\n",
      "46 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "47 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "48 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "49 CompILE: Compositional Imitation Learning and Execution\n",
      "50 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "51 Learning an embedding space for transferable robot skills\n",
      "52 A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "53 DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "54 Feudal networks for hierarchical reinforcement learning\n",
      "55 Stochastic neural networks for hierarchical reinforcement learning\n",
      "56 The option-critic architecture\n",
      "57 Learning and Transfer of Modulated Locomotor Controllers\n"
     ]
    }
   ],
   "source": [
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    if 'citedby' not in paper:\n",
    "         print(key, paper['title'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8b093ca",
   "metadata": {},
   "source": [
    "# Load citedby list from html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ab4b586",
   "metadata": {},
   "source": [
    "### Generate instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d0e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from pkl\n"
     ]
    }
   ],
   "source": [
    "rw = RelatedWork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eac03f5c",
   "metadata": {},
   "source": [
    "### Update citedby page list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff6a3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=523997480481690644&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=18041795639441247667&as_sdt=5,33&sciodt=0,33&hl=en&oe=ASCII\n",
      "update pkl\n",
      "max_page:  0\n",
      "No citation\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=4828484264646918391&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=5877439646516921010&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=5877439646516921010&scipsc=\n",
      "update pkl\n",
      "max_page:  0\n",
      "No citation\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=12891288254842573229&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  0\n",
      "No citation\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=9163542561991441433&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=8889692761740655853&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=9889514503886316391&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=1057545518662014941&as_sdt=5,33&sciodt=0,33&hl=en&oe=ASCII\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=76025098096065425&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=15968271079323780218&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=8367908304037497189&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=8367908304037497189&scipsc=\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=13425458595968178952&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=10914552444848446816&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=15907975694198974812&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  4\n",
      "https://scholar.google.co.kr/scholar?cites=14356602524143341638&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=14356602524143341638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=14356602524143341638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=14356602524143341638&scipsc=\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=15529967354476584636&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  4\n",
      "https://scholar.google.co.kr/scholar?cites=1986650243805735458&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=1986650243805735458&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=1986650243805735458&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=1986650243805735458&scipsc=\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=11314236649785473138&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=11314236649785473138&scipsc=\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=435773060438895549&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=435773060438895549&scipsc=\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=15461268367576192426&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=15461268367576192426&scipsc=\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=1668549134726216054&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  2\n",
      "https://scholar.google.co.kr/scholar?cites=13311269075007662914&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=13311269075007662914&scipsc=\n",
      "update pkl\n",
      "max_page:  3\n",
      "https://scholar.google.co.kr/scholar?cites=6403653946569400429&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=6403653946569400429&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=6403653946569400429&scipsc=\n",
      "update pkl\n",
      "max_page:  3\n",
      "https://scholar.google.co.kr/scholar?cites=13031874054704232682&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=13031874054704232682&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=13031874054704232682&scipsc=\n",
      "update pkl\n",
      "max_page:  14\n",
      "https://scholar.google.co.kr/scholar?cites=12759957383784422449&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=12759957383784422449&scipsc=\n",
      "update pkl\n",
      "max_page:  3\n",
      "https://scholar.google.co.kr/scholar?cites=12112480548646894252&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=12112480548646894252&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=12112480548646894252&scipsc=\n",
      "update pkl\n",
      "max_page:  8\n",
      "https://scholar.google.co.kr/scholar?cites=14885288945978947032&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=14885288945978947032&scipsc=\n",
      "update pkl\n",
      "max_page:  4\n",
      "https://scholar.google.co.kr/scholar?cites=12004036644938436040&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=12004036644938436040&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=12004036644938436040&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=12004036644938436040&scipsc=\n",
      "update pkl\n",
      "max_page:  4\n",
      "https://scholar.google.co.kr/scholar?cites=5054485303778649633&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=5054485303778649633&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=5054485303778649633&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=5054485303778649633&scipsc=\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=11608793867513526995&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  53\n",
      "https://scholar.google.co.kr/scholar?cites=6553865482301757638&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=370&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=380&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=390&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=400&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=410&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=420&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=430&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=440&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=450&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=460&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=470&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=480&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=490&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=500&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=510&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=520&hl=en&as_sdt=5,33&sciodt=0,33&cites=6553865482301757638&scipsc=\n",
      "update pkl\n",
      "max_page:  4\n",
      "https://scholar.google.co.kr/scholar?cites=13301690220356516621&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=13301690220356516621&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=13301690220356516621&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=13301690220356516621&scipsc=\n",
      "update pkl\n",
      "max_page:  10\n",
      "https://scholar.google.co.kr/scholar?cites=13221854970087621935&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=13221854970087621935&scipsc=\n",
      "update pkl\n",
      "max_page:  1\n",
      "https://scholar.google.co.kr/scholar?cites=6723836100639975451&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "update pkl\n",
      "max_page:  9\n",
      "https://scholar.google.co.kr/scholar?cites=837442000331224083&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=837442000331224083&scipsc=\n",
      "update pkl\n",
      "max_page:  6\n",
      "https://scholar.google.co.kr/scholar?cites=5359860145732970662&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=5359860145732970662&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=5359860145732970662&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=5359860145732970662&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=5359860145732970662&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=5359860145732970662&scipsc=\n",
      "update pkl\n",
      "max_page:  28\n",
      "https://scholar.google.co.kr/scholar?cites=17528482615651308176&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=17528482615651308176&scipsc=\n",
      "update pkl\n",
      "max_page:  25\n",
      "https://scholar.google.co.kr/scholar?cites=9659966346850413309&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=9659966346850413309&scipsc=\n",
      "update pkl\n",
      "max_page:  20\n",
      "https://scholar.google.co.kr/scholar?cites=11558193958091287134&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=11558193958091287134&scipsc=\n",
      "update pkl\n",
      "max_page:  16\n",
      "https://scholar.google.co.kr/scholar?cites=12493399866748517630&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=12493399866748517630&scipsc=\n",
      "update pkl\n",
      "max_page:  12\n",
      "https://scholar.google.co.kr/scholar?cites=11172180957185308522&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=11172180957185308522&scipsc=\n",
      "update pkl\n",
      "max_page:  21\n",
      "https://scholar.google.co.kr/scholar?cites=4478323851880436880&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=\n",
      "update pkl\n",
      "max_page:  7\n",
      "https://scholar.google.co.kr/scholar?cites=9780067471177651796&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=9780067471177651796&scipsc=\n",
      "update pkl\n",
      "max_page:  69\n",
      "https://scholar.google.co.kr/scholar?cites=8228365515476642671&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=370&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=380&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=390&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=400&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=410&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=420&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=430&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=440&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=450&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=460&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=470&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=480&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=490&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=500&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=510&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=520&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=530&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=540&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=550&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=560&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=570&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=580&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=590&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=600&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=610&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=620&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=630&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=640&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=650&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=660&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=670&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=680&hl=en&as_sdt=5,33&sciodt=0,33&cites=8228365515476642671&scipsc=\n",
      "update pkl\n",
      "max_page:  9\n",
      "https://scholar.google.co.kr/scholar?cites=12302759254570528216&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=12302759254570528216&scipsc=\n",
      "update pkl\n",
      "max_page:  18\n",
      "https://scholar.google.co.kr/scholar?cites=16469506517224271150&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=16469506517224271150&scipsc=\n",
      "update pkl\n",
      "max_page:  28\n",
      "https://scholar.google.co.kr/scholar?cites=1816310193271208067&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=1816310193271208067&scipsc=\n",
      "update pkl\n",
      "max_page:  38\n",
      "https://scholar.google.co.kr/scholar?cites=15352455767272452459&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=370&hl=en&as_sdt=5,33&sciodt=0,33&cites=15352455767272452459&scipsc=\n",
      "update pkl\n",
      "max_page:  8\n",
      "https://scholar.google.co.kr/scholar?cites=15439784310453297831&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=15439784310453297831&scipsc=\n",
      "update pkl\n",
      "max_page:  84\n",
      "https://scholar.google.co.kr/scholar?cites=2074247135017163310&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=370&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=380&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=390&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=400&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=410&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=420&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=430&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=440&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=450&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=460&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=470&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=480&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=490&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=500&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=510&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=520&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=530&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=540&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=550&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=560&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=570&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=580&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=590&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=600&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=610&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=620&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=630&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=640&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=650&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=660&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=670&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=680&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=690&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=700&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=710&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=720&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=730&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=740&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=750&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=760&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=770&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=780&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=790&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=800&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=810&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=820&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=830&hl=en&as_sdt=5,33&sciodt=0,33&cites=2074247135017163310&scipsc=\n",
      "update pkl\n",
      "max_page:  37\n",
      "https://scholar.google.co.kr/scholar?cites=15662620749719187568&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=15662620749719187568&scipsc=\n",
      "update pkl\n",
      "max_page:  99\n",
      "https://scholar.google.co.kr/scholar?cites=107676057328336895&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=210&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=220&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=230&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=240&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=250&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=260&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=270&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=280&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=290&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=300&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=310&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=320&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=330&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=340&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=350&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=360&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=370&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=380&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=390&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=400&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=410&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=420&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=430&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=440&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=450&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=460&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=470&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=480&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=490&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=500&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=510&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=520&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=530&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=540&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=550&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=560&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=570&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=580&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=590&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=600&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=610&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=620&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=630&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=640&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=650&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=660&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=670&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=680&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=690&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=700&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=710&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=720&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=730&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=740&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=750&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=760&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=770&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=780&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=790&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=800&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=810&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=820&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=830&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=840&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=850&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=860&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=870&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=880&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=890&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=900&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=910&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=920&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=930&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=940&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=950&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=960&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=970&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=980&hl=en&as_sdt=5,33&sciodt=0,33&cites=107676057328336895&scipsc=\n",
      "update pkl\n",
      "max_page:  20\n",
      "https://scholar.google.co.kr/scholar?cites=13819726433345104945&as_sdt=5,33&sciodt=0,33&hl=en\n",
      "https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=13819726433345104945&scipsc=\n",
      "update pkl\n"
     ]
    }
   ],
   "source": [
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    # maximum # of citedby page\n",
    "    max_page = math.ceil(paper['info']['num_citations']/10.0)\n",
    "    print('max_page: ', max_page)\n",
    "\n",
    "    # load citedby page url list\n",
    "    page_list = []\n",
    "\n",
    "    scholar_url = 'https://scholar.google.co.kr'\n",
    "    try:\n",
    "        first_page = scholar_url + paper['info']['citedby_url']\n",
    "        page_list.append(first_page)\n",
    "        print(first_page)\n",
    "\n",
    "        paper_id = first_page.split('cites=')[1].split('&')[0]\n",
    "\n",
    "        for page in range(max_page-1):\n",
    "            start = (page+1)*10\n",
    "            next_page = \"https://scholar.google.co.kr/scholar?start={}&hl=en&as_sdt=5,33&sciodt=0,33&cites={}&scipsc=\".format(start, paper_id)\n",
    "            page_list.append(next_page)\n",
    "            print(next_page)\n",
    "    except KeyError as e:\n",
    "        print('No citation')\n",
    "        \n",
    "    # save page list\n",
    "    rw.paper_list[key]['citedby_page_list'] = page_list\n",
    "    if 'citedby_paper_list' in paper:\n",
    "        rw.paper_list[key]['citedby_paper_num'] = len(paper['citedby_paper_list'])\n",
    "    else:\n",
    "        rw.paper_list[key]['citedby_paper_num'] = 0\n",
    "\n",
    "    # update pickle\n",
    "    rw.is_updated = True\n",
    "    rw.update_pkl()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ee3ca5",
   "metadata": {},
   "source": [
    "### Load citedby papers from citedby pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e51750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_papers_from_citedby_page(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    print(response)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Parse HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #print(soup)\n",
    "        body = soup.select('h3.gs_rt')\n",
    "        print(body)\n",
    "        paper_list = []\n",
    "        if len(body) != 0:\n",
    "            for i, b in enumerate(body):\n",
    "                paper = {}\n",
    "                title = b.text.split('] ')[-1]\n",
    "                print(title)\n",
    "                paper['title'] = title\n",
    "                try:\n",
    "                    paper['url'] = b.select('a')[0]['href']\n",
    "                except IndexError as e:\n",
    "                    print('No url')\n",
    "                paper_list.append(paper)\n",
    "            return paper_list\n",
    "        else:\n",
    "            print('failed to load citedby page')\n",
    "            return None\n",
    "    else:\n",
    "        print('error')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c25de0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [ 1 ] A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2 2\n",
      "already exists\n",
      "\n",
      " [ 2 ] Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "2 2\n",
      "already exists\n",
      "\n",
      " [ 3 ] Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "0 0\n",
      "already exists\n",
      "\n",
      " [ 4 ] Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "1 1\n",
      "already exists\n",
      "\n",
      " [ 5 ] Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning\n",
      "16 16\n",
      "already exists\n",
      "\n",
      " [ 6 ] ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "0 0\n",
      "already exists\n",
      "\n",
      " [ 7 ] Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "1 1\n",
      "already exists\n",
      "\n",
      " [ 8 ] CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "0 0\n",
      "already exists\n",
      "\n",
      " [ 9 ] MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "5 5\n",
      "already exists\n",
      "\n",
      " [ 10 ] Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space\n",
      "2 2\n",
      "already exists\n",
      "\n",
      " [ 11 ] Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning\n",
      "3 3\n",
      "already exists\n",
      "\n",
      " [ 12 ] Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\n",
      "10 10\n",
      "already exists\n",
      "\n",
      " [ 13 ] Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2 2\n",
      "already exists\n",
      "\n",
      " [ 14 ] Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "7 7\n",
      "already exists\n",
      "\n",
      " [ 15 ] Skill-based Meta-Reinforcement Learning\n",
      "19 19\n",
      "already exists\n",
      "\n",
      " [ 16 ] Skill-based Model-based Reinforcement Learning\n",
      "4 4\n",
      "already exists\n",
      "\n",
      " [ 17 ] SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "1 1\n",
      "already exists\n",
      "\n",
      " [ 18 ] Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback\n",
      "1 1\n",
      "already exists\n",
      "\n",
      " [ 19 ] Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "31 31\n",
      "already exists\n",
      "\n",
      " [ 20 ] Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "10 10\n",
      "already exists\n",
      "\n",
      " [ 21 ] Demonstration-Guided Reinforcement Learning with Learned Skills\n",
      "34 34\n",
      "already exists\n",
      "\n",
      " [ 22 ] Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "17 17\n",
      "already exists\n",
      "\n",
      " [ 23 ] Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "14 14\n",
      "already exists\n",
      "\n",
      " [ 24 ] Hierarchical Skills for Efficient Exploration\n",
      "16 16\n",
      "already exists\n",
      "\n",
      " [ 25 ] Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space\n",
      "2 2\n",
      "already exists\n",
      "\n",
      " [ 26 ] Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "12 12\n",
      "already exists\n",
      "\n",
      " [ 27 ] Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning\n",
      "23 23\n",
      "already exists\n",
      "\n",
      " [ 28 ] TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "23 23\n",
      "already exists\n",
      "\n",
      " [ 29 ] Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "131 131\n",
      "already exists\n",
      "\n",
      " [ 30 ] Behavior Priors for Efficient Reinforcement Learning\n",
      "23 23\n",
      "already exists\n",
      "\n",
      " [ 31 ] Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "76 76\n",
      "already exists\n",
      "\n",
      " [ 32 ] CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "31 31\n",
      "already exists\n",
      "\n",
      " [ 33 ] Discovering Motor Programs by Recomposing Demonstrations\n",
      "39 39\n",
      "already exists\n",
      "\n",
      " [ 34 ] Hierarchical reinforcement learning for efficent exploration and transfer\n",
      "8 8\n",
      "already exists\n",
      "\n",
      " [ 35 ] Learning quadrupedal locomotion over challenging terrain\n",
      "526 526\n",
      "already exists\n",
      "\n",
      " [ 36 ] Learning Robot Skills with Temporal Variational Inference\n",
      "39 39\n",
      "already exists\n",
      "\n",
      " [ 37 ] Multi-expert learning of adaptive legged locomotion\n",
      "99 99\n",
      "already exists\n",
      "\n",
      " [ 38 ] Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer\n",
      "8 8\n",
      "already exists\n",
      "\n",
      " [ 39 ] PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "85 85\n",
      "already exists\n",
      "\n",
      " [ 40 ] Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information\n",
      "57 57\n",
      "already exists\n",
      "\n",
      " [ 41 ] Dynamics-Aware Unsupervised Discovery of Skills\n",
      "273 273\n",
      "already exists\n",
      "\n",
      " [ 42 ] Learning Latent Plans from Play\n",
      "245 245\n",
      "already exists\n",
      "\n",
      " [ 43 ] LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "198 198\n",
      "already exists\n",
      "\n",
      " [ 44 ] MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "152 152\n",
      "already exists\n",
      "\n",
      " [ 45 ] Neural probabilistic motor primitives for humanoid control\n",
      "112 112\n",
      "already exists\n",
      "\n",
      " [ 46 ] Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "202 202\n",
      "already exists\n",
      "\n",
      " [ 47 ] Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "67 67\n",
      "already exists\n",
      "\n",
      " [ 48 ] Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "685 685\n",
      "already exists\n",
      "\n",
      " [ 49 ] CompILE: Compositional Imitation Learning and Execution\n",
      "88 88\n",
      "already exists\n",
      "\n",
      " [ 50 ] Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "176 176\n",
      "already exists\n",
      "\n",
      " [ 51 ] Learning an embedding space for transferable robot skills\n",
      "278 278\n",
      "already exists\n",
      "\n",
      " [ 52 ] A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "378 378\n",
      "already exists\n",
      "\n",
      " [ 53 ] DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "76 76\n",
      "already exists\n",
      "\n",
      " [ 54 ] Feudal networks for hierarchical reinforcement learning\n",
      "832 832\n",
      "already exists\n",
      "\n",
      " [ 55 ] Stochastic neural networks for hierarchical reinforcement learning\n",
      "365 365\n",
      "already exists\n",
      "\n",
      " [ 56 ] The option-critic architecture\n",
      "981 981\n",
      "already exists\n",
      "\n",
      " [ 57 ] Learning and Transfer of Modulated Locomotor Controllers\n",
      "200 200\n",
      "already exists\n"
     ]
    }
   ],
   "source": [
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    print('\\n', '[', key, ']', paper['title'])\n",
    "    print(paper['info']['num_citations'], paper['citedby_paper_num'])\n",
    "    if paper['citedby_paper_num'] < paper['info']['num_citations']: # 개수가 적으면\n",
    "        citedby_paper_list = []\n",
    "        if 'citedby_paper_list' not in rw.paper_list[key]:\n",
    "            rw.paper_list[key]['citedby_paper_list'] = [] \n",
    "        for i, page in enumerate(paper['citedby_page_list']):\n",
    "            print(page)\n",
    "            if (i+1)*10 <= paper['citedby_paper_num']: # 채운곳까지는 넘어감\n",
    "                print('continue {} < {}'.format((i+1)*10, paper['citedby_paper_num']))\n",
    "                continue \n",
    "            #time.sleep(2)\n",
    "            return_list = get_papers_from_citedby_page(page)\n",
    "            if return_list is not None:\n",
    "                citedby_paper_list += return_list\n",
    "            else:\n",
    "                break\n",
    "        print(citedby_paper_list)\n",
    "        rw.paper_list[key]['citedby_paper_list'] += citedby_paper_list\n",
    "        rw.paper_list[key]['citedby_paper_num'] += len(citedby_paper_list)\n",
    "        print(rw.paper_list[key]['citedby_paper_num'])\n",
    "        rw.is_updated = True\n",
    "        rw.update_pkl()\n",
    "    else:\n",
    "        print('already exists')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c183aa8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79e2122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2 2\n",
      "2 2 2\n",
      "no citation\n",
      "1 1 1\n",
      "16 16 16\n",
      "no citation\n",
      "1 1 1\n",
      "no citation\n",
      "5 5 5\n",
      "2 2 2\n",
      "3 3 3\n",
      "10 10 10\n",
      "2 2 2\n",
      "7 7 7\n",
      "19 19 19\n",
      "4 4 4\n",
      "1 1 1\n",
      "1 1 1\n",
      "31 31 31\n",
      "10 10 10\n",
      "34 34 34\n",
      "17 17 17\n",
      "14 14 14\n",
      "16 16 16\n",
      "2 2 2\n",
      "12 12 12\n",
      "23 23 23\n",
      "23 23 23\n",
      "131 131 131\n",
      "23 23 23\n",
      "76 76 76\n",
      "31 31 31\n",
      "39 39 39\n",
      "8 8 8\n",
      "no citation\n",
      "39 39 39\n",
      "99 99 99\n",
      "8 8 8\n",
      "85 85 85\n",
      "57 57 57\n",
      "273 273 273\n",
      "245 245 245\n",
      "198 198 198\n",
      "152 152 152\n",
      "112 112 112\n",
      "202 202 202\n",
      "67 67 67\n",
      "685 685 685\n",
      "88 88 88\n",
      "176 176 176\n",
      "278 278 278\n",
      "378 378 378\n",
      "76 76 76\n",
      "832 832 832\n",
      "365 365 365\n",
      "210 210 981\n",
      "0 0 200\n"
     ]
    }
   ],
   "source": [
    "# 실수로 citedby_paper_num를 지웠을 때 복구한 코드\n",
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    \n",
    "    try:\n",
    "        rw.paper_list[key]['citedby_paper_num'] = len(paper['citedby_paper_list'])\n",
    "        print(len(paper['citedby_paper_list']), paper['citedby_paper_num'], paper['info']['num_citations'])\n",
    "        if paper['citedby_paper_num'] > paper['info']['num_citations']:\n",
    "            print(key, '-> to be modified', len(paper['citedby_paper_list'][paper['info']['num_citations']:]))\n",
    "            for item in paper['citedby_paper_list'][paper['info']['num_citations']:]:\n",
    "                print(item)\n",
    "                paper['citedby_paper_list'].remove(item)\n",
    "    except KeyError as e:\n",
    "        print('no citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dd9a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update pkl\n"
     ]
    }
   ],
   "source": [
    "rw.is_updated = True\n",
    "rw.update_pkl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2d0d84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "16\n",
      "0\n",
      "1\n",
      "0\n",
      "5\n",
      "2\n",
      "3\n",
      "10\n",
      "2\n",
      "7\n",
      "19\n",
      "4\n",
      "1\n",
      "1\n",
      "31\n",
      "10\n",
      "34\n",
      "17\n",
      "14\n",
      "16\n",
      "2\n",
      "12\n",
      "23\n",
      "23\n",
      "131\n",
      "23\n",
      "76\n",
      "31\n",
      "39\n",
      "8\n",
      "526\n",
      "39\n",
      "99\n",
      "8\n",
      "85\n",
      "57\n",
      "273\n",
      "245\n",
      "198\n",
      "152\n",
      "112\n",
      "202\n",
      "67\n",
      "685\n",
      "88\n",
      "176\n",
      "278\n",
      "378\n",
      "76\n",
      "832\n",
      "365\n",
      "981\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# 지우는 코드\n",
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    print(paper['info']['num_citations'])\n",
    "    #print(paper['citedby_paper_list'])\n",
    "    try:\n",
    "        del paper['citedby_paper_list']\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "rw.is_updated = True\n",
    "rw.update_pkl()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "937e2473",
   "metadata": {},
   "source": [
    "### 어떤 논문에서 인용이 됐는지 체크하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6150b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce8f4ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9904761904761905\n"
     ]
    }
   ],
   "source": [
    "a = \"Behavior Transformers: Cloning  modes with one stone\"\n",
    "b = \"Behavior Transformers: Cloning k modes with one stone\"\n",
    "print(similar(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2367004d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations, permutations\n",
    "import numpy as np\n",
    "\n",
    "nums = list(range(1, 58))\n",
    "print(nums)\n",
    "perm = list(permutations(nums, 2))\n",
    "#print(combi)\n",
    "\n",
    "citation_matrix = np.zeros((57, 57))\n",
    "\n",
    "# for pair in perm:\n",
    "#     print(pair[0], pair[1])\n",
    "#     #print(rw.paper_list[pair[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a70316cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from pkl\n"
     ]
    }
   ],
   "source": [
    "rw = RelatedWork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "69bbe640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rw.paper_list.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3592e420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[1]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5da74b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning',\n",
       " 'year': '2019',\n",
       " 'info': {'container_type': 'Publication',\n",
       "  'source': <PublicationSource.PUBLICATION_SEARCH_SNIPPET: 'PUBLICATION_SEARCH_SNIPPET'>,\n",
       "  'bib': {'title': 'Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning',\n",
       "   'author': ['A Gupta', 'V Kumar', 'C Lynch', 'S Levine'],\n",
       "   'pub_year': '2019',\n",
       "   'venue': 'arXiv preprint arXiv …',\n",
       "   'abstract': 'We present relay policy learning, a method for imitation and reinforcement learning that can solve multi-stage, long-horizon robotic tasks. This general and universally-applicable, two-phase approach consists of an imitation learning stage that produces goal-conditioned hierarchical policies, and a reinforcement learning phase that finetunes these policies for task performance. Our method, while not necessarily perfect at imitation learning, is very amenable to further improvement via environment interaction, allowing it to scale to'},\n",
       "  'filled': False,\n",
       "  'gsrank': 1,\n",
       "  'pub_url': 'https://arxiv.org/abs/1910.11956',\n",
       "  'author_id': ['1wLVDP4AAAAJ',\n",
       "   'nu3W--sAAAAJ',\n",
       "   'CYWO-oAAAAAJ',\n",
       "   '8R35rCwAAAAJ'],\n",
       "  'url_scholarbib': '/scholar?hl=en&q=info:kBR-7NAzJj4J:scholar.google.com/&output=cite&scirp=0&hl=en',\n",
       "  'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelay%2BPolicy%2BLearning:%2BSolving%2BLong-Horizon%2BTasks%2Bvia%2BImitation%2Band%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&update_op=library_add&info=kBR-7NAzJj4J&ei=uCx9ZMiIH72P6rQP85mLmAc&json=',\n",
       "  'num_citations': 202,\n",
       "  'citedby_url': '/scholar?cites=4478323851880436880&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "  'url_related_articles': '/scholar?q=related:kBR-7NAzJj4J:scholar.google.com/&scioq=Relay+Policy+Learning:+Solving+Long-Horizon+Tasks+via+Imitation+and+Reinforcement+Learning&hl=en&as_sdt=0,33',\n",
       "  'eprint_url': 'https://arxiv.org/pdf/1910.11956'},\n",
       " 'citedby_page_list': ['https://scholar.google.co.kr/scholar?cites=4478323851880436880&as_sdt=5,33&sciodt=0,33&hl=en',\n",
       "  'https://scholar.google.co.kr/scholar?start=10&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=20&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=30&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=40&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=50&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=60&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=70&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=80&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=90&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=100&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=110&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=120&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=130&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=140&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=150&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=160&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=170&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=180&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=190&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc=',\n",
       "  'https://scholar.google.co.kr/scholar?start=200&hl=en&as_sdt=5,33&sciodt=0,33&cites=4478323851880436880&scipsc='],\n",
       " 'citedby_paper_num': 202,\n",
       " 'citedby_paper_list': [{'title': 'Hierarchical reinforcement learning: A comprehensive survey',\n",
       "   'url': 'https://dl.acm.org/doi/abs/10.1145/3453160'},\n",
       "  {'title': 'A survey on offline reinforcement learning: Taxonomy, review, and open problems',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10078377/'},\n",
       "  {'title': 'Conservative q-learning for offline reinforcement learning',\n",
       "   'url': 'https://proceedings.neurips.cc/paper/2020/hash/0d2b2061826a5df3221116a5085a6052-Abstract.html'},\n",
       "  {'title': 'Awac: Accelerating online reinforcement learning with offline datasets',\n",
       "   'url': 'https://arxiv.org/abs/2006.09359'},\n",
       "  {'title': 'Accelerating reinforcement learning with learned skill priors',\n",
       "   'url': 'https://proceedings.mlr.press/v155/pertsch21a.html'},\n",
       "  {'title': 'The unsurprising effectiveness of pre-trained vision models for control',\n",
       "   'url': 'https://proceedings.mlr.press/v162/parisi22a.html'},\n",
       "  {'title': 'Behavior Transformers: Cloning  modes with one stone',\n",
       "   'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/90d17e882adbdda42349db6f50123817-Abstract-Conference.html'},\n",
       "  {'title': 'R3m: A universal visual representation for robot manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2203.12601'},\n",
       "  {'title': 'Goal-conditioned reinforcement learning with imagined subgoals',\n",
       "   'url': 'https://proceedings.mlr.press/v139/chane-sane21a'},\n",
       "  {'title': \"Don't start from scratch: Leveraging prior data to automate robotic reinforcement learning\",\n",
       "   'url': 'https://proceedings.mlr.press/v205/walke23a.html'},\n",
       "  {'title': 'Parrot: Data-driven behavioral priors for reinforcement learning',\n",
       "   'url': 'https://arxiv.org/abs/2011.10024'},\n",
       "  {'title': 'Discovering and achieving goals via world models',\n",
       "   'url': 'https://proceedings.neurips.cc/paper/2021/hash/cc4af25fa9d2d5c953496579b75f6f6c-Abstract.html'},\n",
       "  {'title': 'Variable compliance control for robotic peg-in-hole assembly: A deep-reinforcement-learning approach',\n",
       "   'url': 'https://www.mdpi.com/846340'},\n",
       "  {'title': 'Concept2robot: Learning manipulation concepts from instructions and human demonstrations',\n",
       "   'url': 'https://journals.sagepub.com/doi/pdf/10.1177/02783649211046285'},\n",
       "  {'title': 'Language conditioned imitation learning over unstructured data',\n",
       "   'url': 'https://arxiv.org/abs/2005.07648'},\n",
       "  {'title': 'RvS: What is Essential for Offline RL via Supervised Learning?',\n",
       "   'url': 'https://arxiv.org/abs/2112.10751'},\n",
       "  {'title': 'Latent plans for task-agnostic offline reinforcement learning',\n",
       "   'url': 'https://proceedings.mlr.press/v205/rosete-beas23a.html'},\n",
       "  {'title': 'IKEA furniture assembly environment for long-horizon complex manipulation tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9560986/'},\n",
       "  {'title': 'Blocks assemble! learning to assemble with large-scale structured reinforcement learning',\n",
       "   'url': 'https://proceedings.mlr.press/v162/ghasemipour22a.html'},\n",
       "  {'title': 'Learning to reach goals via iterated supervised learning',\n",
       "   'url': 'https://arxiv.org/abs/1912.06088'},\n",
       "  {'title': 'From motor control to team play in simulated humanoid football',\n",
       "   'url': 'https://arxiv.org/abs/2105.12196'},\n",
       "  {'title': 'C-learning: Learning to achieve goals via recursive classification',\n",
       "   'url': 'https://arxiv.org/abs/2011.08909'},\n",
       "  {'title': 'Grounding language in play',\n",
       "   'url': 'https://www.academia.edu/download/93604914/2005.07648v1.pdf'},\n",
       "  {'title': 'Screwnet: Category-independent articulation model estimation from depth images using screw theory',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9561132/'},\n",
       "  {'title': 'Guided reinforcement learning with learned skills',\n",
       "   'url': 'https://arxiv.org/abs/2107.10253'},\n",
       "  {'title': 'Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9812140/'},\n",
       "  {'title': 'Imitating human behaviour with diffusion models',\n",
       "   'url': 'https://arxiv.org/abs/2301.10677'},\n",
       "  {'title': 'Generalization with lossy affordances: Leveraging broad offline data for learning visuomotor tasks',\n",
       "   'url': 'https://proceedings.mlr.press/v205/fang23a.html'},\n",
       "  {'title': 'Skill-based meta-reinforcement learning',\n",
       "   'url': 'https://arxiv.org/abs/2204.11828'},\n",
       "  {'title': 'Can Foundation Models Perform Zero-Shot Task Specification For Robot Manipulation?',\n",
       "   'url': 'https://proceedings.mlr.press/v168/cui22a.html'},\n",
       "  {'title': 'Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents',\n",
       "   'url': 'https://proceedings.mlr.press/v199/powers22b.html'},\n",
       "  {'title': 'Reinforcement learning based control of imitative policies for near-accident driving',\n",
       "   'url': 'https://arxiv.org/abs/2007.00178'},\n",
       "  {'title': 'Retrospectives on the embodied ai workshop',\n",
       "   'url': 'https://arxiv.org/abs/2210.06849'},\n",
       "  {'title': 'Recent advances in leveraging human guidance for sequential decision-making tasks',\n",
       "   'url': 'https://link.springer.com/article/10.1007/s10458-021-09514-w'},\n",
       "  {'title': 'Interactive language: Talking to robots in real time',\n",
       "   'url': 'https://arxiv.org/abs/2210.06407'},\n",
       "  {'title': 'Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9695333/'},\n",
       "  {'title': 'Asymmetric self-play for automatic goal discovery in robotic manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2101.04882'},\n",
       "  {'title': 'Learning geometric reasoning and control for long-horizon tasks from visual input',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9560934/'},\n",
       "  {'title': 'Vima: General robot manipulation with multimodal prompts',\n",
       "   'url': 'https://arxiv.org/abs/2210.03094'},\n",
       "  {'title': 'Efficient and interpretable robot manipulation with graph neural networks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9684675/'},\n",
       "  {'title': 'Broadly-exploring, local-policy trees for long-horizon task planning',\n",
       "   'url': 'https://arxiv.org/abs/2010.06491'},\n",
       "  {'title': 'Hierarchical few-shot imitation with skill transition models',\n",
       "   'url': 'https://arxiv.org/abs/2107.08981'},\n",
       "  {'title': 'Understanding domain randomization for sim-to-real transfer',\n",
       "   'url': 'https://arxiv.org/abs/2110.03239'},\n",
       "  {'title': 'Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning',\n",
       "   'url': 'https://arxiv.org/abs/2303.05479'},\n",
       "  {'title': 'Offline Reinforcement Learning at Multiple Frequencies',\n",
       "   'url': 'https://proceedings.mlr.press/v205/burns23a.html'},\n",
       "  {'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics',\n",
       "   'url': 'https://proceedings.mlr.press/v205/rana23a.html'},\n",
       "  {'title': 'Bootstrapped autonomous practicing via multi-task reinforcement learning',\n",
       "   'url': 'https://arxiv.org/abs/2203.15755'},\n",
       "  {'title': 'Generalization guarantees for imitation learning',\n",
       "   'url': 'https://proceedings.mlr.press/v155/ren21a.html'},\n",
       "  {'title': 'Example-driven model-based reinforcement learning for solving long-horizon visuomotor tasks',\n",
       "   'url': 'https://arxiv.org/abs/2109.10312'},\n",
       "  {'title': 'NeoRL: A near real-world benchmark for offline reinforcement learning',\n",
       "   'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/9cd828eb8dc81a84fb6bf89a94263e1b-Abstract-Datasets_and_Benchmarks.html'},\n",
       "  {'title': 'Reinforcement learning with sparse rewards using guidance from offline demonstration',\n",
       "   'url': 'https://arxiv.org/abs/2202.04628'},\n",
       "  {'title': 'Offline imitation learning with a misspecified simulator',\n",
       "   'url': 'https://proceedings.neurips.cc/paper/2020/hash/60cb558c40e4f18479664069d9642d5a-Abstract.html'},\n",
       "  {'title': 'Diffusion policy: Visuomotor policy learning via action diffusion',\n",
       "   'url': 'https://arxiv.org/abs/2303.04137'},\n",
       "  {'title': 'Motion generation using bilateral control-based imitation learning with autoregressive learning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9344611/'},\n",
       "  {'title': 'You Only Live Once: Single-Life Reinforcement Learning',\n",
       "   'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/5ec4e93f2cec19d47ef852a0e1fb2c48-Abstract-Conference.html'},\n",
       "  {'title': 'Plato: Predicting latent affordances through object-centric play',\n",
       "   'url': 'https://proceedings.mlr.press/v205/belkhale23a.html'},\n",
       "  {'title': 'You only evaluate once: a simple baseline algorithm for offline rl',\n",
       "   'url': 'https://proceedings.mlr.press/v164/goo22a.html'},\n",
       "  {'title': 'Hybrid trajectory and force learning of complex assembly tasks: A combined learning framework',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9406007/'},\n",
       "  {'title': 'EC2: Emergent Communication for Embodied Control',\n",
       "   'url': 'http://openaccess.thecvf.com/content/CVPR2023/html/Mu_EC2_Emergent_Communication_for_Embodied_Control_CVPR_2023_paper.html'},\n",
       "  {'title': 'Autonomous reinforcement learning: Formalism and benchmarking',\n",
       "   'url': 'https://arxiv.org/abs/2112.09605'},\n",
       "  {'title': 'Manipulation planning from demonstration via goal-conditioned prior action primitive decomposition and alignment',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9669009/'},\n",
       "  {'title': 'Learning multi-stage tasks with one demonstration via self-replay',\n",
       "   'url': 'https://proceedings.mlr.press/v164/palo22a.html'},\n",
       "  {'title': 'Learning task decomposition with ordered memory policy network',\n",
       "   'url': 'https://arxiv.org/abs/2103.10972'},\n",
       "  {'title': 'Hierarchical planning through goal-conditioned offline reinforcement learning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9826807/'},\n",
       "  {'title': 'Goal-conditioned imitation learning using score-based diffusion policies',\n",
       "   'url': 'https://arxiv.org/abs/2304.02532'},\n",
       "  {'title': 'Lossless adaptation of pretrained vision models for robotic manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2304.06600'},\n",
       "  {'title': 'Kitchenshift: Evaluating zero-shot generalization of imitation-based policy learning under domain shifts',\n",
       "   'url': 'https://openreview.net/forum?id=DdglKo8hBq0'},\n",
       "  {'title': 'Planning to practice: Efficient online fine-tuning by composing goals in latent space',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9981999/'},\n",
       "  {'title': 'Transferring Hierarchical Structures with Dual Meta Imitation Learning',\n",
       "   'url': 'https://proceedings.mlr.press/v205/gao23b.html'},\n",
       "  {'title': 'Developing cooperative policies for multi-stage reinforcement learning tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9772966/'},\n",
       "  {'title': 'Versatile offline imitation from observations and examples via regularized state-occupancy matching',\n",
       "   'url': 'https://proceedings.mlr.press/v162/ma22a.html'},\n",
       "  {'title': 'Toward robots that learn to summarize their actions in natural language: a set of tasks',\n",
       "   'url': 'https://proceedings.mlr.press/v164/dechant22a.html'},\n",
       "  {'title': 'Robotic imitation of human assembly skills using hybrid trajectory and force learning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9561619/'},\n",
       "  {'title': 'Skill-based model-based reinforcement learning',\n",
       "   'url': 'https://arxiv.org/abs/2207.07560'},\n",
       "  {'title': 'The Provable Benefits of Unsupervised Data Sharing for Offline Reinforcement Learning',\n",
       "   'url': 'https://arxiv.org/abs/2302.13493'},\n",
       "  {'title': 'SAFARI: Safe and active robot imitation learning with imagination',\n",
       "   'url': 'https://arxiv.org/abs/2011.09586'},\n",
       "  {'title': 'Learning hybrid object kinematics for efficient hierarchical planning under uncertainty',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9340749/'},\n",
       "  {'title': 'C-planning: An automatic curriculum for learning goal-reaching tasks',\n",
       "   'url': 'https://arxiv.org/abs/2110.12080'},\n",
       "  {'title': 'Less Is More: Refining Datasets for Offline Reinforcement Learning with Reward Machines',\n",
       "   'url': 'https://dl.acm.org/doi/abs/10.5555/3545946.3598769'},\n",
       "  {'title': 'Training and Inference on Any-Order Autoregressive Models the Right Way',\n",
       "   'url': 'https://arxiv.org/abs/2205.13554'},\n",
       "  {'title': 'An Independently Learnable Hierarchical Model for Bilateral Control-Based Imitation Learning Applications',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9722836/'},\n",
       "  {'title': 'Boosting Reinforcement Learning and Planning with Demonstrations: A Survey',\n",
       "   'url': 'https://arxiv.org/abs/2303.13489'},\n",
       "  {'title': 'EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought',\n",
       "   'url': 'https://arxiv.org/abs/2305.15021'},\n",
       "  {'title': 'Hierarchical Reinforcement Learning in Complex 3D Environments',\n",
       "   'url': 'https://arxiv.org/abs/2302.14451'},\n",
       "  {'title': 'Learning Video-Conditioned Policies for Unseen Manipulation Tasks',\n",
       "   'url': 'https://arxiv.org/abs/2305.06289'},\n",
       "  {'title': 'Wish you were here: Hindsight Goal Selection for long-horizon dexterous manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2112.00597'},\n",
       "  {'title': 'Deep PQR: Solving inverse reinforcement learning using anchor actions',\n",
       "   'url': 'https://proceedings.mlr.press/v119/geng20a.html'},\n",
       "  {'title': 'RHH-LGP: Receding Horizon And Heuristics-Based Logic-Geometric Programming For Task And Motion Planning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9981797/'},\n",
       "  {'title': 'Pretraining in Deep Reinforcement Learning: A Survey',\n",
       "   'url': 'https://arxiv.org/abs/2211.03959'},\n",
       "  {'title': 'Future-guided offline imitation learning for long action sequences via video interpolation and future-trajectory prediction',\n",
       "   'url': 'https://www.sciencedirect.com/science/article/pii/S0925231223004484'},\n",
       "  {'title': 'Controllability-Aware Unsupervised Skill Discovery',\n",
       "   'url': 'https://arxiv.org/abs/2302.05103'},\n",
       "  {'title': 'DexDeform: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics',\n",
       "   'url': 'https://arxiv.org/abs/2304.03223'},\n",
       "  {'title': 'Imitation learning for variable speed contact motion for operation up to control bandwidth',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9707856/'},\n",
       "  {'title': 'Syntactic Inductive Biases for Deep Learning Methods',\n",
       "   'url': 'https://arxiv.org/abs/2206.04806'},\n",
       "  {'title': 'CACTI: A Framework for Scalable Multi-Task Multi-Scene Visual Imitation Learning',\n",
       "   'url': 'https://arxiv.org/abs/2212.05711'},\n",
       "  {'title': 'Designing an offline reinforcement learning objective from scratch',\n",
       "   'url': 'https://arxiv.org/abs/2301.12842'},\n",
       "  {'title': 'One-shot Visual Imitation via Attributed Waypoints and Demonstration Augmentation',\n",
       "   'url': 'https://arxiv.org/abs/2302.04856'},\n",
       "  {'title': 'Beyond tabula-rasa: a modular reinforcement learning approach for physically embedded 3d sokoban',\n",
       "   'url': 'https://arxiv.org/abs/2010.01298'},\n",
       "  {'title': 'Twin delayed hierarchical actor-critic',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9376459/'},\n",
       "  {'title': 'VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training',\n",
       "   'url': 'https://arxiv.org/abs/2210.00030'},\n",
       "  {'title': 'Learning and Retrieval from Prior Data for Skill-based Imitation Learning',\n",
       "   'url': 'https://arxiv.org/abs/2210.11435'},\n",
       "  {'title': 'Learning Skills from Demonstrations: A Trend from Motion Primitives to Experience Abstraction',\n",
       "   'url': 'https://arxiv.org/abs/2210.08060'},\n",
       "  {'title': 'Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation',\n",
       "   'url': 'https://arxiv.org/abs/2305.16985'},\n",
       "  {'title': 'PATO: Policy Assisted TeleOperation for Scalable Robot Data Collection',\n",
       "   'url': 'https://arxiv.org/abs/2212.04708'},\n",
       "  {'title': 'Markovian policy network for efficient robot learning',\n",
       "   'url': 'https://www.sciencedirect.com/science/article/pii/S0925231222011535'},\n",
       "  {'title': 'FIRL: Fast Imitation and Policy Reuse Learning',\n",
       "   'url': 'https://arxiv.org/abs/2203.00251'},\n",
       "  {'title': 'Ignorance is Bliss: Robust Control via Information Gating',\n",
       "   'url': 'https://arxiv.org/abs/2303.06121'},\n",
       "  {'title': 'FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2305.12821'},\n",
       "  {'title': 'Multi-task Hierarchical Adversarial Inverse Reinforcement Learning',\n",
       "   'url': 'https://arxiv.org/abs/2305.12633'},\n",
       "  {'title': 'CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations',\n",
       "   'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/hash/322e4a595afd9442a89f0bfaa441871e-Abstract-Conference.html'},\n",
       "  {'title': 'Efficient Learning of High Level Plans from Play',\n",
       "   'url': 'https://arxiv.org/abs/2303.09628'},\n",
       "  {'title': 'Curriculum Goal-Conditioned Imitation for Offline Reinforcement Learning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9962804/'},\n",
       "  {'title': 'Cross-Domain Transfer via Semantic Skill Imitation',\n",
       "   'url': 'https://arxiv.org/abs/2212.07407'},\n",
       "  {'title': 'Know Your Boundaries: The Necessity of Explicit Behavioral Cloning in Offline RL',\n",
       "   'url': 'https://arxiv.org/abs/2206.00695'},\n",
       "  {'title': 'Challenges to Solving Combinatorially Hard Long-Horizon Deep RL Tasks',\n",
       "   'url': 'https://arxiv.org/abs/2206.01812'},\n",
       "  {'title': 'Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments',\n",
       "   'url': 'https://arxiv.org/abs/2304.09825'},\n",
       "  {'title': 'Imitation Learning With Time-Varying Synergy for Compact Representation of Spatiotemporal Structures',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10091504/'},\n",
       "  {'title': 'Temporal Logic Imitation: Learning Plan-Satisficing Motion Policies from Demonstrations',\n",
       "   'url': 'https://arxiv.org/abs/2206.04632'},\n",
       "  {'title': 'The StarCraft Multi-Agent Exploration Challenges: Learning Multi-Stage Tasks and Environmental Factors without Precise Reward Functions',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10099458/'},\n",
       "  {'title': 'E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance',\n",
       "   'url': 'https://arxiv.org/abs/2212.02064'},\n",
       "  {'title': 'Hierarchical Learning from Demonstrations for Long-Horizon Tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9561408/'},\n",
       "  {'title': 'Learning to play by imitating humans',\n",
       "   'url': 'https://arxiv.org/abs/2006.06874'},\n",
       "  {'title': 'RIRL: A Recurrent Imitation and Reinforcement Learning Method for Long-Horizon Robotic Tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9700600/'},\n",
       "  {'title': 'Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning',\n",
       "   'url': 'https://arxiv.org/abs/2210.13846'},\n",
       "  {'title': 'PIRLNav: Pretraining with Imitation and RL Finetuning for OBJECTNAV',\n",
       "   'url': 'https://arxiv.org/abs/2301.07302'},\n",
       "  {'title': 'CRISP: Curriculum inducing Primitive Informed Subgoal Prediction for Hierarchical Reinforcement Learning',\n",
       "   'url': 'https://arxiv.org/abs/2304.03535'},\n",
       "  {'title': 'Distance Weighted Supervised Learning for Offline Interaction Data',\n",
       "   'url': 'https://arxiv.org/abs/2304.13774'},\n",
       "  {'title': 'Information Maximizing Curriculum: A Curriculum-Based Approach for Training Mixtures of Experts',\n",
       "   'url': 'https://arxiv.org/abs/2303.15349'},\n",
       "  {'title': 'HAC Explore: Accelerating Exploration with Hierarchical Reinforcement Learning',\n",
       "   'url': 'https://arxiv.org/abs/2108.05872'},\n",
       "  {'title': 'Parallel Sampling of Diffusion Models',\n",
       "   'url': 'https://arxiv.org/abs/2305.16317'},\n",
       "  {'title': 'B2RL: an open-source dataset for building batch reinforcement learning',\n",
       "   'url': 'https://dl.acm.org/doi/abs/10.1145/3563357.3566164'},\n",
       "  {'title': 'Learning Dynamic Manipulation Skills from Haptic-Play',\n",
       "   'url': 'https://arxiv.org/abs/2207.14007'},\n",
       "  {'title': 'A brief survey of Sim2Real methods for robot learning',\n",
       "   'url': 'https://link.springer.com/chapter/10.1007/978-3-031-04870-8_16'},\n",
       "  {'title': 'A Survey of Demonstration Learning',\n",
       "   'url': 'https://arxiv.org/abs/2303.11191'},\n",
       "  {'title': 'Consistent Experience Replay in High-Dimensional Continuous Control with Decayed Hindsights',\n",
       "   'url': 'https://www.mdpi.com/2075-1702/10/10/856'},\n",
       "  {'title': 'Hierarchical Decision Transformer',\n",
       "   'url': 'https://arxiv.org/abs/2209.10447'},\n",
       "  {'title': 'An Open Tele-Impedance Framework to Generate Large Datasets for Contact-Rich Tasks in Robotic Manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2209.10486'},\n",
       "  {'title': 'The Fittest Wins: a Multi-Stage Framework Achieving New SOTA in ViZDoom Competition',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10077442/'},\n",
       "  {'title': 'Multi-State-Space Reasoning Reinforcement Learning for Long-Horizon RFID-Based Robotic Searching and Planning Tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9906938/'},\n",
       "  {'title': 'A Proprioceptive Haptic Device Design for Teaching Bimanual Manipulation',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9811694/'},\n",
       "  {'title': 'Explicit User Manipulation in Reinforcement Learning Based Recommender Systems',\n",
       "   'url': 'https://arxiv.org/abs/2203.10629'},\n",
       "  {'title': 'A Simple Approach for General Task-Oriented Picking using Placing constraints',\n",
       "   'url': 'https://arxiv.org/abs/2304.01290'},\n",
       "  {'title': 'A Memory-Related Multi-Task Method Based on Task-Agnostic Exploration',\n",
       "   'url': 'https://arxiv.org/abs/2209.04100'},\n",
       "  {'title': 'For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal',\n",
       "   'url': 'https://arxiv.org/abs/2304.04591'},\n",
       "  {'title': 'Identifying reward functions using anchor actions',\n",
       "   'url': 'https://assets.amazon.science/07/6b/bffb94094b21936c7d2d15589e66/scipub-1582.pdf'},\n",
       "  {'title': 'Versatile Offline Imitation Learning via State-Occupancy Matching',\n",
       "   'url': 'https://openreview.net/forum?id=B42rnNpNyWq'},\n",
       "  {'title': 'Learning to Schedule Job-Shop Problems via Hierarchical Reinforcement Learning',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9945585/'},\n",
       "  {'title': 'Multi-goal Reinforcement Learning via Exploring Successor Matching',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9893637/'},\n",
       "  {'title': 'Acquiring Robot Navigation Skill with Knowledge Learned from Demonstration',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9515639/'},\n",
       "  {'title': 'Leveraging Efficiency through Hybrid Prioritized Experience Replay in Door Environment',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10011801/'},\n",
       "  {'title': 'Continuous Control with Action Quantization from Demonstrations',\n",
       "   'url': 'https://arxiv.org/abs/2110.10149'},\n",
       "  {'title': 'HRL2E: Hierarchical Reinforcement Learning with Low-level Ensemble',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9892189/'},\n",
       "  {'title': 'Learning from Guided Play: A Scheduled Hierarchical Approach for Improving Exploration in Adversarial Imitation Learning',\n",
       "   'url': 'https://arxiv.org/abs/2112.08932'},\n",
       "  {'title': 'The StarCraft Multi-Agent Challenges+: Learning of Multi-Stage Tasks and Environmental Factors without Precise Reward Functions',\n",
       "   'url': 'https://arxiv.org/abs/2207.02007'},\n",
       "  {'title': 'Revisiting Proprioceptive Sensing for Articulated Object Manipulation',\n",
       "   'url': 'https://arxiv.org/abs/2305.09584'},\n",
       "  {'title': 'Learning compositional neural programs for continuous control',\n",
       "   'url': 'https://arxiv.org/abs/2007.13363'},\n",
       "  {'title': 'Learning a Skill-sequence-dependent Policy for Long-horizon Manipulation Tasks',\n",
       "   'url': 'https://arxiv.org/abs/2105.05484'},\n",
       "  {'title': 'Policy Transfer via Skill Adaptation and Composition',\n",
       "   'url': 'https://dl.acm.org/doi/abs/10.1145/3577530.3577562'},\n",
       "  {'title': 'A Brief Review of Recent Hierarchical Reinforcement Learning for Robotic Manipulation',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10130968/'},\n",
       "  {'title': 'ACQL: An Adaptive Conservative Q-Learning Framework for Offline Reinforcement Learning',\n",
       "   'url': 'https://openreview.net/forum?id=o_HqtIc-oF'},\n",
       "  {'title': 'The skill-action architecture: Learning abstract action embeddings for reinforcement learning',\n",
       "   'url': 'https://openreview.net/forum?id=PU35uLgRZkk'},\n",
       "  {'title': 'Broadly-Exploring, Local-Policy Trees for Long-Horizon Task Planning',\n",
       "   'url': 'https://openreview.net/forum?id=yhy25u-DrjR'},\n",
       "  {'title': 'Imitation learning with auxiliary, suboptimal, and task-agnostic data',\n",
       "   'url': 'https://repositories.lib.utexas.edu/handle/2152/117578'},\n",
       "  {'title': 'Towards Adaptive, Continual Embodied Agents',\n",
       "   'url': 'https://escholarship.org/uc/item/3tk9g0b7'},\n",
       "  {'title': 'Know Your Boundaries: The Advantage of Explicit Behavior Cloning in Offline RL',\n",
       "   'url': 'https://openreview.net/forum?id=MT2l4ziaxeE'},\n",
       "  {'title': 'Improving Meta-imitation Learning with Focused Task Embedding',\n",
       "   'url': 'https://link.springer.com/chapter/10.1007/978-3-031-16075-2_12'},\n",
       "  {'title': 'Pre-training Agents for Design Optimization and Control',\n",
       "   'url': 'https://search.proquest.com/openview/c7ee5b82626e3f496c646be7aac02b96/1?pq-origsite=gscholar&cbl=18750&diss=y'},\n",
       "  {'title': 'Using natural language to aid task specification in sequential decision making problems',\n",
       "   'url': 'https://repositories.lib.utexas.edu/handle/2152/116769'},\n",
       "  {'title': 'The Impact of Approximation Errors on Warm-Start Reinforcement Learning: A Finite-time Analysis',\n",
       "   'url': 'https://openreview.net/forum?id=MuWgF-FVzON'},\n",
       "  {'title': 'Offline Learning for Scalable Decision Making',\n",
       "   'url': 'https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-168.pdf'},\n",
       "  {'title': 'The Ecology of Open-Ended Skill Acquisition',\n",
       "   'url': 'https://hal.inria.fr/tel-03875448/'},\n",
       "  {'title': 'Multi-task Hierarchical Reinforcement Learning for Compositional Tasks',\n",
       "   'url': 'https://deepblue.lib.umich.edu/handle/2027.42/169951'},\n",
       "  {'title': 'How to Train Your Robot: Techniques for Enabling Robotic Learning in the Real World',\n",
       "   'url': 'https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-191.pdf'},\n",
       "  {'title': 'Learning from Guided Play: Improving Exploration for Adversarial Imitation Learning with Simple Auxiliary Tasks',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/10016714/'},\n",
       "  {'title': 'Learning and Leveraging Kinematics for Robot Motion Planning under Uncertainty',\n",
       "   'url': 'https://search.proquest.com/openview/4c366317ecab06e2f52ac6ecc0fa6c2b/1?pq-origsite=gscholar&cbl=18750&diss=y'},\n",
       "  {'title': 'Real World Robot Learning: Learned Rewards, Offline Datasets and Skill Re-Use',\n",
       "   'url': 'https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-179.pdf'},\n",
       "  {'title': 'See, Act, and Conceptualize: A Learning System for Robots to Interact with the World',\n",
       "   'url': 'https://search.proquest.com/openview/3c3096593077402172ccfdd0bc06a2b0/1?pq-origsite=gscholar&cbl=18750&diss=y'},\n",
       "  {'title': 'Efficient algorithms for low-effort human teaching of robots',\n",
       "   'url': 'https://repositories.lib.utexas.edu/handle/2152/117575'},\n",
       "  {'title': 'Ensemble learning through cooperative evolutionary computation',\n",
       "   'url': 'https://ourarchive.otago.ac.nz/handle/10523/12072'},\n",
       "  {'title': 'Towards Exploiting Geometry and Time for Fast Off-Distribution Adaptation in Multi-Task Robot Learning',\n",
       "   'url': 'https://arxiv.org/abs/2106.13237'},\n",
       "  {'title': 'Grasp Planning Based on Deep Reinforcement Learning: A Brief Survey',\n",
       "   'url': 'https://ieeexplore.ieee.org/abstract/document/9727526/'},\n",
       "  {'title': '基于深度强化学习的机器人操作行为研究综述',\n",
       "   'url': 'http://html.rhhz.net/jqr/html/20220211.htm'},\n",
       "  {'title': 'LIV: Language-Image Representations and Rewards for Robotic Control',\n",
       "   'url': 'https://openreview.net/forum?id=F3GXIeMLLSL'},\n",
       "  {'title': 'Learning multi-stage tasks with one demonstration via self-replay',\n",
       "   'url': 'https://spiral.imperial.ac.uk/handle/10044/1/93296'},\n",
       "  {'title': 'Appendix of EC2: Emergent Communication for Embodied Control',\n",
       "   'url': 'https://openaccess.thecvf.com/content/CVPR2023/supplemental/Mu_EC2_Emergent_Communication_CVPR_2023_supplemental.pdf'},\n",
       "  {'title': 'VARIATIONAL REPARAMETRIZED POLICY LEARNING WITH DIFFERENTIABLE PHYSICS',\n",
       "   'url': 'https://openreview.net/forum?id=-23E9fxmI4'},\n",
       "  {'title': 'Diffusion Policy',\n",
       "   'url': 'https://diffusion-policy.cs.columbia.edu/diffusion_policy_2023.pdf'},\n",
       "  {'title': 'Offline RL at Multiple Frequencies',\n",
       "   'url': 'https://openreview.net/forum?id=CFxHg2L902W'},\n",
       "  {'title': 'Fine-tuning Offline Policies with Optimistic Action Selection',\n",
       "   'url': 'https://openreview.net/forum?id=ELmiPlCOSw'},\n",
       "  {'title': 'SPRINT: Scalable Semantic Policy Pre-training via Language Instruction Relabeling',\n",
       "   'url': 'https://openreview.net/forum?id=IfN3tzrKVBr'},\n",
       "  {'title': 'Beyond traits: Social context based personality model',\n",
       "   'url': 'https://ink.library.smu.edu.sg/sis_research/6078/'},\n",
       "  {'title': 'Exploring Visual Pre-training for Robot Manipulation: Datasets, Models and Methods',\n",
       "   'url': 'https://openreview.net/forum?id=WvhoSUZN89i'},\n",
       "  {'title': 'Inducing Reusable Skills From Demonstrations with Option-Controller Network',\n",
       "   'url': 'https://openreview.net/forum?id=62r41yOG5m'},\n",
       "  {'title': 'Hierarchical self-imitation learning in single-agent sparse reward environments',\n",
       "   'url': 'https://theneeloy.github.io/docs/SP21-ECE499-Thesis-Chakraborty,%20Neeloy.pdf'},\n",
       "  {'title': 'Robust Manipulation with Spatial Features',\n",
       "   'url': 'https://openreview.net/forum?id=X7beXNWxYP'},\n",
       "  {'title': 'Concept2Robot 2.0: Improving Learning of Manipulation Concepts Using Enhanced Representations',\n",
       "   'url': 'https://www.cs.utexas.edu/~yukez/cs391r_reports/files/Fall-2021/SS-JHL.pdf'},\n",
       "  {'title': 'One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control',\n",
       "   'url': 'https://openreview.net/forum?id=SOb834a4k-9'},\n",
       "  {'title': 'Behavior policy learning: Learning multi-stage tasks via solution sketches and model-based controllers',\n",
       "   'url': 'https://discovery.ucl.ac.uk/id/eprint/10158852/'},\n",
       "  {'title': 'Evaluating Generalization of Policy Learning Under Domain Shifts'},\n",
       "  {'title': 'CORA: Benchmarks, Baselines, and a Platform for Continual Reinforcement Learning Agents'},\n",
       "  {'title': 'Behavior Policy Learning: Learning Long-Horizon Tasks via Solution Sketches and Model-Based Controllers'},\n",
       "  {'title': '심층 강화 학습을 통한 사람형 로봇손의 롱-호라이즌 주방 사물 조작',\n",
       "   'url': 'https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11197020'}]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "68ceffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_citation(paper1, paper2):\n",
    "    citation_list = paper1['citedby_paper_list'] # paper1을 인용한 논문들\n",
    "    #if citation_list == []:\n",
    "    #    print(citation_list)\n",
    "    for paper in citation_list:\n",
    "        #print(paper)\n",
    "        similarity = similar(paper['title'].lower(), paper2['title'].lower()) # paper2가 paper1을 인용했나?\n",
    "        #print(similarity)\n",
    "        if similarity > 0.9:\n",
    "            #print('citation exists with ', similarity)\n",
    "            return True, similarity\n",
    "    return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f0b595d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics', 'url': 'https://proceedings.mlr.press/v205/rana23a.html'}\n",
      "1.0\n",
      "citation exists with  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 1.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bayesian = rw.paper_list[20]\n",
    "robot_learning = rw.paper_list[14]\n",
    "residual = rw.paper_list[13]\n",
    "check_citation(bayesian, residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "aab685d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) is cited by\n",
      "------------------------------------------------------\n",
      "Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning (2023) is cited by\n",
      "------------------------------------------------------\n",
      "Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (2023) is cited by\n",
      "------------------------------------------------------\n",
      "Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning (2022) is cited by\n",
      "------------------------------------------------------\n",
      "ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Cascaded Compositional Residual Learning for Complex Interactive Behaviors (2022) is cited by\n",
      "------------------------------------------------------\n",
      "CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) is cited by\n",
      "------------------------------------------------------\n",
      "MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Skill-based Meta-Reinforcement Learning (2022) is cited by\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Skill-based Model-based Reinforcement Learning (2022) is cited by\n",
      "------------------------------------------------------\n",
      "SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback (2022) is cited by\n",
      "------------------------------------------------------\n",
      "Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) is cited by\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics (2021) is cited by\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Demonstration-Guided Reinforcement Learning with Learned Skills (2021) is cited by\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Hierarchical Few-Shot Imitation with Skill Transition Models (2021) is cited by\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) is cited by\n",
      "------------------------------------------------------\n",
      "Hierarchical Skills for Efficient Exploration (2021) is cited by\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space (2021) is cited by\n",
      "------------------------------------------------------\n",
      "Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) is cited by\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning (2021) is cited by\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) is cited by\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) is cited by\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer (2022) with similarity 1.0\n",
      "* Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics (2021) with similarity 1.0\n",
      "* Hierarchical Few-Shot Imitation with Skill Transition Models (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Behavior Priors for Efficient Reinforcement Learning (2020) is cited by\n",
      "------------------------------------------------------\n",
      "Catch and carry: Reusable neural controllers for vision-guided whole-body tasks (2020) is cited by\n",
      "* Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) is cited by\n",
      "------------------------------------------------------\n",
      "Discovering Motor Programs by Recomposing Demonstrations (2020) is cited by\n",
      "* Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* Learning Robot Skills with Temporal Variational Inference (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Hierarchical reinforcement learning for efficent exploration and transfer (2020) is cited by\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Learning quadrupedal locomotion over challenging terrain (2020) is cited by\n",
      "* Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning (2022) with similarity 1.0\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Learning Robot Skills with Temporal Variational Inference (2020) is cited by\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Multi-expert learning of adaptive legged locomotion (2020) is cited by\n",
      "* Cascaded Compositional Residual Learning for Complex Interactive Behaviors (2022) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer (2020) is cited by\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Robot Learning of Mobile Manipulation With Reachability Behavior Priors (2022) with similarity 1.0\n",
      "* Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) is cited by\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information (2019) is cited by\n",
      "* Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (2023) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Learning Robot Skills with Temporal Variational Inference (2020) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "* CompILE: Compositional Imitation Learning and Execution (2018) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Dynamics-Aware Unsupervised Discovery of Skills (2019) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "------------------------------------------------------\n",
      "Learning Latent Plans from Play (2019) is cited by\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* Discovering Motor Programs by Recomposing Demonstrations (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT (2019) is cited by\n",
      "* Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning (2023) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) is cited by\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* Cascaded Compositional Residual Learning for Complex Interactive Behaviors (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Catch and carry: Reusable neural controllers for vision-guided whole-body tasks (2020) with similarity 0.9743589743589743\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* Multi-expert learning of adaptive legged locomotion (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Neural probabilistic motor primitives for humanoid control (2019) is cited by\n",
      "* Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) with similarity 1.0\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* Catch and carry: Reusable neural controllers for vision-guided whole-body tasks (2020) with similarity 0.9743589743589743\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) is cited by\n",
      "* Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (2023) with similarity 1.0\n",
      "* CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations (2022) with similarity 1.0\n",
      "* Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Hierarchical Few-Shot Imitation with Skill Transition Models (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) is cited by\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) is cited by\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT (2019) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "CompILE: Compositional Imitation Learning and Execution (2018) is cited by\n",
      "* Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (2023) with similarity 0.9090909090909091\n",
      "* Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) with similarity 1.0\n",
      "* Skill-based Model-based Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Discovering Motor Programs by Recomposing Demonstrations (2020) with similarity 1.0\n",
      "* Learning Robot Skills with Temporal Variational Inference (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Latent Space Policies for Hierarchical Reinforcement Learning (2018) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* Discovering Motor Programs by Recomposing Demonstrations (2020) with similarity 1.0\n",
      "* Multi-expert learning of adaptive legged locomotion (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Learning an embedding space for transferable robot skills (2018) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* Skill-based Meta-Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics (2021) with similarity 1.0\n",
      "* Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* Learning Latent Plans from Play (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "A Deep Hierarchical Approach to Lifelong Learning in Minecraft (2017) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) with similarity 0.9345794392523364\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "* Feudal networks for hierarchical reinforcement learning (2017) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations (2017) is cited by\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* Discovering Motor Programs by Recomposing Demonstrations (2020) with similarity 1.0\n",
      "* Learning Robot Skills with Temporal Variational Inference (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* CompILE: Compositional Imitation Learning and Execution (2018) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Feudal networks for hierarchical reinforcement learning (2017) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "* Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) with similarity 0.9345794392523364\n",
      "* CompILE: Compositional Imitation Learning and Execution (2018) with similarity 1.0\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "* DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations (2017) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Stochastic neural networks for hierarchical reinforcement learning (2017) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* Multi-task Hierarchical Adversarial Inverse Reinforcement Learning (2023) with similarity 1.0\n",
      "* ASPiRe: Adaptive Skill Priors for Reinforcement Learning (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "* Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) with similarity 0.9345794392523364\n",
      "* CompILE: Compositional Imitation Learning and Execution (2018) with similarity 1.0\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "* DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations (2017) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "The option-critic architecture (2017) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning (2023) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills (2022) with similarity 1.0\n",
      "* Cascaded Compositional Residual Learning for Complex Interactive Behaviors (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives (2021) with similarity 1.0\n",
      "* Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans (2021) with similarity 1.0\n",
      "* Hierarchical Skills for Efficient Exploration (2021) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* TRAIL: Near-Optimal Imitation Learning with Suboptimal Data (2021) with similarity 1.0\n",
      "* Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL) (2020) with similarity 0.9384615384615385\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* Discovering Motor Programs by Recomposing Demonstrations (2020) with similarity 1.0\n",
      "* Hierarchical reinforcement learning for efficent exploration and transfer (2020) with similarity 0.9931972789115646\n",
      "* Learning Robot Skills with Temporal Variational Inference (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning (2019) with similarity 1.0\n",
      "* Sub-policy Adaptation for Hierarchical Reinforcement Learning (2019) with similarity 1.0\n",
      "* Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) with similarity 0.9345794392523364\n",
      "* CompILE: Compositional Imitation Learning and Execution (2018) with similarity 1.0\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "* A Deep Hierarchical Approach to Lifelong Learning in Minecraft (2017) with similarity 1.0\n",
      "* DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations (2017) with similarity 1.0\n",
      "* Feudal networks for hierarchical reinforcement learning (2017) with similarity 1.0\n",
      "* Stochastic neural networks for hierarchical reinforcement learning (2017) with similarity 1.0\n",
      "------------------------------------------------------\n",
      "Learning and Transfer of Modulated Locomotor Controllers (2016) is cited by\n",
      "* A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning (2023) with similarity 1.0\n",
      "* Cascaded Compositional Residual Learning for Complex Interactive Behaviors (2022) with similarity 1.0\n",
      "* Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics (2022) with similarity 1.0\n",
      "* SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration (2022) with similarity 1.0\n",
      "* Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies (2021) with similarity 1.0\n",
      "* Behavior Priors for Efficient Reinforcement Learning (2020) with similarity 1.0\n",
      "* Catch and carry: Reusable neural controllers for vision-guided whole-body tasks (2020) with similarity 0.9743589743589743\n",
      "* CoMic: Complementary Task Learning & Mimicry for Reusable Skills (2020) with similarity 1.0\n",
      "* PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING (2020) with similarity 1.0\n",
      "* Dynamics-Aware Unsupervised Discovery of Skills (2019) with similarity 1.0\n",
      "* MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies (2019) with similarity 1.0\n",
      "* Data-Efficient Hierarchical Reinforcement Learning (HIRO) (2018) with similarity 0.9345794392523364\n",
      "* Latent Space Policies for Hierarchical Reinforcement Learning (2018) with similarity 1.0\n",
      "* Stochastic neural networks for hierarchical reinforcement learning (2017) with similarity 1.0\n"
     ]
    }
   ],
   "source": [
    "ref = 0\n",
    "for pair in perm:\n",
    "   #pair = (46, 21)\n",
    "   #print(pair[0], pair[1])\n",
    "   paper1 = rw.paper_list[pair[0]]\n",
    "   paper2 = rw.paper_list[pair[1]]\n",
    "    \n",
    "   if pair[0] != ref:\n",
    "      print(\"------------------------------------------------------\")\n",
    "      print(\"{} ({}) is cited by\".format(paper1['title'], paper1['year']))\n",
    "      ref = pair[0]  \n",
    "   # for paper in paper1['citedby_paper_list']:\n",
    "   #    if \"demonstration\" in paper['title'].lower():\n",
    "   #       print(paper)\n",
    "   # print(paper2['title'])\n",
    "   \n",
    "   \n",
    "   exists, similarity = check_citation(paper1, paper2)\n",
    "   if exists:\n",
    "      print(\"* {} ({}) with similarity {}\".format(paper2['title'], paper2['year'], similarity))\n",
    "      citation_matrix[pair[0]-1, pair[1]-1] = similarity\n",
    "   #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c6e8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2 Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "3 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "4 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "5 Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning\n",
      "6 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "7 Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "8 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "9 MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "10 Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space\n",
      "11 Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning\n",
      "12 Relay Hindsight Experience Replay: Self-Guided Continual Reinforcement Learning for Sequential Object Manipulation Tasks with Sparse Rewards\n",
      "13 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "14 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "15 Skill-based Meta-Reinforcement Learning\n",
      "16 Skill-based Model-based Reinforcement Learning\n",
      "17 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "18 Zero-shot sim-to-real transfer of reinforcement learning framework for robotics manipulation with demonstration and force feedback\n",
      "19 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "20 Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "21 Demonstration-Guided Reinforcement Learning with Learned Skills\n",
      "22 Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "23 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "24 Hierarchical Skills for Efficient Exploration\n",
      "25 Learning Robotic Manipulation Skills Using an Adaptive Force-Impedance Action Space\n",
      "26 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "27 Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning\n",
      "28 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "29 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "30 Behavior Priors for Efficient Reinforcement Learning\n",
      "31 Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "32 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "33 Discovering Motor Programs by Recomposing Demonstrations\n",
      "34 Hierarchical reinforcement learning for efficent exploration and transfer\n",
      "35 Learning quadrupedal locomotion over challenging terrain\n",
      "36 Learning Robot Skills with Temporal Variational Inference\n",
      "37 Multi-expert learning of adaptive legged locomotion\n",
      "38 Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer\n",
      "39 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "40 Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information\n",
      "41 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "42 Learning Latent Plans from Play\n",
      "43 LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "44 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "45 Neural probabilistic motor primitives for humanoid control\n",
      "46 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "47 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "48 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "49 CompILE: Compositional Imitation Learning and Execution\n",
      "50 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "51 Learning an embedding space for transferable robot skills\n",
      "52 A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "53 DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "54 Feudal networks for hierarchical reinforcement learning\n",
      "55 Stochastic neural networks for hierarchical reinforcement learning\n",
      "56 The option-critic architecture\n",
      "57 Learning and Transfer of Modulated Locomotor Controllers\n"
     ]
    }
   ],
   "source": [
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    print(key, paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e33a61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Residual skill policies: Learning an adaptable skill-based action space for reinforcement learning for robotics',\n",
       "  'url': 'https://proceedings.mlr.press/v205/rana23a.html'},\n",
       " {'title': 'Robot learning of mobile manipulation with reachability behavior priors',\n",
       "  'url': 'https://ieeexplore.ieee.org/abstract/document/9813580/'},\n",
       " {'title': 'Energy optimization of wind turbines via a neural control policy based on reinforcement learning Markov chain Monte Carlo algorithm',\n",
       "  'url': 'https://www.sciencedirect.com/science/article/pii/S0306261923004725'},\n",
       " {'title': 'SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration',\n",
       "  'url': 'https://arxiv.org/abs/2211.13743'},\n",
       " {'title': 'Verification of safety critical control policies using kernel methods',\n",
       "  'url': 'https://ieeexplore.ieee.org/abstract/document/9838224/'},\n",
       " {'title': 'Renaissance Robot: Optimal Transport Policy Fusion for Learning Diverse Skills',\n",
       "  'url': 'https://ieeexplore.ieee.org/abstract/document/9981105/'},\n",
       " {'title': 'Robotic Packaging Optimization with Reinforcement Learning',\n",
       "  'url': 'https://arxiv.org/abs/2303.14693'},\n",
       " {'title': 'Hybrid LMC: Hybrid Learning and Model-based Control for Wheeled Humanoid Robot via Ensemble Deep Reinforcement Learning',\n",
       "  'url': 'https://ieeexplore.ieee.org/abstract/document/9981913/'},\n",
       " {'title': 'Learning to Drive Using Sparse Imitation Reinforcement Learning',\n",
       "  'url': 'https://ieeexplore.ieee.org/abstract/document/9956121/'},\n",
       " {'title': 'Control strategies for reactive manipulation',\n",
       "  'url': 'https://eprints.qut.edu.au/236551/'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rw.paper_list[20]['citedby_paper_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "387acc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Skill-based Meta-Reinforcement Learning' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[8]\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "\n",
      "\n",
      "'Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[14 16 26]\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "\n",
      "\n",
      "'Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[13 14 17]\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "\n",
      "\n",
      "'Demonstration-Guided Reinforcement Learning with Learned Skills' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 8  9 13 15 16 26 28]\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2022 MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "\n",
      "\n",
      "'Hierarchical Few-Shot Imitation with Skill Transition Models' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 8 28]\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "\n",
      "\n",
      "'Hierarchical Skills for Efficient Exploration' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[13]\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "\n",
      "\n",
      "'Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies' is cited by\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 6 17]\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "\n",
      "\n",
      "'Model predictive actor-critic: Accelerating robot skill acquisition with deep reinforcement learning' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[14]\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "\n",
      "\n",
      "'TRAIL: Near-Optimal Imitation Learning with Suboptimal Data' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[8]\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "\n",
      "\n",
      "'Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)' is cited by\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 6  8  9 11 13 14 15 16 17 20 22 24 26 28]\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2022 MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "2022 Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "2021 Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "\n",
      "\n",
      "'Catch and carry: Reusable neural controllers for vision-guided whole-body tasks' is cited by\n",
      "[0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         1.         0.         0.         0.93846154 0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 4 13 15 24 26 29 32]\n",
      "2022 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "\n",
      "\n",
      "'Discovering Motor Programs by Recomposing Demonstrations' is cited by\n",
      "[0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         1.         1.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.93846154 0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 4 13 16 17 19 28 29 36 39]\n",
      "2022 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 Learning Robot Skills with Temporal Variational Inference\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "\n",
      "\n",
      "'Hierarchical reinforcement learning for efficent exploration and transfer' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[13]\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "\n",
      "\n",
      "'Learning quadrupedal locomotion over challenging terrain' is cited by\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 5 14]\n",
      "2022 Advanced Skills through Multiple Adversarial Motion Priors in Reinforcement Learning\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "\n",
      "\n",
      "'Learning Robot Skills with Temporal Variational Inference' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[16 19 24 28]\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "\n",
      "\n",
      "'Multi-expert learning of adaptive legged locomotion' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[7]\n",
      "2022 Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "\n",
      "\n",
      "'Multiplicative controller fusion: Leveraging algorithmic priors for sample-efficient reinforcement learning and safe sim-to-real transfer' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[13 14 20]\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Robot Learning of Mobile Manipulation With Reachability Behavior Priors\n",
      "2021 Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "\n",
      "\n",
      "'PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING' is cited by\n",
      "[0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 6  8  9 13 17 19 23 26 28]\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2022 MPR-RL: Multi-Prior Regularized Reinforcement Learning for Knowledge Transfer\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "\n",
      "\n",
      "'Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information' is cited by\n",
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 3 13 36 46 47 49]\n",
      "2023 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2020 Learning Robot Skills with Temporal Variational Inference\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "2018 CompILE: Compositional Imitation Learning and Execution\n",
      "\n",
      "\n",
      "'Dynamics-Aware Unsupervised Discovery of Skills' is cited by\n",
      "[1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         0.         0.\n",
      " 1.         0.         0.         0.         0.         1.\n",
      " 0.         1.         0.         1.         0.93846154 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 1 15 16 19 24 26 28 29]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "\n",
      "\n",
      "'Learning Latent Plans from Play' is cited by\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 1.         0.         1.         1.         1.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 0.         1.         0.         1.         0.93846154 0.\n",
      " 0.         1.         1.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 8 13 15 16 17 19 23 26 28 29 32 33 39 46]\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 Discovering Motor Programs by Recomposing Demonstrations\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "\n",
      "\n",
      "'LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT' is cited by\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[2]\n",
      "2023 Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "\n",
      "\n",
      "'MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies' is cited by\n",
      "[0.         0.         0.         0.         0.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.97435897 1.         0.         0.         0.         0.\n",
      " 1.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 6  7 17 24 28 31 32 37 39]\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 Multi-expert learning of adaptive legged locomotion\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "\n",
      "\n",
      "'Neural probabilistic motor primitives for humanoid control' is cited by\n",
      "[0.         0.         0.         1.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         1.         0.         0.         0.93846154 1.\n",
      " 0.97435897 1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 4  6 13 17 24 26 29 30 31 32 44]\n",
      "2022 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "\n",
      "\n",
      "'Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning' is cited by\n",
      "[0.         0.         1.         0.         0.         0.\n",
      " 0.         1.         0.         1.         0.         0.\n",
      " 1.         0.         1.         1.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.93846154 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 3  8 10 13 15 16 22 29 39]\n",
      "2023 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "2022 CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations\n",
      "2022 Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Hierarchical Few-Shot Imitation with Skill Transition Models\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "\n",
      "\n",
      "'Sub-policy Adaptation for Hierarchical Reinforcement Learning' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[17 19 24]\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "\n",
      "\n",
      "'Data-Efficient Hierarchical Reinforcement Learning (HIRO)' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[16 19 23 24 26 28 30 39 41 43 46 47]\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "\n",
      "\n",
      "'CompILE: Compositional Imitation Learning and Execution' is cited by\n",
      "[0.         0.         0.90909091 1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         1.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         1.         0.         0.\n",
      " 0.         0.         1.         0.         0.         1.\n",
      " 0.         0.         1.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 3  4 16 17 26 28 33 36 39 46]\n",
      "2023 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "2022 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022 Skill-based Model-based Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Discovering Motor Programs by Recomposing Demonstrations\n",
      "2020 Learning Robot Skills with Temporal Variational Inference\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "\n",
      "\n",
      "'Latent Space Policies for Hierarchical Reinforcement Learning' is cited by\n",
      "[1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[ 1  6 13 17 26 30 32 33 37 39 44 47]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 Discovering Motor Programs by Recomposing Demonstrations\n",
      "2020 Multi-expert learning of adaptive legged locomotion\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "\n",
      "\n",
      "'Learning an embedding space for transferable robot skills' is cited by\n",
      "[1.         0.         0.         0.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         1.         0.         1.         0.\n",
      " 1.         1.         0.         0.         1.         1.\n",
      " 0.         1.         0.         1.         0.93846154 1.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         1.\n",
      " 0.         1.         0.         0.         1.         0.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 1  6 13 15 17 19 20 23 24 26 28 29 30 32 41 42 44 47 50]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 Skill-based Meta-Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Bayesian controller fusion: Leveraging control priors in deep reinforcement learning for robotics\n",
      "2021 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 Learning Latent Plans from Play\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "\n",
      "\n",
      "'A Deep Hierarchical Approach to Lifelong Learning in Minecraft' is cited by\n",
      "[1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         0.93457944\n",
      " 0.         1.         0.         0.         0.         1.\n",
      " 0.         0.         0.        ]\n",
      "[ 1 44 48 50 54]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2018 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "2017 Feudal networks for hierarchical reinforcement learning\n",
      "\n",
      "\n",
      "'DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations' is cited by\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[13 28 30 33 36 39 40 44 49]\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 Discovering Motor Programs by Recomposing Demonstrations\n",
      "2020 Learning Robot Skills with Temporal Variational Inference\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Directed-info GAIL: Learning hierarchical policies from unsegmented demonstrations using directed information\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2018 CompILE: Compositional Imitation Learning and Execution\n",
      "\n",
      "\n",
      "'Feudal networks for hierarchical reinforcement learning' is cited by\n",
      "[1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         1.\n",
      " 0.         0.         0.         1.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 1.         1.         0.         1.         1.         0.93457944\n",
      " 1.         1.         0.         0.         1.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 1 19 24 28 30 41 43 44 46 47 48 49 50 53]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "2018 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "2018 CompILE: Compositional Imitation Learning and Execution\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "2017 DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "\n",
      "\n",
      "'Stochastic neural networks for hierarchical reinforcement learning' is cited by\n",
      "[1.         0.         1.         0.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 0.         1.         0.         0.         0.         1.\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         1.         0.\n",
      " 0.         1.         0.         0.         1.         0.93457944\n",
      " 1.         1.         0.         0.         1.         0.\n",
      " 0.         0.         0.        ]\n",
      "[ 1  3  6 17 24 26 30 32 39 41 44 47 48 49 50 53]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2023 Multi-task Hierarchical Adversarial Inverse Reinforcement Learning\n",
      "2022 ASPiRe: Adaptive Skill Priors for Reinforcement Learning\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "2018 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "2018 CompILE: Compositional Imitation Learning and Execution\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "2017 DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "\n",
      "\n",
      "'The option-critic architecture' is cited by\n",
      "[1.         1.         0.         1.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 1.         0.         0.         0.         1.         1.\n",
      " 0.         1.         0.         1.         0.93846154 1.\n",
      " 0.         1.         1.         0.99319728 0.         1.\n",
      " 0.         0.         1.         0.         1.         0.\n",
      " 1.         1.         0.         1.         1.         0.93457944\n",
      " 1.         1.         0.         1.         1.         1.\n",
      " 1.         0.         0.        ]\n",
      "[ 1  2  4  7 13 17 19 23 24 26 28 29 30 32 33 34 36 39 41 43 44 46 47 48\n",
      " 49 50 52 53 54 55]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2023 Hierarchical Kickstarting for Skill Transfer in Reinforcement Learning\n",
      "2022 Accelerating Reinforcement Learning for Autonomous Driving using Task-Agnostic and Ego-Centric Motion Skills\n",
      "2022 Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives\n",
      "2021 Hierarchical Policies for Cluttered-Scene Grasping with Latent Plans\n",
      "2021 Hierarchical Skills for Efficient Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2021 TRAIL: Near-Optimal Imitation Learning with Suboptimal Data\n",
      "2020 Accelerating Reinforcement Learning with Learned Skill Priors (SPiRL)\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 Discovering Motor Programs by Recomposing Demonstrations\n",
      "2020 Hierarchical reinforcement learning for efficent exploration and transfer\n",
      "2020 Learning Robot Skills with Temporal Variational Inference\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 LEARNING MULTI-LEVEL HIERARCHIES WITH HINDSIGHT\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2019 Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning\n",
      "2019 Sub-policy Adaptation for Hierarchical Reinforcement Learning\n",
      "2018 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "2018 CompILE: Compositional Imitation Learning and Execution\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "2017 A Deep Hierarchical Approach to Lifelong Learning in Minecraft\n",
      "2017 DDCO: Discovery of Deep Continuous Options for Robot Learning from Demonstrations\n",
      "2017 Feudal networks for hierarchical reinforcement learning\n",
      "2017 Stochastic neural networks for hierarchical reinforcement learning\n",
      "\n",
      "\n",
      "'Learning and Transfer of Modulated Locomotor Controllers' is cited by\n",
      "[1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         1.\n",
      " 0.97435897 1.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         1.         0.\n",
      " 0.         1.         0.         0.         0.         0.93457944\n",
      " 0.         1.         0.         0.         0.         0.\n",
      " 1.         0.         0.        ]\n",
      "[ 1  7 13 17 26 30 31 32 39 41 44 48 50 55]\n",
      "2023 A Task-Agnostic Regularizer for Diverse Subpolicy Discovery in Hierarchical Reinforcement Learning\n",
      "2022 Cascaded Compositional Residual Learning for Complex Interactive Behaviors\n",
      "2022 Residual Skill Policies: Learning an Adaptable Skill-based Action Space for Reinforcement Learning for Robotics\n",
      "2022 SkillS: Adaptive Skill Sequencing for Efficient Temporally-Extended Exploration\n",
      "2021 Learning Transferable Motor Skills with Hierarchical Latent Mixture Policies\n",
      "2020 Behavior Priors for Efficient Reinforcement Learning\n",
      "2020 Catch and carry: Reusable neural controllers for vision-guided whole-body tasks\n",
      "2020 CoMic: Complementary Task Learning & Mimicry for Reusable Skills\n",
      "2020 PARROT: DATA-DRIVEN BEHAVIORAL PRIORS FOR REINFORCEMENT LEARNING\n",
      "2019 Dynamics-Aware Unsupervised Discovery of Skills\n",
      "2019 MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies\n",
      "2018 Data-Efficient Hierarchical Reinforcement Learning (HIRO)\n",
      "2018 Latent Space Policies for Hierarchical Reinforcement Learning\n",
      "2017 Stochastic neural networks for hierarchical reinforcement learning\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in rw.paper_list.keys():\n",
    "    paper = rw.paper_list[key]\n",
    "    if citation_matrix[key-1].sum() > 0:\n",
    "        print(\"'{}' is cited by\".format(paper['title']))\n",
    "        b = np.array(citation_matrix[key-1])\n",
    "        print(b)\n",
    "        citing_key = np.where(b>0)[0] + 1\n",
    "        print(citing_key)\n",
    "        for k in citing_key:\n",
    "            print(rw.paper_list[k]['year'], rw.paper_list[k]['title'])\n",
    "        print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
